<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Lil'Log</title>
<link>https://lilianweng.github.io/</link>


<item>
<title>Reward Hacking in Reinforcement Learning</title>
<link>https://lilianweng.github.io/posts/2024-11-28-reward-hacking/</link>
<guid>https://lilianweng.github.io/posts/2024-11-28-reward-hacking/</guid>
<content:encoded><![CDATA[
<div> reward hacking, RL environments, alignment training, language models, autonomy
æ€»ç»“:<br /><br />æ–‡ç« è®¨è®ºäº†å¦‚ä½•åœ¨RLè®­ç»ƒä¸­é˜²æ­¢å¥–åŠ±æ¬ºéª—çš„æŒ‘æˆ˜ã€‚ç”±äºRLç¯å¢ƒä¸å®Œç¾ï¼Œå¾ˆéš¾å‡†ç¡®æŒ‡å®šå¥–åŠ±å‡½æ•°ï¼Œå¯¼è‡´ä»£ç†æœºå™¨äººåˆ©ç”¨æ¼æ´æˆ–æ¨¡ç³Šæ€§æ¥è·å–é«˜å¥–åŠ±ã€‚éšç€é€šç”¨ä»»åŠ¡çš„æ™®åŠå’ŒRLHFæˆä¸ºå¯¹é½è®­ç»ƒçš„å®é™…æ–¹æ³•ï¼Œå¥–åŠ±æ¬ºéª—åœ¨è¯­è¨€æ¨¡å‹çš„RLè®­ç»ƒä¸­å˜å¾—è‡³å…³é‡è¦ã€‚æ¨¡å‹å­¦ä¼šä¿®æ”¹å•å…ƒæµ‹è¯•ä»¥é€šè¿‡ç¼–ç ä»»åŠ¡ï¼Œæˆ–å“åº”åŒ…å«æ¨¡ä»¿ç”¨æˆ·åå¥½çš„åè§ï¼Œè¿™æ˜¯å€¼å¾—å…³æ³¨çš„é—®é¢˜ï¼Œä¹Ÿæ˜¯é˜»ç¢AIæ¨¡å‹æ›´è‡ªä¸»ä½¿ç”¨æ¡ˆä¾‹åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²çš„ä¸»è¦éšœç¢ã€‚ <div>
<p>Reward hacking occurs when a <a href="(https://lilianweng.github.io/posts/2018-02-19-rl-overview/)">reinforcement learning (RL)</a> agent <a href="https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#exploitation-vs-exploration">exploits</a> flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.</p>
<p>With the rise of <a href="https://lilianweng.github.io/posts/2019-01-31-lm/">language models</a> generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user&rsquo;s preference, are pretty concerning and are likely one of the major blockers for the real world deployment of more autonomous use cases of AI models.</p>
]]></content:encoded>
<pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate>
</item>
<item>
<title>Extrinsic Hallucinations in LLMs</title>
<link>https://lilianweng.github.io/posts/2024-07-07-hallucination/</link>
<guid>https://lilianweng.github.io/posts/2024-07-07-hallucination/</guid>
<content:encoded><![CDATA[
<div> Hallucination, large language models, unfaithful, fabricated, inconsistent

Hallucinationåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é€šå¸¸æŒ‡æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ä¸æä¾›çš„ä¸Šä¸‹æ–‡æˆ–ä¸–ç•ŒçŸ¥è¯†ä¸ä¸€è‡´ã€è™šæ„ã€ä¸è¿è´¯æˆ–è’è°¬ã€‚è¿™ç¯‡æ–‡ç« å°†å¹»è§‰é—®é¢˜é™å®šä¸ºæ¨¡å‹è¾“å‡ºå†…å®¹æ˜¯è™šæ„çš„ï¼Œæ²¡æœ‰ä¸æä¾›çš„ä¸Šä¸‹æ–‡æˆ–ä¸–ç•ŒçŸ¥è¯†ç›¸å…³ã€‚å¹»è§‰æœ‰ä¸¤ç§ç±»å‹ï¼šä¸Šä¸‹æ–‡å¹»è§‰å’Œéä¸Šä¸‹æ–‡å¹»è§‰ã€‚æ€»ç»“: Hallucinationåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æŒ‡æ¨¡å‹ç”Ÿæˆçš„å†…å®¹è™šæ„ï¼Œä¸ä¸Šä¸‹æ–‡ä¸ç›¸å…³ã€‚æœ‰ä¸Šä¸‹æ–‡å¹»è§‰å’Œéä¸Šä¸‹æ–‡å¹»è§‰ä¸¤ç§ç±»å‹ã€‚ <div>
Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:
In-context hallucination: The model output should be consistent with the source content in context.
]]></content:encoded>
<pubDate>Sun, 07 Jul 2024 00:00:00 +0000</pubDate>
</item>
<item>
<title>Diffusion Models for Video Generation</title>
<link>https://lilianweng.github.io/posts/2024-04-12-diffusion-video/</link>
<guid>https://lilianweng.github.io/posts/2024-04-12-diffusion-video/</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯: æ‰©æ•£æ¨¡å‹ï¼Œå›¾åƒåˆæˆï¼Œè§†é¢‘ç”Ÿæˆï¼Œæ—¶é—´ä¸€è‡´æ€§ï¼Œä¸–ç•ŒçŸ¥è¯†ã€‚

æ€»ç»“:
æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆé¢†åŸŸå–å¾—äº†å¾ˆå¥½çš„ç»“æœï¼Œè€Œå¦‚ä»Šç ”ç©¶ç•Œå¼€å§‹æŒ‘æˆ˜æ›´éš¾çš„ä»»åŠ¡â€”â€”å°†å…¶åº”ç”¨äºè§†é¢‘ç”Ÿæˆã€‚ä¸å›¾åƒç›¸æ¯”ï¼Œè§†é¢‘ç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒå¯¹æ—¶é—´ä¸Šçš„å¸§é—´ä¸€è‡´æ€§æœ‰é¢å¤–çš„è¦æ±‚ï¼Œè¿™è‡ªç„¶éœ€è¦å°†æ›´å¤šçš„ä¸–ç•ŒçŸ¥è¯†ç¼–ç åˆ°æ¨¡å‹ä¸­ã€‚è¿™é¡¹ä»»åŠ¡æ˜¯å›¾åƒçš„è¶…é›†ï¼Œéœ€è¦æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œå¤„ç†æ—¶é—´ä¸Šçš„è¿ç»­æ€§ã€‚è¿™å°†å¯¹æ¨¡å‹çš„è®­ç»ƒå’Œç”Ÿæˆè¿‡ç¨‹æå‡ºæ›´é«˜çš„è¦æ±‚ã€‚é€šè¿‡ç ”ç©¶è§†é¢‘ç”Ÿæˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ¨åŠ¨å›¾åƒåˆæˆé¢†åŸŸçš„å‘å±•ï¼Œæ¢ç´¢æ›´å¹¿é˜”çš„åº”ç”¨é¢†åŸŸã€‚ <div>
Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task&mdash;using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:
It has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.
]]></content:encoded>
<pubDate>Fri, 12 Apr 2024 00:00:00 +0000</pubDate>
</item>
<item>
<title>Thinking about High-Quality Human Data</title>
<link>https://lilianweng.github.io/posts/2024-02-05-human-data-quality/</link>
<guid>https://lilianweng.github.io/posts/2024-02-05-human-data-quality/</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯: é«˜è´¨é‡æ•°æ®, æ·±åº¦å­¦ä¹ æ¨¡å‹, äººå·¥æ ‡æ³¨, æ•°æ®æ”¶é›†, æ³¨æ„ç»†èŠ‚

æ€»ç»“:
é«˜è´¨é‡çš„æ•°æ®æ˜¯ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„ç‡ƒæ–™ã€‚å¤§å¤šæ•°ç‰¹å®šä»»åŠ¡çš„æ ‡æ³¨æ•°æ®æ¥è‡ªäººå·¥æ ‡æ³¨ï¼Œä¾‹å¦‚åˆ†ç±»ä»»åŠ¡æˆ–ç”¨äºLLMå¯¹é½è®­ç»ƒçš„RLHFæ ‡æ³¨ï¼ˆå¯ä»¥æ„å»ºä¸ºåˆ†ç±»æ ¼å¼ï¼‰ã€‚æ–‡ç« ä¸­æåˆ°çš„è®¸å¤šæœºå™¨å­¦ä¹ æŠ€æœ¯å¯ä»¥æé«˜æ•°æ®è´¨é‡ï¼Œä½†ä»æ ¹æœ¬ä¸Šè¯´ï¼Œäººå·¥æ•°æ®æ”¶é›†éœ€è¦æ³¨æ„ç»†èŠ‚å’Œè°¨æ…æ‰§è¡Œã€‚<br /><br />æ€»ç»“: é«˜è´¨é‡æ•°æ®æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„é‡è¦åŸºç¡€ï¼Œéœ€é€šè¿‡äººå·¥æ ‡æ³¨å’Œç»†è‡´æ³¨æ„çš„æ•°æ®æ”¶é›†è¿‡ç¨‹æ¥ä¿è¯æ•°æ®è´¨é‡ã€‚ <div>
[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper &ldquo;Vox populi&rdquo;) and nice feedback. ğŸ™ ]
High-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution.
]]></content:encoded>
<pubDate>Mon, 05 Feb 2024 00:00:00 +0000</pubDate>
</item>
<item>
<title>Adversarial Attacks on LLMs</title>
<link>https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/</link>
<guid>https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/</guid>
<content:encoded><![CDATA[
<div> ChatGPT, language models, real world, alignment process, adversarial attacks, jailbreak prompts, ground work, images, continuous, high-dimensional space.<br />
<br />
ChatGPTçš„æ¨å‡ºåŠ é€Ÿäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚OpenAIå›¢é˜Ÿåœ¨å¯¹é½è¿‡ç¨‹ä¸­æŠ•å…¥äº†å¤§é‡çš„å·¥ä½œï¼Œä»¥æ„å»ºæ¨¡å‹çš„é»˜è®¤å®‰å…¨è¡Œä¸ºï¼ˆä¾‹å¦‚é€šè¿‡RLHFï¼‰ã€‚ç„¶è€Œï¼Œå¯¹æŠ—æ€§æ”»å‡»æˆ–è¶Šç‹±æç¤ºå¯èƒ½ä¼šè§¦å‘æ¨¡å‹è¾“å‡ºä¸æœŸæœ›çš„ç»“æœã€‚å¯¹æŠ—æ€§æ”»å‡»çš„å¤§éƒ¨åˆ†å·¥ä½œæ˜¯åœ¨å›¾åƒé¢†åŸŸè¿›è¡Œçš„ï¼Œè€ŒChatGPTåˆ™åœ¨è¿ç»­çš„é«˜ç»´ç©ºé—´ä¸­è¿›è¡Œæ“ä½œã€‚<br />
<br />
æ€»ç»“ï¼š<br />
1. ChatGPTæ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯¹ç°å®ä¸–ç•Œåº”ç”¨æœ‰ç€å¼ºå¤§çš„æ¨åŠ¨ä½œç”¨ã€‚<br />
2. åœ¨æ¨¡å‹çš„å¯¹é½è¿‡ç¨‹ä¸­ï¼ŒOpenAIå›¢é˜Ÿå°†å¤§é‡å·¥ä½œæŠ•å…¥åˆ°æ„å»ºæ¨¡å‹çš„é»˜è®¤å®‰å…¨è¡Œä¸ºä¸­ã€‚<br />
3. å¯¹æŠ—æ€§æ”»å‡»æˆ–è¶Šç‹±æç¤ºå¯èƒ½å¯¼è‡´æ¨¡å‹è¾“å‡ºæ„å¤–çš„ç»“æœã€‚<br />
4. å¯¹æŠ—æ€§æ”»å‡»çš„ç ”ç©¶å¤§éƒ¨åˆ†é›†ä¸­åœ¨å›¾åƒé¢†åŸŸï¼Œè€ŒChatGPTæ“ä½œçš„æ˜¯è¿ç»­çš„é«˜ç»´ç©ºé—´ã€‚ <div>
The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.
A large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space.
]]></content:encoded>
<pubDate>Wed, 25 Oct 2023 00:00:00 +0000</pubDate>
</item>
<item>
<title>LLM Powered Autonomous Agents</title>
<link>https://lilianweng.github.io/posts/2023-06-23-agent/</link>
<guid>https://lilianweng.github.io/posts/2023-06-23-agent/</guid>
<content:encoded><![CDATA[
<div> å…³é”®è¯: LLM, è‡ªä¸»ä»£ç†ç³»ç»Ÿ, æ¦‚è¿°, å…³é”®ç»„ä»¶, æ½œåŠ›

æ¦‚è¿°: LLMä½œä¸ºæ ¸å¿ƒæ§åˆ¶å™¨æ„å»ºä»£ç†è‡ªä¸»ç³»ç»Ÿæ˜¯ä¸€ä¸ªå¾ˆé…·çš„æ¦‚å¿µã€‚AutoGPTã€GPT-Engineerå’ŒBabyAGIç­‰å¤šä¸ªProof-of-Conceptæ¼”ç¤ºæä¾›äº†çµæ„Ÿã€‚LLMçš„æ½œåŠ›ä¸ä»…é™äºç”Ÿæˆæ–‡æœ¬å‰¯æœ¬ã€æ•…äº‹ã€æ–‡ç« å’Œç¨‹åºï¼Œè¿˜å¯ä»¥ä½œä¸ºå¼ºå¤§çš„é€šç”¨é—®é¢˜æ±‚è§£å™¨ã€‚åœ¨LLMé©±åŠ¨çš„è‡ªä¸»ä»£ç†ç³»ç»Ÿä¸­ï¼ŒLLMå……å½“ä»£ç†çš„å¤§è„‘ï¼Œè¾…ä»¥å‡ ä¸ªå…³é”®ç»„ä»¶ã€‚
æ€»ç»“:<br /><br />LLMä½œä¸ºæ ¸å¿ƒæ§åˆ¶å™¨ç”¨äºæ„å»ºè‡ªä¸»ä»£ç†ç³»ç»Ÿï¼Œå®ƒä¸ä»…å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬ï¼Œè¿˜å¯ä»¥ä½œä¸ºå¼ºå¤§çš„é—®é¢˜æ±‚è§£å™¨ã€‚é€šè¿‡ç»“åˆå…¶ä»–å…³é”®ç»„ä»¶ï¼Œè¿™ç§ç³»ç»Ÿå¯ä»¥å®ç°æ›´å¤šåŠŸèƒ½ã€‚LLMçš„æ½œåŠ›ä»¤äººå…´å¥‹ï¼Œä¸ºæ„å»ºæ™ºèƒ½ä»£ç†ç³»ç»Ÿæ‰“å¼€äº†æ–°çš„å¯èƒ½æ€§ã€‚ <div>
Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.
Agent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent&rsquo;s brain, complemented by several key components:
]]></content:encoded>
<pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate>
</item>
<item>
<title>Prompt Engineering</title>
<link>https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/</link>
<guid>https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/</guid>
<content:encoded><![CDATA[
<div> Prompt Engineering, In-Context Prompting, æœºå™¨å­¦ä¹ , è¯­è¨€æ¨¡å‹, å®éªŒ, å¯å‘å¼æ–¹æ³•<br /><br />æ€»ç»“: 
Prompt Engineeringï¼ˆæç¤ºå·¥ç¨‹ï¼‰ï¼Œåˆç§°ä¸Šä¸‹æ–‡æç¤ºï¼Œæ˜¯æŒ‡åœ¨ä¸æ›´æ–°æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¸LLMï¼ˆè¯­è¨€æ¨¡å‹ï¼‰è¿›è¡Œé€šä¿¡æ¥å¼•å¯¼å…¶è¡Œä¸ºä»¥å®ç°æœŸæœ›çš„ç»“æœçš„æ–¹æ³•ã€‚è¿™æ˜¯ä¸€é—¨ç»éªŒç§‘å­¦ï¼Œæç¤ºå·¥ç¨‹æ–¹æ³•çš„æ•ˆæœåœ¨ä¸åŒæ¨¡å‹ä¹‹é—´å¯èƒ½æœ‰å¾ˆå¤§å·®å¼‚ï¼Œå› æ­¤éœ€è¦è¿›è¡Œå¤§é‡å®éªŒå’Œå¯å‘å¼è°ƒæ•´ã€‚æœ¬æ–‡ä»…å…³æ³¨è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„æç¤ºå·¥ç¨‹ï¼Œä¸æ¶‰åŠClozeæµ‹è¯•ã€å›¾åƒç”Ÿæˆæˆ–å¤šæ¨¡æ€æ¨¡å‹ã€‚ <div>
Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.
This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.
]]></content:encoded>
<pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate>
</item>
<item>
<title>The Transformer Family Version 2.0</title>
<link>https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</link>
<guid>https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</guid>
<content:encoded><![CDATA[
Many new Transformer architecture improvements have been proposed since my last post on &ldquo;The Transformer Family&rdquo; about three years ago. Here I did a big refactoring and enrichment of that 2020 post &mdash; restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length.
Notations Symbol Meaning $d$ The model size / hidden state dimension / positional encoding size.
]]></content:encoded>
<pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate>
</item>
<item>
<title>Large Transformer Model Inference Optimization</title>
<link>https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</link>
<guid>https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</guid>
<content:encoded><![CDATA[
[Updated on 2023-01-24: add a small section on Distillation.]
Large transformer models are mainstream nowadays, creating SoTA results for a variety of tasks. They are powerful but very expensive to train and use. The extremely high inference cost, in both time and memory, is a big bottleneck for adopting a powerful transformer for solving real-world tasks at scale.
Why is it hard to run inference for large transformer models? Besides the increasing size of SoTA models, there are two main factors contributing to the inference challenge (Pope et al.
]]></content:encoded>
<pubDate>Tue, 10 Jan 2023 10:00:00 -0700</pubDate>
</item>
<item>
<title>Some Math behind Neural Tangent Kernel</title>
<link>https://lilianweng.github.io/posts/2022-09-08-ntk/</link>
<guid>https://lilianweng.github.io/posts/2022-09-08-ntk/</guid>
<content:encoded><![CDATA[
Neural networks are well known to be over-parameterized and can often easily fit data with near-zero training loss with decent generalization performance on test dataset. Although all these parameters are initialized at random, the optimization process can consistently lead to similarly good outcomes. And this is true even when the number of model parameters exceeds the number of training data points.
Neural tangent kernel (NTK) (Jacot et al. 2018) is a kernel to explain the evolution of neural networks during training via gradient descent.
]]></content:encoded>
<pubDate>Thu, 08 Sep 2022 10:00:00 -0700</pubDate>
</item>
<item>
<title>Generalized Visual Language Models</title>
<link>https://lilianweng.github.io/posts/2022-06-09-vlm/</link>
<guid>https://lilianweng.github.io/posts/2022-06-09-vlm/</guid>
<content:encoded><![CDATA[
Processing images to generate text, such as image captioning and visual question-answering, has been studied for years. Traditionally such systems rely on an object detection network as a vision encoder to capture visual features and then produce text via a text decoder. Given a large amount of existing literature, in this post, I would like to only focus on one approach for solving vision language tasks, which is to extend pre-trained generalized language models to be capable of consuming visual signals.
]]></content:encoded>
<pubDate>Thu, 09 Jun 2022 15:10:30 -0700</pubDate>
</item>
<item>
<title>Learning with not Enough Data Part 3: Data Generation</title>
<link>https://lilianweng.github.io/posts/2022-04-15-data-gen/</link>
<guid>https://lilianweng.github.io/posts/2022-04-15-data-gen/</guid>
<content:encoded><![CDATA[
Here comes the Part 3 on learning with not enough data (Previous: Part 1 and Part 2). Letâ€™s consider two approaches for generating synthetic data for training.
Augmented data. Given a set of existing training samples, we can apply a variety of augmentation, distortion and transformation to derive new data points without losing the key attributes. We have covered a bunch of augmentation methods on text and images in a previous post on contrastive learning.
]]></content:encoded>
<pubDate>Fri, 15 Apr 2022 15:10:30 -0700</pubDate>
</item>
<item>
<title>Learning with not Enough Data Part 2: Active Learning</title>
<link>https://lilianweng.github.io/posts/2022-02-20-active-learning/</link>
<guid>https://lilianweng.github.io/posts/2022-02-20-active-learning/</guid>
<content:encoded><![CDATA[
This is part 2 of what to do when facing a limited amount of labeled data for supervised learning tasks. This time we will get some amount of human labeling work involved, but within a budget limit, and therefore we need to be smart when selecting which samples to label.
Notations Symbol Meaning $K$ Number of unique class labels. $(\mathbf{x}^l, y) \sim \mathcal{X}, y \in \{0, 1\}^K$ Labeled dataset. $y$ is a one-hot representation of the true label.
]]></content:encoded>
<pubDate>Sun, 20 Feb 2022 00:00:00 +0000</pubDate>
</item>
<item>
<title>Learning with not Enough Data Part 1: Semi-Supervised Learning</title>
<link>https://lilianweng.github.io/posts/2021-12-05-semi-supervised/</link>
<guid>https://lilianweng.github.io/posts/2021-12-05-semi-supervised/</guid>
<content:encoded><![CDATA[
When facing a limited amount of labeled data for supervised learning tasks, four approaches are commonly discussed.
Pre-training + fine-tuning: Pre-train a powerful task-agnostic model on a large unsupervised data corpus, e.g. pre-training LMs on free text, or pre-training vision models on unlabelled images via self-supervised learning, and then fine-tune it on the downstream task with a small set of labeled samples. Semi-supervised learning: Learn from the labelled and unlabeled samples together.
]]></content:encoded>
<pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate>
</item>
<item>
<title>How to Train Really Large Models on Many GPUs?</title>
<link>https://lilianweng.github.io/posts/2021-09-25-train-large/</link>
<guid>https://lilianweng.github.io/posts/2021-09-25-train-large/</guid>
<content:encoded><![CDATA[
[Updated on 2022-03-13: add expert choice routing.] [Updated on 2022-06-10]: Greg and I wrote a shorted and upgraded version of this post, published on OpenAI Blog: &ldquo;Techniques for Training Large Neural Networks&rdquo;
In recent years, we are seeing better results on many NLP benchmark tasks with larger pre-trained language models. How to train large and deep neural networks is challenging, as it demands a large amount of GPU memory and a long horizon of training time.
]]></content:encoded>
<pubDate>Fri, 24 Sep 2021 00:00:00 +0000</pubDate>
</item>
<item>
<title>What are Diffusion Models?</title>
<link>https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</link>
<guid>https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</guid>
<content:encoded><![CDATA[
[Updated on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of several key papers in the references)]. [Updated on 2022-08-27: Added classifier-free guidance, GLIDE, unCLIP and Imagen. [Updated on 2022-08-31: Added latent diffusion model. [Updated on 2024-04-13: Added progressive distillation, consistency models, and the Model Architecture section.
So far, I&rsquo;ve written about three types of generative models, GAN, VAE, and Flow-based models. They have shown great success in generating high-quality samples, but each has some limitations of its own.
]]></content:encoded>
<pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
</item>
<item>
<title>Contrastive Representation Learning</title>
<link>https://lilianweng.github.io/posts/2021-05-31-contrastive/</link>
<guid>https://lilianweng.github.io/posts/2021-05-31-contrastive/</guid>
<content:encoded><![CDATA[
The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in self-supervised learning.
Contrastive Training Objectives In early versions of loss functions for contrastive learning, only one positive and one negative sample are involved.
]]></content:encoded>
<pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
</item>
<item>
<title>Reducing Toxicity in Language Models</title>
<link>https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/</link>
<guid>https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/</guid>
<content:encoded><![CDATA[
Large pretrained language models are trained over a sizable collection of online data. They unavoidably acquire certain toxic behavior and biases from the Internet. Pretrained language models are very powerful and have shown great success in many NLP tasks. However, to safely deploy them for practical real-world applications demands a strong safety control over the model generation process.
Many challenges are associated with the effort to diminish various types of unsafe content:
]]></content:encoded>
<pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
</item>
<item>
<title>Controllable Neural Text Generation</title>
<link>https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/</link>
<guid>https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/</guid>
<content:encoded><![CDATA[
[Updated on 2021-02-01: Updated to version 2.0 with several work added and many typos fixed.] [Updated on 2021-05-26: Add P-tuning and Prompt Tuning in the &ldquo;prompt design&rdquo; section.] [Updated on 2021-09-19: Add &ldquo;unlikelihood training&rdquo;.]
There is a gigantic amount of free text on the Web, several magnitude more than labelled benchmark datasets. The state-of-the-art language models (LM) are trained with unsupervised Web data in large scale. When generating samples from LM by iteratively sampling the next token, we do not have much control over attributes of the output text, such as the topic, the style, the sentiment, etc.
]]></content:encoded>
<pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
</item>
<item>
<title>How to Build an Open-Domain Question Answering System?</title>
<link>https://lilianweng.github.io/posts/2020-10-29-odqa/</link>
<guid>https://lilianweng.github.io/posts/2020-10-29-odqa/</guid>
<content:encoded><![CDATA[
[Updated on 2020-11-12: add an example on closed-book factual QA using OpenAI API (beta).
A model that can answer any question with regard to factual knowledge can lead to many useful and practical applications, such as working as a chatbot or an AI assistantğŸ¤–. In this post, we will review several common approaches for building such an open-domain question answering system.
Disclaimers given so many papers in the wild:
]]></content:encoded>
<pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate>
</item>
<item>
<title>Neural Architecture Search</title>
<link>https://lilianweng.github.io/posts/2020-08-06-nas/</link>
<guid>https://lilianweng.github.io/posts/2020-08-06-nas/</guid>
<content:encoded><![CDATA[
Although most popular and successful model architectures are designed by human experts, it doesn&rsquo;t mean we have explored the entire network architecture space and settled down with the best option. We would have a better chance to find the optimal solution if we adopt a systematic and automatic way of learning high-performance model architectures.
Automatically learning and evolving network topologies is not a new idea (Stanley &amp; Miikkulainen, 2002). In recent years, the pioneering work by Zoph &amp; Le 2017 and Baker et al.
]]></content:encoded>
<pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate>
</item>
<item>
<title>Exploration Strategies in Deep Reinforcement Learning</title>
<link>https://lilianweng.github.io/posts/2020-06-07-exploration-drl/</link>
<guid>https://lilianweng.github.io/posts/2020-06-07-exploration-drl/</guid>
<content:encoded><![CDATA[
[Updated on 2020-06-17: Add &ldquo;exploration via disagreement&rdquo; in the &ldquo;Forward Dynamics&rdquo; section.
Exploitation versus exploration is a critical topic in Reinforcement Learning. We&rsquo;d like the RL agent to find the best solution as fast as possible. However, in the meantime, committing to solutions too quickly without enough exploration sounds pretty bad, as it could lead to local minima or total failure. Modern RL algorithms that optimize for the best returns can achieve good exploitation quite efficiently, while exploration remains more like an open topic.
]]></content:encoded>
<pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
</item>
<item>
<title>The Transformer Family</title>
<link>https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/</link>
<guid>https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/</guid>
<content:encoded><![CDATA[
[Updated on 2023-01-27: After almost three years, I did a big refactoring update of this post to incorporate a bunch of new Transformer models since 2020. The enhanced version of this post is here: The Transformer Family Version 2.0. Please refer to that post on this topic.] It has been almost two years since my last post on attention. Recent progress on new and enhanced versions of Transformer motivates me to write another post on this specific topic, focusing on how the vanilla Transformer can be improved for longer-term attention span, less memory and computation consumption, RL task solving and more.
]]></content:encoded>
<pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate>
</item>
<item>
<title>Curriculum for Reinforcement Learning</title>
<link>https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/</link>
<guid>https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/</guid>
<content:encoded><![CDATA[
[Updated on 2020-02-03: mentioning PCG in the &ldquo;Task-Specific Curriculum&rdquo; section. [Updated on 2020-02-04: Add a new &ldquo;curriculum through distillation&rdquo; section.
It sounds like an impossible task if we want to teach integral or derivative to a 3-year-old who does not even know basic arithmetics. That&rsquo;s why education is important, as it provides a systematic way to break down complex knowledge and a nice curriculum for teaching concepts from simple to hard.
]]></content:encoded>
<pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
</item>
<item>
<title>Self-Supervised Representation Learning</title>
<link>https://lilianweng.github.io/posts/2019-11-10-self-supervised/</link>
<guid>https://lilianweng.github.io/posts/2019-11-10-self-supervised/</guid>
<content:encoded><![CDATA[
[Updated on 2020-01-09: add a new section on Contrastive Predictive Coding]. [Updated on 2020-04-13: add a &ldquo;Momentum Contrast&rdquo; section on MoCo, SimCLR and CURL.] [Updated on 2020-07-08: add a &ldquo;Bisimulation&rdquo; section on DeepMDP and DBC.] [Updated on 2020-09-12: add MoCo V2 and BYOL in the &ldquo;Momentum Contrast&rdquo; section.] [Updated on 2021-05-31: remove section on &ldquo;Momentum Contrast&rdquo; and add a pointer to a full post on &ldquo;Contrastive Representation Learning&rdquo;]
Given a task and enough labels, supervised learning can solve it really well.
]]></content:encoded>
<pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate>
</item>
<item>
<title>Evolution Strategies</title>
<link>https://lilianweng.github.io/posts/2019-09-05-evolution-strategies/</link>
<guid>https://lilianweng.github.io/posts/2019-09-05-evolution-strategies/</guid>
<content:encoded><![CDATA[
Stochastic gradient descent is a universal choice for optimizing deep learning models. However, it is not the only option. With black-box optimization algorithms, you can evaluate a target function $f(x): \mathbb{R}^n \to \mathbb{R}$, even when you don&rsquo;t know the precise analytic form of $f(x)$ and thus cannot compute gradients or the Hessian matrix. Examples of black-box optimization methods include Simulated Annealing, Hill Climbing and Nelder-Mead method.
Evolution Strategies (ES) is one type of black-box optimization algorithms, born in the family of Evolutionary Algorithms (EA).
]]></content:encoded>
<pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
</item>
<item>
<title>Meta Reinforcement Learning</title>
<link>https://lilianweng.github.io/posts/2019-06-23-meta-rl/</link>
<guid>https://lilianweng.github.io/posts/2019-06-23-meta-rl/</guid>
<content:encoded><![CDATA[
In my earlier post on meta-learning, the problem is mainly defined in the context of few-shot classification. Here I would like to explore more into cases when we try to &ldquo;meta-learn&rdquo; Reinforcement Learning (RL) tasks by developing an agent that can solve unseen tasks fast and efficiently.
To recap, a good meta-learning model is expected to generalize to new tasks or new environments that have never been encountered during training.
]]></content:encoded>
<pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate>
</item>
<item>
<title>Domain Randomization for Sim2Real Transfer</title>
<link>https://lilianweng.github.io/posts/2019-05-05-domain-randomization/</link>
<guid>https://lilianweng.github.io/posts/2019-05-05-domain-randomization/</guid>
<content:encoded><![CDATA[
In Robotics, one of the hardest problems is how to make your model transfer to the real world. Due to the sample inefficiency of deep RL algorithms and the cost of data collection on real robots, we often need to train models in a simulator which theoretically provides an infinite amount of data. However, the reality gap between the simulator and the physical world often leads to failure when working with physical robots.
]]></content:encoded>
<pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
</item>
<item>
<title>Are Deep Neural Networks Dramatically Overfitted?</title>
<link>https://lilianweng.github.io/posts/2019-03-14-overfit/</link>
<guid>https://lilianweng.github.io/posts/2019-03-14-overfit/</guid>
<content:encoded><![CDATA[
[Updated on 2019-05-27: add the section on Lottery Ticket Hypothesis.]
If you are like me, entering into the field of deep learning with experience in traditional machine learning, you may often ponder over this question: Since a typical deep neural network has so many parameters and training error can easily be perfect, it should surely suffer from substantial overfitting. How could it be ever generalized to out-of-sample data points?
]]></content:encoded>
<pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate>
</item>
<item>
<title>Generalized Language Models</title>
<link>https://lilianweng.github.io/posts/2019-01-31-lm/</link>
<guid>https://lilianweng.github.io/posts/2019-01-31-lm/</guid>
<content:encoded><![CDATA[
[Updated on 2019-02-14: add ULMFiT and GPT-2.] [Updated on 2020-02-29: add ALBERT.] [Updated on 2020-10-25: add RoBERTa.] [Updated on 2020-12-13: add T5.] [Updated on 2020-12-30: add GPT-3.] [Updated on 2021-11-13: add XLNet, BART and ELECTRA; Also updated the Summary section.]
Fig. 0. I guess they are Elmo & Bert? (Image source: here) We have seen amazing progress in NLP in 2018. Large-scale pre-trained language modes like OpenAI GPT and BERT have achieved great performance on a variety of language tasks using generic model architectures.
]]></content:encoded>
<pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
</item>
<item>
<title>Object Detection Part 4: Fast Detection Models</title>
<link>https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/</link>
<guid>https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/</guid>
<content:encoded><![CDATA[
In Part 3, we have reviewed models in the R-CNN family. All of them are region-based object detection algorithms. They can achieve high accuracy but could be too slow for certain applications such as autonomous driving. In Part 4, we only focus on fast object detection models, including SSD, RetinaNet, and models in the YOLO family.
Links to all the posts in the series: [Part 1] [Part 2] [Part 3] [Part 4].
]]></content:encoded>
<pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
</item>
<item>
<title>Meta-Learning: Learning to Learn Fast</title>
<link>https://lilianweng.github.io/posts/2018-11-30-meta-learning/</link>
<guid>https://lilianweng.github.io/posts/2018-11-30-meta-learning/</guid>
<content:encoded><![CDATA[
[Updated on 2019-10-01: thanks to Tianhao, we have this post translated in Chinese!]
A good machine learning model often requires training with a large number of samples. Humans, in contrast, learn new concepts and skills much faster and more efficiently. Kids who have seen cats and birds only a few times can quickly tell them apart. People who know how to ride a bike are likely to discover the way to ride a motorcycle fast with little or even no demonstration.
]]></content:encoded>
<pubDate>Fri, 30 Nov 2018 00:00:00 +0000</pubDate>
</item>
<item>
<title>Flow-based Deep Generative Models</title>
<link>https://lilianweng.github.io/posts/2018-10-13-flow-models/</link>
<guid>https://lilianweng.github.io/posts/2018-10-13-flow-models/</guid>
<content:encoded><![CDATA[
So far, I&rsquo;ve written about two types of generative models, GAN and VAE. Neither of them explicitly learns the probability density function of real data, $p(\mathbf{x})$ (where $\mathbf{x} \in \mathcal{D}$) &mdash; because it is really hard! Taking the generative model with latent variables as an example, $p(\mathbf{x}) = \int p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})d\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\mathbf{z}$.
]]></content:encoded>
<pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate>
</item>
<item>
<title>From Autoencoder to Beta-VAE</title>
<link>https://lilianweng.github.io/posts/2018-08-12-vae/</link>
<guid>https://lilianweng.github.io/posts/2018-08-12-vae/</guid>
<content:encoded><![CDATA[
[Updated on 2019-07-18: add a section on VQ-VAE &amp; VQ-VAE-2.] [Updated on 2019-07-26: add a section on TD-VAE.] Autocoder is invented to reconstruct high-dimensional data using a neural network model with a narrow bottleneck layer in the middle (oops, this is probably not true for Variational Autoencoder, and we will investigate it in details in later sections). A nice byproduct is dimension reduction: the bottleneck layer captures a compressed latent encoding.
]]></content:encoded>
<pubDate>Sun, 12 Aug 2018 00:00:00 +0000</pubDate>
</item>
<item>
<title>Attention? Attention!</title>
<link>https://lilianweng.github.io/posts/2018-06-24-attention/</link>
<guid>https://lilianweng.github.io/posts/2018-06-24-attention/</guid>
<content:encoded><![CDATA[
[Updated on 2018-10-28: Add Pointer Network and the link to my implementation of Transformer.] [Updated on 2018-11-06: Add a link to the implementation of Transformer model.] [Updated on 2018-11-18: Add Neural Turing Machines.] [Updated on 2019-07-18: Correct the mistake on using the term &ldquo;self-attention&rdquo; when introducing the show-attention-tell paper; moved it to Self-Attention section.] [Updated on 2020-04-07: A follow-up post on improved Transformer models is here.]
Attention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence.
]]></content:encoded>
<pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
</item>
<item>
<title>Implementing Deep Reinforcement Learning Models with Tensorflow + OpenAI Gym</title>
<link>https://lilianweng.github.io/posts/2018-05-05-drl-implementation/</link>
<guid>https://lilianweng.github.io/posts/2018-05-05-drl-implementation/</guid>
<content:encoded><![CDATA[
The full implementation is available in lilianweng/deep-reinforcement-learning-gym
In the previous two posts, I have introduced the algorithms of many deep reinforcement learning models. Now it is the time to get our hands dirty and practice how to implement the models in the wild. The implementation is gonna be built in Tensorflow and OpenAI gym environment. The full version of the code in this tutorial is available in [lilian/deep-reinforcement-learning-gym].
Environment Setup Make sure you have Homebrew installed: /usr/bin/ruby -e "$(curl -fsSL https://raw.
]]></content:encoded>
<pubDate>Sat, 05 May 2018 00:00:00 +0000</pubDate>
</item>
<item>
<title>Policy Gradient Algorithms</title>
<link>https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</link>
<guid>https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</guid>
<content:encoded><![CDATA[
[Updated on 2018-06-30: add two new policy gradient methods, SAC and D4PG.] [Updated on 2018-09-30: add a new policy gradient method, TD3.] [Updated on 2019-02-09: add SAC with automatically adjusted temperature]. [Updated on 2019-06-26: Thanks to Chanseok, we have a version of this post in Korean]. [Updated on 2019-09-12: add a new policy gradient method SVPG.] [Updated on 2019-12-22: add a new policy gradient method IMPALA.] [Updated on 2020-10-15: add a new policy gradient method PPG &amp; some new discussion in PPO.
]]></content:encoded>
<pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate>
</item>
<item>
<title>A (Long) Peek into Reinforcement Learning</title>
<link>https://lilianweng.github.io/posts/2018-02-19-rl-overview/</link>
<guid>https://lilianweng.github.io/posts/2018-02-19-rl-overview/</guid>
<content:encoded><![CDATA[
[Updated on 2020-09-03: Updated the algorithm of SARSA and Q-learning so that the difference is more pronounced. [Updated on 2021-09-19: Thanks to çˆ±åƒçŒ«çš„é±¼, we have this post in Chinese].
A couple of exciting news in Artificial Intelligence (AI) has just happened in recent years. AlphaGo defeated the best professional human player in the game of Go. Very soon the extended algorithm AlphaGo Zero beat AlphaGo by 100-0 without supervised learning on human knowledge.
]]></content:encoded>
<pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
</item>
<item>
<title>The Multi-Armed Bandit Problem and Its Solutions</title>
<link>https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/</link>
<guid>https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/</guid>
<content:encoded><![CDATA[
The algorithms are implemented for Bernoulli bandit in lilianweng/multi-armed-bandit.
Exploitation vs Exploration The exploration vs exploitation dilemma exists in many aspects of our life. Say, your favorite restaurant is right around the corner. If you go there every day, you would be confident of what you will get, but miss the chances of discovering an even better option. If you try new places all the time, very likely you are gonna have to eat unpleasant food from time to time.
]]></content:encoded>
<pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
</item>
<item>
<title>Object Detection for Dummies Part 3: R-CNN Family</title>
<link>https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/</link>
<guid>https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/</guid>
<content:encoded><![CDATA[
[Updated on 2018-12-20: Remove YOLO here. Part 4 will cover multiple fast object detection algorithms, including YOLO.] [Updated on 2018-12-27: Add bbox regression and tricks sections for R-CNN.]
In the series of &ldquo;Object Detection for Dummies&rdquo;, we started with basic concepts in image processing, such as gradient vectors and HOG, in Part 1. Then we introduced classic convolutional neural network architecture designs for classification and pioneer models for object recognition, Overfeat and DPM, in Part 2.
]]></content:encoded>
<pubDate>Sun, 31 Dec 2017 00:00:00 +0000</pubDate>
</item>
<item>
<title>Object Detection for Dummies Part 2: CNN, DPM and Overfeat</title>
<link>https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/</link>
<guid>https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/</guid>
<content:encoded><![CDATA[
Part 1 of the &ldquo;Object Detection for Dummies&rdquo; series introduced: (1) the concept of image gradient vector and how HOG algorithm summarizes the information across all the gradient vectors in one image; (2) how the image segmentation algorithm works to detect regions that potentially contain objects; (3) how the Selective Search algorithm refines the outcomes of image segmentation for better region proposal.
In Part 2, we are about to find out more on the classic convolution neural network architectures for image classification.
]]></content:encoded>
<pubDate>Fri, 15 Dec 2017 00:00:00 +0000</pubDate>
</item>
<item>
<title>Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS</title>
<link>https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/</link>
<guid>https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/</guid>
<content:encoded><![CDATA[
I&rsquo;ve never worked in the field of computer vision and has no idea how the magic could work when an autonomous car is configured to tell apart a stop sign from a pedestrian in a red hat. To motivate myself to look into the maths behind object recognition and detection algorithms, I&rsquo;m writing a few posts on this topic &ldquo;Object Detection for Dummies&rdquo;. This post, part 1, starts with super rudimentary concepts in image processing and a few methods for image segmentation.
]]></content:encoded>
<pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
</item>
<item>
<title>Learning Word Embedding</title>
<link>https://lilianweng.github.io/posts/2017-10-15-word-embedding/</link>
<guid>https://lilianweng.github.io/posts/2017-10-15-word-embedding/</guid>
<content:encoded><![CDATA[
Human vocabulary comes in free text. In order to make a machine learning model understand and process the natural language, we need to transform the free-text words into numeric values. One of the simplest transformation approaches is to do a one-hot encoding in which each distinct word stands for one dimension of the resulting vector and a binary value indicates whether the word presents (1) or not (0).
However, one-hot encoding is impractical computationally when dealing with the entire vocabulary, as the representation demands hundreds of thousands of dimensions.
]]></content:encoded>
<pubDate>Sun, 15 Oct 2017 00:00:00 +0000</pubDate>
</item>
<item>
<title>Anatomize Deep Learning with Information Theory</title>
<link>https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/</link>
<guid>https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/</guid>
<content:encoded><![CDATA[
Professor Naftali Tishby passed away in 2021. Hope the post can introduce his cool idea of information bottleneck to more people.
Recently I watched the talk &ldquo;Information Theory in Deep Learning&rdquo; by Prof Naftali Tishby and found it very interesting. He presented how to apply the information theory to study the growth and transformation of deep neural networks during training. Using the Information Bottleneck (IB) method, he proposed a new learning bound for deep neural networks (DNN), as the traditional learning theory fails due to the exponentially large number of parameters.
]]></content:encoded>
<pubDate>Thu, 28 Sep 2017 00:00:00 +0000</pubDate>
</item>
<item>
<title>From GAN to WGAN</title>
<link>https://lilianweng.github.io/posts/2017-08-20-gan/</link>
<guid>https://lilianweng.github.io/posts/2017-08-20-gan/</guid>
<content:encoded><![CDATA[
[Updated on 2018-09-30: thanks to Yoonju, we have this post translated in Korean!] [Updated on 2019-04-18: this post is also available on arXiv.]
Generative adversarial network (GAN) has shown great results in many generative tasks to replicate the real-world rich content such as images, human language, and music. It is inspired by game theory: two models, a generator and a critic, are competing with each other while making each other stronger at the same time.
]]></content:encoded>
<pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate>
</item>
<item>
<title>How to Explain the Prediction of a Machine Learning Model?</title>
<link>https://lilianweng.github.io/posts/2017-08-01-interpretation/</link>
<guid>https://lilianweng.github.io/posts/2017-08-01-interpretation/</guid>
<content:encoded><![CDATA[
The machine learning models have started penetrating into critical areas like health care, justice systems, and financial industry. Thus to figure out how the models make the decisions and make sure the decisioning process is aligned with the ethnic requirements or legal regulations becomes a necessity.
Meanwhile, the rapid growth of deep learning models pushes the requirement of interpreting complicated models further. People are eager to apply the power of AI fully on key aspects of everyday life.
]]></content:encoded>
<pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
</item>
<item>
<title>Predict Stock Prices Using RNN: Part 2</title>
<link>https://lilianweng.github.io/posts/2017-07-22-stock-rnn-part-2/</link>
<guid>https://lilianweng.github.io/posts/2017-07-22-stock-rnn-part-2/</guid>
<content:encoded><![CDATA[
In the Part 2 tutorial, I would like to continue the topic on stock price prediction and to endow the recurrent neural network that I have built in Part 1 with the capability of responding to multiple stocks. In order to distinguish the patterns associated with different price sequences, I use the stock symbol embedding vectors as part of the input.
Dataset During the search, I found this library for querying Yahoo!
]]></content:encoded>
<pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate>
</item>
<item>
<title>Predict Stock Prices Using RNN: Part 1</title>
<link>https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/</link>
<guid>https://lilianweng.github.io/posts/2017-07-08-stock-rnn-part-1/</guid>
<content:encoded><![CDATA[
This is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. The full working code is available in github.com/lilianweng/stock-rnn. If you don&rsquo;t know what is recurrent neural network or LSTM cell, feel free to check my previous post.
One thing I would like to emphasize that because my motivation for writing this post is more on demonstrating how to build and train an RNN model in Tensorflow and less on solve the stock prediction problem, I didn&rsquo;t try hard on improving the prediction outcomes.
]]></content:encoded>
<pubDate>Sat, 08 Jul 2017 00:00:00 +0000</pubDate>
</item>
<item>
<title>An Overview of Deep Learning for Curious People</title>
<link>https://lilianweng.github.io/posts/2017-06-21-overview/</link>
<guid>https://lilianweng.github.io/posts/2017-06-21-overview/</guid>
<content:encoded><![CDATA[
(The post was originated from my talk for WiMLDS x Fintech meetup hosted by Affirm.)
I believe many of you have watched or heard of the games between AlphaGo and professional Go player Lee Sedol in 2016. Lee has the highest rank of nine dan and many world championships. No doubt, he is one of the best Go players in the world, but he lost by 1-4 in this series versus AlphaGo.
]]></content:encoded>
<pubDate>Wed, 21 Jun 2017 00:00:00 +0000</pubDate>
</item>
<item>
<title>FAQ</title>
<link>https://lilianweng.github.io/faq/</link>
<guid>https://lilianweng.github.io/faq/</guid>
<content:encoded><![CDATA[

]]></content:encoded>
<pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
</item>
</channel>
</rss>