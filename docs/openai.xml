<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>OpenAI Blog</title>
<link>https://openai.com/blog</link>

<item>
<title>Introducing more enterprise-grade features for API customers</title>
<link>https://openai.com/blog/more-enterprise-grade-features-for-api-customers</link>
<guid>https://openai.com/blog/more-enterprise-grade-features-for-api-customers</guid>
<content:encoded><![CDATA[
<div> 企业级安全、管理员控制、成本管理、AI解决方案、API改进<br><br>总结: OpenAI为企业提供了改进的企业级安全功能，如Private Link和多因素身份验证，并提供了更好的管理员控制，如角色和访问限制。同时，OpenAI还改进了助手API，提供了更好的文件检索、实时对话响应和成本管理。此外，OpenAI还推出了面向承诺吞吐量的折扣服务和异步工作负载降低成本服务。OpenAI致力于在企业级安全、管理员控制和成本管理方面继续不断改进。 <div>
<img alt="Dalle 2024 04 05 07 40" src="https://images.openai.com/blob/659e0aa5-140d-47f7-81fe-0eb92f4a1b48/DALLE2024-04-0507.40.png?trim=411%2C0%2C308%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We work with many enterprises like <a href="https://openai.com/customer-stories/klarna" rel="noopener noreferrer">Klarna</a>, <a href="https://openai.com/customer-stories/morgan-stanley" rel="noopener noreferrer">Morgan Stanley</a>, <a href="https://openai.com/customer-stories/oscar" rel="noopener noreferrer">Oscar</a>, <a href="https://openai.com/customer-stories/salesforce" rel="noopener noreferrer">Salesforce</a>, and <a href="https://openai.com/customer-stories/wix" rel="noopener noreferrer">Wix</a> to help them build AI solutions from scratch and safely deploy AI across their organizations and products. We’re deepening our support for enterprises with new features that are useful for both large businesses and any developers who are scaling quickly on our platform.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="enhanced-enterprise-grade-security"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Enhanced enterprise-grade security</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We’ve introduced Private Link, a new way that customers can ensure direct communication between Azure and OpenAI while minimizing exposure to the open internet. We’ve also released native <a href="https://help.openai.com/en/articles/7967234-enabling-multi-factor-authentication-mfa-with-openai" rel="noopener noreferrer" target="_blank">Multi-Factor Authentication</a> (MFA) to help ensure compliance with increasing access control requirements. These are new additions to our existing stack of <a href="https://trust.openai.com/" rel="noopener noreferrer" target="_blank">enterprise security features</a> including SOC 2 Type II certification, single sign-on (SSO), data encryption at rest using AES-256 and in transit using TLS 1.2, and role-based access controls. We also offer <a href="https://help.openai.com/en/articles/8660679-how-can-i-get-a-business-associate-agreement-baa-with-openai" rel="noopener noreferrer" target="_blank">Business Associate Agreements</a> for healthcare companies that require HIPAA compliance and a zero data retention policy for API customers with a qualifying use case.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="better-administrative-control"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Better administrative control</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>With our new <a href="https://help.openai.com/en/articles/9186755" rel="noopener noreferrer" target="_blank">Projects</a> feature, organizations will have more granular control and oversight over individual projects in OpenAI. This includes the ability to scope roles and API keys to specific projects, restrict/allow which models to make available, and set usage- and rate-based limits to give access and avoid unexpected overages. Project owners will also have the ability to create service account API keys, which give access to projects without being tied to an individual user.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--image"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><figure class=""><div class=""><img alt="Projects Crop 2" class="w-full" height="746" src="https://images.openai.com/blob/d2e85bb1-41e9-4c0c-83ed-9cab72ce9290/projects-crop-2.gif" width="1098" /></div><figcaption class="f-caption-1 relative mt-8"></figcaption></figure></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="assistants-api-improvements"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Assistants API improvements</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We’ve introduced several updates to the <a href="https://platform.openai.com/docs/assistants/overview?context=with-streaming" rel="noopener noreferrer" target="_blank">Assistants API</a> for more accurate retrieval, flexibility around model behavior and tools used to complete tasks, and better control over costs. These features include:<br class="softbreak" /><br class="softbreak" /></p><ul><li><strong>Improved retrieval with ‘file_search’</strong> which can ingest up to 10,000 files per assistant—a 500x increase from the previous file limit of 20. The tool is faster, supports parallel queries through multi-threaded searches, and has enhanced reranking and query rewriting.</li><li><strong>Streaming support </strong>for real-time, conversational responses—one of the top requests from developers and enterprises.</li><li><strong>New ‘vector_store’ objects</strong> in the API so files can be added to a vector store and automatically parsed, chunked, and embedded in preparation for file search. Vector stores can be used across assistants and threads, simplifying file management and billing.</li><li><strong>Control over the maximum number of tokens used per run</strong>, plus limits on previous and recent messages used in each run, so you can manage token usage costs.</li><li><strong>New ‘tool_choice’ parameter</strong> to select a specific tool (like ‘file_search’, ‘code_interpreter’, or ‘function’) in a particular run.</li><li><strong>Support for fine-tuned GPT-3.5 Turbo models </strong>in the API (to start, we’ll support fine-tunes of ‘gpt-3.5-turbo-0125’).<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--image"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><figure class=""><div class=""><img alt="Sarea46" class="w-full" height="738" src="https://images.openai.com/blob/8baf79d9-0c5b-4dcf-917c-63aad0092150/sArea46.gif" width="1199" /></div><figcaption class="f-caption-1 relative mt-8"></figcaption></figure></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="more-options-for-cost-management"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">More options for cost management</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>To help organizations scale their AI usage without over-extending their budgets, we’ve added two new ways to reduce costs on consistent and asynchronous workloads:</p><ul><li><strong>Discounted usage on committed throughput: </strong>Customers with a sustained level of tokens per minute (TPM) usage on GPT-4 or GPT-4 Turbo can request access to provisioned throughput to get discounts ranging from 10–50% based on the size of the commitment.</li><li><strong>Reduced costs on asynchronous workloads: </strong>Customers can use our new <a href="https://platform.openai.com/docs/api-reference/batch" rel="noopener noreferrer" target="_blank">Batch API</a> to run non-urgent workloads asynchronously. Batch API requests are priced at 50% off shared prices, offer much higher rate limits, and return results within 24 hours. This is ideal for use cases like model evaluation, offline classification, summarization, and synthetic data generation.</li></ul><p><span class="ql-anchor" id="docs-internal-guid-9610cd65-7fff-3c1b-d7d8-b4c3fafdbe47"><br class="softbreak" /></span>We plan to keep adding new features focused on enterprise-grade security, administrative controls, and cost management. For more information on these launches, visit our <a href="https://platform.openai.com/docs/introduction" rel="noopener noreferrer" target="_blank">API documentation</a> or <a href="https://openai.com/contact-sales" rel="noopener noreferrer" target="_blank">get in touch with our team</a> to discuss custom solutions for your enterprise.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Mon, 22 Apr 2024 20:51:50 GMT</pubDate>
<pubDate>Mon, 22 Apr 2024 20:51:50 GMT</pubDate>
</item>
<item>
<title>OpenAI’s commitment to child safety: adopting safety by design principles</title>
<link>https://openai.com/blog/child-safety-adopting-sbd-principles</link>
<guid>https://openai.com/blog/child-safety-adopting-sbd-principles</guid>
<content:encoded><![CDATA[
<div> OpenAI, child safety measures, generative AI technologies, Safety by Design principles, commitment.

总结:
OpenAI与行业领导者一起承诺在生成式AI技术的开发、部署和维护中实施强有力的儿童安全措施。他们采纳了全面的设计安全原则，致力于保护儿童的安全，并减轻生成式AI对儿童造成的风险。OpenAI和其他参与者已经采取了一系列措施，以最大程度地减少模型生成损害儿童的内容的可能性，并对ChatGPT设置了年龄限制。他们积极与国家失踪和被剥削儿童中心（NCMEC）、科技联盟和其他政府和行业相关方合作，解决儿童保护问题，并加强举报机制。通过承诺开展全方位的安全设计工作，OpenAI及其同行们将确保儿童安全是AI开发的每一阶段的重中之重。同时，他们还承诺在源码选择、检测、删除儿童性虐待材料、以及部署和维护过程中持续关注儿童安全风险。通过这一倡议，OpenAI等机构将共同努力，避免滥用AI技术来制造或传播针对儿童的性虐待材料，为最脆弱的社会成员的福祉做出努力。 <div>
<img alt="Dalle 2024 02 27 20 39" src="https://images.openai.com/blob/b264c2d2-bcd1-4e28-af98-ce85c244c242/DALLE2024-02-2720.39.png?trim=270%2C0%2C202%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>OpenAI, alongside industry leaders including Amazon, Anthropic, Civitai, Google, Meta, Metaphysic, Microsoft, Mistral AI, and Stability AI, has committed to implementing robust child safety measures in the development, deployment, and maintenance of generative AI technologies as articulated in the Safety by Design principles. This initiative, led by <a href="http://thorn.org/" rel="noopener noreferrer" target="_blank">Thorn</a>, a nonprofit dedicated to defending children from sexual abuse, and <a href="https://alltechishuman.org/" rel="noopener noreferrer" target="_blank">All Tech Is Human</a>, an organization dedicated to tackling tech and society's complex problems, aims to mitigate the risks generative AI poses to children. By adopting comprehensive Safety by Design principles, OpenAI and our peers are ensuring that child safety is prioritized at every stage in the development of AI. To date, we have made significant effort to minimize the potential for our models to generate content that harms children, set age restrictions for ChatGPT, and actively engage with the National Center for Missing and Exploited Children (NCMEC), Tech Coalition, and other government and industry stakeholders on child protection issues and enhancements to reporting mechanisms.&nbsp;</p><p>As part of this Safety by Design effort, we commit to:</p>

<ol>
  <li>
    <strong>Develop:</strong> Develop, build, and train generative AI models
    that proactively address child safety risks.
    <ul style="margin-top: 0; padding-left: 2em;">
      <li style="text-indent: 0; padding: 0;">
        Responsibly source our training datasets, detect and remove child sexual
        abuse material (CSAM) and child sexual exploitation material (CSEM) from
        training data, and report any confirmed CSAM to the relevant
        authorities.
      </li>
      <li style="text-indent: 0; padding: 0;">
        Incorporate feedback loops and iterative stress-testing strategies in
        our development process.
      </li>
      <li style="text-indent: 0; padding: 0;">Deploy solutions to address adversarial misuse.</li>
    </ul>
  </li>
  <li>
    <strong>Deploy: </strong>Release and distribute generative AI models after
    they have been trained and evaluated for child safety, providing protections
    throughout the process.
    <ul style="margin-top: 0; padding-left: 2em;">
      <li style="text-indent: 0; padding: 0;">
        Combat and respond to abusive content and conduct, and incorporate
        prevention efforts.
      </li>
      <li style="text-indent: 0; padding: 0;">Encourage developer ownership in safety by design.</li>
    </ul>
  </li>
  <li>
    <strong>Maintain: </strong>Maintain model and platform safety by continuing
    to actively understand and respond to child safety risks.
    <ul style="margin-top: 0; padding-left: 2em;">
      <li style="text-indent: 0; padding: 0;">
        Committed to removing new AIG-CSAM generated by bad actors from our
        platform.&nbsp;
      </li>
      <li style="text-indent: 0; padding: 0;">Invest in research and future technology solutions.</li>
      <li style="text-indent: 0; padding: 0;">Fight CSAM, AIG-CSAM and CSEM on our platforms.</li>
    </ul>
  </li>
</ol>

<p>This commitment marks an important step in preventing the misuse of AI technologies to create or spread child sexual abuse material (AIG-CSAM) and other forms of sexual harm against children. As part of the working group, we have also agreed to release progress updates every year.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--quote"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="f-body-1 md:w-6-cols lg:ml-2-cols lg:w-10-cols"><figure><blockquote class="f-quote-1"><p class="before:content-['“'] after:content-['”'] relative before:absolute before:left-0 before:-translate-x-full"><span>We care deeply about the safety and responsible use of our tools, which is why we’ve built strong guardrails and safety measures into ChatGPT and DALL-E. We are committed to working alongside Thorn, All Tech is Human and the broader tech community to uphold the Safety by Design principles and continue our work in mitigating potential harms to children.</span></p></blockquote><figcaption class="f-subhead-2 mt-spacing-4">Chelsea Carlson, Child Safety TPM</figcaption></figure></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>This collective action underscores our shared approach to child safety, demonstrating a shared commitment to ethical innovation and the well-being of the most vulnerable members of society. Thorn has published the principles at <a href="https://teamthorn.co/gen-ai" rel="noopener noreferrer" target="_blank">https://teamthorn.co/gen-ai</a><br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Mon, 22 Apr 2024 19:34:22 GMT</pubDate>
<pubDate>Mon, 22 Apr 2024 19:34:22 GMT</pubDate>
</item>
<item>
<title>Introducing OpenAI Japan</title>
<link>https://openai.com/blog/introducing-openai-japan</link>
<guid>https://openai.com/blog/introducing-openai-japan</guid>
<content:encoded><![CDATA[
<div> Tokyo, Japan; AI tools; local businesses; research institutions; GPT-4 custom model; Tadao Nagasaki; ChatGPT Enterprise; Japanese government; AGI benefits; OpenAI Japan; regional challenges.
<br><br>
总结: OpenAI将在东京设立新办事处，致力于与日本政府、本地企业和研究机构合作，开发满足日本独特需求的安全AI工具，解锁新机遇。他们已经推出了日本语言优化的GPT-4自定义模型，并与当地企业和政府展开合作，旨在自动化业务流程、改善公共服务效率，探索AI对社会挑战的应用。OpenAI计划通过在全球扩大存在，汲取多样性的观点，确保智能机器的利益惠及全人类。 <div>
<img alt="Japan Wht" src="https://images.openai.com/blob/4a1491a9-14b5-41f5-863a-b38c5168a584/Japan_Wht.png?trim=43%2C0%2C429%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><em>Editor’s note: Japanese follows English (日本語は英語の後に続きます)</em><br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>As we grow our operations internationally, we’re expanding into Asia with a new office in Tokyo, Japan. We are committed to collaborating with the Japanese government, local businesses, and research institutions to develop safe AI tools that serve Japan’s unique needs and to unlock new opportunities. We chose Tokyo as our first Asian office for its global leadership in technology, culture of service, and a community that embraces innovation.&nbsp;</p></div></div></div></div></div></div></div><div class="ui-block ui-block--quote"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="f-body-1 md:w-6-cols lg:ml-2-cols lg:w-10-cols"><figure><blockquote class="f-quote-1"><p class="before:content-['“'] after:content-['”'] relative before:absolute before:left-0 before:-translate-x-full"><span>We’re excited to be in Japan which has a rich history of people and technology coming together to do more. We believe AI will accelerate work by empowering people to be more creative and productive, while also delivering broad value to current and new industries that have yet to be imagined.</span></p></blockquote><figcaption class="f-subhead-2 mt-spacing-4">Sam Altman, CEO of OpenAI</figcaption></figure></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>To spearhead our efforts in Japan and ensure we are deeply integrated within the local community, we welcome Tadao Nagasaki as our new President of OpenAI Japan. Mr. Nagasaki will lead our commercial and market engagement efforts and help build our local team that will advance Global Affairs, Go-to-Market, Communications, Operations and other functions in serving Japan.&nbsp;</p><p>As a first step in our long-term commitment to the region, we’re providing local businesses with early access to a GPT-4 custom model specifically optimized for the Japanese language. This custom model offers improved performance in translating and summarizing Japanese text, is cost effective, and operates up to 3x faster than its predecessor. <a href="https://www.speak.com/jp" rel="noopener noreferrer" target="_blank">Speak</a>, a top English learning app in Japan, is seeing 2.8x faster tutor explanations in Japanese when users make a mistake with a 47% reduction in token cost, unlocking higher quality tutor feedback in more places and with higher limits per user. We plan to release the custom model more broadly in the API<em> </em>in the coming months.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--image"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><figure class=""><div class=""><img alt="Blog Japan 1" class="w-full" height="1080" src="https://images.openai.com/blob/3f51af3a-4ebb-42f3-b027-e60efd75acd9/Blog_Japan1.gif" width="1919" /></div><figcaption class="f-caption-1 relative mt-8 ui-richtext"><p>We are releasing a GPT-4 custom model optimized for the Japanese language which offers improved performance in Japanese text and operates up to 3x faster than GPT-4 Turbo.<br class="softbreak" /></p></figcaption></figure></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Our new local presence also gets us closer to leading businesses like Daikin, Rakuten, and TOYOTA Connected who are using ChatGPT Enterprise to automate complex business processes, assist in data analysis, and optimize internal reporting. ChatGPT also helps accelerate the efforts of local governments, such as Yokosuka City, which is leveraging the technology to improve the efficiency of public services in Japan. Over the past year, the city has gradually provided ChatGPT access to almost all city employees, and 80% have reported increases in productivity. Now Yokosuka City has formed a network with 21 local governments—including the Tokyo Metropolitan Government and the City of Kobe—to share best practices of ChatGPT use in government.</p><p>As a key global voice on AI policy, the Japanese government chaired the G7 Hiroshima AI Process and worked to implement AI policies that align with its goals for human dignity, diversity and inclusion, and sustainable societies, while helping Japan realize solutions to its rural depopulation and labor shortage. We look forward to contributing to the local ecosystem, while exploring how AI can help with these societal challenges in the region.&nbsp;</p><p>Growing our presence across the world allows us to learn from a wide range of diverse perspectives, which is critical to our mission of ensuring AGI benefits all of humanity. If you are interested in joining us, please see our <a href="https://openai.com/careers/search" rel="noopener noreferrer" target="_blank">Careers</a> page for all open positions.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="openai-japan"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">OpenAI Japan 始動</h2></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-6" id="gpt-4-"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">東京にアジア初のオフィスを開設するとともに、日本語に最適化されたGPT-4カスタムモデルの提供を開始します。</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>OpenAI がグローバルに事業を拡大する中、本日、東京に新しいオフィスを設立し、アジアへと展開していきます。アジアでの最初の拠点として技術、サービスの文化、イノベーションを受け入れるコミュニティにおいて、世界をリードする東京を選びました。日本の独自のニーズに応える安全なAIツールの開発を目指し、政府、地元企業、研究機関と協力していくことに尽力していきます。<br class="softbreak" /></p><p>「日本にオフィスを開設できたことを嬉しく思います。日本は長い歴史を通じ、人々と技術が協力し、大変多くのことを成し遂げています。AIが、人々をより創造的で生産的になるのを助け、まだ想像されていない新しい産業にも広範囲に価値を提供することを加速できると信じています。」- サム・アルトマン、OpenAI CEO</p><p>OpenAI の日本における活動をリードし、日本のコミュニティに深く溶け込んで貢献するため、長﨑忠雄がOpenAI Japanの社長に着任しました。長﨑は、セールスと事業開発をリードし、併せて渉外、製品およびサービスに関する計画、コミュニケーション、オペレーションなどを担うチームを構築していきます。</p><p>日本への長期的なコミットメントの第一歩として、私たちは日本の企業に日本語に特化して最適化されたGPT-4カスタムモデルの提供を開始しています。このカスタムモデルは、日本語のテキストの翻訳と要約のパフォーマンス、およびコスト効率を向上させ、前モデルと比較して、最大3倍高速に動作します。 この技術を活用した日本で最も利用されている英語学習アプリ「Speak」は、ユーザーが間違えた際のチューター（指導者）の説明が2.8倍速くなりました。トークン数が減り、効率化されたことでそのコストが47％削減されています。より多くの場所でより高品質なフィードバックが可能になりました。</p><p>このカスタムモデルは、数か月以内にAPIで広くリリースされる予定です。<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--image"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><figure class=""><div class=""><img alt="Blog Japan 1" class="w-full" height="1080" src="https://images.openai.com/blob/3f51af3a-4ebb-42f3-b027-e60efd75acd9/Blog_Japan1.gif" width="1919" /></div><figcaption class="f-caption-1 relative mt-8 ui-richtext"><p>写真キャプション：GPT-4のカスタムモデルを日本語向けに最適化しました。日本語テキストの性能向上と、GPT-4 Turboより最大3倍高速な動作を提供します。<br class="softbreak" /></p></figcaption></figure></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>日本においてはすでに、ダイキン、楽天、トヨタコネクテッドなどの日本の主要企業に導入され、ChatGPTエンタープライズを利用して複雑なビジネスプロセスの自動化、データ分析の支援、社内報告の最適化を図っています。また、ChatGPTは、横須賀市などの地方自治体に活用され、地域の公共サービスの生産性向上に貢献しています。横須賀市によると、過去1年間、全市職員のほとんどにChatGPTのアクセスを段階的に提供し、80%が生産性の向上を報告されています。現在、横須賀市は東京都や神戸市を含む21の地方自治体とネットワークを形成し、行政におけるChatGPT使用に関するベストプラクティスを共有しています。</p><p>AI政策における世界の主要な声として、日本政府はG7広島AIプロセスを主導し、人間の尊厳、多様性と包摂、持続可能な社会という目標に合致するAI政策の実施に取り組んでいます。まずは地方の過疎化と労働力不足への解決策を実現していくことでしょう。OpenAI もこのエコシステムに貢献し、日本の社会的課題に対してAIがどのように役立つかを探求していくことを楽しみにしています。</p><p>私たちが成長し、日本を含む世界で存在感を高めることで、私たちは多様な視点から学ぶことができます。それは人類全体にAGIの利益を確実にするという私たちの使命にとって、きわめて重要です。</p><p>OpenAI Japanでは、一緒に働く仲間を募集しています。詳細は日本の採用ページで公開していきますのでぜひご覧ください。<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Fri, 12 Apr 2024 19:09:59 GMT</pubDate>
<pubDate>Fri, 12 Apr 2024 19:09:59 GMT</pubDate>
</item>
<item>
<title>Introducing improvements to the fine-tuning API and expanding our custom models program</title>
<link>https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program</link>
<guid>https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program</guid>
<content:encoded><![CDATA[
<div> 自定义模型、API、特性、Fine-tuning、性能控制<br><br>总结: 该文章介绍了如何通过自定义模型、Fine-tuning和API的特性来提高模型性能，以减少延迟、提高准确性和降低成本。文章提到了自助Fine-tuning的API特性，以及支持第三方集成和全面验证指标等新功能。此外，文章还介绍了自定义模型计划中的辅助Fine-tuning和全定制模型的实施方式，并提供了一些案例和结果。作者认为未来大多数组织会开发定制模型，以实现更具有个性化和特定影响力的AI实施。 <div>
<img alt="Customblogcover" src="https://images.openai.com/blob/b44edb0b-77de-46a9-b4dc-0ff956143d94/CustomBlogCover.png?trim=160%2C0%2C288%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>There are a <a href="https://www.youtube.com/watch?v=ahnGLM-RC1Y&amp;list=PLOXw6I10VTv-exVCRuRjbT6bqkfO74rWz&amp;index=4" rel="noopener noreferrer" target="_blank">variety of techniques</a> that developers can use to increase model performance in an effort to reduce latency, improve accuracy, and reduce costs. Whether it’s extending model knowledge with retrieval-augmented generation (RAG), customizing a model’s behavior with fine-tuning, or building a custom-trained model with new domain-specific knowledge, we have developed a range of options to support our customers’ AI implementations. Today, we’re launching new features to give developers more control over fine-tuning with the API and introducing more ways to work with our team of AI experts and researchers to build custom models.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="new-fine-tuning-api-features"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">New fine-tuning API features</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We launched the self-serve <a href="https://platform.openai.com/docs/guides/fine-tuning" rel="noopener noreferrer" target="_blank">fine-tuning API</a> for GPT-3.5 in August 2023. Since then, thousands of organizations have trained hundreds of thousands of models using our API. Fine-tuning can help models deeply understand content and augment a model’s existing knowledge and capabilities for a specific task. Our fine-tuning API also supports a larger volume of examples than can fit in a single prompt to achieve higher quality results while reducing cost and latency. Some of the common use cases of fine-tuning include training a model to generate better code in a particular programming language, to summarize text in a specific format, or to craft personalized content based on user behavior.&nbsp;</p><p>For example,&nbsp;<a href="https://www.indeed.com/" rel="noopener noreferrer" target="_blank">Indeed</a>, a global job matching and hiring platform, wants to simplify the hiring process. As part of this, Indeed launched a feature that sends personalized recommendations to job seekers, highlighting relevant jobs based on their skills, experience, and preferences. They fine-tuned GPT-3.5 Turbo to generate higher quality and more accurate explanations. As a result, Indeed was able to improve cost and latency by reducing the number of tokens in prompt by 80%. This let them scale from less than one million messages to job seekers per month to roughly 20 million.</p><p>Today, we’re introducing <a href="https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model" rel="noopener noreferrer" target="_blank">new features</a> to give developers even more control over their fine-tuning jobs, including:</p><ul><li><strong>Epoch-based Checkpoint Creation:</strong> Automatically produce one full fine-tuned model checkpoint during each training epoch, which reduces the need for subsequent retraining, especially in the cases of overfitting</li><li><strong>Comparative Playground</strong>: A new side-by-side Playground UI for comparing model quality and performance, allowing human evaluation of the outputs of multiple models or fine-tune snapshots against a single prompt</li><li><strong>Third-party Integration: </strong>Support for integrations with third-party platforms (starting with <a href="https://wandb.ai/site" rel="noopener noreferrer" target="_blank">Weights and Biases</a> this week) to let developers share detailed fine-tuning data to the rest of their stack</li><li><strong>Comprehensive Validation Metrics</strong>: The ability to compute metrics like loss and accuracy over the entire validation dataset instead of a sampled batch, providing better insight on model quality</li><li><strong>Hyperparameter Configuration</strong>: The ability to configure available hyperparameters from the <a href="https://platform.openai.com/finetune" rel="noopener noreferrer" target="_blank">Dashboard</a> (rather than only through the API or SDK)&nbsp;</li><li><strong>Fine-Tuning Dashboard Improvements</strong>: Including the ability to configure hyperparameters, view more detailed training metrics, and rerun jobs from previous configurations<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--image"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><figure class=""><div class=""><img alt="Fine Tuning in the API" class="w-full" height="924" src="https://images.openai.com/blob/23b755f0-f6f0-4d9f-a90a-b9264ed93731/fine-tuning-in-api.gif" width="1396" /></div><figcaption class="f-caption-1 relative mt-8"></figcaption></figure></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="expanding-our-custom-models-program"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Expanding our Custom Models Program</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><strong>Assisted Fine-Tuning</strong></p><p>At DevDay last November, we <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday" rel="noopener noreferrer" target="_blank">announced</a> a Custom Model program designed to train and optimize models for a specific domain, in partnership with a dedicated group of OpenAI researchers. Since then, we've met with dozens of customers to assess their custom model needs and evolved our program to further maximize performance.</p><p>Today, we are formally announcing our assisted fine-tuning offering as part of the Custom Model program. Assisted fine-tuning is a collaborative effort with our technical teams to leverage techniques beyond the fine-tuning API, such as additional hyperparameters and various parameter efficient fine-tuning (PEFT) methods at a larger scale. It’s particularly helpful for organizations that need support setting up efficient training data pipelines, evaluation systems, and bespoke parameters and methods to maximize model performance for their use case or task.</p><p>For example, <a href="https://www.sktelecom.com/index_en.html" rel="noopener noreferrer" target="_blank">SK Telecom</a>, a telecommunications operator serving over 30 million subscribers in South Korea, wanted to customize a model to be an expert in the telecommunications domain with an initial focus on customer service. They worked with OpenAI to fine-tune GPT-4 to improve its performance in telecom-related conversations in the Korean language. Over the course of multiple weeks, SKT and OpenAI drove meaningful performance improvement in telecom customer service tasks—a 35% increase in conversation summarization quality, a 33% increase in intent recognition accuracy, and an increase in satisfaction scores from 3.6 to 4.5 (out of 5) when comparing the fine-tuned model to GPT-4.&nbsp;</p><p><strong>Custom-Trained Model</strong></p><p>In some cases, organizations need to train a purpose-built model from scratch that understands their business, industry, or domain. Fully custom-trained models imbue new knowledge from a specific domain by modifying key steps of the model training process using novel mid-training and post-training techniques. Organizations that see success with a fully custom-trained model often have large quantities of proprietary data—millions of examples or billions of tokens—that they want to use to teach the model new knowledge or complex, unique behaviors for highly specific use cases.&nbsp;</p><p>For example, <a href="https://www.harvey.ai/" rel="noopener noreferrer" target="_blank">Harvey</a>, an AI-native legal tool for attorneys, partnered with OpenAI to <a href="https://openai.com/customer-stories/harvey" rel="noopener noreferrer" target="_blank">create a custom-trained large language model for case law</a>. While foundation models were strong at reasoning, they lacked the extensive knowledge of legal case history and other knowledge required for legal work. After testing out prompt engineering, RAG, and fine-tuning, Harvey worked with our team to add the depth of context needed to the model—the equivalent of 10 billion tokens worth of data. Our team modified every step of the model training process, from domain-specific mid-training to customizing post-training processes and incorporating expert attorney feedback. The resulting model achieved an 83% increase in factual responses and attorneys preferred the customized model’s outputs 97% of the time over GPT-4.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--image"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><figure class=""><div class=""><img alt="Harvey Sbs" class="w-full" height="1080" src="https://images.openai.com/blob/188d9d1e-5695-463c-a987-07f6f04047a2/harvey-optimized.gif" width="1919" /></div><figcaption class="f-caption-1 relative mt-8"></figcaption></figure></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="what-s-next-for-model-customization"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">What’s next for model customization</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We believe that in the future, the vast majority of organizations will develop customized models that are personalized to their industry, business, or use case. With a variety of techniques available to build a custom model, organizations of all sizes can develop personalized models to realize more meaningful, specific impact from their AI implementations. The key is to clearly scope the use case, design and implement evaluation systems, choose the right techniques, and be prepared to iterate over time for the model to reach optimal performance.&nbsp;</p><p>With OpenAI, most organizations can see meaningful results quickly with the self-serve fine-tuning API. For any organizations that need to more deeply fine-tune their models or imbue new, domain-specific knowledge into the model, our Custom Model programs can help.&nbsp;</p><p>Visit our <a href="https://platform.openai.com/docs/guides/fine-tuning" rel="noopener noreferrer" target="_blank">fine-tuning API</a> docs to start fine-tuning our models. For more information on how we can help customize models for your use case, <a href="https://openai.com/form/custom-models" rel="noopener noreferrer" target="_blank">reach out to us</a>.&nbsp;<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Wed, 03 Apr 2024 17:44:32 GMT</pubDate>
<pubDate>Wed, 03 Apr 2024 17:44:32 GMT</pubDate>
</item>
<item>
<title>Start using ChatGPT instantly</title>
<link>https://openai.com/blog/start-using-chatgpt-instantly</link>
<guid>https://openai.com/blog/start-using-chatgpt-instantly</guid>
<content:encoded><![CDATA[
<div> 核心任务是使像ChatGPT这样的工具广泛可用，以便人们可以体验到人工智能的好处。每周有超过1亿人来自185个国家使用ChatGPT，通过它们来学习新知识、获得创意灵感和回答问题。从今天开始，您可以立即使用ChatGPT，无需注册。我们将逐步推出这一功能，旨在让任何对人工智能能力感兴趣的人都能够获取。我们可能会利用您在ChatGPT中提供的内容来改进我们的模型，使其惠及所有用户。如果您愿意，可以通过设置关闭此功能，无论您是否创建帐户。了解更多关于我们如何使用内容来训练我们的模型以及您的选择的信息。我们还为这一使用体验引入了额外的内容安全措施，例如在更广泛的类别中屏蔽提示和生成的内容。创建帐户有许多好处，包括保存和查看聊天记录、分享聊天内容以及解锁其他功能，如语音对话和自定义指令。如果您曾对人工智能的潜力感到好奇，但又不想经历设置帐户的步骤，请从今天开始使用ChatGPT。

关键词：ChatGPT，AI，广泛可用，用户，不需要注册

总结：<br><br>
1. ChatGPT是一个广泛可用的人工智能工具，让用户能够体验到人工智能的好处。
2. 每周有超过1亿人来自185个国家使用ChatGPT，学习新知识、创意灵感和回答问题。
3. 从今天开始，用户可以立即使用ChatGPT，无需注册。
4. 用户可以选择关闭将其提供给ChatGPT的内容用于改进模型的功能。
5. 创建帐户可以享有许多额外的功能和好处，如保存聊天记录、分享聊天内容以及解锁其他功能。

总结：ChatGPT是一个广泛可用的人工智能工具，每周有超过1亿用户来自185个国家使用它。现在用户可以无需注册即可立即使用。用户可以选择关闭将内容提供给ChatGPT用于模型改进。创建帐户还可以享有其他功能和好处。 <div>
<img alt="Noauth Cover" src="https://images.openai.com/blob/891a2b86-dd64-42ea-b642-b867fb3b4c27/NoAuth_Cover.png?trim=371%2C0%2C101%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>It's core to our mission to make tools like ChatGPT broadly available so that people can experience the benefits of AI. More than 100 million people across 185 countries use ChatGPT weekly to learn something new, find creative inspiration, and get answers to their questions. Starting today, you can use ChatGPT instantly, without needing to sign-up. We're rolling this out gradually, with the aim to make AI accessible to anyone curious about its capabilities.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--image"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><figure class=""><div class=""><img alt="Comp Noauth 328" class="w-full" height="1080" src="https://images.openai.com/blob/ebff4f07-9c31-48cb-a0f4-6bc777ab41ca/Comp_NoAuth_328.gif" width="1919" /></div><figcaption class="f-caption-1 relative mt-8"></figcaption></figure></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We may use what you provide to ChatGPT to improve our models for everyone. If you’d like, you can turn this off through your Settings - whether you create an account or not. Learn more about how we use content to train our models and your choices in our <a href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance" rel="noopener noreferrer" target="_blank">Help Center</a>.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--image"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><figure class=""><div class=""><img alt="Comp Noauth Gif2" class="w-full" height="1080" src="https://images.openai.com/blob/ebff4f07-9c31-48cb-a0f4-6bc777ab41ca/Comp_NoAuth_Gif2.gif" width="1919" /></div><figcaption class="f-caption-1 relative mt-8"></figcaption></figure></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We’ve also introduced additional content safeguards for this experience, such as blocking prompts and generations in a wider range of categories.<br class="softbreak" /></p><p>There are many benefits to creating an account including the ability to save and review your chat history, share chats, and unlock additional features like voice conversations and custom instructions.<br class="softbreak" /></p><p>For anyone that has been curious about AI’s potential but didn’t want to go through the steps to set-up an account, start using ChatGPT today.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Wed, 27 Mar 2024 18:16:27 GMT</pubDate>
<pubDate>Wed, 27 Mar 2024 18:16:27 GMT</pubDate>
</item>
<item>
<title>Sora: first impressions</title>
<link>https://openai.com/blog/sora-first-impressions</link>
<guid>https://openai.com/blog/sora-first-impressions</guid>
<content:encoded><![CDATA[
<div> Sora, visual artists, creative process, surreal, filmmakers <br>
<br>
总结: Sora是一个能帮助视觉艺术家和创意人士实现他们的创作想法的工具。它的最大特点是能够生成超现实主义的作品，让艺术家能够创造出以前无法想象的作品。本文介绍了一些艺术家如何利用Sora在他们的工作流程和业务中应用的早期见解，以及他们对于Sora的期待和赞扬。通过Sora，艺术家们能够更自由地表达自己的创意，超越传统的限制，探索新的艺术领域。Sora的出现让艺术家们能够更快地将想法转化为现实，为他们的创造力提供更大的发挥空间。 <div>
<img alt="Sora" src="https://images.openai.com/blob/e2012037-b375-4b88-a256-30dd73166ae6/Sora.png?trim=32%2C0%2C416%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Since we <a href="https://openai.com/sora" rel="noopener noreferrer" target="_blank">introduced Sora</a> to the world last month, we’ve been working with visual artists, designers, creative directors and filmmakers to learn how Sora might aid in their creative process. <br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--quote"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="f-body-1 md:w-6-cols lg:ml-2-cols lg:w-10-cols"><figure><blockquote class="f-quote-1"><p class="before:content-['“'] after:content-['”'] relative before:absolute before:left-0 before:-translate-x-full"><span>Sora is at its most powerful when you’re not replicating the old but bringing to life new and impossible ideas we would have otherwise never had the opportunity to see.</span></p></blockquote><figcaption class="f-subhead-2 mt-spacing-4">Paul Trillo, Director</figcaption></figure></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>While we have many improvements to make to Sora, we're already getting a glimpse of how the model can help creatives bring ideas to reality.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--quote"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="f-body-1 md:w-6-cols lg:ml-2-cols lg:w-10-cols"><figure><blockquote class="f-quote-1"><p class="before:content-['“'] after:content-['”'] relative before:absolute before:left-0 before:-translate-x-full"><span>As great as Sora is at generating things that appear real - what excites us is its ability to make things that are totally surreal.</span></p></blockquote><figcaption class="f-subhead-2 mt-spacing-4">shy kids</figcaption></figure></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Below are a few examples of the artists’ work, with early thoughts from them on how they see Sora fitting into their workflows and businesses. <br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--video"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="ui-video overflow-hidden"><div class="group theme-dark-gray bg-transparent"><div class="left-0"><div style="padding: 56.25% 0 0 0;"></div></div><div class="absolute top-0 right-0 bottom-0 left-0 transition duration-500 group-hover:brightness-90 opacity-100"><div class="w-full h-full"><img alt="Poster Shy Kids" class="ratio-content h-full w-full object-cover" height="1440" src="https://images.openai.com/blob/d4c58911-9afe-44fe-ba16-a11b1b6bb8ad/poster-shy-kids.png?trim=0,0,0,0&amp;width=10&amp;height=10&amp;quality=50" width="2560" /></div></div><div class="absolute top-0 right-0 bottom-0 left-0 flex h-full w-full cursor-pointer items-end py-16 px-16 transition-opacity duration-300 after:absolute after:top-0 after:right-0 after:bottom-0 after:left-0 after:bg-gradient-to-t after:from-[rgba(0,0,0,0.56)] after:content-[''] md:top-auto md:after:top-auto md:after:h-[364px] visible opacity-100"><button class="ui-link group f-ui-1 inline-block relative text-[currentColor] relative"><span class="flex items-center"><span class="ui-button relative inline-block border border-inverse bg-primary px-16 pt-7 pb-9 text-primary hover:border-primary hover:bg-inverse hover:text-inverse active:border-primary active:bg-inverse active:text-inverse"><svg class="a-icon--play400 a-icon--text mr-12 inline-block" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="2 2 14 8 2 14 2 2"></polygon></svg><span class="f-ui-1">Play video</span></span></span></button></div></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="shy-kids-air-head"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">shy kids – “Air Head”</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Based in Toronto, <a href="https://www.instagram.com/shykids_/" rel="noopener noreferrer" target="_blank">shy kids</a> are a multimedia production company who utilized Sora for their short film about a balloon man. “We now have the ability to expand on stories we once thought impossible,” shares the trio made up of Walter Woodman, Sidney Leeder and Patrick Cederberg.&nbsp; Walter, who directed <em>Air Head</em>, remarks that “as great as Sora is at generating things that appear real, what excites us is its ability to make things that are totally surreal. A new era of abstract expressionism.” Speaking to the wider industry, “people from all over the world with stories ready to burst out of their chests finally have the opportunity to show the world what’s inside.”<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--video"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="ui-video overflow-hidden"><div class="group theme-dark-gray bg-transparent"><div class="left-0"><div style="padding: 56.25% 0 0 0;"></div></div><div class="absolute top-0 right-0 bottom-0 left-0 transition duration-500 group-hover:brightness-90 opacity-100"><div class="w-full h-full"><img alt="Poster Paul Trillo" class="ratio-content h-full w-full object-cover" height="721" src="https://images.openai.com/blob/3ba7a271-1c68-46ce-b64d-1fba551919ce/poster-paul-trillo.png?trim=0,0,3,0&amp;width=10&amp;height=10&amp;quality=50" width="1282" /></div></div><div class="absolute top-0 right-0 bottom-0 left-0 flex h-full w-full cursor-pointer items-end py-16 px-16 transition-opacity duration-300 after:absolute after:top-0 after:right-0 after:bottom-0 after:left-0 after:bg-gradient-to-t after:from-[rgba(0,0,0,0.56)] after:content-[''] md:top-auto md:after:top-auto md:after:h-[364px] visible opacity-100"><button class="ui-link group f-ui-1 inline-block relative text-[currentColor] relative"><span class="flex items-center"><span class="ui-button relative inline-block border border-inverse bg-primary px-16 pt-7 pb-9 text-primary hover:border-primary hover:bg-inverse hover:text-inverse active:border-primary active:bg-inverse active:text-inverse"><svg class="a-icon--play400 a-icon--text mr-12 inline-block" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="2 2 14 8 2 14 2 2"></polygon></svg><span class="f-ui-1">Play video</span></span></span></button></div></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="paul-trillo-director"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Paul Trillo, Director</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><a href="https://www.instagram.com/paultrillo?utm_source=ig_web_button_share_sheet&amp;igsh=ZDNlZDc0MzIxNw==" rel="noopener noreferrer" target="_blank">Paul Trillo</a> is a multi-disciplinary artist, writer, and director whose work has earned accolades from outlets like Rolling Stone and The New Yorker. Paul has garnered 19 Vimeo Staff Picks, an honor given to the best short films hosted on Vimeo. “Working with Sora is the first time I’ve felt unchained as a filmmaker,” he states. “Not restricted by time, money, other people’s permission, I can ideate and experiment in bold and exciting ways.” His experimental videos reflect this approach. “Sora is at its most powerful when you’re not replicating the old but bringing to life new and impossible ideas we would have otherwise never had the opportunity to see.”<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--video"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="ui-video overflow-hidden"><div class="group theme-dark-gray bg-transparent"><div class="left-0"><div style="padding: 56.25% 0 0 0;"></div></div><div class="absolute top-0 right-0 bottom-0 left-0 transition duration-500 group-hover:brightness-90 opacity-100"><div class="w-full h-full"><img alt="Poster Nik Kleverov" class="ratio-content h-full w-full object-cover" height="720" src="https://images.openai.com/blob/bbf0e316-2011-4fc8-84d2-0f3319eec4b9/poster-nik-kleverov.png?trim=0,0,0,0&amp;width=10&amp;height=10&amp;quality=50" width="1280" /></div></div><div class="absolute top-0 right-0 bottom-0 left-0 flex h-full w-full cursor-pointer items-end py-16 px-16 transition-opacity duration-300 after:absolute after:top-0 after:right-0 after:bottom-0 after:left-0 after:bg-gradient-to-t after:from-[rgba(0,0,0,0.56)] after:content-[''] md:top-auto md:after:top-auto md:after:h-[364px] visible opacity-100"><button class="ui-link group f-ui-1 inline-block relative text-[currentColor] relative"><span class="flex items-center"><span class="ui-button relative inline-block border border-inverse bg-primary px-16 pt-7 pb-9 text-primary hover:border-primary hover:bg-inverse hover:text-inverse active:border-primary active:bg-inverse active:text-inverse"><svg class="a-icon--play400 a-icon--text mr-12 inline-block" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="2 2 14 8 2 14 2 2"></polygon></svg><span class="f-ui-1">Play video</span></span></span></button></div></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="nik-kleverov-creative-director-native-foreign"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Nik Kleverov, Creative Director / Native Foreign</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><a href="http://nativeforeign.tv/" rel="noopener noreferrer" target="_blank">Native Foreign</a> is an Emmy-nominated creative agency from Los Angeles, California specializing in brand storytelling, motion and title design, and generative AI workflows. Co-Founder <a href="https://linktr.ee/kleverov" rel="noopener noreferrer" target="_blank">Nik Kleverov</a>, who is using Sora “to visualize concepts and rapidly iterate on creative for brand partners,” suggests that budgetary restraints no longer have to entirely shape the narrative of creativity. “I’m one of those creatives that thinks in motion, so when I’m in Sora it really feels like I can bring any idea to life.”<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--video"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="ui-video overflow-hidden"><div class="group theme-dark-gray bg-transparent"><div class="left-0"><div style="padding: 56.25% 0 0 0;"></div></div><div class="absolute top-0 right-0 bottom-0 left-0 transition duration-500 group-hover:brightness-90 opacity-100"><div class="w-full h-full"><img alt="Poster August Kamp" class="ratio-content h-full w-full object-cover" height="726" src="https://images.openai.com/blob/6af4d0d4-9902-4138-858e-07297b61a64b/poster-august-kamp.jpg?trim=0,0,86,0&amp;width=10&amp;height=10&amp;quality=50" width="1290" /></div></div><div class="absolute top-0 right-0 bottom-0 left-0 flex h-full w-full cursor-pointer items-end py-16 px-16 transition-opacity duration-300 after:absolute after:top-0 after:right-0 after:bottom-0 after:left-0 after:bg-gradient-to-t after:from-[rgba(0,0,0,0.56)] after:content-[''] md:top-auto md:after:top-auto md:after:h-[364px] visible opacity-100"><button class="ui-link group f-ui-1 inline-block relative text-[currentColor] relative"><span class="flex items-center"><span class="ui-button relative inline-block border border-inverse bg-primary px-16 pt-7 pb-9 text-primary hover:border-primary hover:bg-inverse hover:text-inverse active:border-primary active:bg-inverse active:text-inverse"><svg class="a-icon--play400 a-icon--text mr-12 inline-block" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="2 2 14 8 2 14 2 2"></polygon></svg><span class="f-ui-1">Play video</span></span></span></button></div></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="august-kamp-artist-musician"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">August Kamp, Artist/Musician</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><a href="https://linktr.ee/augustkamp" rel="noopener noreferrer" target="_blank">August Kamp</a> is a musician, researcher, creative activist and multidisciplinary artist. “Sora represents a real turning point for me as an artist whose scope has always been limited by imagination being at odds with means,” she explains. “Being able to build and iterate on cinematic visuals this intuitively has opened up categorically new lanes of artistry to me...I truly cannot wait to see what other forms of storytelling will come into reach with the future of these tools."<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--video"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="ui-video overflow-hidden"><div class="group theme-dark-gray bg-transparent"><div class="left-0"><div style="padding: 56.25% 0 0 0;"></div></div><div class="absolute top-0 right-0 bottom-0 left-0 transition duration-500 group-hover:brightness-90 opacity-100"><div class="w-full h-full"><img alt="Poster Josephine Miller" class="ratio-content h-full w-full object-cover" height="725" src="https://images.openai.com/blob/51945c52-e591-4359-879e-94c004e23b55/poster-josephine-miller.jpg?trim=0,3,6,0&amp;width=10&amp;height=10&amp;quality=50" width="1287" /></div></div><div class="absolute top-0 right-0 bottom-0 left-0 flex h-full w-full cursor-pointer items-end py-16 px-16 transition-opacity duration-300 after:absolute after:top-0 after:right-0 after:bottom-0 after:left-0 after:bg-gradient-to-t after:from-[rgba(0,0,0,0.56)] after:content-[''] md:top-auto md:after:top-auto md:after:h-[364px] visible opacity-100"><button class="ui-link group f-ui-1 inline-block relative text-[currentColor] relative"><span class="flex items-center"><span class="ui-button relative inline-block border border-inverse bg-primary px-16 pt-7 pb-9 text-primary hover:border-primary hover:bg-inverse hover:text-inverse active:border-primary active:bg-inverse active:text-inverse"><svg class="a-icon--play400 a-icon--text mr-12 inline-block" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="2 2 14 8 2 14 2 2"></polygon></svg><span class="f-ui-1">Play video</span></span></span></button></div></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="josephine-miller-creative-director"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Josephine Miller, Creative Director</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><a href="https://www.instagram.com/josephinemiller?igsh=ZWdodzdwZW5rZmVx&amp;utm_source=qr" rel="noopener noreferrer" target="_blank">Josephine Miller</a> is the Co-Founder and Creative Director of London based Oraar Studio, specializing in the design of 3D visuals, augmented reality and digital fashion. "Sora has opened up the potential to bring to life ideas I've had for years, ideas that were previously technically impossible,” she states. “The ability to rapidly conceptualize at such a high level of quality is not only challenging my creative process but also helping me evolve in storytelling. It's enabling me to translate my imagination with fewer technical constraints."<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--video"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="ui-video overflow-hidden"><div class="group theme-dark-gray bg-transparent"><div class="left-0"><div style="padding: 56.25% 0 0 0;"></div></div><div class="absolute top-0 right-0 bottom-0 left-0 transition duration-500 group-hover:brightness-90 opacity-100"><div class="w-full h-full"><img alt="Poster Don Allen" class="ratio-content h-full w-full object-cover" height="663" src="https://images.openai.com/blob/c7bec38b-e51c-4175-bae9-f5faca055835/poster-don-allen.jpg?trim=104,0,113,0&amp;width=10&amp;height=10&amp;quality=50" width="1179" /></div></div><div class="absolute top-0 right-0 bottom-0 left-0 flex h-full w-full cursor-pointer items-end py-16 px-16 transition-opacity duration-300 after:absolute after:top-0 after:right-0 after:bottom-0 after:left-0 after:bg-gradient-to-t after:from-[rgba(0,0,0,0.56)] after:content-[''] md:top-auto md:after:top-auto md:after:h-[364px] visible opacity-100"><button class="ui-link group f-ui-1 inline-block relative text-[currentColor] relative"><span class="flex items-center"><span class="ui-button relative inline-block border border-inverse bg-primary px-16 pt-7 pb-9 text-primary hover:border-primary hover:bg-inverse hover:text-inverse active:border-primary active:bg-inverse active:text-inverse"><svg class="a-icon--play400 a-icon--text mr-12 inline-block" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="2 2 14 8 2 14 2 2"></polygon></svg><span class="f-ui-1">Play video</span></span></span></button></div></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="don-allen-stevenson-iii-digital-ar-xr-artist"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Don Allen Stevenson III, Digital AR/XR Artist</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Starting his career at DreamWorks Animation, <a href="http://www.instagram.com/donalleniii" rel="noopener noreferrer" target="_blank">Don Allen III</a> is a multidisciplinary creator, speaker and consultant who collaborates with major tech and entertainment companies on mixed reality, virtual reality and AI applications. “For a long time I've been making augmented reality hybrid creatures that I think would be fun combinations in my head. Now I have a much easier way of prototyping the ideas before I fully build out the 3-D characters to place in spatial computers.” Don cites Sora’s “weirdness” as its greatest strength: “It’s not bound by traditional laws of physics or conventions of thought.” He says that working with Sora shifted his focus from “technical hurdles to pure creativity…unlocking a world of instant visualization and rapid prototyping.” At the same time, Don says “I feel like this allows me to focus more of my time and energy in the right places… and the emotional impact that I would like my characters to have.”<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--video"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="ui-video overflow-hidden"><div class="group theme-dark-gray bg-transparent"><div class="left-0"><div style="padding: 56.25% 0 0 0;"></div></div><div class="absolute top-0 right-0 bottom-0 left-0 transition duration-500 group-hover:brightness-90 opacity-100"><div class="w-full h-full"><img alt="Poster Alex Reben" class="ratio-content h-full w-full object-cover" height="1080" src="https://images.openai.com/blob/57ee0ed8-2db4-46db-9b42-ee94c99c940a/poster-alex-reben.jpg?trim=0,0,0,0&amp;width=10&amp;height=10&amp;quality=50" width="1920" /></div></div><div class="absolute top-0 right-0 bottom-0 left-0 flex h-full w-full cursor-pointer items-end py-16 px-16 transition-opacity duration-300 after:absolute after:top-0 after:right-0 after:bottom-0 after:left-0 after:bg-gradient-to-t after:from-[rgba(0,0,0,0.56)] after:content-[''] md:top-auto md:after:top-auto md:after:h-[364px] visible opacity-100"><button class="ui-link group f-ui-1 inline-block relative text-[currentColor] relative"><span class="flex items-center"><span class="ui-button relative inline-block border border-inverse bg-primary px-16 pt-7 pb-9 text-primary hover:border-primary hover:bg-inverse hover:text-inverse active:border-primary active:bg-inverse active:text-inverse"><svg class="a-icon--play400 a-icon--text mr-12 inline-block" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="2 2 14 8 2 14 2 2"></polygon></svg><span class="f-ui-1">Play video</span></span></span></button></div></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="alex-reben-sculptor-artist-and-openai-s-artist-in-residence"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Alex Reben, Sculptor/Artist and OpenAI’s Artist In Residence</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><a href="https://linktr.ee/artboffin" rel="noopener noreferrer" target="_blank">Alexander Reben</a> is an artist who has spent the last decade creating work that explores the humor and absurdity of human nature in artificial intelligence. Alex has been <a href="https://www.instagram.com/p/C4Q8J-9vSjM/" rel="noopener noreferrer" target="_blank">creating sculptures</a> that originate from AI-generated imagery, manually transforming those AI creations into 3D models materialized in the physical world. “My experience of using Sora was as a starting point to develop 3D sculpture. My thoughts drifted towards exploring the realm of photogrammetry and its potential applications to sculpture. The prospect of transforming video into 3D models intrigued me, as it hinted at propelling the AI system beyond its initial scope.”&nbsp; <br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Fri, 22 Mar 2024 16:48:46 GMT</pubDate>
<pubDate>Fri, 22 Mar 2024 16:48:46 GMT</pubDate>
</item>
<item>
<title>Global news partnerships: Le Monde and Prisa Media</title>
<link>https://openai.com/blog/global-news-partnerships-le-monde-and-prisa-media</link>
<guid>https://openai.com/blog/global-news-partnerships-le-monde-and-prisa-media</guid>
<content:encoded><![CDATA[
<div> ChatGPT, Le Monde, Prisa Media, partnerships, news industry, interactive news content, AI technologies, journalism support, authoritative information.
<br><br>
总结: OpenAI宣布与法国报纸Le Monde和Prisa Media达成合作伙伴关系，在ChatGPT中提供高质量的互动新闻内容，同时也可用于模型的训练。这一合作旨在改进新闻行业的报道方式，让ChatGPT用户通过与Le Monde和Prisa Media合作方的内容互动和交流，并且取得对应的原始文章链接。这些合作是OpenAI加强新闻业支持的一部分，打造更先进的AI工具，解决现有难题。 <div>
<img alt="Newspartnership Cover" src="https://images.openai.com/blob/ffffbd46-a171-41c5-bba2-76e490e7b940/NewsPartnership_Cover.png?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We are continually making improvements to ChatGPT and are supporting the essential role of the news industry in delivering real-time, authoritative information to users. We’re excited to announce partnerships with <strong>Le Monde </strong>and <strong>Prisa Media</strong> along with its publications like El País, Cinco Días, As, and El Huffpost. Our partnerships will enable ChatGPT users to engage with Le Monde and Prisa Media’s high-quality content on recent events in ChatGPT, and their content will also contribute to the training of our models.&nbsp;</p><p><strong>Brad Lightcap, COO of OpenAI,</strong> said, "We're dedicated to supporting journalism by applying new AI technologies and enhancing opportunities for content creators. In partnership with Le Monde and Prisa Media, our goal is to enable ChatGPT users around the world to connect with the news in new ways that are interactive and insightful.”&nbsp;&nbsp;</p></div></div></div></div></div></div></div><div class="ui-block ui-block--quote"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="f-body-1 md:w-6-cols lg:ml-2-cols lg:w-10-cols"><figure><blockquote class="f-quote-1"><p class="before:content-['“'] after:content-['”'] relative before:absolute before:left-0 before:-translate-x-full"><span>In partnership with Le Monde and Prisa Media, our goal is to enable ChatGPT users around the world to connect with the news in new ways that are interactive and insightful.</span></p></blockquote><figcaption class="f-subhead-2 mt-spacing-4">Brad Lightcap, COO of OpenAI</figcaption></figure></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Over the coming months, ChatGPT users will be able to interact with relevant news content from these publishers through select summaries with attribution and enhanced links to the original articles, giving users the ability to access additional information or related articles from their news sites.</p><p>Echoing this sentiment, <strong>Louis Dreyfus, CEO of Le Monde</strong>, stated, "At the moment we are celebrating the 80th anniversary of Le Monde, this partnership with OpenAI allows us to expand our reach and uphold our commitment to providing accurate, verified, balanced news stories at scale. Collaborating with OpenAI ensures that our authoritative content can be accessed and appreciated by a broader, more diverse audience.&nbsp;&nbsp;</p><p>Every shift in the media landscape has presented Le Monde with new opportunities. From the transition to digital platforms to embracing the era of free media, Le Monde has consistently seized these moments to underscore its commitment to independence, expertise, and journalistic integrity.&nbsp;</p><p>Since 2010, Le Monde has emerged as a digital media trailblazer, adapting its organizational structure and operational methods while steadfastly adhering to its core principles. By 2024, Le Monde has established itself as France's leading news outlet, boasting more than 600,000 subscribers, 2.2M unique users a day and generating over 632 million page views per month.&nbsp;</p><p>Our partnership with OpenAI is a strategic move to ensure the dissemination of reliable information to AI users, safeguarding our journalistic integrity and revenue streams in the process.”</p><p><strong>Carlos Nuñez, Chairman and CEO of Prisa Media</strong> added, “Joining forces with OpenAI opens new avenues for us to engage with our audience. Leveraging ChatGPT's capabilities allows us to present our in-depth, quality journalism in novel ways, reaching individuals who seek credible and independent content. This is a definite step towards the future of news, where technology and human expertise merge to enrich the reader's experience.</p><p>This is a new chapter in Prisa Media’s digital journey, where we are continuously improving our position as the largest Hispanic mediahouse, operating the leading media brands in our core markets: Spain, Latam and USA. We have developed a reach of more than 7 million daily unique users with over 1,650 million page views per month and a clear focus on developing content in digital formats beyond text, both in audio, where we provide 90 million total listening hours and 51 million audio downloads per month, and in video, with more than 141 million monthly video views.”</p><p>Our partnerships with Le Monde and Prisa Media, as well as Axel Springer, help empower news organizations to reach audiences in new ways. They build on our collaborations with American Journalism Project to support innovative local news initiatives, and The Associated Press, which contributes to the training of our models. Our partnerships underscore our vision to develop advanced AI tools that empower industries, such as journalism, and solve problems that are otherwise out of reach.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Wed, 13 Mar 2024 00:21:13 GMT</pubDate>
<pubDate>Wed, 13 Mar 2024 00:21:13 GMT</pubDate>
</item>
<item>
<title>OpenAI announces new members to board of directors</title>
<link>https://openai.com/blog/openai-announces-new-members-to-board-of-directors</link>
<guid>https://openai.com/blog/openai-announces-new-members-to-board-of-directors</guid>
<content:encoded><![CDATA[
<div> 关键词：OpenAI、扩展、董事会成员、经验、人物介绍

总结:
OpenAI宣布了三名新的董事会成员：Bill和Melinda Gates基金会的前CEO Sue Desmond-Hellmann博士、索尼公司的前执行副总裁兼总法律顾问Nicole Seligman、Instacart的首席执行官兼董事长Fidji Simo。Sam Altman将重新加入OpenAI董事会。这三位新成员在领导全球组织和应对复杂监管环境方面具有丰富的经验，涵盖了技术、非营利组织和董事会治理等领域。他们将与现有董事会成员和OpenAI的高级管理团队密切合作。总结: OpenAI宣布了三名新的董事会成员，他们在领导全球组织和应对复杂监管环境方面具有丰富的经验。他们的加入将有助于OpenAI继续实现其确保人工智能对全人类有益的使命。 <div>
<img alt="Facebook" src="https://images.openai.com/blob/d1a9e364-550d-42d7-b228-28e4612e20e9/facebook.png?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We’re announcing three new members to our Board of Directors as a first step towards our commitment to expansion: <strong>Dr.</strong> <strong>Sue Desmond-Hellmann</strong>, <em>former CEO of the Bill and Melinda Gates Foundation</em>, <strong>Nicole Seligman</strong>, <em>former EVP and General Counsel at Sony Corporation</em> and <strong>Fidji Simo</strong>, <em>CEO and Chair of Instacart. </em>Additionally, <strong>Sam Altman</strong>, CEO, will rejoin the OpenAI Board of Directors.&nbsp;</p><p>Sue, Nicole and Fidji have experience in leading global organizations and navigating complex regulatory environments, including backgrounds in technology, nonprofit and board governance. They will work closely with current board members Adam D’Angelo, Larry Summers and Bret Taylor as well as Sam and OpenAI’s senior management.&nbsp;</p><p>Bret Taylor, Chair of the OpenAI board, stated, “I am excited to welcome Sue, Nicole, and Fidji to the OpenAI Board of Directors. Their experience and leadership will enable the Board to oversee OpenAI’s growth, and to ensure that we pursue OpenAI’s mission of ensuring artificial general intelligence benefits all of humanity.”</p><p><strong>Dr. Sue Desmond-Hellmann</strong>&nbsp;is a non-profit leader and physician. Dr. Desmond-Hellmann currently serves on the Boards of Pfizer and the President’s Council of Advisors on Science and Technology. She previously was a Director at Proctor and Gamble, Meta (Facebook), and the Bill &amp; Melinda Gates Medical Research institute.&nbsp;She served as the Chief Executive Officer of the&nbsp;Bill &amp; Melinda Gates Foundation&nbsp;from 2014 to 2020. From 2009-2014 she was Professor and Chancellor of the&nbsp;University of California, San Francisco&nbsp;(UCSF), the first woman to hold the position. She also previously served as President of&nbsp;Product Development&nbsp;at&nbsp;Genentech, where she played a leadership role in the development of the first gene-targeted cancer drugs.&nbsp;</p><p><strong>Nicole Seligman</strong> is a globally recognized corporate and civic leader and lawyer. She currently serves on three public company corporate boards - Paramount Global, MeiraGTx Holdings PLC, and Intuitive Machines, Inc. Seligman held several senior leadership positions at Sony entities, including EVP and General Counsel at Sony Corporation, where she oversaw functions including global legal and compliance matters. She also served as President of Sony Entertainment, Inc., and simultaneously served as President of Sony Corporation of America. Seligman also currently holds nonprofit leadership roles at the Schwarzman Animal Medical Center and The Doe Fund in New York City. Previously, Seligman was a partner in the litigation practice at Williams &amp; Connolly LLP in Washington, D.C., working on complex civil and criminal matters and counseling a wide range of clients, including President William Jefferson Clinton and Hillary Clinton. She served as a law clerk to Justice Thurgood Marshall on the Supreme Court of the United States.</p><p><strong>Fidji Simo</strong> is a consumer technology industry veteran, having spent more than 15 years leading the operations, strategy and product development for some of the world’s leading businesses. She is the Chief Executive Officer and Chair of Instacart. She also serves as a member of the Board of Directors at Shopify. Prior to joining Instacart, Simo was Vice President and Head of the Facebook App. Over the last decade at Facebook, she oversaw the Facebook App, including News Feed, Stories, Groups, Video, Marketplace, Gaming, News, Dating, Ads and more. Simo founded the&nbsp;<a href="https://www.metrodora.co/" rel="noopener noreferrer" target="_blank">Metrodora Institute</a>, a multidisciplinary medical clinic and research foundation dedicated to the care and cure of neuroimmune axis disorders and serves as President of the Metrodora Foundation.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Fri, 08 Mar 2024 19:46:26 GMT</pubDate>
<pubDate>Fri, 08 Mar 2024 19:46:26 GMT</pubDate>
</item>
<item>
<title>Review completed &amp; Altman, Brockman to continue to lead OpenAI</title>
<link>https://openai.com/blog/review-completed-altman-brockman-to-continue-to-lead-openai</link>
<guid>https://openai.com/blog/review-completed-altman-brockman-to-continue-to-lead-openai</guid>
<content:encoded><![CDATA[
<div> OpenAI, review, leadership, Board of Directors, governance structure<br><br>总结: 在经历了一次全面审查后，OpenAI的董事会对Sam Altman和Greg Brockman的领导能力表示充分信任，并决定让Sam Altman重新加入OpenAI的董事会。此外，OpenAI的董事会还选举了三名新成员，他们将与现有的董事们一起合作，共同推动OpenAI的发展。董事会还宣布了一系列重要的改进措施，包括制定新的公司治理指南、加强利益冲突政策、设立举报热线和创建额外的董事会委员会等。这些改进旨在更好地实现OpenAI的使命。总体而言，OpenAI的董事会对公司的前景充满信心，并致力于推动公司的可持续发展。 <div>
<img alt="Facebook" src="https://images.openai.com/blob/d1a9e364-550d-42d7-b228-28e4612e20e9/facebook.png?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>The Special Committee of the OpenAI Board today announced the completion of the review by WilmerHale. The firm conducted dozens of interviews with members of OpenAI’s prior Board, OpenAI executives, advisors to the prior Board, and other pertinent witnesses; reviewed more than 30,000 documents; and evaluated various corporate actions. Based on the record developed by WilmerHale and following the recommendation of the Special Committee, the Board expressed its full confidence in Mr. Sam Altman and Mr. Greg Brockman’s ongoing leadership of OpenAI.</p><p>“We have unanimously concluded that Sam and Greg are the right leaders for OpenAI,” stated Bret Taylor, Chair of the OpenAI Board.</p><p>Sam Altman, as CEO, will rejoin the OpenAI Board of Directors.&nbsp;</p><p>The OpenAI Board also announced today the election of three new Board members as one part of its commitment to expansion, including:&nbsp;</p><ul><li><strong>Dr. Sue Desmond-Hellmann</strong>, <em>former CEO of the Bill and Melinda Gates Foundation and on the Board of Directors at Pfizer and on the President’s Council of Advisors on Science and Technology.</em></li><li><strong>Nicole Seligman</strong>, <em>former EVP and Global General Counsel of Sony and President of Sony Entertainment and on the Board of Directors at Paramount Global, Meira GTx, and Intuitive Machines, Inc.</em></li><li><strong>Fidji Simo</strong>, <em>CEO and Chair of Instacart and on the Board of Directors at Shopify</em></li></ul><p><span class="ql-anchor" id="docs-internal-guid-aa92a213-7fff-6106-8def-d9716091a4ac"><br class="softbreak" /></span>The new members have experience in leading global organizations and navigating complex regulatory environments, including backgrounds in technology, nonprofit and board governance. They will work closely with current board members Adam D’Angelo, Larry Summers and Bret Taylor as well as Greg, Sam, and OpenAI’s senior management.&nbsp;</p><p>Taylor further stated, “As Chair of the Board, I am excited to welcome Sue, Nicole, and Fidji to the OpenAI Board of Directors. Their experience and leadership will enable the Board to oversee OpenAI’s growth and to ensure that we pursue OpenAI’s mission of ensuring artificial general intelligence benefits all of humanity.”</p><p>The Board also announced the adoption of important improvements to OpenAI’s governance structure. Key enhancements include:</p><ul><li>Adopting a new set of corporate governance guidelines;</li><li>Strengthening OpenAI’s Conflict of Interest Policy;</li><li>Creating a whistleblower hotline to serve as an anonymous reporting resource for all OpenAI employees and contractors; and</li><li>Creating additional Board committees, including a Mission &amp; Strategy committee focused on implementation and advancement of the core mission of OpenAI.&nbsp;</li></ul><p>The expanded board will prioritize its crucial work to enhance the governance procedures to best achieve OpenAI’s mission. “We recognize the magnitude of our role in stewarding transformative technologies for the global good,” added Taylor.</p><p>The Special Committee acknowledged the important work done by WilmerHale in conducting this extensive review and thanked OpenAI current and former Board members, advisors and employees for their cooperation. The Special Committee of OpenAI’s Board of Directors released a summary of findings.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="summary-of-wilmerhale-review-and-findings"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Summary of WilmerHale review &amp; findings</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>On December 8, 2023, the Special Committee retained WilmerHale to conduct a review of the events concerning the November 17, 2023 removal of Sam Altman and Greg Brockman from the OpenAI Board of Directors and Mr. Altman’s termination as CEO. WilmerHale reviewed more than 30,000 documents; conducted dozens of interviews, including of members of OpenAI’s prior Board, OpenAI executives, advisors to the prior Board, and other pertinent witnesses; and evaluated various corporate actions.&nbsp;&nbsp;</p><p>The Special Committee provided WilmerHale with the resources and authority necessary to conduct a comprehensive review. Many OpenAI employees, as well as current and former Board members, cooperated with the review process. WilmerHale briefed the Special Committee several times on the progress and conclusions of the review.&nbsp;&nbsp;</p><p>WilmerHale evaluated management and governance issues that had been brought to the prior Board’s attention, as well as additional issues that WilmerHale identified in the course of its review. WilmerHale found there was a breakdown in trust between the prior Board and Mr. Altman that precipitated the events of November 17.&nbsp;&nbsp;</p><p>WilmerHale reviewed the public post issued by the prior Board on November 17 and concluded that the statement accurately recounted the prior Board’s decision and rationales. WilmerHale found that the prior Board believed at the time that its actions would mitigate internal management challenges and did not anticipate that its actions would destabilize the Company. WilmerHale also found that the prior Board’s decision did not arise out of concerns regarding product safety or security, the pace of development, OpenAI’s finances, or its statements to investors, customers, or business partners. Instead, it was a consequence of a breakdown in the relationship and loss of trust between the prior Board and Mr. Altman. WilmerHale found the prior Board implemented its decision on an abridged timeframe, without advance notice to key stakeholders, and without a full inquiry or an opportunity for Mr. Altman to address the prior Board’s concerns. WilmerHale found that the prior Board acted within its broad discretion to terminate Mr. Altman, but also found that his conduct did not mandate removal.&nbsp;&nbsp;</p><p>After reviewing the WilmerHale findings, the Special Committee recommended to the full Board that it endorse the November 21 decision to rehire Mr. Altman and Mr. Brockman. With knowledge of the review’s findings, the Special Committee expressed its full confidence in Mr. Altman and Mr. Brockman’s ongoing leadership of OpenAI.&nbsp;&nbsp;&nbsp;</p><p>The Special Committee is pleased to conclude this review and looks forward to continuing with the important work of OpenAI.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Fri, 08 Mar 2024 19:46:19 GMT</pubDate>
<pubDate>Fri, 08 Mar 2024 19:46:19 GMT</pubDate>
</item>
<item>
<title>Navigating the Challenges and Opportunities of Synthetic Voices</title>
<link>https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices</link>
<guid>https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices</guid>
<content:encoded><![CDATA[
<div> OpenAI, Voice Engine, synthetic voices, small-scale preview, preliminary insights.
Using text input and a 15-second audio sample, OpenAI's Voice Engine generates natural-sounding speech that resembles the original speaker. The technology has potential applications such as providing reading assistance, translating content, reaching global communities, supporting non-verbal individuals, and helping patients recover their voice. OpenAI is committed to the responsible deployment of synthetic voices and is engaging with various partners to address potential risks. They have implemented safeguards and safety measures and are exploring voice authentication experiences. OpenAI encourages steps like phasing out voice-based authentication, protecting individuals' voices in AI, educating the public about AI capabilities and limitations, and developing techniques to track the origin of audiovisual content. OpenAI aims to engage in discussions about the challenges and opportunities of synthetic voices with policymakers, researchers, developers, and creatives.

总结：<br><br>OpenAI推出了名为Voice Engine的模型，在一个小规模预览中展示了初步的洞察和结果。Voice Engine可以通过文本输入和15秒的音频样本生成与原始说话者极为相似的自然音质语音。该技术可应用于多个领域，包括提供阅读帮助、翻译内容、全球社区服务、支持非语言人士以及帮助患有言语障碍的患者恢复声音。OpenAI致力于负责任地推广合成语音，并在与合作伙伴展开合作以应对潜在风险。他们已经采取了多项安全措施，同时也在探索语音验证技术。OpenAI呼吁逐步淘汰基于语音的身份验证，保护个人声音在人工智能中的使用，向公众普及人工智能的能力与限制，并加快开发追踪音视频内容源头的技术。OpenAI希望能与决策者、研究人员、开发者和创意人士就合成语音的挑战和机遇展开讨论。 <div>
<img alt="Tts Custom Voice Cover" src="https://images.openai.com/blob/1be0c17b-707d-4a3c-9812-1a90ab423830/tts-custom-voice-cover.png?trim=96%2C0%2C352%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>OpenAI is committed to <a href="https://openai.com/charter" rel="noopener noreferrer" target="_blank">developing safe and broadly beneficial AI</a>. Today we are sharing preliminary insights and results from a small-scale preview of a model called Voice Engine, which uses text input and a single 15-second audio sample to generate natural-sounding speech that closely resembles the original speaker. It is notable that a small model with a single 15-second sample can create emotive and realistic voices.</p><p>We first developed Voice Engine in late 2022, and have used it to power the preset voices available in the <a href="https://platform.openai.com/docs/guides/text-to-speech" rel="noopener noreferrer" target="_blank">text-to-speech API</a> as well as <a href="https://openai.com/blog/chatgpt-can-now-see-hear-and-speak" rel="noopener noreferrer" target="_blank">ChatGPT Voice and Read Aloud</a>.&nbsp;At the same time, we are taking a cautious and informed approach to a broader release due to the potential for synthetic voice misuse. We hope to start a dialogue on the responsible deployment of synthetic voices, and how society can adapt to these new capabilities. Based on these conversations and the results of these small scale tests, we will make a more informed decision about whether and how to deploy this technology at scale.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="early-applications-of-voice-engine"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Early applications of Voice Engine</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>To better understand the potential uses of this technology, late last year we started privately testing it with a small group of trusted partners. We've been impressed by the applications this group has developed. These small scale deployments are helping to inform our approach, safeguards, and thinking about how Voice Engine could be used for good across various industries. A few early examples include:</p><ul><li><strong>Providing reading assistance</strong> to non-readers and children through natural-sounding, emotive voices representing a wider range of speakers than what's possible with preset voices. <a href="https://www.ageoflearning.com/" rel="noopener noreferrer" target="_blank">Age of Learning</a>, an education technology company dedicated to the academic success of children, has been using this to generate pre-scripted voice-over content. They also use Voice Engine and GPT-4 to create real-time, personalized responses to interact with students. With this technology, Age of Learning has been able to create more content for a wider audience. <br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="CustomVoiceEngineDemo-3"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div class="flex flex-col pl-[1em] mt-[calc(-1*var(--spacing-spacing-7))] pt-36"><div class="flex flex-col"><h3 class="f-heading-4">1. Reference audio </h3><div class="flex flex-col mt-spacing-3"><div class="overflow-auto no-scrollbar"><div class="min-w-max relative"><ul class="flex flex-row min-w-max"><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-primary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesSpeakertabenglish">English</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesSpeakertabspanish">Spanish</a></li></ul><div class="absolute w-full min-w-max h-1 bottom-0 left-0 bg-[var(--border-secondary)]"><div class="bg-[var(--text-primary)] h-1 w-[200px] absolute bottom-0 left-0 transition-500 transition-all origin-left"></div></div></div></div></div><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/age-of-learning-reference.mp3"></audio></div></div><div class="mt-36"><h3 class="f-heading-4">2. Generated audio </h3><div class="flex flex-col mt-spacing-3"><div class="overflow-auto no-scrollbar"><div class="min-w-max relative"><ul class="flex flex-row min-w-max"><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-primary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabbiology">Biology</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabreading">Reading</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabchemistry">Chemistry</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabmath">Math</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabphysics">Physics</a></li></ul><div class="absolute w-full min-w-max h-1 bottom-0 left-0 bg-[var(--border-secondary)]"><div class="bg-[var(--text-primary)] h-1 w-[200px] absolute bottom-0 left-0 transition-500 transition-all origin-left"></div></div></div></div></div><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/age-of-learning-rainforest.mp3"></audio></div><div class="mt-16 f-body-1"><span>Some</span> <span>of</span> <span>the</span> <span>most</span> <span>amazing</span> <span>habitats</span> <span>on</span> <span>Earth</span> <span>are</span> <span>found</span> <span>in</span> <span>the</span> <span>rainforest.</span> <span>A</span> <span>rainforest</span> <span>is</span> <span>a</span> <span>place</span> <span>with</span> <span>a</span> <span>lot</span> <span>of</span> <span>precipitation</span> <span>and</span> <span>it</span> <span>has</span> <span>many</span> <span>kinds</span> <span>of</span> <span>animals</span> <span>trees</span> <span>and</span> <span>other</span> <span>plants.</span> <span>Tropical</span> <span>rainforests</span> <span>are</span> <span>usually</span> <span>not</span> <span>too</span> <span>far</span> <span>from</span> <span>the</span> <span>equator</span> <span>and</span> <span>are</span> <span>warm</span> <span>all</span> <span>year.</span></div></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><ul><li><strong>Translating content</strong>, like videos and podcasts, so creators and businesses can reach more people around the world, fluently and in their own voices. One early adopter of this is <a href="https://www.heygen.com/" rel="noopener noreferrer" target="_blank">HeyGen</a>, an AI visual storytelling platform that works with their enterprise customers to create custom, human-like avatars for a variety of content, from product marketing to sales demos. They use Voice Engine for video translation, so they can translate a speaker's voice into multiple languages and reach a global audience. When used for translation, Voice Engine preserves the native accent of the original speaker: for example generating English with an audio sample from a French speaker would produce speech with a French accent.<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="CustomVoiceEngineDemo-5"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div class="flex flex-col pl-[1em] mt-[calc(-1*var(--spacing-spacing-7))] pt-36"><div class="flex flex-col"><h3 class="f-heading-4">1. Reference audio </h3><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/heygen-reference.mp3"></audio></div></div><div class="mt-36"><h3 class="f-heading-4">2. Generated audio </h3><div class="flex flex-col mt-spacing-3"><div class="overflow-auto no-scrollbar"><div class="min-w-max relative"><ul class="flex flex-row min-w-max"><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-primary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabspanish">Spanish</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabmandarin">Mandarin</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabgerman">German</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabfrench">French</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabjapanese">Japanese</a></li></ul><div class="absolute w-full min-w-max h-1 bottom-0 left-0 bg-[var(--border-secondary)]"><div class="bg-[var(--text-primary)] h-1 w-[200px] absolute bottom-0 left-0 transition-500 transition-all origin-left"></div></div></div></div></div><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/heygen-spanish.mp3"></audio></div><div class="mt-16 f-body-1"><span>La</span> <span>amistad</span> <span>es</span> <span>un</span> <span>tesoro</span> <span>universal</span> <span>aporta</span> <span>alegría</span> <span>apoyo</span> <span>y</span> <span>risas</span> <span>a</span> <span>nuestras</span> <span>vidas</span> <span>sin</span> <span>importar</span> <span>donde</span> <span>estemos</span> <span>en</span> <span>el</span> <span>mundo.</span> <span>Los</span> <span>verdaderos</span> <span>amigos</span> <span>están</span> <span>con</span> <span>nosotros</span> <span>en</span> <span>las</span> <span>buenas</span> <span>y</span> <span>en</span> <span>las</span> <span>malas</span> <span>compartiendo</span> <span>nuestras</span> <span>alegrías</span> <span>y</span> <span>aliviando</span> <span>nuestras</span> <span>penas.</span> <span>Celebremos</span> <span>los</span> <span>lazos</span> <span>de</span> <span>amistad</span> <span>que</span> <span>nos</span> <span>conectan</span> <span>a</span> <span>todos</span> <span>a</span> <span>través</span> <span>de</span> <span>cada</span> <span>idioma</span> <span>y</span> <span>cultura.</span></div></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><ul><li><strong>Reaching global communities</strong>, by improving essential service delivery in remote settings. <a href="https://www.dimagi.com/" rel="noopener noreferrer" target="_blank">Dimagi</a> is building tools for community health workers to provide a variety of essential services, such as counseling for breastfeeding mothers. To help these workers develop their skills, Dimagi uses Voice Engine and GPT-4 to give interactive feedback in each worker's primary language including Swahili or more informal languages like Sheng, a code-mixed language popular in Kenya.<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="CustomVoiceEngineDemo-7"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div class="flex flex-col pl-[1em] mt-[calc(-1*var(--spacing-spacing-7))] pt-36"><div class="flex flex-col"><h3 class="f-heading-4">1. Reference audio </h3><div class="flex flex-col mt-spacing-3"><div class="overflow-auto no-scrollbar"><div class="min-w-max relative"><ul class="flex flex-row min-w-max"><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-primary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesSpeakertabswahili">Swahili</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesSpeakertabsheng">Sheng</a></li></ul><div class="absolute w-full min-w-max h-1 bottom-0 left-0 bg-[var(--border-secondary)]"><div class="bg-[var(--text-primary)] h-1 w-[200px] absolute bottom-0 left-0 transition-500 transition-all origin-left"></div></div></div></div></div><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/dimagi-swahili-reference.mp3"></audio></div></div><div class="mt-36"><h3 class="f-heading-4">2. Generated audio </h3><div class="flex flex-col mt-spacing-3"><div class="overflow-auto no-scrollbar"><div class="min-w-max relative"><ul class="flex flex-row min-w-max"><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-primary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabnutrition">Nutrition</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabvitamin-a">Vitamin A</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabbreastfeeding">Breastfeeding</a></li></ul><div class="absolute w-full min-w-max h-1 bottom-0 left-0 bg-[var(--border-secondary)]"><div class="bg-[var(--text-primary)] h-1 w-[200px] absolute bottom-0 left-0 transition-500 transition-all origin-left"></div></div></div></div></div><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/dimagi-swahili-nutrition.mp3"></audio></div><div class="mt-16 f-body-1"><span>Lishe</span> <span>bora</span> <span>ni</span> <span>muhimu</span> <span>katika</span> <span>kuhakikisha</span> <span>kwamba</span> <span>watoto</span> <span>wanakua</span> <span>vizuri,</span> <span>kimwili</span> <span>na</span> <span>kiakili.</span> <span>Vyakula</span> <span>kama</span> <span>matunda,</span> <span>mboga,</span> <span>protini,</span> <span>kalsiamu,</span> <span>na</span> <span>vitamini</span> <span>mbalibali</span> <span>ni</span> <span>muhimu</span> <span>sana</span> <span>kwa</span> <span>ukuaji</span> <span>wa</span> <span>mifupa</span> <span>na</span> <span>maendeleo</span> <span>ya</span> <span>ubongo.</span> <span>Kula</span> <span>vizuri</span> <span>kunamaanisha</span> <span>kwamba</span> <span>mtoto</span> <span>anakuwa</span> <span>na</span> <span>mfumu</span> <span>wa</span> <span>kinga</span> <span>imara</span> <span>unaomwezesha</span> <span>kupambana</span> <span>na</span> <span>magonjwa.</span> <span>Hii</span> <span>ina maana</span> <span>kwamba,</span> <span>hata</span> <span>kama</span> <span>kuna</span> <span>mafua</span> <span>yanayoenea</span> <span>mtaani,</span> <span>mtoto</span> <span>atakuwa</span> <span>na</span> <span>uwezo</span> <span>mkubwa</span> <span>wa</span> <span>kukabiliana</span> <span>nayo.</span> <span>Hivyo,</span> <span>hakutakuwa</span> <span>na</span> <span>haja</span> <span>ya</span> <span>kumpeleka</span> <span>hospitalini</span> <span>mara</span> <span>kwa</span> <span>mara.</span> <span>Kwa</span> <span>kufanya</span> <span>hivyo,</span> <span>tunakuwa</span> <span>tunajenga</span> <span>kizazi</span> <span>cha</span> <span>watu</span> <span>imara.</span> <span>Kama</span> <span>unavyojua,</span> <span>mustakabali</span> <span>wa</span> <span>jamii</span> <span>yetu</span> <span>uko</span> <span>mikononi</span> <span>mwa vijana</span> <span>hawa.</span> <span>Ni vyema</span> <span>tuwape</span> <span>mwanzo</span> <span>bora</span> <span>maishani.</span></div></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><ul><li><strong>Supporting people who are non-verbal</strong>, such as therapeutic applications for individuals with conditions that affect speech and educational enhancements for those with learning needs. <a href="https://livox.com.br/en/" rel="noopener noreferrer" target="_blank">Livox</a>, an AI alternative communication app, powers Augmentative &amp; Alternative Communication (AAC) devices that enable people with disabilities to communicate. By using Voice Engine, they are able to offer people who are non-verbal unique and non-robotic voices across many languages. Their users can choose speech that best represents them, and for multilingual users, maintain a consistent voice across each spoken language. <br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="CustomVoiceEngineDemo-9"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div class="flex flex-col pl-[1em] mt-[calc(-1*var(--spacing-spacing-7))] pt-36"><div class="flex flex-col"><h3 class="f-heading-4">1. Reference audio </h3><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/livox-reference.mp3"></audio></div></div><div class="mt-36"><h3 class="f-heading-4">2. Generated audio </h3><div class="flex flex-col mt-spacing-3"><div class="overflow-auto no-scrollbar"><div class="min-w-max relative"><ul class="flex flex-row min-w-max"><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-primary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabenglish">English</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabportuguese">Portuguese</a></li></ul><div class="absolute w-full min-w-max h-1 bottom-0 left-0 bg-[var(--border-secondary)]"><div class="bg-[var(--text-primary)] h-1 w-[200px] absolute bottom-0 left-0 transition-500 transition-all origin-left"></div></div></div></div></div><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/livox-english.mp3"></audio></div><div class="mt-16 f-body-1"><span>Excuse</span> <span>me</span> <span>can</span> <span>I</span> <span>get</span> <span>your</span> <span>attention?</span> <span>Thank</span> <span>you</span> <span>for</span> <span>your</span> <span>help.</span> <span>Can</span> <span>we</span> <span>watch</span> <span>a</span> <span>movie</span> <span>tonight?</span> <span>Could</span> <span>you</span> <span>please</span> <span>help</span> <span>me</span> <span>find</span> <span>my</span> <span>glasses?</span> <span>Thank</span> <span>you</span> <span>for</span> <span>your</span> <span>understanding,</span> <span>it</span> <span>means</span> <span>a</span> <span>lot</span> <span>to</span> <span>me.</span></div></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><ul><li><strong>Helping patients recover their voice</strong>, for those suffering from sudden or degenerative speech conditions. The Norman Prince Neurosciences Institute at <a href="https://livox.com.br/en/" rel="noopener noreferrer" target="_blank">Lifespan</a>, a not-for-profit health system that serves as the primary teaching affiliate of Brown University's medical school, is exploring uses of AI in clinical contexts. They've been piloting a program offering Voice Engine to individuals with oncologic or neurologic etiologies for speech impairment. Since Voice Engine requires such a short audio sample, doctors Fatima Mirza, Rohaid Ali and  Konstantina Svokos were able to restore the voice of a young patient who lost her fluent speech due to a vascular brain tumor, using audio from a video recorded for a school project.<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="CustomVoiceEngineDemo-11"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div class="flex flex-col pl-[1em] mt-[calc(-1*var(--spacing-spacing-7))] pt-36"><div class="flex flex-col"><h3 class="f-heading-4">1. Current voice</h3><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/lifespan-current.mp3"></audio></div></div><div class="mt-36 flex flex-col"><h3 class="f-heading-4">2. Reference audio </h3><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/lifespan-reference.mp3"></audio></div></div><div class="mt-36"><h3 class="f-heading-4">3. Generated audio </h3><div class="flex flex-col mt-spacing-3"><div class="overflow-auto no-scrollbar"><div class="min-w-max relative"><ul class="flex flex-row min-w-max"><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-primary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabtalking">Talking</a></li><li class="mr-16 lg:mr-24 last:mr-0"><a class="ui-link f-ui-1 relative block pb-8 lg:pb-12 whitespace-nowrap text-secondary" href="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices" id="CustomVoicesAudiotabordering">Ordering</a></li></ul><div class="absolute w-full min-w-max h-1 bottom-0 left-0 bg-[var(--border-secondary)]"><div class="bg-[var(--text-primary)] h-1 w-[200px] absolute bottom-0 left-0 transition-500 transition-all origin-left"></div></div></div></div></div><div class="mt-16"><audio class="w-full" controls="" src="https://cdn.openai.com/previewing-voice-engine/lifespan-talking.mp3"></audio></div><div class="mt-16 f-body-1"><span>Hi</span> <span>everyone,</span> <span>this</span> <span>is</span> <span>what</span> <span>my</span> <span>voice</span> <span>sounds</span> <span>like</span> <span>using</span> <span>OpenAI's</span> <span>new</span> <span>text</span> <span>to</span> <span>speech</span> <span>model</span> <span>called</span> <span>Voice Engine.</span> <span>I</span> <span>was</span> <span>able</span> <span>to</span> <span>use</span> <span>just</span> <span>15</span> <span>seconds</span> <span>of</span> <span>a</span> <span>video</span> <span>that</span> <span>I</span> <span>made</span> <span>for</span> <span>a</span> <span>class</span> <span>project</span> <span>to</span> <span>be</span> <span>the</span> <span>reference</span> <span>audio</span> <span>source</span> <span>for</span> <span>the</span> <span>voice</span> <span>you</span> <span>hear</span> <span>right</span> <span>now.</span> <span>What</span> <span>do</span> <span>you</span> <span>think? </span></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="building-voice-engine-safely"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Building Voice Engine safely</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We recognize that generating speech that resembles people's voices has serious risks, which are especially top of mind in an election year. We are engaging with U.S. and international partners from across government, media, entertainment, education, civil society and beyond to ensure we are incorporating their feedback as we build.&nbsp;</p><p>The partners testing Voice Engine today have agreed to our <a href="https://openai.com/policies/usage-policies" rel="noopener noreferrer" target="_blank">usage policies</a>, which prohibit the impersonation of another individual or organization without consent or legal right. In addition, our terms with these partners require explicit and informed consent from the original speaker and we don’t allow developers to build ways for individual users to create their own voices. Partners must also clearly disclose to their audience that the voices they're hearing are AI-generated. Finally, we have implemented a set of safety measures, including watermarking to trace the origin of any audio generated by Voice Engine, as well as proactive monitoring of how it's being used.&nbsp;</p><p>We believe that any broad deployment of synthetic voice technology should be accompanied by voice authentication experiences that verify that the original speaker is knowingly adding their voice to the service and a no-go voice list that detects and prevents the creation of voices that are too similar to prominent figures.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="looking-ahead"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Looking ahead</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Voice Engine is a continuation of our commitment to understand the technical frontier and openly share what is becoming possible with AI. In line with our <a href="https://openai.com/blog/our-approach-to-ai-safety" rel="noopener noreferrer" target="_blank">approach to AI safety</a> and our <a href="https://openai.com/blog/moving-ai-governance-forward" rel="noopener noreferrer" target="_blank">voluntary commitments</a>, we are choosing to preview but not widely release this technology at this time. We hope this preview of Voice Engine both underscores its potential and also motivates the need to bolster societal resilience against the challenges brought by ever more convincing generative models. Specifically, we encourage steps like:</p><ul><li>Phasing out voice based authentication as a security measure for accessing bank accounts and other sensitive information</li><li>Exploring policies to protect the use of individuals' voices in AI</li><li>Educating the public in understanding the capabilities and limitations of AI technologies, including the possibility of deceptive AI content</li><li>Accelerating the development and adoption of techniques for tracking the origin of audiovisual content, so it's always clear when you're interacting with a real person or with an AI</li></ul><p>It's important that people around the world understand where this technology is headed, whether we ultimately deploy it widely ourselves or not. We look forward to continuing to engage in conversations around the challenges and opportunities of synthetic voices with policymakers, researchers, developers and creatives.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Wed, 06 Mar 2024 15:50:42 GMT</pubDate>
<pubDate>Wed, 06 Mar 2024 15:50:42 GMT</pubDate>
</item>
<item>
<title>OpenAI and Elon Musk</title>
<link>https://openai.com/blog/openai-elon-musk</link>
<guid>https://openai.com/blog/openai-elon-musk</guid>
<content:encoded><![CDATA[

<img alt="Facebook" src="https://images.openai.com/blob/d1a9e364-550d-42d7-b228-28e4612e20e9/facebook.png?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>The mission of OpenAI is to ensure AGI benefits all of humanity, which means both building safe and beneficial AGI and helping create broadly distributed benefits. We are now sharing what we've learned about achieving our mission, and some facts about our relationship with Elon.</p><p>Update March 27, 2024: We have filed papers seeking dismissal of all of Elon's claims.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="we-realized-building-agi-will-require-far-more-resources-than-we-d-initially-imagined"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">We realized building AGI will require far more resources than we’d initially imagined</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><em>Elon said we should announce an initial $1B funding commitment to OpenAI. In total, the non-profit has raised less than $45M from Elon and more than $90M from other donors.</em></p><p>When starting OpenAI in late 2015, Greg and Sam had initially planned to raise $100M. Elon said in an email: “We need to go with a much bigger number than $100M to avoid sounding hopeless… I think we should say that we are starting with a $1B funding commitment… I will cover whatever anyone else doesn't provide.” <a href="https://openai.com/blog/openai-elon-musk#email-1" rel="noopener noreferrer">[1]</a></p><p>We spent a lot of time trying to envision a plausible path to AGI. In early 2017, we came to the realization that building AGI will require vast quantities of compute. We began calculating how much compute an AGI might plausibly require. We all understood we were going to need a lot more capital to succeed at our mission—billions of dollars per year, which was far more than any of us, especially Elon, thought we’d be able to raise as the non-profit.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="we-and-elon-recognized-a-for-profit-entity-would-be-necessary-to-acquire-those-resources"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">We and Elon recognized a for-profit entity would be necessary to acquire those resources</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><em>As we discussed a for-profit structure in order to further the mission, Elon wanted us to merge with Tesla or he wanted full control. Elon left OpenAI, saying there needed to be a relevant competitor to Google/DeepMind and that he was going to do it himself. He said he’d be supportive of us finding our own path.</em></p><p>In late 2017, we and Elon decided the next step for the mission was to create a for-profit entity. Elon wanted majority equity, initial board control, and to be CEO. In the middle of these discussions, he withheld funding. Reid Hoffman bridged the gap to cover salaries and operations.</p><p>We couldn’t agree to terms on a for-profit with Elon because we felt it was against the mission for any individual to have absolute control over OpenAI. He then suggested instead merging OpenAI into Tesla. In early February 2018, Elon forwarded us an email suggesting that OpenAI should “attach to Tesla as its cash cow”, commenting that it was “exactly right… Tesla is the only path that could even hope to hold a candle to Google. Even then, the probability of being a counterweight to Google is small. It just isn’t zero”. <a href="https://openai.com/blog/openai-elon-musk#email-2" rel="noopener noreferrer">[2]</a></p><p>Elon soon chose to leave OpenAI, saying that our probability of success was 0, and that he planned to build an AGI competitor within Tesla. When he left in late February 2018, he told our team he was supportive of us finding our own path to raising billions of dollars. In December 2018, Elon sent us an email saying “Even raising several hundred million won’t be enough. This needs billions per year immediately or forget it.” <a href="https://openai.com/blog/openai-elon-musk#email-3" rel="noopener noreferrer">[3]</a></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="we-advance-our-mission-by-building-widely-available-beneficial-tools"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">We advance our mission by building widely-available beneficial tools</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><em>We’re making our technology broadly usable in ways that empower people and improve their daily lives, including via open-source contributions.</em></p><p>We provide broad access to today's most powerful AI, including a free version that hundreds of millions of people use every day. For example, Albania is using OpenAI’s tools to accelerate its EU accession by as much as 5.5 years; Digital Green is helping boost farmer income in Kenya and India by dropping the cost of agricultural extension services 100x by building on OpenAI; Lifespan, the largest healthcare provider in Rhode Island, uses GPT-4 to simplify its surgical consent forms from a college reading level to a 6th grade one; Iceland is using GPT-4 to preserve the Icelandic language.</p><p>Elon understood the mission did not imply open-sourcing AGI. As Ilya told Elon: “As we get closer to building AI, it will make sense to start being less open.&nbsp; The Open in openAI means that everyone should benefit from the fruits of AI after its built, but it's totally OK to not share the science...”, to which Elon replied: “Yup”. <a href="https://openai.com/blog/openai-elon-musk#email-4" rel="noopener noreferrer">[4]</a></p></div></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We're sad that it's come to this with someone whom we’ve deeply admired—someone who inspired us to aim higher, then told us we would fail, started a competitor, and then sued us when we started making meaningful progress towards OpenAI’s mission without him.</p><p>We are focused on advancing our mission and have a long way to go. As we continue to make our tools better and better, we are excited to deploy these systems so they empower every individual.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Update March 11, 2024: We are seeking to have the lawsuit assigned to dedicated case management, since it involves AI technology and the claims span almost a decade.</p></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="EmailAttachment-9"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div class="flex flex-col f-body-1" id="email-1"><div class="mb-spacing-3">[1]</div><div class="border-l-[var(--gray-400)] border-l-2 pl-spacing-3"><div><span class="font-semibold">From:</span>&nbsp; <div class="inline-block">Elon Musk &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold inline">To:</span>&nbsp; <div class="inline">Greg Brockman &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold inline">CC:</span>&nbsp; <div class="inline">Sam Altman &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold">Date:</span> Sun, Nov 22, 2015 at 7:48 PM</div><div><span class="font-semibold">Subject:</span> follow up from call</div><div class="mt-spacing-4 ui-richtext italic">Blog sounds good, assuming adjustments for neutrality vs being YC-centric.<br /><br />I'd favor positioning the blog to appeal a bit more to the general public -- there is a lot of value to having the public root for us to succeed -- and then having a longer, more detailed and inside-baseball version for recruiting, with a link to it at the end of the general public version.<br /><br />We need to go with a much bigger number than $100M to avoid sounding hopeless relative to what Google or Facebook are spending. I think we should say that we are starting with a $1B funding commitment. This is real. I will cover whatever anyone else doesn't provide.<br /><br />Template seems fine, apart from shifting to a vesting cash bonus as default, which can optionally be turned into YC or potentially SpaceX (need to understand how much this will be) stock.</div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="OpenAIAndElonMuskTwo-10"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div class="mb-spacing-3 f-body-1" id="email-2">[2]</div><div class="flex flex-col f-body-1"><div class="flex flex-col f-body-1"><div class="border-l-[var(--gray-400)] border-l-2 pl-spacing-3"><div><span class="font-semibold">From:</span>&nbsp; <div class="inline-block">Elon Musk &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold inline">To:</span>&nbsp; <div class="inline">Ilya Sutskever &lt;<div class="inline-block"></div>&gt;, Greg Brockman &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold">Date:</span> Thu, Feb 1, 2018 at 3:52 AM</div><div><span class="font-semibold">Subject:</span> Fwd: Top AI institutions today</div><div class="mt-spacing-4"><div class="italic"><div class="inline-block"></div> is exactly right. We may wish it otherwise, but, in my and <div class="inline-block"></div>’s opinion, Tesla is the only path that could even hope to hold a candle to Google. Even then, the probability of being a counterweight to Google is small. It just isn't zero. </div><div class="mt-spacing-4 mb-spacing-4">Begin forwarded message:</div><div class="flex flex-col f-body-1"><div class="border-l-[var(--gray-400)] border-l-2 pl-spacing-3"><div><span class="font-semibold">From:</span>&nbsp; <div class="inline-block"><div class="inline-block"></div> &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold inline">To:</span>&nbsp; <div class="inline">Elon Musk &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold">Date:</span> January 31, 2018 at 11:54:30 PM PST</div><div><span class="font-semibold">Subject:</span> Re: Top AI institutions today</div><div class="mt-spacing-4 ui-richtext italic">Working at the cutting edge of AI is unfortunately expensive. For example,<div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div>In addition to DeepMind, Google also has Google Brain, Research, and Cloud. And TensorFlow, TPUs, and they own about a third of all research (in fact, they hold their own AI conferences).<br /><br />I also strongly suspect that compute horsepower will be necessary (and possibly even sufficient) to reach AGI. If historical trends are any indication, progress in AI is primarily driven by systems - compute, data, infrastructure. The core algorithms we use today have remained largely unchanged from the ~90s. Not only that, but any algorithmic advances published in a paper somewhere can be almost immediately re-implemented and incorporated. Conversely, algorithmic advances alone are inert without the scale to also make them scary.<br /><br />It seems to me that OpenAI today is burning cash and that the funding model cannot reach the scale to seriously compete with Google (an 800B company). If you can't seriously compete but continue to do research in open, you might in fact be making things worse and helping them out “for free”, because any advances are fairly easy for them to copy and immediately incorporate, at scale.<br /><br />A for-profit pivot might create a more sustainable revenue stream over time and would, with the current team, likely bring in a lot of investment. However, building out a product from scratch would steal focus from AI research, it would take a long time and it's unclear if a company could “catch up” to Google scale, and the investors might exert too much pressure in the wrong directions.The most promising option I can think of, as I mentioned earlier, would be for OpenAI to attach to Tesla as its cash cow. I believe attachments to other large suspects (e.g. Apple? Amazon?) would fail due to an incompatible company DNA. Using a rocket analogy, Tesla already built the “first stage” of the rocket with the whole supply chain of Model 3 and its onboard computer and a persistent internet connection. The “second stage” would be a full self driving solution based on large-scale neural network training, which OpenAI expertise could significantly help accelerate. With a functioning full self-driving solution in ~2-3 years we could sell a lot of cars/trucks. If we do this really well, the transportation industry is large enough that we could increase Tesla's market cap to high O(~100K), and use that revenue to fund the AI work at the appropriate scale.<br /><br />I cannot see anything else that has the potential to reach sustainable Google-scale capital within a decade.<br /><br /><div class="inline-block"></div></div></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="EmailAttachment-11"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div class="flex flex-col f-body-1" id="email-3"><div class="mb-spacing-3">[3]</div><div class="border-l-[var(--gray-400)] border-l-2 pl-spacing-3"><div><span class="font-semibold">From:</span>&nbsp; <div class="inline-block">Elon Musk &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold inline">To:</span>&nbsp; <div class="inline">Ilya Sutskever &lt;<div class="inline-block"></div>&gt;, Greg Brockman &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold inline">CC:</span>&nbsp; <div class="inline">Sam Altman &lt;<div class="inline-block"></div>&gt;, <div class="inline-block"></div> &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold">Date:</span> Wed, Dec 26, 2018 at 12:07 PM</div><div><span class="font-semibold">Subject:</span> I feel I should reiterate</div><div class="mt-spacing-4 ui-richtext italic">My probability assessment of OpenAI being relevant to DeepMind/Google without a dramatic change in execution and resources is 0%. Not 1%. I wish it were otherwise.<br /><br />Even raising several hundred million won't be enough. This needs billions per year immediately or forget it.<br /><br />Unfortunately, humanity's future is in the hands of <div class="inline-block"></div>.<br /><br /><div class="inline-block"></div><br /><br />And they are doing a lot more than this.<br /><br /><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><div class="inline-block"></div><br /><br />I really hope I'm wrong.<br /><br />Elon</div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="OpenAIAndElonMuskFour-12"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div class="mb-spacing-3 f-body-1" id="email-4">[4]</div><div class="flex flex-col f-body-1"><div class="border-t-2 border-b-2 border-[color:var(--gray-300)] py-spacing-2 mb-spacing-4"><span class="block f-heading-4">Fwd: congrats on the falcon 9</span><span class="block text-[color:var(--gray-800)]">3 messages</span></div><div class="flex flex-col f-body-1"><div class="border-l-[var(--gray-400)] border-l-2 pl-spacing-3"><div><span class="font-semibold">From:</span>&nbsp; <div class="inline-block">Elon Musk &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold inline">To:</span>&nbsp; <div class="inline">Sam Altman &lt;<div class="inline-block"></div>&gt;, Ilya Sutskever &lt;<div class="inline-block"></div>&gt;, Greg Brockman &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold">Date:</span> Sat, Jan 2, 2016 at 8:18 AM</div><div><span class="font-semibold">Subject:</span> Fwd: congrats on the falcon 9</div><div class="mt-spacing-4"><div class="mb-spacing-4">Begin forwarded message:</div><div class="flex flex-col f-body-1"><div class="border-l-[var(--gray-400)] border-l-2 pl-spacing-3"><div><span class="font-semibold">From:</span>&nbsp; <div class="inline-block"><div class="inline-block"></div> &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold inline">To:</span>&nbsp; <div class="inline">Elon Musk &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold">Date:</span> January 2, 2016 at 10:12:32 AM CST</div><div><span class="font-semibold">Subject:</span> congrats on the falcon 9</div><div class="mt-spacing-4 ui-richtext italic">Hi Elon<br /><br />Happy new year to you, <div class="inline-block"></div>!<br /><br />Congratulations on landing the Falcon 9, what an amazing achievement. Time to build out the fleet now!<br /><br />I've seen you (and Sam and other OpenAI people) doing a lot of interviews recently extolling the virtues of open sourcing AI, but I presume you realise that this is not some sort of panacea that will somehow magically solve the safety problem? There are many good arguments as to why the approach you are taking is actually very dangerous and in fact may increase the risk to the world. Some of the more obvious points are well articulated in this blog post, that I'm sure you've seen, but there are also other important considerations:<br />http://slatestarcodex.com/2015/12/17/should-ai-be-open/<br /><br />I’d be interested to hear your counter-arguments to these points.<br /><br />Best<br /><div class="inline-block"></div></div></div></div></div></div></div><div class="my-20 lg:my-36 border-t-2 border-t-[color:var(--gray-400)]"></div><div class="flex flex-col f-body-1"><div class="border-l-[var(--gray-400)] border-l-2 pl-spacing-3"><div><span class="font-semibold">From:</span>&nbsp; <div class="inline-block">Ilya Sutskever &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold inline">To:</span>&nbsp; <div class="inline">Elon Musk &lt;<div class="inline-block"></div>&gt;, Sam Altman &lt;<div class="inline-block"></div>&gt;, Greg Brockman &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold">Date:</span> Sat, Jan 2, 2016 at 9:06 AM</div><div><span class="font-semibold">Subject:</span> Fwd: congrats on the falcon 9</div><div class="mt-spacing-4 ui-richtext italic">The article is concerned with a hard takeoff scenario:  if a hard takeoff occurs, and a safe AI is harder to build than an unsafe one, then by opensorucing everything, we make it easy for someone unscrupulous with access to overwhelming amount of hardware to build an unsafe AI, which will experience a hard takeoff.<br /><br />As we get closer to building AI, it will make sense to start being less open.  The Open in openAI means that everyone should benefit from the fruits of AI after its built, but it's totally OK to not share the science (even though sharing everything is definitely the right strategy in the short and possibly medium term for recruitment purposes).</div></div></div><div class="my-20 lg:my-36 border-t-2 border-t-[color:var(--gray-400)]"></div><div class="flex flex-col f-body-1"><div class="border-l-[var(--gray-400)] border-l-2 pl-spacing-3"><div><span class="font-semibold">From:</span>&nbsp; <div class="inline-block">Elon Musk &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold inline">To:</span>&nbsp; <div class="inline">Ilya Sutskever &lt;<div class="inline-block"></div>&gt;</div></div><div><span class="font-semibold">Date:</span> Sat, Jan 2, 2016 at 9:11 AM</div><div><span class="font-semibold">Subject:</span> Fwd: congrats on the falcon 9</div><div class="mt-spacing-4 ui-richtext italic">Yup</div></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Tue, 05 Mar 2024 16:35:33 GMT</pubDate>
<pubDate>Tue, 05 Mar 2024 16:35:33 GMT</pubDate>
</item>
<item>
<title>Disrupting malicious uses of AI by state-affiliated threat actors</title>
<link>https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors</link>
<guid>https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors</guid>
<content:encoded><![CDATA[

<img alt="" src="https://images.openai.com/blob/5080983d-9c4d-4479-8e11-e88d309079eb/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors.jpg?trim=192%2C0%2C256%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We build AI tools that improve lives and help solve complex challenges, but we know that malicious actors will sometimes try to abuse our tools to harm others, including in furtherance of cyber operations. Among those malicious actors, state-affiliated groups—which may have access to advanced technology, large financial resources, and skilled personnel—can pose unique risks to the digital ecosystem and human welfare.&nbsp;</p><p>In partnership with Microsoft Threat Intelligence, we have disrupted five state-affiliated actors that sought to use AI services in support of malicious cyber activities. We also outline our approach to detect and disrupt such actors in order to promote information sharing and transparency regarding their activities.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="disruption-of-threat-actors"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Disruption of threat actors</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Based on collaboration and information sharing with Microsoft, we disrupted five state-affiliated malicious actors: two China-affiliated threat actors known as Charcoal Typhoon and Salmon Typhoon; the Iran-affiliated threat actor known as Crimson Sandstorm; the North Korea-affiliated actor known as Emerald Sleet; and the Russia-affiliated actor known as Forest Blizzard. The identified OpenAI accounts associated with these actors were terminated.</p><p>These actors generally sought to use OpenAI services for querying open-source information, translating, finding coding errors, and running basic coding tasks.&nbsp;</p><p>Specifically:&nbsp;</p><ul><li>Charcoal Typhoon used our services to research various companies and cybersecurity tools, debug code and generate scripts, and create content likely for use in phishing campaigns.</li><li>Salmon Typhoon used our services to translate technical papers, retrieve publicly available information on multiple intelligence agencies and regional threat actors, assist with coding, and research common ways processes could be hidden on a system.</li><li>Crimson Sandstorm used our services for scripting support related to app and web development, generating content likely for spear-phishing campaigns, and researching common ways malware could evade detection.</li><li>Emerald Sleet used our services to identify experts and organizations focused on defense issues in the Asia-Pacific region, understand publicly available vulnerabilities, help with basic scripting tasks, and draft content that could be used in phishing campaigns.</li><li>Forest Blizzard used our services primarily for open-source research into satellite communication protocols and radar imaging technology, as well as for support with scripting tasks.</li></ul><p>Additional technical details on the nature of the threat actors and their activities can be found in the <a href="https://aka.ms/emerging-AI-threats" rel="noopener noreferrer" target="_blank">Microsoft blog post</a> published today.&nbsp;</p><p>The activities of these actors are consistent with previous <a href="https://cdn.openai.com/papers/gpt-4.pdf" rel="noopener noreferrer" target="_blank">red team assessments</a><em> </em>we conducted in partnership with external cybersecurity experts, which found that GPT-4 offers only limited, incremental capabilities for malicious cybersecurity tasks beyond what is already achievable with publicly available, non-AI powered tools<em>. </em><br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="a-multi-pronged-approach-to-ai-safety"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">A multi-pronged approach to AI safety</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Although the capabilities of our current models for malicious cybersecurity tasks are limited, we believe it’s important to stay ahead of significant and evolving threats. To respond to the threat, we are taking a multi-pronged approach to combating malicious state-affiliated actors’ use of our platform:&nbsp;</p><ul><li><strong>Monitoring and disrupting malicious state affiliated actors.</strong> We invest in technology and teams to identify and disrupt sophisticated threat actors’ activities.&nbsp;Our Intelligence and Investigations, Safety, Security, and Integrity teams investigate malicious actors in a variety of ways, including using our models to pursue leads, analyze how adversaries are interacting with our platform, and assess their broader intentions. Upon detection, OpenAI takes appropriate action to disrupt their activities, such as disabling their accounts, terminating services, or limiting access to resources.&nbsp;</li><li><strong>Working together with the AI ecosystem. </strong>OpenAI collaborates with industry partners and other stakeholders to regularly exchange information about malicious state-affiliated actors’ detected use of AI. This collaboration reflects our <a href="https://openai.com/blog/moving-ai-governance-forward" rel="noopener noreferrer" target="_blank">voluntary commitment</a> to promote the safe, secure and transparent development and use of AI technology, and aims to promote collective responses to ecosystem-wide risks via information sharing.&nbsp;</li><li><strong>Iterating on safety mitigations. </strong>Learning from real-world use (and misuse) is a key component of creating and releasing increasingly safe AI systems over time. We take lessons learned from these actors' abuse and use them to inform our iterative approach to safety. Understanding how the most sophisticated malicious actors seek to use our systems for harm gives us a signal into practices that may become more widespread in the future, and allows us to continuously evolve our safeguards.&nbsp;</li><li><strong>Public transparency. </strong>We have long sought to highlight potential misuses of AI [<a href="https://cdn.openai.com/papers/gpt-4-system-card.pdf" rel="noopener noreferrer" target="_blank">link 1</a>,<a href="https://openai.com/research/forecasting-misuse" rel="noopener noreferrer" target="_blank"> link 2</a>] and share what we have <a href="https://openai.com/research/language-model-safety-and-misuse" rel="noopener noreferrer" target="_blank">learned</a> about safety [<a href="https://openai.com/research/language-model-safety-and-misuse" rel="noopener noreferrer" target="_blank">link 1,</a> <a href="https://openai.com/blog/best-practices-for-deploying-language-models" rel="noopener noreferrer" target="_blank">link 2]</a> with the industry and the public. As part of our ongoing efforts to advance responsible use of AI, OpenAI will continue to inform the public and stakeholders about the nature and extent of malicious state-affiliated actors’ use of AI detected within our systems and the measures taken against them, when warranted.&nbsp;We believe that sharing and transparency foster greater awareness and preparedness among all stakeholders, leading to stronger collective defense against ever-evolving adversaries.&nbsp;</li></ul><p>The vast majority of people use our systems to help improve their daily lives, from virtual tutors for students to apps that can transcribe the world for people who are seeing impaired. As is the case with many other ecosystems, there are a handful of malicious actors that require sustained attention so that everyone else can continue to enjoy the benefits. Although we work to minimize potential misuse by such actors, we will not be able to stop every instance. But by continuing to innovate, investigate, collaborate, and share, we make it harder for malicious actors to remain undetected across the digital ecosystem and improve the experience for everyone else.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Tue, 13 Feb 2024 22:06:03 GMT</pubDate>
<pubDate>Tue, 13 Feb 2024 22:06:03 GMT</pubDate>
</item>
<item>
<title>Memory and new controls for ChatGPT</title>
<link>https://openai.com/blog/memory-and-new-controls-for-chatgpt</link>
<guid>https://openai.com/blog/memory-and-new-controls-for-chatgpt</guid>
<content:encoded><![CDATA[

<img alt="The &quot;Manage Memory&quot; settings dialog" src="https://images.openai.com/blob/d77c453f-fa97-4a74-9363-5fee31a478be/memory-and-new-controls-for-chatgpt.png?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We’re testing memory with ChatGPT. Remembering things you discuss across all chats saves you from having to repeat information and makes future conversations more helpful.</p><p>You’re in control of ChatGPT’s memory. You can explicitly tell it to remember something, ask it what it remembers, and tell it to forget conversationally or through settings. You can also turn it off entirely.</p><p>We are rolling out to a small portion of ChatGPT free and Plus users this week to learn how useful it is. We will share plans for broader roll out soon.&nbsp;<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="how-memory-works"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">How memory works</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>As you chat with ChatGPT, you can ask it to remember something specific or let it pick up details itself. ChatGPT’s memory will get better the more you use it and you'll start to notice the improvements over time. For example:&nbsp;</p><ul><li>You’ve explained that you prefer meeting notes to have headlines, bullets and action items summarized at the bottom. ChatGPT remembers this and recaps meetings this way.</li><li>You’ve told ChatGPT you own a neighborhood coffee shop. When brainstorming messaging for a social post celebrating a new location, ChatGPT knows where to start.&nbsp;</li><li>You mention that you have a toddler and that she loves jellyfish. When you ask ChatGPT to help create her birthday card, it suggests a jellyfish wearing a party hat.&nbsp;</li><li>As a kindergarten teacher with 25 students, you prefer 50-minute lessons with follow-up activities. ChatGPT remembers this when helping you create lesson plans.<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="you-re-in-control"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">You’re in control</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>You can turn off memory at any time (Settings &gt; Personalization &gt; Memory). While memory is off, you won't create or use memories.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="InlineVideo-5"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div><video controls="" loop="" src="https://cdn.openai.com/memory-ga/OM020_Find_1920x1080_TitleCard_OptionA.mp4"></video></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>If you want ChatGPT to forget something, just tell it. You can also view and delete specific memories or clear all memories in settings (Settings &gt; Personalization &gt; Manage Memory). ChatGPT's memories evolve with your interactions and aren't linked to specific conversations. Deleting a chat doesn't erase its memories; you must delete the memory itself. You can find more details in our <a href="https://help.openai.com/en/articles/8590148-memory-faq" rel="noopener noreferrer" target="_blank">Help Center</a>.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="InlineVideo-7"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div><video controls="" loop="" src="https://cdn.openai.com/memory-ga/OM030_Delete_1920x1080_TitleCard_OptionC.mp4"></video></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We may use content that you provide to ChatGPT, including memories, to improve our models for everyone. If you’d like, you can turn this off through your Data Controls. As always, we won't train on content from ChatGPT Team and Enterprise customers. Learn more about how we use content to train our models and your choices in our <a href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance" rel="noopener noreferrer" target="_blank">Help Center</a>.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="use-temporary-chat-for-conversations-without-memory"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Use temporary chat for conversations without memory</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>If you’d like to have a conversation without using memory, use temporary chat. Temporary chats won't appear in history, won't use memory, and won't be used to train our models. Learn more about temporary chats in our <a href="https://help.openai.com/en/articles/8914046-temporary-chat-faq" rel="noopener noreferrer" target="_blank">Help Center</a>.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="InlineVideo-11"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div><video controls="" loop="" src="https://cdn.openai.com/memory-ga/OM040_Temp_1920x1080_TitleCard_OptionA_v18.mp4"></video></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="custom-instructions-also-allow-chatgpt-to-be-more-helpful"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Custom instructions also allow ChatGPT to be more helpful</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><a href="https://openai.com/blog/custom-instructions-for-chatgpt" rel="noopener noreferrer" target="_blank">Custom Instructions</a> continue to allow you to provide ChatGPT with direct guidance on what you’d like it to know about you and how you’d like it to respond. For explicit information or instructions, you can add it to your Custom Instructions. For information shared via conversations, ChatGPT can remember relevant details for you.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="evolving-our-privacy-and-safety-standards"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Evolving our privacy and safety standards</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Memory brings additional privacy and safety considerations, such as what type of information should be remembered and how it’s used. We’re taking steps to assess and mitigate biases, and steer ChatGPT away from proactively remembering sensitive information, like your health details - unless you explicitly ask it to.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="team-and-enterprise-customers-can-work-more-efficiently"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Team and Enterprise customers can work more efficiently</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>For Enterprise and Team users, memory can be useful when using ChatGPT for work. It can learn your style and preferences, and build upon past interactions. This saves you time and leads to more relevant and insightful responses. For example:</p><ul><li>ChatGPT can remember your tone, voice, and format preferences, and automatically apply them to blog post drafts without needing repetition.</li><li>When coding, you tell ChatGPT your programming language and frameworks. It can remember these preferences for subsequent tasks, streamlining the process.</li><li>For monthly business reviews, you securely upload your data to ChatGPT and it creates your preferred charts with three takeaways each.</li></ul><p>As with any ChatGPT feature, you’re in control of your organization’s data. Memories and any other information on your workspace are excluded from training our models. Users have control on how and when their memories are used in chats. In addition, Enterprise account owners can turn memory off for their organization at any time.</p><p>Enterprise and Team users will have access to memory as part of our wider rollout.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="gpts-will-also-have-memory"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">GPTs will also have memory</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>GPTs will have their own distinct memory. Builders will have the option to enable memory for their GPTs. Like your chats, memories are not shared with builders. To interact with a memory-enabled GPT, you will also need to have memory on. For example:&nbsp;</p><ul><li>The <a href="https://chat.openai.com/g/g-z77yDe7Vu-books" rel="noopener noreferrer" target="_blank">Books GPT</a> helps you find your next read. With memory enabled, it remembers your preferences, such as favorite genres or top books, and tailors recommendations accordingly, without needing repeated inputs.</li></ul><p>Each GPT has its own memory, so you might need to repeat details you’ve previously shared with ChatGPT. For example:</p><ul><li>If you're using the <a href="https://chat.openai.com/g/g-SnF78wo4p-artful-greeting-ai-cards" rel="noopener noreferrer" target="_blank">Artful Greeting Card GPT</a> to create a birthday card for your daughter, it won’t know her age or that she loves jellyfish. You’ll need to tell it the relevant details.</li></ul><p>Memory for GPTs will be available when we roll it out more broadly.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Mon, 12 Feb 2024 17:52:14 GMT</pubDate>
<pubDate>Mon, 12 Feb 2024 17:52:14 GMT</pubDate>
</item>
<item>
<title>New embedding models and API updates</title>
<link>https://openai.com/blog/new-embedding-models-and-api-updates</link>
<guid>https://openai.com/blog/new-embedding-models-and-api-updates</guid>
<content:encoded><![CDATA[

<img alt="New Embeddings Models And API Updates" src="https://images.openai.com/blob/ec19d092-e613-41c0-8dcb-57e030658420/new-embeddings-models-and-api-updates.jpg?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We are releasing new models, reducing prices for GPT-3.5 Turbo, and introducing new ways for developers to manage API keys and understand API usage. The new models include:</p><ul><li>Two new embedding models</li><li>An updated GPT-4 Turbo preview model&nbsp;</li><li>An updated GPT-3.5 Turbo model</li><li>An updated text moderation model</li></ul><p>By default, data sent to the OpenAI API will not be used to train or improve OpenAI models.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="new-embedding-models-with-lower-pricing"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">New embedding models with lower pricing</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We are introducing two new embedding models: a smaller and highly efficient <code>text-embedding-3-small</code> model, and a larger and more powerful <code>text-embedding-3-large</code> model.</p><p>An <a href="https://openai.com/blog/introducing-text-and-code-embeddings" rel="noopener noreferrer" target="_blank">embedding</a> is a sequence of numbers that represents the concepts within content such as natural language or code. Embeddings make it easy for machine learning models and other algorithms to understand the relationships between content and to perform tasks like clustering or retrieval. They power applications like knowledge retrieval in both ChatGPT and the Assistants API, and many retrieval augmented generation (RAG) developer tools.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="NewAndImprovedEmbeddingModelTextAsVector-3"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div><figure class="max-w-[280px] mx-auto sm:max-w-none"><img class="sm:hidden" src="https://cdn.openai.com/new-and-improved-embedding-model/draft-20221214a/vectors-mobile-1.svg" /><img class="hidden sm:block" src="https://cdn.openai.com/new-and-improved-embedding-model/draft-20221214a/vectors-1.svg" /></figure><figure class="max-w-[280px] mx-auto sm:max-w-none" style="display: none;"><img class="sm:hidden" src="https://cdn.openai.com/new-and-improved-embedding-model/draft-20221214a/vectors-mobile-2.svg" /><img class="hidden sm:block" src="https://cdn.openai.com/new-and-improved-embedding-model/draft-20221214a/vectors-2.svg" /></figure><figure class="max-w-[280px] mx-auto sm:max-w-none" style="display: none;"><img class="sm:hidden" src="https://cdn.openai.com/new-and-improved-embedding-model/draft-20221214a/vectors-mobile-3.svg" /><img class="hidden sm:block" src="https://cdn.openai.com/new-and-improved-embedding-model/draft-20221214a/vectors-3.svg" /></figure></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="a-new-small-text-embedding-model"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">A new small text embedding model</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><code>text-embedding-3-small</code> is our new highly efficient embedding model and provides a significant upgrade over its predecessor, the <code>text-embedding-ada-002</code> model released in <a href="https://openai.com/blog/new-and-improved-embedding-model" rel="noopener noreferrer" target="_blank">December 2022</a>.&nbsp;</p><p><strong>Stronger performance. </strong>Comparing <code>text-embedding-ada-002</code> to <code>text-embedding-3-small</code>, the average score on a commonly used benchmark for multi-language retrieval (<a href="https://github.com/project-miracl/miracl" rel="noopener noreferrer" target="_blank">MIRACL</a>) has increased from 31.4% to 44.0%, while the average score on a commonly used benchmark for English tasks (<a href="https://github.com/embeddings-benchmark/mteb" rel="noopener noreferrer" target="_blank">MTEB</a>) has increased from 61.0% to 62.3%.</p><p><strong>Reduced price. </strong><code>text-embedding-3-small</code> is also substantially more efficient than our previous generation <code>text-embedding-ada-002</code> model. Pricing for <code>text-embedding-3-small</code> has therefore been reduced by 5X compared to <code>text-embedding-ada-002</code>, from a price per 1k tokens of $0.0001 to $0.00002.</p><p>We are not deprecating <code>text-embedding-ada-002</code>, so while we recommend the newer model, customers are welcome to continue using the previous generation model.</p><p>A new large text embedding model: <code>text-embedding-3-large</code></p><p><code>text-embedding-3-large</code> is our new next generation larger embedding model and creates embeddings with up to 3072 dimensions.</p><p><strong>Stronger performance. </strong><code>text-embedding-3-large</code> is our new best performing model. Comparing <code>text-embedding-ada-002</code> to <code>text-embedding-3-large</code>: on MIRACL, the average score has increased from 31.4% to 54.9%, while on MTEB, the average score has increased from 61.0% to 64.6%. <br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--table"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols overflow-hidden xs:w-6-cols relative"><div class="overflow-x-auto"><table class="w-full border-t border-t-primary block max-w-[calc(100vw_-_(var(--outer-gutter)*2))] overflow-x-auto md:table"><tbody><tr class="border-b border-secondary"><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-heading-5"><p>Eval benchmark<br class="softbreak" /></p></span></td><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-heading-5"><p>ada v2<br class="softbreak" /></p></span></td><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-heading-5"><p>text-embedding-3-small<br class="softbreak" /></p></span></td><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-heading-5"><p>text-embedding-3-large<br class="softbreak" /></p></span></td></tr><tr class="border-b border-secondary"><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-body-1 whitespace-pre-line ui-richtext"><p><strong>MIRACL average</strong><br class="softbreak" /></p></span></td><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-body-1 whitespace-pre-line ui-richtext"><p>31.4<br class="softbreak" /></p></span></td><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-body-1 whitespace-pre-line ui-richtext"><p>44.0<br class="softbreak" /></p></span></td><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-body-1 whitespace-pre-line ui-richtext"><p>54.9<br class="softbreak" /></p></span></td></tr><tr class="border-b border-secondary"><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-body-1 whitespace-pre-line ui-richtext"><p><strong>MTEB average</strong><br class="softbreak" /></p></span></td><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-body-1 whitespace-pre-line ui-richtext"><p>61.0<br class="softbreak" /></p></span></td><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-body-1 whitespace-pre-line ui-richtext"><p>62.3<br class="softbreak" /></p></span></td><td class="pb-4 pt-4 pr-[1em] align-top xs:min-w-[calc(50vw_-_var(--inner-gutter))] sm:min-w-0"><span class="f-body-1 whitespace-pre-line ui-richtext"><p>64.6<br class="softbreak" /></p></span></td></tr></tbody></table></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><code>text-embedding-3-large</code> will be priced at $0.00013 / 1k tokens.</p><p>You can learn more about using the new embedding models in our <a href="https://platform.openai.com/docs/guides/embeddings" rel="noopener noreferrer" target="_blank">Embeddings guide</a>.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-6" id="native-support-for-shortening-embeddings"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">Native support for shortening embeddings</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Using larger embeddings, for example storing them in a vector store for retrieval, generally costs more and consumes more compute, memory and storage than using smaller embeddings.</p><p>Both of our new embedding models were trained with a technique<span class="ui-fn"><sup class="inline-block min-w-[1.5ch] indent-0 not-italic [em_&amp;]:indent-2"><span class="error">[^footnote-technique]</span></sup></span> that allows developers to trade-off performance and cost of using embeddings. Specifically, developers can shorten embeddings (i.e. remove some numbers from the end of the sequence) without the embedding losing its concept-representing properties by passing in the <code>dimensions</code> API parameter. For example, on the MTEB benchmark, a <code>text-embedding-3-large</code> embedding can be shortened to a size of 256 while still outperforming an unshortened <code>text-embedding-ada-002</code> embedding with a size of 1536.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet" id="NewEmbeddingsModelsAndAPIUpdatesSizeScoreComparison-10"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><div class="f-ui-1"><table class="border-t border-t-primary"><thead><tr><td></td><td>ada v2</td><td colspan="2">text-embedding-3-small</td><td colspan="3">text-embedding-3-large</td></tr></thead><tbody><tr><td>Embedding size</td><td>1536</td><td>512</td><td>1536</td><td>256</td><td>1024</td><td>3072</td></tr><tr><td>Average MTEB score</td><td>61.0</td><td>61.6</td><td>62.3</td><td>62.0</td><td>64.1</td><td>64.6</td></tr></tbody></table></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>This enables very flexible usage. For example, when using a vector data store that only supports embeddings up to 1024 dimensions long, developers can now still use our best embedding model <code>text-embedding-3-large</code> and specify a value of 1024 for the <code>dimensions</code> API parameter, which will shorten the embedding down from 3072 dimensions, trading off some accuracy in exchange for the smaller vector size.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="other-new-models-and-lower-pricing"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Other new models and lower pricing</h2></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-6" id="updated-gpt-3-5-turbo-model-and-lower-pricing"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">Updated GPT-3.5 Turbo model and lower pricing</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Next week we are introducing a new GPT-3.5 Turbo model, <code>gpt-3.5-turbo-0125</code>, and for the third time in the past year, we will be decreasing prices on GPT-3.5 Turbo to help our customers scale. Input prices for the new model are reduced by 50% to $0.0005 /1K tokens and output prices are reduced by 25% to $0.0015 /1K tokens. This model will also have various improvements including higher accuracy at responding in requested formats and a fix for <a href="https://community.openai.com/t/gpt-4-1106-preview-is-not-generating-utf-8/482839" rel="noopener noreferrer" target="_blank">a bug</a> which caused a text encoding issue for non-English language function calls.</p><p>Customers using the unpinned <code>gpt-3.5-turbo</code> model alias will be automatically upgraded from <code>gpt-3.5-turbo-0613</code>&nbsp;to <code>gpt-3.5-turbo-0125</code> two weeks after this model launches.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-6" id="updated-gpt-4-turbo-preview"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">Updated GPT-4 Turbo preview</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Over 70% of requests from GPT-4 API customers have transitioned to GPT-4 Turbo since its release, as developers take advantage of its updated knowledge cutoff, larger 128k context windows, and lower prices.&nbsp;</p><p>Today, we are releasing an updated GPT-4 Turbo preview model, <code>gpt-4-0125-preview</code>. This model completes tasks like code generation more thoroughly than the previous preview model and is intended to reduce cases of “laziness” where the model doesn’t complete a task. The new model also includes the fix for the bug impacting non-English UTF-8 generations.</p><p>For those who want to be automatically upgraded to new GPT-4 Turbo preview versions, we are also introducing a new <code>gpt-4-turbo-preview</code> model name alias, which will always point to our latest GPT-4 Turbo preview model.&nbsp;</p><p>We plan to launch GPT-4 Turbo with vision in general availability in the coming months.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-6" id="updated-moderation-model"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">Updated moderation model</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>The free Moderation API allows developers to identify potentially harmful text. As part of our ongoing safety work, we are releasing <code>text-moderation-007</code>, our most robust moderation model to-date. The <code>text-moderation-latest</code> and <code>text-moderation-stable</code> aliases have been updated to point to it. You can learn more about building safe AI systems through our <a href="https://platform.openai.com/docs/guides/safety-best-practices" rel="noopener noreferrer" target="_blank">safety best practices guide</a>.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="new-ways-to-understand-api-usage-and-manage-api-keys"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">New ways to understand API usage and manage API keys</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We are launching two platform improvements to give developers both more visibility into their usage and control over API keys.</p><p>First, developers can now assign permissions to API keys from the <a href="https://platform.openai.com/api-keys" rel="noopener noreferrer" target="_blank">API keys page</a>. For example, a key could be assigned read-only access to power an internal tracking dashboard, or restricted to only access certain endpoints.</p><p>Second, the usage dashboard and usage export function now expose metrics on an API key level after <a href="https://platform.openai.com/api-keys" rel="noopener noreferrer" target="_blank">turning on tracking</a>. This makes it simple to view usage on a per feature, team, product, or project level, simply by having separate API keys for each.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--image"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><figure class=""><div class=""><img alt="Image2" class="w-full" height="1252" src="https://images.openai.com/blob/77c78c23-137a-452f-9c39-fbaa78b8dc8f/image2.png?trim=0,0,0,0&amp;width=10&amp;height=10&amp;quality=50" width="1999" /></div><figcaption class="f-caption-1 relative mt-8"></figcaption></figure></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>In the coming months, we plan to further improve the ability for developers to view their API usage and manage API keys, especially in larger organizations.</p><p>For the latest updates on OpenAI's APIs, follow us on X at <a href="https://twitter.com/OpenAIDevs" rel="noopener noreferrer" target="_blank">@OpenAIDevs</a>.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Tue, 23 Jan 2024 16:46:06 GMT</pubDate>
<pubDate>Tue, 23 Jan 2024 16:46:06 GMT</pubDate>
</item>
<item>
<title>How OpenAI is approaching 2024 worldwide elections</title>
<link>https://openai.com/blog/how-openai-is-approaching-2024-worldwide-elections</link>
<guid>https://openai.com/blog/how-openai-is-approaching-2024-worldwide-elections</guid>
<content:encoded><![CDATA[

<img alt="How OpenAI Is Approaching 2024 Worldwide Elections" src="https://images.openai.com/blob/d78a3b67-71f0-4487-901e-5b0099cbaa80/how-openai-is-approaching-2024-worldwide-elections.jpg?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Protecting the integrity of elections requires collaboration from every corner of the democratic process, and we want to make sure our technology is not used in a way that could undermine this process.&nbsp;</p><p>Our tools empower people to improve their daily lives and solve complex problems—from using AI to <a href="https://www.governor.pa.gov/newsroom/shapiro-administration-and-openai-launch-first-in-the-nation-generative-ai-pilot-for-commonwealth-employees/" rel="noopener noreferrer" target="_blank">enhance state services</a> to <a href="https://www.bostonglobe.com/2023/08/23/metro/can-chatgpt-help-with-medical-forms/" rel="noopener noreferrer" target="_blank">simplifying medical forms for patients</a>.</p><p>We want to make sure that our AI systems are built, deployed, and used <a href="https://openai.com/blog/our-approach-to-ai-safety" rel="noopener noreferrer" target="_blank">safely</a>. Like any new technology, these tools come with benefits and challenges. They are also unprecedented, and we will keep evolving our approach as we learn more about how our tools are used.</p><p>As we prepare for elections in 2024 across the world’s largest democracies, our approach is to continue our platform safety work by elevating accurate voting information, enforcing measured policies, and improving transparency.<strong> </strong>We have a cross-functional effort dedicated to election work, bringing together expertise from our safety systems, threat intelligence, legal, engineering, and policy teams to quickly investigate and address potential abuse.&nbsp;</p><p>The following are key initiatives our teams are investing in to prepare for elections this year:<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="preventing-abuse"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Preventing abuse</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We expect and aim for people to use our tools safely and responsibly, and elections are no different. We work to anticipate and prevent relevant abuse—such as misleading “deepfakes”, scaled influence operations, or chatbots impersonating candidates. Prior to releasing new systems, we red team them, engage users and external partners for feedback, and build safety mitigations to reduce the potential for harm. For years, we’ve been iterating on tools to improve factual accuracy, reduce bias, and decline certain requests. These tools provide a strong foundation for our work around election integrity. For instance, DALL·E has guardrails to decline requests that ask for image generation of real people, including candidates.</p><p>We regularly refine our <a href="https://openai.com/policies/usage-policies" rel="noopener noreferrer" target="_blank">Usage Policies</a> for ChatGPT and the API as we learn more about how people use or attempt to abuse our technology. A few to highlight for elections:&nbsp;</p><ul><li>We’re still working to understand how effective our tools might be for personalized persuasion. Until we know more, we don’t allow people to build applications for political campaigning and lobbying.&nbsp;</li><li>People want to know and trust that they are interacting with a real person, business, or government. For that reason, we don’t allow builders to create chatbots that pretend to be real people (e.g., candidates) or institutions (e.g., local government).&nbsp;</li><li>We don’t allow applications that deter people from participation in democratic processes—for example, misrepresenting voting processes and qualifications (e.g., when, where, or who is eligible to vote) or that discourage voting (e.g., claiming a vote is meaningless).</li><li>With our new GPTs, users can report potential violations to us.<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--image"><div class="mt-spacing-7"><div class="container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols relative"><figure class=""><div class=""><img alt="Report GPT Flow" class="w-full" height="1000" src="https://images.openai.com/blob/aca6067c-bb65-4248-bbbd-1ca102df7a89/report-gpt-flow.gif" width="1597" /></div><figcaption class="f-caption-1 relative mt-8 ui-richtext"><p>With our new GPTs, users can report potential violations to us.<br class="softbreak" /></p></figcaption></figure></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="transparency-around-ai-generated-content"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Transparency around AI-generated content</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Better transparency around image provenance—including the ability to detect which tools were used to produce an image—can empower voters to assess an image with trust and confidence in how it was made. We’re working on several provenance efforts. We implemented the&nbsp;<a href="https://c2pa.org/post/contentcredentials/" rel="noopener noreferrer" target="_blank">Coalition for Content Provenance and Authenticity’s&nbsp;</a>digital credentials—an approach that encodes details about the content’s provenance using cryptography—for&nbsp;<a href="https://help.openai.com/en/articles/8912793-c2pa-in-dall-e-3" rel="noopener noreferrer" target="_blank">images</a>&nbsp;generated by DALL·E 3.</p><p>We are also experimenting with a provenance classifier, a new tool for detecting images generated by DALL·E. Our internal testing has shown promising early results, even where images have been subject to common types of modifications. We plan to soon make it available to our first group of testers—including journalists, platforms, and researchers—for feedback. <br class="softbreak" /><br class="softbreak" />Finally, ChatGPT is increasingly integrating with existing sources of information—for example, users will start to get access to real-time news reporting globally, including attribution and links. Transparency around the origin of information and balance in news sources can help voters better assess information and decide for themselves what they can trust.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="improving-access-to-authoritative-voting-information"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Improving access to authoritative voting information</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>In the United States, we are working with the <a href="https://www.nass.org/can-I-vote" rel="noopener noreferrer" target="_blank">National Association of Secretaries of State</a> (NASS), the nation's oldest nonpartisan professional organization for public officials. ChatGPT will direct users to CanIVote.org, the authoritative website on US voting information, when asked certain procedural election related questions—for example, where to vote. Lessons from this work will inform our approach in other countries and regions.&nbsp;</p><p>We’ll have more to share in the coming months. We look forward to continuing to work with and learn from partners to anticipate and prevent potential abuse of our tools in the lead up to this year’s global elections.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Fri, 12 Jan 2024 21:01:35 GMT</pubDate>
<pubDate>Fri, 12 Jan 2024 21:01:35 GMT</pubDate>
</item>
<item>
<title>OpenAI and journalism</title>
<link>https://openai.com/blog/openai-and-journalism</link>
<guid>https://openai.com/blog/openai-and-journalism</guid>
<content:encoded><![CDATA[

<img alt="OpenAI and Journalism" src="https://images.openai.com/blob/dac215bf-e4e6-4ba8-ab65-361fd2e310ef/openai-and-journalism.jpg?trim=271%2C0%2C177%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Our goal is to develop AI tools that <a href="https://twitter.com/peakcooper/status/1639716822680236032?s=20" rel="noopener noreferrer" target="_blank">empower people</a> to solve problems that are otherwise out of reach. People worldwide are already using our technology to <a href="https://www.tiktok.com/@lucyedwards/video/7210863732203769093?lang=en" rel="noopener noreferrer" target="_blank">improve their daily lives</a>. Millions of developers and more than 92% of Fortune 500 are building on our products today.</p><p>While we disagree with the claims in The New York Times lawsuit, we view it as an opportunity to clarify our business, our intent, and how we build our technology. Our position can be summed up in these four points, which we flesh out below:</p><ol><li>We collaborate with news organizations and are creating new opportunities</li><li>Training is fair use, but we provide an opt-out because it’s the right thing to do</li><li>“Regurgitation” is a rare bug that we are working to drive to zero</li><li>The New York Times is not telling the full story<br class="softbreak" /></li></ol></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="1-we-collaborate-with-news-organizations-and-are-creating-new-opportunities"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">1. We collaborate with news organizations and are creating new opportunities</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We work hard in our technology design process to support news organizations. We’ve met with dozens, as well as leading industry organizations like the News/Media Alliance, to explore opportunities, discuss their concerns, and provide solutions. We aim to learn, educate, listen to feedback, and adapt.</p><p>Our goals are to support a healthy news ecosystem, be a good partner, and create mutually beneficial opportunities. With this in mind, we have pursued partnerships with news organizations to achieve these objectives:</p><ol><li>Deploy our products to benefit and support reporters and editors, by assisting with time-consuming tasks like analyzing voluminous public records and translating stories.</li><li>Teach our AI models about the world by training on additional historical, non-publicly available content.</li><li>Display real-time content with attribution in ChatGPT, providing new ways for news publishers to connect with readers.</li></ol><p>Our early partnerships with the <a href="https://www.ap.org/press-releases/2023/ap-open-ai-agree-to-share-select-news-content-and-technology-in-new-collaboration" rel="noopener noreferrer" target="_blank">Associated Press</a>, <a href="https://www.axelspringer.com/en/ax-press-release/axel-springer-and-openai-partner-to-deepen-beneficial-use-of-ai-in-journalism" rel="noopener noreferrer" target="_blank">Axel Springer</a>, <a href="https://theajp.org/news-insights/announcements/american-journalism-project-announces-new-partnership-with-openai-to-support-local-news/" rel="noopener noreferrer" target="_blank">American Journalism Project</a> and <a href="https://www.nyu.edu/about/news-publications/news/2023/august/arthur-l--carter-journalism-institute-launches-ethics-initiative.html" rel="noopener noreferrer" target="_blank">NYU</a> offer a glimpse into our approach.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="2-training-is-fair-use-but-we-provide-an-opt-out-because-it-s-the-right-thing-to-do"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">2. Training is fair use, but we provide an opt-out because it’s the right thing to do</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Training AI models using publicly available internet materials is fair use, as supported by long-standing and widely accepted precedents. We view this principle as fair to creators, necessary for innovators, and critical for US competitiveness.</p><p>The principle that training AI models is permitted as a fair use is supported by a wide range of <a href="https://www.regulations.gov/comment/COLC-2023-0006-8854" rel="noopener noreferrer" target="_blank">academics</a>, <a href="https://www.regulations.gov/comment/COLC-2023-0006-8452" rel="noopener noreferrer" target="_blank">library associations</a>, <a href="https://www.regulations.gov/comment/COLC-2023-0006-8735" rel="noopener noreferrer" target="_blank">civil</a> <a href="https://www.regulations.gov/comment/COLC-2023-0006-8554" rel="noopener noreferrer" target="_blank">society</a> <a href="https://www.regulations.gov/comment/COLC-2023-0006-8302" rel="noopener noreferrer" target="_blank">groups</a>, <a href="https://www.regulations.gov/comment/COLC-2023-0006-7623" rel="noopener noreferrer" target="_blank">startups</a>, <a href="https://www.regulations.gov/comment/COLC-2023-0006-8719" rel="noopener noreferrer" target="_blank">leading</a> <a href="https://www.regulations.gov/comment/COLC-2023-0006-8750" rel="noopener noreferrer" target="_blank">US</a> <a href="https://www.regulations.gov/comment/COLC-2023-0006-8594" rel="noopener noreferrer" target="_blank">companies</a>, <a href="https://www.regulations.gov/comment/COLC-2023-0006-8426" rel="noopener noreferrer" target="_blank">creators</a>, <a href="https://www.regulations.gov/comment/COLC-2023-0006-8976" rel="noopener noreferrer" target="_blank">authors</a>, and <a href="https://www.regulations.gov/comment/COLC-2023-0006-9057" rel="noopener noreferrer" target="_blank">others</a> that recently submitted comments to the US Copyright Office. Other regions and countries, including the <a href="https://eur-lex.europa.eu/eli/dir/2019/790/oj" rel="noopener noreferrer" target="_blank">European Union</a>, <a href="https://www.cric.or.jp/english/clj/cl2.html#:~:text=the%20Results%20Thereof)-,Article%2047%2D5,-(1)%E3%80%80A%20person" rel="noopener noreferrer" target="_blank">Japan</a>, <a href="https://sso.agc.gov.sg/Act/CA2021/Uncommenced/20231103112754?DocDate=20211007&amp;ValidDt=20240501&amp;ProvIds=pr243-,pr244-" rel="noopener noreferrer" target="_blank">Singapore</a>, and <a href="https://www.gov.il/BlobFolder/legalinfo/machine-learning/he/machine-learning.pdf" rel="noopener noreferrer" target="_blank">Israel</a> also have laws that permit training models on copyrighted content—an advantage for AI innovation, advancement, and investment.</p><p>That being said, legal right is less important to us than being good citizens. We have led the AI industry in providing a simple opt-out <a href="https://platform.openai.com/docs/gptbot" rel="noopener noreferrer" target="_blank">process</a> for publishers (which The New York Times adopted in August 2023) to prevent our tools from accessing their sites.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="3-regurgitation-is-a-rare-bug-that-we-are-working-to-drive-to-zero"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">3. “Regurgitation” is a rare bug that we are working to drive to zero</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Our models were designed and trained to learn concepts in order to apply them to <a href="https://openai.com/research/gpt-4#capabilities" rel="noopener noreferrer" target="_blank">new problems</a>.</p><p>Memorization is a rare failure of the learning process that we are continually making progress on, but it’s more common when particular content appears more than once in training data, like if pieces of it appear on lots of different public websites. So we have measures in place to limit inadvertent memorization and prevent regurgitation in model outputs. We also expect our users to act responsibly; intentionally manipulating our models to regurgitate is not an appropriate use of our technology and is against our terms of use.</p><p>Just as humans obtain a broad education to learn how to solve new problems, we want our AI models to observe the range of the world’s information, including from every language, culture, and industry. Because models learn from the enormous aggregate of human knowledge, any one sector—including news—is a tiny slice of overall training data, and any single data source—including The New York Times—is not significant for the model’s intended learning.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="4-the-new-york-times-is-not-telling-the-full-story"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">4. The New York Times is not telling the full story</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Our discussions with The New York Times had appeared to be progressing constructively through our last communication on December 19. The negotiations focused on a high-value partnership around real-time display with attribution in ChatGPT, in which The New York Times would gain a new way to connect with their existing and new readers, and our users would gain access to their reporting. We had explained to The New York Times that, like any single source, their content didn't meaningfully contribute to the training of our existing models and also wouldn't be sufficiently impactful for future training. Their lawsuit on December 27—which we learned about by reading The New York Times—came as a surprise and disappointment to us.</p><p>Along the way, they had mentioned seeing some regurgitation of their content but repeatedly refused to share any examples, despite our commitment to investigate and fix any issues. We’ve demonstrated how seriously we treat this as a priority, such as in July when we <a href="https://twitter.com/OpenAI/status/1676072388436594688" rel="noopener noreferrer" target="_blank">took down a ChatGPT feature</a> immediately after we learned it could reproduce real-time content in unintended ways.</p><p>Interestingly, the regurgitations The New York Times induced appear to be from years-old articles that have proliferated on <a href="https://incidentdatabase.ai/cite/267/" rel="noopener noreferrer" target="_blank">multiple</a> <a href="https://m.blog.naver.com/yhkim13/221777199209" rel="noopener noreferrer" target="_blank">third</a>-<a href="https://www.transtutors.com/questions/i-have-attached-the-file-below-here-i-need-a-distinction-quality-assignment-since-it-5775363.htm" rel="noopener noreferrer" target="_blank">party</a> <a href="https://www.punkinfinland.net/forum/viewtopic.php?t=106205&amp;start=375" rel="noopener noreferrer" target="_blank">websites</a>. It seems they intentionally manipulated prompts, often including lengthy excerpts of articles, in order to get our model to regurgitate. Even when using such prompts, our models don’t typically behave the way The New York Times insinuates, which suggests they either instructed the model to regurgitate or cherry-picked their examples from many attempts.</p><p>Despite their claims, this misuse is not typical or allowed user activity, and is not a substitute for The New York Times. Regardless, we are continually making our systems more resistant to adversarial attacks to regurgitate training data, and have already made much progress in our recent models.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We regard The New York Times’ lawsuit to be without merit. Still, we are hopeful for a constructive partnership with The New York Times and respect its long history, which includes reporting the <a href="https://www.nytimes.com/1958/07/13/archives/electronic-brain-teaches-itself.html" rel="noopener noreferrer" target="_blank">first working neural network</a> over 60 years ago and championing First Amendment freedoms.</p><p>We look forward to continued collaboration with news organizations, helping elevate their ability to produce quality journalism by realizing the transformative potential of AI.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Sun, 07 Jan 2024 00:04:41 GMT</pubDate>
<pubDate>Sun, 07 Jan 2024 00:04:41 GMT</pubDate>
</item>
<item>
<title>Introducing ChatGPT Team</title>
<link>https://openai.com/blog/introducing-chatgpt-team</link>
<guid>https://openai.com/blog/introducing-chatgpt-team</guid>
<content:encoded><![CDATA[

<img alt="ChatGPT Team" src="https://images.openai.com/blob/cc305f9e-cb40-426f-a8b8-b18ddd576588/team-hero-for-blog.jpg?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We launched ChatGPT Enterprise a few months ago and industry leaders like Block, Canva, Carlyle, The Estée Lauder Companies, PwC, and Zapier are already using it to redefine how their organizations operate. Today, we’re adding a new self-serve plan: <a href="https://openai.com/chatgpt/team" rel="noopener noreferrer" target="_blank">ChatGPT Team</a>.</p><p>ChatGPT Team offers access to our advanced models like GPT-4 and DALL·E 3, and tools like Advanced Data Analysis. It additionally includes a dedicated collaborative workspace for your team and admin tools for team management. As with ChatGPT Enterprise, you own and control your business data—we do not train on your business data or conversations, and our models don’t learn from your usage. More details on our data privacy practices can be found on our <a href="https://openai.com/enterprise-privacy" rel="noopener noreferrer" target="_blank">privacy page</a> and <a href="https://trust.openai.com/" rel="noopener noreferrer" target="_blank">Trust Portal</a>.</p></div></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><strong>ChatGPT Team includes</strong>:</p><ul><li>Access to GPT-4 with 32K context window</li><li>Tools like DALL·E 3, GPT-4 with Vision, Browsing, Advanced Data Analysis—with higher message caps</li><li>No training on your business data or conversations</li><li>Secure workspace for your team</li><li>Create and share custom GPTs with your workspace</li><li>Admin console for workspace and team management</li><li>Early access to new features and improvements<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--links"><div class="mt-spacing-6"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="flex flex-row items-center"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor] ml-16 first:ml-0" href="https://chat.openai.com/#pricing" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Start now</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="customize-chatgpt-for-any-type-of-work"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Customize ChatGPT for any type of work</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We recently <a href="https://openai.com/blog/introducing-gpts" rel="noopener noreferrer" target="_blank">announced GPTs</a>—custom versions of ChatGPT that you can create for a specific purpose with instructions, expanded knowledge, and custom capabilities. These can be especially useful for businesses and teams. With GPTs, you can customize ChatGPT to your team’s specific needs and workflows (no code required) and publish them securely to your team’s workspace. GPTs can help with a wide range of tasks, such as assisting in project management, team onboarding, generating code, performing data analysis, securely taking action in your existing systems and tools, or creating collateral to match your brand tone and voice. Today, we announced the <a href="http://chat.openai.com/gpts" rel="noopener noreferrer" target="_blank">GPT Store</a> where you can find useful and popular GPTs from your workspace.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="improve-team-efficiency-and-work-quality"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Improve team efficiency and work quality</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Integrating AI into everyday organizational workflows can make your team more productive. In a recent study by the Harvard Business School, employees at Boston Consulting Group who were given access to GPT-4 reported completing tasks 25% faster and achieved a 40% higher quality in their work as compared to their peers who did not have access.<span class="ui-fn"><sup class="inline-block min-w-[1.5ch] indent-0 not-italic [em_&amp;]:indent-2"><span class="error">[^study]</span></sup></span></p><p>Connor O’Brien, VP of GTM Strategy &amp; Operations at Sourcegraph, shares, "We use ChatGPT in almost every part of our business, from financial modeling for pricing and packaging to internal and external communications to board prep to recruiting and note taking—it’s accelerated everything we do allowing us to execute at a high level."</p><p>Dr. John Brownstein, Chief Innovation Officer at Boston Children’s Hospital says, “With ChatGPT Team, we’ve been able to pilot innovative GPTs that enhance our team’s productivity and collaboration. As we integrate GPTs safely and responsibly across internal operations, we know the transformative impact this will have in strengthening the systems that enable our doctors, researchers, students, and administrative staff to provide exceptional care to every patient that walks through our doors.”</p><p>ChatGPT Team costs $25/month per user when billed annually, or $30/month per user when billed monthly. You can <a href="https://openai.com/chatgpt/team" rel="noopener noreferrer" target="_blank">explore the details</a> or get started now by upgrading in your ChatGPT settings.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--links"><div class="mt-spacing-6"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="flex flex-row items-center"><a class="ui-button relative inline-block px-16 xs:pt-9 xs:pb-10 lg:pt-10 lg:pb-12 h-44 lg:h-48 border border-primary text-primary hover-hover:hover:bg-inverse hover-hover:hover:text-inverse active:bg-inverse active:text-inverse f-ui-1 ml-16 first:ml-0" href="https://openai.com/chatgpt/team"><span class="flex items-center justify-center"><span class="block">Explore ChatGPT Team</span></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor] ml-16 first:ml-0" href="https://chat.openai.com/#pricing" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Start now</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Sat, 06 Jan 2024 23:43:09 GMT</pubDate>
<pubDate>Sat, 06 Jan 2024 23:43:09 GMT</pubDate>
</item>
<item>
<title>Introducing the GPT Store</title>
<link>https://openai.com/blog/introducing-the-gpt-store</link>
<guid>https://openai.com/blog/introducing-the-gpt-store</guid>
<content:encoded><![CDATA[

<img alt="Gpt Store R3" src="https://images.openai.com/blob/0b8b7a8b-25ef-4d64-9dc6-6c26242dffbb/gpt-store-r3.png?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>It’s been two months since we&nbsp;<a href="https://openai.com/blog/introducing-gpts" rel="noopener noreferrer" target="_blank">announced&nbsp;</a>GPTs, and users have already created over 3 million custom versions of ChatGPT. Many builders have shared their GPTs for others to use. Today, we're starting to roll out the GPT Store to ChatGPT Plus, Team and Enterprise users so you can find useful and popular GPTs. Visit chat.openai.com/gpts to explore.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="discover-what-s-trending-in-the-store"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Discover what’s trending in the store</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>The store features a diverse range of GPTs developed by our partners and the community. Browse popular and trending GPTs on the community leaderboard, with categories like DALL·E, writing, research, programming, education, and lifestyle. <br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="new-featured-gpts-every-week"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">New featured GPTs every week</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We will also highlight useful and impactful GPTs. Some of our first featured GPTs include:</p><ul><li>Personalized trail recommendations from <a href="https://chat.openai.com/g/g-KpF6lTka3-alltrails" rel="noopener noreferrer" target="_blank">AllTrails</a></li><li>Search and synthesize results from 200M academic papers with <a href="https://chat.openai.com/g/g-bo0FiWLY7-researchgpt" rel="noopener noreferrer" target="_blank">Consensus</a></li><li>Expand your coding skills with Khan Academy’s <a href="https://chat.openai.com/g/g-HxPrv1p8v-code-tutor-khanmigo-lite" rel="noopener noreferrer" target="_blank">Code Tutor</a></li><li>Design presentations or social posts with <a href="https://chat.openai.com/g/g-alKfVrz9K-canva" rel="noopener noreferrer" target="_blank">Canva</a></li><li>Find your next read with <a href="https://chat.openai.com/g/g-z77yDe7Vu" rel="noopener noreferrer" target="_blank">Books</a></li><li>Learn math and science anytime, anywhere with the <a href="https://chat.openai.com/g/g-cEEXd8Dpb-ck-12-flexi" rel="noopener noreferrer" target="_blank">CK-12 Flexi</a> AI tutor<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="include-your-gpt-in-the-store"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Include your GPT in the store</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Building your own GPT is simple and doesn't require any coding skills.</p><p>If you’d like to share a GPT in the store, you’ll need to:</p><ol><li>Save your GPT for <strong>Everyone </strong>(<strong>Anyone with a link</strong> will not be shown in the store).</li><li>Verify your Builder Profile (<strong>Settings</strong> → <strong>Builder profile</strong> → <strong>Enable your name or a verified website</strong>).</li></ol><p>Please review our latest <a href="https://openai.com/policies/usage-policies" rel="noopener noreferrer" target="_blank">usage policies</a> and <a href="https://openai.com/brand#gpts-in-chatgpt" rel="noopener noreferrer" target="_blank">GPT brand guidelines</a> to ensure your GPT is compliant. To help ensure GPTs adhere to our policies, we've established a new review system in addition to the existing safety measures we've built into our products. The review process includes both human and automated review. Users are also<a href="https://help.openai.com/en/articles/8554982-how-can-i-report-an-inappropriate-gpt" rel="noopener noreferrer" target="_blank"> able to report</a> GPTs.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="builders-can-earn-based-on-gpt-usage"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Builders can earn based on GPT usage</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>In Q1 we will launch a GPT builder revenue program. As a first step, US builders will be paid based on user engagement with their GPTs. We'll provide details on the criteria for payments as we get closer.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="team-and-enterprise-customers-can-manage-gpts"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Team and Enterprise customers can manage GPTs</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Today, we announced our new <a href="http://openai.com/chatgpt/team" rel="noopener noreferrer" target="_blank">ChatGPT Team</a> plan for teams of all sizes. Team customers have access to a private section of the GPT Store which includes GPTs securely published to your workspace. The GPT Store will be available soon for <a href="https://openai.com/enterprise" rel="noopener noreferrer" target="_blank">ChatGPT Enterprise</a> customers and will include enhanced admin controls like choosing how internal-only GPTs are shared and which external GPTs may be used inside your business. Like all usage on ChatGPT Team and Enterprise, we do not use your conversations with GPTs to improve our models.</p><p>Explore GPTs at <a href="https://chat.openai.com/gpts" rel="noopener noreferrer" target="_blank">chat.openai.com/gpts</a>.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--links"><div class="mt-spacing-6"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="flex flex-row items-center"><a class="ui-button relative inline-block px-16 xs:pt-9 xs:pb-10 lg:pt-10 lg:pb-12 h-44 lg:h-48 border border-primary text-primary hover-hover:hover:bg-inverse hover-hover:hover:text-inverse active:bg-inverse active:text-inverse f-ui-1 ml-16 first:ml-0" href="https://chat.openai.com/gpts" rel="noopener" target="_blank"><span class="flex items-center justify-center"><span class="block">Explore GPTs</span><svg class="a-icon--arrow-north-east400 a-icon--text relative only:ml-0 a-icon--no-align top-[0.05em] f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Thu, 04 Jan 2024 23:33:54 GMT</pubDate>
<pubDate>Thu, 04 Jan 2024 23:33:54 GMT</pubDate>
</item>
<item>
<title>Superalignment Fast Grants</title>
<link>https://openai.com/blog/superalignment-fast-grants</link>
<guid>https://openai.com/blog/superalignment-fast-grants</guid>
<content:encoded><![CDATA[

<img alt="Superalignment Fast Grants" src="https://images.openai.com/blob/a351542d-47d2-4814-a5a5-810c19cc5450/superalignment-fast-grants.jpg?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We believe superintelligence could arrive within the next 10 years. These AI systems would have vast capabilities—they could be hugely beneficial, but also potentially pose large risks.</p><p>Today, we <a href="https://openai.com/research/instruction-following" rel="noopener noreferrer" target="_blank">align AI systems</a> to ensure they are safe using reinforcement learning from human feedback (RLHF). However, aligning future superhuman AI systems will pose fundamentally new and qualitatively different technical challenges.&nbsp;</p><p>Superhuman AI systems will be capable of complex and creative behaviors that humans cannot fully understand. For example, if a superhuman model generates a million lines of extremely complicated code, humans will not be able to reliably evaluate whether the code is safe or dangerous to execute. Existing alignment techniques like RLHF that rely on human supervision may no longer be sufficient. <strong>This leads to the fundamental challenge: how can humans steer and trust AI systems much smarter than them?&nbsp;</strong></p><p>This is one of the most important unsolved technical problems in the world. But we think it is solvable with a concerted effort. There are many promising approaches and exciting directions, with lots of low-hanging fruit. We think there is an enormous opportunity for the ML research community and individual researchers to make major progress on this problem today.&nbsp;</p><p>As part of our <a href="https://openai.com/blog/introducing-superalignment" rel="noopener noreferrer" target="_blank">Superalignment</a> project, we want to rally the best researchers and engineers in the world to meet this challenge—and we’re especially excited to bring new people into the field.<br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="superalignment-fast-grants"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Superalignment Fast Grants</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>In partnership with Eric Schmidt, we are launching a $10M grants program to support technical research towards ensuring superhuman AI systems are aligned and safe:</p><ul><li>We are offering $100K–$2M grants for academic labs, nonprofits, and individual researchers.</li><li>For graduate students, we are sponsoring a one-year <strong>$150K OpenAI Superalignment Fellowship:</strong> $75K in stipend and $75K in compute and research funding.</li><li>No prior experience working on alignment is required; we are actively looking to support researchers who are excited to work on alignment for the first time.</li><li>Our application process is simple, and we’ll get back to you within four weeks of applications closing.&nbsp;<span class="ql-anchor" id="docs-internal-guid-f73c13be-7fff-4b8a-ae69-df10fb5543f6"><br class="softbreak" /></span></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--links"><div class="mt-spacing-6"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><div class="flex flex-row items-center"><a class="ui-button relative inline-block px-16 xs:pt-9 xs:pb-10 lg:pt-10 lg:pb-12 h-44 lg:h-48 border border-primary text-primary hover-hover:hover:bg-inverse hover-hover:hover:text-inverse active:bg-inverse active:text-inverse f-ui-1 ml-16 first:ml-0" href="https://airtable.com/appnIXmOlWAJBzrJp/paghnoKL6EHiKmKbf/form" rel="noopener" target="_blank"><span class="flex items-center justify-center"><span class="block">Apply by February 18</span><svg class="a-icon--arrow-north-east400 a-icon--text relative only:ml-0 a-icon--no-align top-[0.05em] f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>With these grants, we are particularly interested in funding the <a href="https://openai.notion.site/Research-directions-0df8dd8136004615b0936bf48eb6aeb8" rel="noopener noreferrer" target="_blank">following research directions</a>:</p><ul><li><strong>Weak-to-strong generalization</strong>: Humans will be weak supervisors relative to superhuman models. Can we understand and control how strong models <a href="https://openai.com/research/weak-to-strong-generalization" rel="noopener noreferrer" target="_blank">generalize from weak supervision</a>?&nbsp;</li><li><strong>Interpretability</strong>: How can we understand model internals? And can we use this to e.g. build an AI lie detector?</li><li><strong>Scalable oversight</strong>: How can we use AI systems to assist humans in evaluating the outputs of other AI systems on complex tasks?</li><li>Many other research directions, including but not limited to: <strong>honesty</strong>, <strong>chain-of-thought faithfulness</strong>, <strong>adversarial robustness</strong>, <strong>evals and testbeds</strong>, and more.</li></ul><p>For more on the research directions, FAQs, and other details, see our <a href="https://openai.notion.site/Superalignment-Fast-Grants-and-OpenAI-Generalization-Prizes-fd12c66a286a4cbc9dc0f2fef1c62e92" rel="noopener noreferrer" target="_blank">Superalignment Fast Grants page</a>. <br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="join-us-in-this-challenge"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Join us in this challenge</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p><strong>We think new researchers could make enormous contributions! </strong>This is a young field with many tractable research problems; outstanding contributions could not just help shape the field, but be critical for the future of AI. There has never been a better time to start working on alignment.<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Wed, 13 Dec 2023 22:50:25 GMT</pubDate>
<pubDate>Wed, 13 Dec 2023 22:50:25 GMT</pubDate>
</item>
<item>
<title>Democratic inputs to AI grant program: lessons learned and implementation plans</title>
<link>https://openai.com/blog/democratic-inputs-to-ai-grant-program-update</link>
<guid>https://openai.com/blog/democratic-inputs-to-ai-grant-program-update</guid>
<content:encoded><![CDATA[

<img alt="Democratic Inputs To AI Grant Program Update" src="https://images.openai.com/blob/f50ce1d2-4f61-4ed2-86ca-556773bee5ee/democratic-inputs-to-ai-grant-program-update.png?trim=0%2C0%2C0%2C0&amp;width=1000&amp;quality=80" /> <div class="ui-blocks" id="content"><div class="ui-block ui-block--text"><div class="mt-spacing-7"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>As AI gets more advanced and widely used, it is essential to involve the public in deciding <a href="https://openai.com/blog/how-should-ai-systems-behave" rel="noopener noreferrer" target="_blank">how AI should behave</a> in order to better <a href="https://openai.com/research/learning-from-human-preferences" rel="noopener noreferrer" target="_blank">align</a> our models to the values of humanity. In May, we announced the <a href="https://openai.com/blog/democratic-inputs-to-ai" rel="noopener noreferrer" target="_blank">Democratic Inputs to AI grant program</a>. We then awarded $100,000 to 10 teams out of nearly 1000 applicants to design, build, and test ideas that use democratic methods to decide the rules that govern AI systems. Throughout, the teams tackled challenges like recruiting diverse participants across the digital divide, producing a coherent output that represents diverse viewpoints, and designing processes with sufficient transparency to be trusted by the public.&nbsp;</p><p>At OpenAI, we’ll build on this momentum by designing an end-to-end process for collecting inputs from external stakeholders and using those inputs to train and shape the behavior of our models. We’re excited to combine our research with ideas and prototypes developed by the grant teams in the coming months.</p><p>In this update, we will cover:</p><ul><li>How our grant recipients innovated on democratic technology</li><li>Key learnings from the grant program</li><li>Our implementation plans<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="how-our-grant-recipients-innovated-on-democratic-technology"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">How our grant recipients innovated on democratic technology</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>We received nearly 1,000 applications across 113 countries. There were far more than 10 qualified teams, but a joint committee of OpenAI employees and external experts in democratic governance selected the final 10 teams to span a set of diverse backgrounds and approaches: the chosen teams have members from 12 different countries and their expertise spans various fields, including law, journalism, peace-building, machine learning, and social science research.</p><p>During the program, teams received hands-on support and guidance. To facilitate collaboration, teams were encouraged to describe and document their processes in a structured way (via “process cards” and “run reports”). This enabled faster iteration and easier identification of opportunities to integrate with other teams’ prototypes. Additionally, OpenAI facilitated a special <a href="https://vimeo.com/875039398/c777de0595" rel="noopener noreferrer" target="_blank">Demo Day</a> in September for the teams to showcase their concepts to one another, OpenAI staff, and researchers from other AI labs and academia.&nbsp;</p><p>The projects spanned different aspects of participatory engagement, such as novel video deliberation interfaces, platforms for crowdsourced audits of AI models, mathematical formulations of representation guarantees, and approaches to map beliefs to dimensions that can be used to fine-tune model behavior. Notably, across nearly all projects, AI itself played a <a href="https://arxiv.org/abs/2306.11932" rel="noopener noreferrer" target="_blank">useful role as a part of the processes</a> in the form of customized chat interfaces, voice-to-text transcription, data synthesis, and more.&nbsp;</p><p>Today, along with lessons learned, we share <a href="https://github.com/openai/democratic-inputs/tree/main" rel="noopener noreferrer" target="_blank">the code that teams created for this grant program</a>, and&nbsp;present brief summaries of the work accomplished by each of the ten teams:</p><p><br class="softbreak" /></p></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet ui-block--snug" id="DemocraticInputsToAIProcessBox-3"><div class="mt-spacing-7"><div class=""><div class="w-full"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-8-cols lg:w-12-cols"><div class="pb-16 md:pb-32"><div class="grid grid-cols-12 border-t border-primary pt-16 md:pt-32"><div class="col-span-11 grid grid-cols-1 md:grid-cols-2"><div class="pb-16 lg:pb-32"><h2 class="f-heading-3 pr-16 md:pr-32">⚖️ Case Law for AI Policy</h2><div class="flex flex-row gap-16 pt-8 md:pt-12"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://social.cs.washington.edu/case-law-ai-policy/" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Report</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="mailto: sfl-case-law@cs.washington.edu" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Contact</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div><p class="f-body-1 pb-16 lg:pb-32 pr-16">Creating a robust case repository around AI interaction scenarios that can be used to make case-law-inspired judgments through a process that democratically engages experts, laypeople, and key stakeholders.</p></div><div class="col-span-1 justify-self-end"><button><svg class="a-icon--plus600 a-icon--small" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="14 7 9 7 9 2 7 2 7 7 2 7 2 9 7 9 7 14 9 14 9 9 14 9 14 7"></polygon></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet ui-block--snug ui-block--next-snug" id="DemocraticInputsToAIProcessBox-4"><div class="mt-spacing-7"><div class=""><div class="w-full"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-8-cols lg:w-12-cols"><div class="pb-16 md:pb-32"><div class="grid grid-cols-12 border-t border-primary pt-16 md:pt-32"><div class="col-span-11 grid grid-cols-1 md:grid-cols-2"><div class="pb-16 lg:pb-32"><h2 class="f-heading-3 pr-16 md:pr-32">💬 Collective Dialogues for Democratic Policy Development</h2><div class="flex flex-row gap-16 pt-8 md:pt-12"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://arxiv.org/pdf/2311.02242.pdf" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Report</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://forms.gle/r1ynWC2Q92teCwtz6" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Contact</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div><p class="f-body-1 pb-16 lg:pb-32 pr-16">Developing policies that reflect informed public will using collective dialogues to efficiently scale democratic deliberation and find areas of consensus.</p></div><div class="col-span-1 justify-self-end"><button><svg class="a-icon--plus600 a-icon--small" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="14 7 9 7 9 2 7 2 7 7 2 7 2 9 7 9 7 14 9 14 9 9 14 9 14 7"></polygon></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet ui-block--snug ui-block--next-snug" id="DemocraticInputsToAIProcessBox-5"><div class="mt-spacing-7"><div class=""><div class="w-full"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-8-cols lg:w-12-cols"><div class="pb-16 md:pb-32"><div class="grid grid-cols-12 border-t border-primary pt-16 md:pt-32"><div class="col-span-11 grid grid-cols-1 md:grid-cols-2"><div class="pb-16 lg:pb-32"><h2 class="f-heading-3 pr-16 md:pr-32">🤝 Deliberation at Scale: Socially democratic inputs to AI</h2><div class="flex flex-row gap-16 pt-8 md:pt-12"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://findcommonground.online/top-level-pages/final-report-democratic-inputs-to-ai" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Report</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://findcommonground.online/top-level-pages/contact-us" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Contact</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div><p class="f-body-1 pb-16 lg:pb-32 pr-16">Enabling democratic deliberation in small group conversations conducted via AI-facilitated video calls.</p></div><div class="col-span-1 justify-self-end"><button><svg class="a-icon--plus600 a-icon--small" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="14 7 9 7 9 2 7 2 7 7 2 7 2 9 7 9 7 14 9 14 9 9 14 9 14 7"></polygon></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet ui-block--snug ui-block--next-snug" id="DemocraticInputsToAIProcessBox-6"><div class="mt-spacing-7"><div class=""><div class="w-full"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-8-cols lg:w-12-cols"><div class="pb-16 md:pb-32"><div class="grid grid-cols-12 border-t border-primary pt-16 md:pt-32"><div class="col-span-11 grid grid-cols-1 md:grid-cols-2"><div class="pb-16 lg:pb-32"><h2 class="f-heading-3 pr-16 md:pr-32">🦉 Democratic Fine-Tuning</h2><div class="flex flex-row gap-16 pt-8 md:pt-12"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://meaningalignment.substack.com/p/the-first-moral-graph" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Report</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://www.meaningalignment.org/" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Website</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="mailto: hello@meaningalignment.org" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Contact</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div><p class="f-body-1 pb-16 lg:pb-32 pr-16">Eliciting values from participants in a chat dialogue in order to create a moral graph of values that can be used to fine-tune models.</p></div><div class="col-span-1 justify-self-end"><button><svg class="a-icon--plus600 a-icon--small" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="14 7 9 7 9 2 7 2 7 7 2 7 2 9 7 9 7 14 9 14 9 9 14 9 14 7"></polygon></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet ui-block--snug ui-block--next-snug" id="DemocraticInputsToAIProcessBox-7"><div class="mt-spacing-7"><div class=""><div class="w-full"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-8-cols lg:w-12-cols"><div class="pb-16 md:pb-32"><div class="grid grid-cols-12 border-t border-primary pt-16 md:pt-32"><div class="col-span-11 grid grid-cols-1 md:grid-cols-2"><div class="pb-16 lg:pb-32"><h2 class="f-heading-3 pr-16 md:pr-32">⚡ Energize AI: Aligned - a Platform for Alignment</h2><div class="flex flex-row gap-16 pt-8 md:pt-12"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://energize.ai/openai/" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Report</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://energize.ai/" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Website</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="mailto:ethan@energize.ai" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Contact</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div><p class="f-body-1 pb-16 lg:pb-32 pr-16">Developing guidelines for aligning AI models with live, large-scale participation and a 'community notes' algorithm.</p></div><div class="col-span-1 justify-self-end"><button><svg class="a-icon--plus600 a-icon--small" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="14 7 9 7 9 2 7 2 7 7 2 7 2 9 7 9 7 14 9 14 9 9 14 9 14 7"></polygon></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet ui-block--snug ui-block--next-snug" id="DemocraticInputsToAIProcessBox-8"><div class="mt-spacing-7"><div class=""><div class="w-full"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-8-cols lg:w-12-cols"><div class="pb-16 md:pb-32"><div class="grid grid-cols-12 border-t border-primary pt-16 md:pt-32"><div class="col-span-11 grid grid-cols-1 md:grid-cols-2"><div class="pb-16 lg:pb-32"><h2 class="f-heading-3 pr-16 md:pr-32">👫 Generative Social Choice</h2><div class="flex flex-row gap-16 pt-8 md:pt-12"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="http://procaccia.info/GenSoc_OpenAI_report.pdf" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Report</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="mailto: generative.social.choice@gmail.com" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Contact</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div><p class="f-body-1 pb-16 lg:pb-32 pr-16">Distilling a large number of free-text opinions into a concise slate that guarantees fair representation using mathematical arguments from social choice theory.</p></div><div class="col-span-1 justify-self-end"><button><svg class="a-icon--plus600 a-icon--small" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="14 7 9 7 9 2 7 2 7 7 2 7 2 9 7 9 7 14 9 14 9 9 14 9 14 7"></polygon></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet ui-block--snug ui-block--next-snug" id="DemocraticInputsToAIProcessBox-9"><div class="mt-spacing-7"><div class=""><div class="w-full"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-8-cols lg:w-12-cols"><div class="pb-16 md:pb-32"><div class="grid grid-cols-12 border-t border-primary pt-16 md:pt-32"><div class="col-span-11 grid grid-cols-1 md:grid-cols-2"><div class="pb-16 lg:pb-32"><h2 class="f-heading-3 pr-16 md:pr-32">🌎 Inclusive.AI: Engaging Underserved Populations in Democratic Decision-Making on AI</h2><div class="flex flex-row gap-16 pt-8 md:pt-12"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://socialcomputing.web.illinois.edu/images/Report-InclusiveAI.pdf" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Report</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://medium.com/@tanusreesharma207/inclusive-ai-engaging-underserved-populations-in-democratic-decision-making-on-ai-8ca6b108e4e3" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Blog post</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://socialcomputing.web.illinois.edu/inclusiveai.html#team" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Contact</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div><p class="f-body-1 pb-16 lg:pb-32 pr-16">Facilitating decision-making processes related to AI using a platform with decentralized governance mechanisms (e.g., a DAO) that empower underserved groups.</p></div><div class="col-span-1 justify-self-end"><button><svg class="a-icon--plus600 a-icon--small" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="14 7 9 7 9 2 7 2 7 7 2 7 2 9 7 9 7 14 9 14 9 9 14 9 14 7"></polygon></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet ui-block--snug ui-block--next-snug" id="DemocraticInputsToAIProcessBox-10"><div class="mt-spacing-7"><div class=""><div class="w-full"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-8-cols lg:w-12-cols"><div class="pb-16 md:pb-32"><div class="grid grid-cols-12 border-t border-primary pt-16 md:pt-32"><div class="col-span-11 grid grid-cols-1 md:grid-cols-2"><div class="pb-16 lg:pb-32"><h2 class="f-heading-3 pr-16 md:pr-32">📰 Making AI Transparent and Accountable by Rappler</h2><div class="flex flex-row gap-16 pt-8 md:pt-12"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://www.rappler.com/technology/features/generative-ai-use-enriching-democratic-consultations/" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Report</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="mailto: openai-experiments@rappler.com" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Contact</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div><p class="f-body-1 pb-16 lg:pb-32 pr-16">Enabling discussion and understanding of participants' views on complex, polarizing topics via linked offline and online processes.</p></div><div class="col-span-1 justify-self-end"><button><svg class="a-icon--plus600 a-icon--small" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="14 7 9 7 9 2 7 2 7 7 2 7 2 9 7 9 7 14 9 14 9 9 14 9 14 7"></polygon></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet ui-block--snug ui-block--next-snug" id="DemocraticInputsToAIProcessBox-11"><div class="mt-spacing-7"><div class=""><div class="w-full"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-8-cols lg:w-12-cols"><div class="pb-16 md:pb-32"><div class="grid grid-cols-12 border-t border-primary pt-16 md:pt-32"><div class="col-span-11 grid grid-cols-1 md:grid-cols-2"><div class="pb-16 lg:pb-32"><h2 class="f-heading-3 pr-16 md:pr-32">🎨 Ubuntu-AI: A Platform for Equitable and Inclusive Model Training</h2><div class="flex flex-row gap-16 pt-8 md:pt-12"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://www.researchgate.net/publication/374870830_Interim_Report_for_Ubuntu-AI_A_Bottom-up_Approach_to_More_Democratic_and_Equitable_Training_and_Outcomes_for_Machine_Learning" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Report</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://generativejustice.org/contact-2/" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Contact</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div><p class="f-body-1 pb-16 lg:pb-32 pr-16">Returning value to those who help create it while facilitating LLM development and ensuring more inclusive knowledge of African creative work.</p></div><div class="col-span-1 justify-self-end"><button><svg class="a-icon--plus600 a-icon--small" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="14 7 9 7 9 2 7 2 7 7 2 7 2 9 7 9 7 14 9 14 9 9 14 9 14 7"></polygon></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--code-snippet ui-block--snug" id="DemocraticInputsToAIProcessBox-12"><div class="mt-spacing-7"><div class=""><div class="w-full"><div class="container"><div class="cols-container"><div class="xs:w-6-cols md:w-8-cols lg:w-12-cols"><div class="pb-16 md:pb-32"><div class="grid grid-cols-12 border-t border-primary pt-16 md:pt-32"><div class="col-span-11 grid grid-cols-1 md:grid-cols-2"><div class="pb-16 lg:pb-32"><h2 class="f-heading-3 pr-16 md:pr-32">🔁 vTaiwan and Chatham House: Bridging the Recursive Public</h2><div class="flex flex-row gap-16 pt-8 md:pt-12"><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Report</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="https://www.recursivepublic.com/" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Website</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a><a class="ui-link group f-ui-1 inline-block ui-link--underline relative text-[currentColor]" href="mailto: akrasodomski@chathamhouse.org" rel="noopener" target="_blank"><span class="flex items-center"><span class="underline-thickness-1 underline-offset-4 underline">Contact</span><svg class="a-icon--arrow-north-east400 a-icon--text a-icon--no-align top-[0.05em] relative f-ui-1 ml-2 -mr-4" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="5 4.31 5 5.69 9.33 5.69 2.51 12.51 3.49 13.49 10.31 6.67 10.31 11 11.69 11 11.69 4.31 5 4.31"></polygon></svg></span></a></div></div><p class="f-body-1 pb-16 lg:pb-32 pr-16">Using an adapted vTaiwan methodology to create a recursive, connected participatory process for AI.</p></div><div class="col-span-1 justify-self-end"><button><svg class="a-icon--plus600 a-icon--small" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><polygon fill="currentColor" points="14 7 9 7 9 2 7 2 7 7 2 7 2 9 7 9 7 14 9 14 9 9 14 9 14 7"></polygon></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="key-learnings-from-the-grant-program-so-far"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Key learnings from the grant program so far</h2></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-6" id="public-opinion-can-change-frequently"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">Public opinion can change frequently</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Teams captured views in multiple ways. Many teams found that public views changed often.</p><ul><li>The<strong> Democratic Fine-Tuning </strong>team created a chatbot that presented scenarios to participants and produced “value cards” that participants could review and evaluate. The <strong>Case Law </strong>team held expert workshops, and represented their opinions as a set of dimensions and considerations over a specific set of scenarios. The<strong> Inclusive.AI</strong> team captured both statements and how strongly people felt about these statements by allowing them to distribute voting tokens across many statements (versus a single vote). Many other teams presented statements accompanied by the proportion of participants in support.&nbsp;</li><li>Interestingly, many teams found that public opinion changed frequently, even day-to-day, which could have meaningful implications for how frequently input-collecting processes should take place. A collective process should be thorough enough to capture hard-to-change and perhaps more fundamental values, and simultaneously be sensitive enough (or recur frequently enough) to detect meaningful changes of views over time.<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-6" id="bridging-across-the-digital-divide-is-still-difficult-and-this-can-skew-results"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">Bridging across the digital divide is still difficult and this can skew results</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Reaching relevant participants across digital and cultural divides might require additional investments in better outreach and better tooling.&nbsp;</p><ul><li>Some teams found that participants recruited online leaned more optimistic toward AI, a trait that was correlated with increased support and enthusiasm for AI model behavior in general.&nbsp;</li><li>Furthermore, due to lack of reach or availability on most platforms we consulted, most teams faced serious difficulty in recruiting participants across the digital divide.</li><li>More subtly, even when citizens of global majority countries are included, the tools might be less useful to them due to limitations in understanding the local language or context. For example, in their online and onground focus group discussions, the <strong>Rappler</strong> team found that the documented <a href="https://www.rappler.com/technology/features/generative-ai-use-enriching-democratic-consultations/" rel="noopener noreferrer" target="_blank">disparities in performance across languages</a> of readily available speech recognition tools like <a href="https://openai.com/research/whisper" rel="noopener noreferrer" target="_blank">Whisper</a> made transcription difficult in participants’ spoken languages, e.g. Tagalog, Binisaya, Hiligaynon, which are major Filipino languages.</li><li>The <strong>Ubuntu-AI</strong> team<strong> </strong>chose to directly incentivize participation, by developing a platform that allows African creatives to receive compensation for contributing to machine learning about their own designs and backgrounds.<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-6" id="finding-agreement-within-polarized-groups"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">Finding agreement within polarized groups</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Finding a compromise can be hard when a small group has strong opinions on a particular issue.</p><ul><li>The <strong>Collective Dialogues </strong>team found that each session always contained a small group of people who felt strongly that restricting AI assistants from answering certain questions was wrong no matter what. In this case, because the group was small, majority voting yielded outcomes that they strongly disagreed with.&nbsp;</li><li>The <strong>Collective Dialogues</strong>, <strong>Energize.AI</strong>, and <strong>Recursive Public </strong>teams’<strong> </strong>processes were designed to find policy proposals that would be strongly supported across polarized groups. For example, all policy guidelines generated by the <strong>Collective Dialogues</strong> process with U.S. participants —including on vaccine information, a known divisive issue— had over 72% support across Democrats, Independents, and Republicans.<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-6" id="reaching-consensus-vs-representing-diversity"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">Reaching consensus vs. representing diversity</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>When trying to produce a single outcome or make a single decision to represent a group, there might be tension between trying to reach consensus and adequately representing the diversity of various opinions. It’s not just about siding with the majority, but also giving a platform to different viewpoints.&nbsp;</p><ul><li>The <strong>Generative Social Choice</strong> team devised a method that highlights a few key positions, showcasing the range of opinions while finding some common ground. They used mathematical theory to help navigate this balance.&nbsp;</li><li>Meanwhile, the<strong> Inclusive.AI </strong>team investigated different voting mechanisms and how they are perceived. They found that methods which show how strongly people feel about their choices, and that ensure everyone has an equal say, are perceived as more democratic and fair.<br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-6" id="hopes-and-anxieties-about-the-future-of-ai-governance"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h3 class="f-heading-4">Hopes and anxieties about the future of AI governance</h3></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-3"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Some participants felt nervous about the use of AI in writing policy and would like transparency regarding when and how AI is applied in democratic processes. Post-deliberation sessions, many teams found that participants became more hopeful about the public's ability to help guide AI.</p><ul><li>In collaborations with a municipal government and roundtables with various stakeholders, both the<strong> Deliberation at Scale </strong>and<strong> Recursive Public</strong> teams<strong> </strong>found that while there is clear interest in the role AI itself might play in improving democratic processes, there is also an air of caution around how much power or influence democratic institutions should grant to these systems and their developers.&nbsp;</li><li>The <strong>Collective Dialogues</strong> team found that combining AI in a decision making process with non-AI decision steps – like expert curation of AI-generated policy clauses, or a final vote on a policy informed by AI – resulted in a process that had AI-enabled efficiency while still being perceived as trustworthy and legitimate by the public.&nbsp;</li><li>In the <strong>Collective Dialogues</strong> team’s process, a popular clause emerged during deliberations – across different participant groups –&nbsp;which roughly states that the chosen policy should be “expanded on and updated regularly as new issues arise, better understanding is developed, and AI's capabilities evolve.” On average, this clause was supported by 85% of participants.&nbsp;</li><li>The <strong>Collective Dialogues </strong>and <strong>Deliberation at Scale</strong> teams found that the act of participating in a deliberation session on an AI policy issue made people more likely to think the public was capable of helping guide AI behavior in general. <br class="softbreak" /></li></ul></div></div></div></div></div></div></div><div class="ui-block ui-block--heading"><div class="mt-spacing-7" id="our-implementation-plans"><div class="container"><div class="cols-container"><div class="md:w-6-cols lg:ml-2-cols lg:w-6-cols"><h2 class="f-heading-3">Our implementation plans</h2></div></div></div></div></div><div class="ui-block ui-block--text"><div class="mt-spacing-4"><div class="container"><div class="cols-container"><div class="xs:w-12-cols md:w-6-cols lg:ml-2-cols lg:w-6-cols relative f-body-1"><div class="ui-richtext"><div><p>Our goal is to design systems that incorporate public inputs to steer powerful AI models while addressing the above challenges. To help ensure that we continue to make progress on this research, we are forming a “Collective Alignment” team consisting of researchers and engineers that will:</p><ul><li>Implement a system for collecting and encoding public input on model behavior into our systems.</li><li>Continue to work with external advisors and grant teams, including running pilots to incorporate the grant prototypes into steering our models.</li></ul><p>We are recruiting exceptional research engineers from diverse technical backgrounds to help build this work with us. If you’re excited about what we’re doing and would like to apply, <a href="https://openai.com/careers/research-engineer-collective-alignment" rel="noopener noreferrer" target="_blank">please apply to join us</a>!<br class="softbreak" /></p></div></div></div></div></div></div></div></div>
]]></content:encoded>
<pubDate>Mon, 13 Nov 2023 23:44:18 GMT</pubDate>
<pubDate>Mon, 13 Nov 2023 23:44:18 GMT</pubDate>
</item>

</channel>
</rss>