<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>ByteByteGo Newsletter</title>
<link>https://blog.bytebytego.com</link>


<item>
<title>A Pattern Every Modern Developer Should Know: CQRS</title>
<link>https://blog.bytebytego.com/p/a-pattern-every-modern-developer</link>
<guid>https://blog.bytebytego.com/p/a-pattern-every-modern-developer</guid>
<content:encoded><![CDATA[
<div> CQRS, Command Query Responsibility Segregation, Greg Young, 2010, Command-Query Separation<br />
<br />
总结:<br />
CQRS是一种架构模式，将数据的读写分离成命令和查询两个不同的部分。由Greg Young在2010年首次引入，其核心概念涵盖命令端和查询端的责任分离，继承了CQS原则。CQRS在领域驱动设计和事件驱动架构中得到广泛应用，适用于需要考虑性能、扩展性和复杂性的应用场景。核心概念包括命令、查询、分离和分层，决策矩阵可用于确定什么时候应用CQRS。 <div>
<p>CQRS, which stands for Command Query Responsibility Segregation, is an architectural pattern that separates the concerns of reading and writing data.&nbsp;</p><p>It divides an application into two distinct parts:&nbsp;</p><ul><li><p><strong>The Command Side:</strong> Responsible for managing create, update, and delete requests.&nbsp;</p></li><li><p><strong>The Query Side:</strong> Responsible for handling read requests.</p></li></ul><p>The CQRS pattern was first introduced by Greg Young, a software developer and architect, in 2010. He described it as a way to separate the responsibility of handling commands (write operations) from handling queries (read operations) in a system.</p><p>The origins of CQRS can be traced back to the Command-Query Separation (CQS) principle, introduced by Bertrand Meyer. CQS states that every method should either be a command that performs an action or a query that returns data, but not both. CQRS takes the CQS principle further by applying it at an architectural level, separating the command and query responsibilities into different models, services, or even databases.</p><p>Since its introduction, CQRS has gained popularity in the software development community, particularly in the context of domain-driven design (DDD) and event-driven architectures.&nbsp;</p><p>It has been successfully applied in various domains, such as e-commerce, financial systems, and collaborative applications, where performance, scalability, and complexity are critical concerns.</p><p>In this post, we&#8217;ll learn about CQRS in comprehensive detail. We will cover the various aspects of the pattern along with a decision matrix on when to use it.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdc0d3d5-8453-4920-ab8a-c31032216a84_1460x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1596" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffdc0d3d5-8453-4920-ab8a-c31032216a84_1460x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>Core Concepts of CQRS</h2><p>The overall CQRS pattern is made up of a few core concepts:</p>
      <p>
          <a href="https://blog.bytebytego.com/p/a-pattern-every-modern-developer">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 07 Nov 2024 16:30:27 GMT</pubDate>
</item>
<item>
<title>How McDonald Sells Millions of Burgers Per Day With Event-Driven Architecture</title>
<link>https://blog.bytebytego.com/p/how-mcdonald-sells-millions-of-burgers</link>
<guid>https://blog.bytebytego.com/p/how-mcdonald-sells-millions-of-burgers</guid>
<content:encoded><![CDATA[
<div> McDonald's, AWS, event-driven architecture, scalability, schema registry
总结:<br /><br />这篇文章介绍了麦当劳的事件驱动架构设计，通过AWS实现大规模全球事件处理平台。架构采用AWS MSK、架构注册表等组件，实现了可伸缩性、可靠性和简单性。关键挑战包括数据治理、集群自动扩缩容和基于领域的分片。最佳实践包括统一SDK、集中化架构管理、强大的错误处理等。未来计划包括正式事件规范支持、迁移至无服务器MSK等改进。整体而言，该平台在保持核心设计原则的同时不断完善，以满足日益增长的事件量需求。 <div>
<h2><a href="https://bit.ly/Datadog_110524">Cloud-scale monitoring with AWS and Datadog (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Datadog_110524" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1201" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b25b5ad-9ef5-4d22-94c2-08fe21b086fc_1201x1201.png" width="1201" /><div></div></div></a></figure></div><p>In this eBook, you&#8217;ll learn about the benefits of migrating workloads to AWS and how to get deep visibility into serverless and containerized applications with Datadog.<br /><br />You&#8217;ll also learn how to:</p><ul><li><p>Plan and track every stage of your migration to AWS</p></li><li><p>Monitor your entire serverless architecture in one place</p></li><li><p>Ensure your AWS container workloads are operating efficiently</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Datadog_110524"><span>Download the ebook</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the McDonald&#8217;s Technical Blog. All credit for the technical details goes to the McDonald&#8217;s engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>Over the years, McDonald&#8217;s has undergone a significant digital transformation to enhance customer experiences, strengthen its brand, and optimize overall operations.&nbsp;</p><p>At the core of this transformation is a robust technological infrastructure that unifies processes across various channels and touchpoints throughout their global operations.</p><p>The need for unified event processing emerged from McDonald's extensive digital ecosystem, where events are utilized across the technology stack. There were three key processing types:</p><ul><li><p>Asynchronous operations</p></li><li><p>Transactional processing</p></li><li><p>Analytical data handling</p></li></ul><p>The events were used across use cases such as mobile-order progress tracking and sending customers marketing communications (deals and promotions).</p><p>Coupled with the scale of McDonald&#8217;s operations, the system needed an architecture that could handle:</p><ul><li><p>Global deployment requirements</p></li><li><p>Real-time event processing</p></li><li><p>Cross-channel integration</p></li><li><p>High-volume transaction processing</p></li></ul><p>In this article, we&#8217;re going to look at McDonald&#8217;s journey of developing a unified platform enabling real-time, event-driven architectures.</p><h2>Design Goals of the Platform</h2><p>McDonald's unified event-driven platform was built with specific foundational principles to support its global operations and customer-facing services.&nbsp;</p><p>Each design goal was carefully considered to ensure the platform's robustness and efficiency. Let&#8217;s look at the goals in a little more detail.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15341d70-9afa-4786-830b-b23add060f53_1600x1142.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1039" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15341d70-9afa-4786-830b-b23add060f53_1600x1142.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Scalability</h3><p>The platform needed the ability to auto-scale to accommodate demand.</p><p>For this purpose, they engineered it to handle growing event volumes through domain-based sharding across multiple MSK clusters. This approach enables horizontal scaling and efficient resource utilization as transaction volumes increase.</p><h3>High Availability</h3><p>The platform had to be capable enough to withstand failures in components.</p><p>System resilience is achieved through redundant components and failover mechanisms. The architecture includes a standby event store that maintains operation continuity when the primary MSK service experiences issues.</p><h3>Performance</h3><p>The goal was to deliver events in real time with the ability to handle highly concurrent workloads.</p><p>Real-time event delivery is facilitated through optimized processing paths and schema caching mechanisms. The system maintains low latency while handling high-throughput scenarios across different geographical regions.</p><h3>Security</h3><p>The data needed to adhere to data security guidelines.</p><p>The platform implements comprehensive security measures, including:</p><ul><li><p>Authentication layers for external partner integrations</p></li><li><p>Secure event gateways</p></li><li><p>Adherence to strict data security protocols</p></li></ul><h3>Reliability</h3><p>The platform must be dependable with controls to avoid losing any events.</p><p>Event loss prevention is achieved through:</p><ul><li><p>Dead-letter topic management</p></li><li><p>Robust error-handling mechanisms</p></li><li><p>Reliable delivery guarantees</p></li><li><p>Automated recovery procedures</p></li></ul><h3>Consistency</h3><p>The platform should maintain consistency around important patterns related to error handling, resiliency, schema evolution, and monitoring.</p><p>Standardization is maintained using:</p><ul><li><p>Custom SDKs for different programming languages</p></li><li><p>Unified implementation patterns</p></li><li><p>Centralized schema registry</p></li><li><p>Consistent event contract management</p></li></ul><h3>Simplicity</h3><p>The platform should reduce operational complexity so that teams can build on the platform with ease.</p><p>Operational complexity is minimized with:</p><ul><li><p>Automated cluster management</p></li><li><p>Streamlined developer tools</p></li><li><p>Simplified administrative interfaces</p></li><li><p>Clear implementation patterns</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/AppFlowy_110524">The leading open source Notion alternative (Sponsored) </a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/AppFlowy_110524" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="Image" class="sizing-normal" height="630" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F911512f9-3ae9-43e2-8801-f06959c203fb_1200x630.png" title="Image" width="1200" /><div></div></div></a></figure></div><p>AppFlowy is the AI collaborative workspace where you achieve more without losing control of your data. It works offline and supports self-hosting. Own your data and embrace a smarter way to work. Get started for free! </p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/AppFlowy_110524"><span>Try for Free</span></a></p><div><hr /></div><h2>Key Components of the Architecture</h2><p>The diagram below shows the high-level architecture of McDonald&#8217;s event-driven architecture.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfdda41e-1eae-4259-a957-ee494b6d7c61_1600x1017.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="925" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcfdda41e-1eae-4259-a957-ee494b6d7c61_1600x1017.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The key components of the architecture are as follows:</p><h3>Event Broker</h3><p>The core component of the platform is AWS Managed Streaming for Kafka (MSK), which handles:</p><ul><li><p>Communication management between producers and consumers.</p></li><li><p>Topic organization and management.</p></li><li><p>Event distribution across the platform.</p></li></ul><h3>Schema Registry</h3><p>A schema registry is a critical component that maintains data quality by storing all event schemas.</p><p>This enables schema validation for producers as well as consumers. It also allows the consumers to determine which schema to follow for message processing.</p><h3>Standby Event Store</h3><p>This component helps avoid the loss of messages if MSK is unavailable.&nbsp;</p><p>It performs the following functions:</p><ul><li><p>Acts as a fallback mechanism when Kafka is unavailable.</p></li><li><p>Temporarily stores events that couldn't be published to Kafka.</p></li><li><p>Works with AWS Lambda function to retry publishing events to Kafka once it is available.</p></li></ul><h3>Custom SDKs</h3><p>The McDonald&#8217;s engineering team built language-specific libraries for producers and consumers.</p><p>Here are the features supported by these SDKs:</p><ul><li><p>Standardized interfaces for both producers and consumers.</p></li><li><p>Built-in schema validation capabilities.</p></li><li><p>Automated error handling and retry mechanisms.</p></li><li><p>Abstraction of complex platform operations.</p></li></ul><h3>Event Gateway</h3><p>McDonald&#8217;s event-based architecture is required to support internally generated events and events produced by external partner applications.</p><p>The event gateway serves as an interface for external integrations by:</p><ul><li><p>Providing HTTP endpoints for external partners.</p></li><li><p>Converting HTTP requests to Kafka events.</p></li><li><p>Implementing authentication and authorization layers.</p></li></ul><h3>Supporting Utilities</h3><p>These are administrative tools that offer capabilities such as:</p><ul><li><p>Management of dead-letter topics</p></li><li><p>Error handling for failed events</p></li><li><p>Administrative interfaces for event monitoring</p></li><li><p>Cluster management capabilities</p></li></ul><h2>Event Processing Flow</h2><p>The event processing system at McDonald's follows a sophisticated flow that ensures data integrity and efficient processing.&nbsp;</p><p>The diagram below shows the overall processing flow.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0de61156-dbf6-45bf-bd3d-e6442a9ce5de_1600x1014.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="923" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0de61156-dbf6-45bf-bd3d-e6442a9ce5de_1600x1014.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Let&#8217;s look at it in more detail by dividing the flow in two major themes - event creation and event reception.</p><h3>Event Creation and Sharing</h3><ul><li><p>The first step is to create a blueprint (schema) for each type of event and store it in a central library also known as the schema registry.</p></li><li><p>Apps that want to create events use a special tool (producer SDK) to do so.</p></li><li><p>When an app starts up, it saves a copy of the event blueprint for quick access.</p></li><li><p>The tool checks if the event matches the blueprint before sending it out.</p></li><li><p>If everything looks good, the event is sent to the main message board, which is the primary topic.</p></li><li><p>If there's a problem with the event or a fixable error, it's sent to a separate area (dead-letter topic) for that app.</p></li><li><p>If the message system (MSK) is down, the event is saved in a backup database (DynamoDB).</p></li></ul><h3>Event Reception</h3><ul><li><p>Apps that want to receive events use the consumer SDK. This SDK also checks if the received events match their blueprints.</p></li><li><p>When an event is successfully received, the application marks it as "read" and moves on to the next one.</p></li><li><p>Events in the problem area (dead-letter topic) can be fixed later and sent back to the main message board.</p></li><li><p>Events from partner companies ("Outer Events") come in through the event gateway as mentioned earlier.</p></li></ul><h2>Techniques for Key Challenges</h2><p>The McDonald&#8217;s engineering team also used some interesting techniques to solve common challenges associated with the setup.</p><p>Let&#8217;s look at a few important ones:</p><h3>Data Governance</h3><p>Ensuring data accuracy is crucial when different systems share information. If the data is reliable, it makes designing and building these systems much simpler.&nbsp;</p><p>MSK and Schema Registry help maintain data integrity by enforcing "data contracts" between systems.</p><p>A schema is like a blueprint that defines what information should be present in each message and in what format. It specifies the required and optional data fields and their types (e.g., text, number, date). Every message is checked against this blueprint in real time. If a message doesn't match the schema, it's sent to a separate area to be fixed.</p><p>Here's how schemas work:</p><ul><li><p>When a system starts, it saves a list of known schemas for quick reference.</p></li><li><p>Schemas can be updated to include more fields or change data types.</p></li><li><p>When a system sends a message, it includes a version number to indicate which schema was used.</p></li><li><p>The receiving system uses this version number to process the message with the correct schema.</p></li><li><p>This approach handles messages with different schemas without interruption and allows easy updates and rollbacks.</p></li></ul><p>See the diagram below for reference:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca204509-fc56-4390-ac0f-d80dc90cddb0_1600x1014.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="923" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca204509-fc56-4390-ac0f-d80dc90cddb0_1600x1014.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Using a schema registry to validate data contracts ensures that the information flowing between systems is accurate and consistent. This saves time and effort in designing and operating the systems that rely on this data, especially for analytics purposes.</p><h3>Cluster Autoscaling</h3><p>MSK is a messaging system that helps different parts of an application communicate with each other. It uses brokers to store and manage the messages.&nbsp;</p><p>As the amount of data grows, MSK automatically increases the storage space for each broker. However, they needed a way to add more brokers to the system when the existing ones got overloaded.</p><p>To solve this problem, they created an Autoscaler function. See the diagram below:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334dce97-1569-4a89-bd9e-0751eac0511a_1600x1026.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="934" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F334dce97-1569-4a89-bd9e-0751eac0511a_1600x1026.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Think of this function as a watchdog that keeps an eye on how hard each broker is working. When a broker's workload (measured by CPU utilization) goes above a certain level, the Autoscaler function kicks in and does two things:</p><ul><li><p>It adds a new broker to the MSK system to help handle the increased workload.</p></li><li><p>It triggers a lambda function to redistribute the data evenly across all the brokers, including the new one.</p></li></ul><p>This way, the MSK system can automatically adapt to handle more data and traffic without the need to add brokers or move data around manually.&nbsp;</p><h3>Domain-Based Sharding</h3><p>To ensure that the messaging system can handle a lot of data and minimize the risk of failures, they divide events into separate groups based on their domain.&nbsp;</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26149502-6adc-41a3-8f60-8f8d135dccda_1600x1027.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="935" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26149502-6adc-41a3-8f60-8f8d135dccda_1600x1027.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Each group has its own dedicated MSK cluster. This is like having separate mailrooms for different departments in a large company. The domain of an event determines which cluster and topic it belongs to. For example, events related to user profiles might go to one cluster, while events related to product orders might go to another.</p><p>Applications that need to receive events can choose to get them from any of these domain-based topics. This improves flexibility and helps distribute the workload across the system.</p><p>To make sure the platform is always available and can serve users globally, it is set up to work across multiple regions. In each region, there is a high-availability configuration. This means that if one part of the system goes down, another part can take over seamlessly, ensuring uninterrupted service.</p><h2>Conclusion</h2><p>McDonald's event-driven architecture demonstrates a successful implementation of a large-scale, global event processing platform. The system effectively handles diverse use cases from mobile order tracking to marketing communications while maintaining high reliability and performance.</p><p>Key success factors include the robust implementation of AWS MSK, effective schema management, and comprehensive error-handling mechanisms. The architecture's domain-based sharding approach and auto-scaling capabilities have proven crucial for handling growing event volumes.</p><p>Some best practices established through this implementation include:</p><ul><li><p>A standardized SDK usage across different programming languages.</p></li><li><p>Centralized schema management.</p></li><li><p>Robust error handling with dead-letter topics.</p></li><li><p>Performance optimization through schema caching.</p></li></ul><p>Looking ahead, McDonald's platform is positioned to evolve with planned enhancements including:</p><ul><li><p>Formal event specification support.</p></li><li><p>Transition to serverless MSK.</p></li><li><p>Implementation of partition autoscaling.</p></li><li><p>Enhanced developer tooling and experience.</p></li></ul><p>These improvements will further strengthen the platform's capabilities while maintaining its core design principles of scalability, reliability, and simplicity.</p><p><strong>References:</strong></p><ul><li><p><a href="https://medium.com/mcdonalds-technical-blog/behind-the-scenes-mcdonalds-event-driven-architecture-51a6542c0d86">Behind the scenes: McDonald&#8217;s event-driven architecture</a></p></li><li><p><a href="https://medium.com/mcdonalds-technical-blog/mcdonalds-event-driven-architecture-the-data-journey-and-how-it-works-4591d108821f">McDonald&#8217;s event-driven architecture: The data journey and how it works</a></p></li><li><p><a href="https://aws.amazon.com/msk/">Amazon Managed Streaming for Apache Kafka</a></p></li></ul><p><br /></p>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 16:31:04 GMT</pubDate>
</item>
<item>
<title>EP136: The Ultimate DevOps Developer Roadmap</title>
<link>https://blog.bytebytego.com/p/ep136-the-ultimate-devops-developer</link>
<guid>https://blog.bytebytego.com/p/ep136-the-ultimate-devops-developer</guid>
<content:encoded><![CDATA[
<div> SDKs, Web Applications, DevOps, Redis, Architectural Patterns, Eventual Consistency
<br />
<br />
总结: Speakeasy提供了可以生成多达10种语言的SDK的解决方案，帮助团队创建优秀的SDK，并提供本地化的API文档。对于系统设计，需要了解Web应用程序、DevOps开发人员的路线图、Redis基础知识、软件架构模式和最终一致性模式。具体内容包括主要编程语言、操作系统、源代码控制管理、网络基础知识、持续集成和持续交付、脚本编程和终端、托管平台、基础设施即代码、监控和日志、软件开发基础等。另外还讨论了Redis的基本概念和命令、软件架构模式的选择、以及实现最终一致性的不同模式。 <div>
<h2><a href="https://bit.ly/Speakeasy_110224">Generate Handwritten SDKs (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Speakeasy_110224" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1512" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F213370db-d098-41fe-995f-f184aea4b3e2_4623x4800.png" width="1456" /><div></div></div></a></figure></div><p>Invest hundreds of hours your team doesn't have in maintaining SDKs by hand or generate crappy SDKs that leave a bad impression on your users. That's two bad options. Fortunately, you can now use Speakeasy to create ergonomic type-safe SDKs in over 10 languages. We've worked with language experts to create a generator that gets the details right. With Speakeasy, you can build SDKs that your team is proud of and offer code-native API docs as well.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Speakeasy_110224"><span>Try for free</span></a></p><div><hr /></div><p>This week&#8217;s system design refresher:</p><ul><li><p>Everything You NEED to KNOW About Web Applications (Youtube video)</p></li><li><p>The Ultimate DevOps Developer Roadmap</p></li><li><p>6 Software Architectural Patterns You Must Know</p></li><li><p>Top Eventual Consistency Patterns You Must Know</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2>Everything You NEED to KNOW About Web Applications</h2><div class="youtube-wrap" id="youtube2-_higfXfhjdo"><div class="youtube-inner"></div></div><div><hr /></div><h2>The Ultimate DevOps Developer Roadmap</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="1778" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0b3d948-49b2-499f-a1c3-d794e6356858_1280x1778.gif" title="graphical user interface" width="1280" /><div></div></div></a></figure></div><ol><li><p>Programming Languages<br />Pick and master one or two programming languages. Choose from options like Python, JavaScript, Go, Ruby, etc.</p></li><li><p>Operating Systems<br />Master the ins and outs of major operating systems like Linux, Windows, Mac, and so on.</p></li><li><p>Source Control Management<br />Learn about source control management tools such as Git, GitHub, GitLab, and Bitbucket.</p></li><li><p>Networking<br />Master the basics of networking concepts such as DNS, IP, TCP, and SSH.</p></li><li><p>CI/CD<br />Pick tools like GitHub Actions, Jenkins, or CircleCI to learn about continuous integration and continuous delivery.</p></li><li><p>Scripting and Terminals<br />Learn scripting in bash, and PowerShell along with knowledge of various terminals and editors.</p></li><li><p>Hosting and Platforms<br />Master multiple hosting platforms such as AWS, Azure, GCP, Docker, Kubernetes, Digital Ocean, Lambda, Azure Functions, etc.</p></li><li><p>Infrastructure as Code<br />Learn infrastructure as code tools like Terraform, Pulumi, Ansible, Chef, Puppet, Kubernetes, etc.</p></li><li><p>Monitoring and Logging<br />Master the key tools for monitoring and logging for infrastructure and applications such as Prometheus, Elasticsearch, Logstash, Kibana, etc.</p></li><li><p>Basics of Software Development<br />Learn the basics of software development such as system availability, data management, design patterns, and team collaboration.</p></li></ol><p>Over to you: What else would you add to this roadmap?</p><div><hr /></div><h2><a href="https://bit.ly/NewRelic_110224">New Relic unveils the industry's first Intelligent Observability Platform (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/NewRelic_110224" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="630" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97975fa2-8aa3-48d8-8129-57746968a235_1200x630.jpeg" width="1200" /><div></div></div></a></figure></div><p>The&nbsp;New Relic Intelligent Observability Platform, transforms observability from ensuring uptime and reliability into a key driver of business growth and developer velocity for enterprises worldwide. With new innovations such as New Relic AI and GitHub Copilot integration, New Relic Pathpoint Plus for business observability, and New Relic Retail Solution.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/NewRelic_110224"><span>Get started</span></a></p><div><hr /></div><h2>The Ultimate Redis 101</h2><p>Redis is one of the most popular data stores in the world and is packed with features.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb83f415-ec18-4755-8b9f-a6bae1e3ac0f_1294x1536.gif" title="graphical user interface" width="1294" /><div></div></div></a></figure></div><p>Here are 8 simple steps that can help you understand the fundamentals of Redis.</p><ol><li><p>What is Redis?<br />Redis (Remote Dictionary Server) is a multi-modal database that provides sub-millisecond latency. The core idea behind Redis is that a cache can also act as a full-fledged database.<br /></p></li><li><p>Redis Adoption<br />High-traffic internet websites like Airbnb, Uber, Slack, and many others have adopted Redis in their technology stack.<br /></p></li><li><p>How Redis Changed the Database Game?<br />Redis supported main memory read/writes while still supporting fully durable storage. Read and writes are served from the main memory but the data is also persisted to the disk. This is done using snapshots (RDB) and AOF.<br /></p></li><li><p>Redis Data Structures<br />Redis stores data in key-value format. It supports various data structures such as strings, bitmaps, lists, sets, sorted sets, hash, JSON, etc.<br /></p></li><li><p>Basic Redis Commands<br />Some of the most used Redis commands are SET, GET, DELETE, INCR, HSET, etc. There are many more commands available.<br /></p></li><li><p>Redis Modules<br />Redis modules are add-ons that extend Redis functionality beyond its core features. Some prominent modules are RediSearch, RedisJSON, RedisGraph, RedisBloom, RedisAI, RedisTimeSeries, RedisGears, RedisML, and so on.<br /></p></li><li><p>Redis Pub/Sub<br />Redis also supports even-driven architecture using a publish-subscribe communication model.<br /></p></li><li><p>Redis Use Cases<br />Top Redis use cases are Distributed Caching, Session Storage, Message Queue, Rate Limiting, High-Speed Database, etc.</p></li></ol><p>Over to you: What else will you add to get a better understanding of Redis?</p><div><hr /></div><h2>6 Software Architectural Patterns You Must Know</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1615" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63440d88-f57b-4010-a9a6-64926cf4fbb8_1280x1615.gif" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><p>Choosing the right software architecture pattern is essential for solving problems efficiently.</p><ol><li><p>Layered Architecture<br />Each layer plays a distinct and clear role within the application context. <br />Great for applications that need to be built quickly. On the downside, source code can become unorganized if proper rules aren&#8217;t followed<br /></p></li><li><p>Microservices Architecture<br />Break down a large system into smaller and more manageable components.<br />Systems built with microservices architecture are fault tolerant. Also, each component can be scaled individually. On the downside, it might increase the complexity of the application.<br /></p></li><li><p>Event-Driven Architecture<br />Services talk to each other by emitting events that other services may or may not consume.<br />This style promotes loose coupling between components. However, testing individual components becomes challenging<br /></p></li><li><p>Client-Server Architecture<br />It comprises two main components - clients and servers communicating over a network.<br />Great for real-time services. However, servers can become a single point of failure.<br /></p></li><li><p>Plugin-based Architecture<br />This pattern consists of two types of components - a core system and plugins. The plugin modules are independent components providing a specialized functionality.<br />Great for applications that have to be expanded over time like IDEs. However, changing the core is difficult.<br /></p></li><li><p>Hexagonal Architecture<br />This pattern creates an abstraction layer that protects the core of an application and isolates it from external integrations for better modularity. Also known as ports and adapters architecture.<br />On the downside, this pattern can lead to increased development time and learning curve.<br /></p></li></ol><p>Over to you: Which other architectural pattern have you seen?</p><div><hr /></div><h2>Top Eventual Consistency Patterns You Must Know</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1519" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb032242e-c29c-4226-896e-f8c9e9f6de54_1280x1519.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><p>Eventual consistency is a data consistency model that ensures that updates to a distributed database are eventually reflected across all nodes. Techniques like async replication help achieve eventual consistency.<br /><br />However, eventual consistency can also result in data inconsistency. Here are 4 patterns that can help you design applications.<br /><br />Pattern#1 - Event-based Eventual Consistency<br />Services emit events and other services listen to these events to update their database instances. This makes services loosely coupled but delays data consistency.<br /><br />Pattern#2 - Background Sync Eventual Consistency<br />In this pattern, a background job makes the data across databases consistent. It results in slower eventual consistency since the background job runs on a specific schedule.<br /><br />Pattern#3 - Saga-based Eventual Consistency<br />Saga is a sequence of local transactions where each transaction updates data with a single service. It is used to manage long-lived transactions that are eventually consistent.<br /><br />Pattern#4 - CQRS-based Eventual Consistency<br />Separate read and write operations into different databases that are eventually consistent. Read and write models can be optimized for specific requirements.<br /><br />Over to you: Which other eventual consistency patterns have you seen?</p><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 02 Nov 2024 15:30:55 GMT</pubDate>
</item>
<item>
<title>Why Executives Seem Out of Touch, and How to Reach Them</title>
<link>https://blog.bytebytego.com/p/why-executives-seem-out-of-touch</link>
<guid>https://blog.bytebytego.com/p/why-executives-seem-out-of-touch</guid>
<content:encoded><![CDATA[
<div> Amazon VP, Prime Video, career growth coach, newsletter, course

要点1: 本文介绍了一位前亚马逊副总裁，他在创建一些突破性服务方面发挥了关键作用，如Prime Video、Amazon Video、Amazon Appstore等。
要点2: 除了担任工程领导者的经验外，Ethan还是一位职业发展教练，帮助许多人摆脱困境、职业晋升。
要点3: 喜欢的读者可以通过Twitter、LinkedIn等社交媒体与Ethan联系，也可以订阅他的新闻简报和职业社区。
要点4: Ethan的课程被评价很高，教授如何在职业中迈向下一个层次。
要点5: 文章指出，较大组织中的员工常感到领导人的行为和决策令人惊讶和脱节，领导者与员工之间的距离越大，领导者的选择就越容易令人意外和“脱离实际”。

<br /><br />总结: 本文介绍了一位前亚马逊副总裁Ethan，他在创建许多突破性服务方面发挥重要作用，同时也是一位职业发展教练。读者可以通过社交媒体与他联系，并了解他的新闻简报和职业社区。文章还推荐了他的高评价课程，教授如何在职业中提升。同时，文章指出大型组织中的员工常感到领导者的行为和决策令人意外和脱节。 <div>
<p>For this week's issue, I am pleased to introduce our guest author, <a href="https://www.linkedin.com/in/ethanevansvp/">Ethan Evans</a>, an ex-Amazon VP who played a pivotal role in creating some of the groundbreaking services we use today, such as Prime Video, Amazon Video, the Amazon Appstore, Prime Gaming (formerly Twitch Prime), and Twitch Commerce.</p><p>Beyond his experience as an engineering leader, Ethan is also a career growth coach who has assisted numerous individuals to get unstuck and level up their careers.</p><p>If you would like to connect with Ethan, you can do so on <a href="https://twitter.com/EthanEvansVP">Twitter</a> and <a href="https://www.linkedin.com/in/ethanevansvp/">LinkedIn</a>. Also, be sure to check out his newsletter and career community, <a href="https://levelupwithethanevans.substack.com/BBG25">Level Up</a> (ByteByteGo readers get a 25% discount off the annual subscription&#8212;highly recommended for more deep executive insights), and consider his highly rated live course on how to <a href="https://maven.com/ethan-evans/break-through-to-executive">get unstuck and break through</a> to the next level in your career.</p><div><hr /></div><p>Employees in larger organizations often feel surprised and disconnected from the actions and decisions of their leaders.&nbsp; The bigger the organization and the more distance to the leader, the more likely it is that their choices will be surprising and seem &#8220;out of touch.&#8221;</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdddfd303-9b05-4d9d-8216-7215223d127b_1600x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1456" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdddfd303-9b05-4d9d-8216-7215223d127b_1600x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Comic by <a href="https://workchronicles.com/">Work Chronicles</a> for ByteByteGo</figcaption></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/why-executives-seem-out-of-touch">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 15:31:21 GMT</pubDate>
</item>
<item>
<title>1.8 Trillion Events Per Day with Kafka: How Agoda Handles it</title>
<link>https://blog.bytebytego.com/p/18-trillion-events-per-day-with-kafka</link>
<guid>https://blog.bytebytego.com/p/18-trillion-events-per-day-with-kafka</guid>
<content:encoded><![CDATA[
<div> CodeRabbit, Kafka, Agoda, 2-step logging architecture, load balancing
总结:<br /><br />CodeRabbit是一个AI代码审查工具，帮助团队提高代码质量并加快代码合并速度。Agoda使用Kafka处理每天1.8万亿次事件，并面临各种挑战。他们实施了2步日志架构、基于用例拆分Kafka集群、监控和审核系统以及动态负载均衡解决方案。通过实施这些策略，Agoda成功管理了大规模事件的处理，并为其他开发人员提供了宝贵的学习经验。 <div>
<h2><a href="https://bit.ly/CodeRabbit_102924">Cut Code Review Time &amp; Bugs into Half with CodeRabbit (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/CodeRabbit_102924" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="877" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ff8cdd8-5f43-40a0-8c56-1674579670fe_2400x1446.png" width="1456" /><div></div></div></a></figure></div><p>CodeRabbit is an AI Code Reviewer that helps you or your team merge your code changes faster with superior code quality. CodeRabbit doesn&#8217;t just point out issues; it suggests fixes and explains the reasoning behind the suggestions. Elevate code quality with AI-powered, context-aware reviews and 1-click fixes.</p><p>CodeRabbit provides:</p><p>&#8226; Automatic PR summaries and file-change walkthroughs.</p><p>&#8226; Runs popular linters like Biome, Ruff, PHPStan, etc.</p><p>&#8226; Highlights code and configuration security issues.</p><p>&#8226; Enables you to write custom code review instructions and AST grep rules.</p><p>To date, CodeRabbit has reviewed more than 5 million PRs, is installed on a million repositories, has 15k+ daily developer interactions, and is used by 1000+ organizations.</p><p>PS: CodeRabbit is free for open-source.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/CodeRabbit_102924"><span>Try for free</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the Agoda Engineering Blog. All credit for the technical details goes to the Agoda engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>Agoda sends around 1.8 trillion events per day through Apache Kafka.</p><p>Since 2015, the Kafka usage at Agoda has grown tremendously with a 2x growth YOY on average.</p><p>Kafka supports multiple use cases at Agoda, which are as follows:</p><ul><li><p>Analytical data management</p></li><li><p>Feeding data into the data lake</p></li><li><p>Near real-time monitoring and alerting solutions</p></li><li><p>Building asynchronous APIs</p></li><li><p>Data replication across data centers</p></li><li><p>Serving data to and from Machine Learning pipelines</p></li></ul><p>As the scale and Kafka usage grew, multiple challenges forced Agoda&#8217;s engineering team to develop solutions.</p><p>In this post, we&#8217;ll examine some key challenges that Agoda faced and the solutions they implemented.</p><h2>Simplifying how Developers Send Data to Kafka</h2><p>One of the first changes Agoda made was around sending data to Kafka.</p><p>Agoda built a 2-step logging architecture:</p><ul><li><p>A client library writes events to disk. It handles file rotations and manages the write file locations.</p></li><li><p>A separate daemon process (Forwarder) reads the events and forwards them to Kafka. It is responsible for reading the files, sending the events to Kafka, tracking file offsets, and managing the deletion of completed files.</p></li></ul><p>See the diagram below:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc29c6ec-e563-4503-81eb-e3216005ce7e_1600x1019.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="927" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc29c6ec-e563-4503-81eb-e3216005ce7e_1600x1019.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The architecture separates operational concerns away from development teams, allowing the Kafka team to perform tasks like dynamic configuration, optimizations, and upgrades independently. The client library has a simplified API for producers, enforces serialization standards, and adds a layer of resiliency.</p><p>The tradeoff is increased latency for better resiliency and flexibility, with a 99-percentile latency of 10s for analytics workloads. For critical and time-sensitive use cases requiring sub-second latency, applications can bypass the 2-step logging architecture and write to Kafka directly.</p><h2>Splitting Kafka Clusters Based On Use Cases</h2><p>Agoda made a strategic decision early on to split their Kafka clusters based on use cases instead of having a single large Kafka cluster per data center.&nbsp;</p><p>This means that instead of having one massive Kafka cluster serving all kinds of workloads, they have multiple smaller Kafka clusters, each dedicated to a specific use case or set of use cases.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c70937b-4bbf-4878-8739-55406cb89e7e_1600x1017.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="925" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c70937b-4bbf-4878-8739-55406cb89e7e_1600x1017.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The main reasons for this approach are:</p><ul><li><p>By having separate clusters for different use cases, any issues that arise in one cluster will be contained within that cluster and won&#8217;t affect the others.</p></li><li><p>Different use cases may have different requirements in terms of performance, reliability, and data retention.</p></li></ul><p>For example, a cluster used for real-time data processing might be configured with lower data retention periods and higher network throughput to handle the high volume of data.</p><p>In addition to splitting Kafka clusters by use case, Agoda also provisions dedicated physical nodes for Zookeeper, separate from the Kafka broker nodes. Zookeeper is a critical component in a Kafka cluster, responsible for managing the cluster's metadata, coordinating broker leader elections, and maintaining configuration information.</p><div><hr /></div><h2><a href="https://bit.ly/FusionAuth_101224">Stop renting auth. Make it yours instead.(Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/FusionAuth_101224" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1456" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5b65d1-d667-4be1-b633-b22149b248d2_2400x2400.png" title="" width="1456" /><div></div></div></a></figure></div><ul><li><p>Developer-Centric: True API first design, quick integration, built on standards, highly flexible &amp; customizable.</p></li><li><p>Hosting Flexibility: You host or we host - the choice is yours with no loss of features.</p></li><li><p>Unlimited: Unlimited IDPs, unlimited users, unlimited tenants, unlimited applications, always free.</p></li><li><p>Total Control: Deploy on any computer, anywhere in the world and integrate easily with any tech stack.</p></li><li><p>Scale Confidently: Lightning-fast performance for 10 users or 10 million users (or more).</p></li><li><p>Data Isolation: Single tenant by design means your data is physically isolated from everyone else&#8217;s.</p></li></ul><p>FusionAuth is a complete auth &amp; user platform that has 15M+ downloads and is trusted by industry leaders.</p><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/FusionAuth_101224"><span>Start For Free</span></a></p><div><hr /></div><h2>Monitoring and Auditing Kafka</h2><p>From a monitoring point of view, Agoda uses JMXTrans to collect Kafka broker metrics.&nbsp;</p><p>JMXTrans is a tool that connects to JMX (Java Management Extensions) endpoints and collects metrics. These metrics are then sent to Graphite, a time-series database that stores numeric time-series data.</p><p>The collected metrics include things like broker throughput, partition counts, consumer lag, and various other Kafka-specific performance indicators.&nbsp;</p><p>The metrics stored in Graphite are visualized using Grafana, a popular open-source platform for monitoring and observability. Grafana allows the creation of customizable dashboards that display real-time and historical data from Graphite.</p><p>For auditing, Agoda implemented a custom Kafka auditing system. The primary goal of this auditing system is to ensure data completeness, reliability, accuracy, and timeliness across the entire Kafka pipeline.</p><p>Here&#8217;s how it works:</p><ul><li><p>Audit counts are generated at various points throughout the pipeline.&nbsp;</p></li><li><p>A separate thread runs in the background on Agoda&#8217;s client libraries as part of the 2-step logging architecture we discussed earlier. This thread asynchronously aggregates message counts across time buckets to generate audits.</p></li><li><p>The generated audit data is stored in a separate Kafka cluster dedicated to audit information. This ensures the audit data doesn&#8217;t interfere with the main data pipelines.</p></li><li><p>The audit information ultimately ends up in two places:</p><ul><li><p><strong>Whitefalcon</strong>: Agoda&#8217;s internal near real-time analytics platform</p></li><li><p><strong>Hadoop</strong>: For longer-term storage and analysis.</p></li></ul></li></ul><h2>Authentication and ACLs</h2><p>Initially, Agoda&#8217;s Kafka clusters were used primarily for application telemetry data, and authentication wasn&#8217;t deemed necessary.</p><p>As Kafka usage grew exponentially, concerns arose about the inability to identify and manage users who might be abusing or negatively impacting Kafka cluster performance. Agoda completed and released its Kafka Authentication and Authorization system in 2021.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb970a035-b6b1-4dee-aabe-8c418c84e977_1600x1017.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="925" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb970a035-b6b1-4dee-aabe-8c418c84e977_1600x1017.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The Authentication and Authorization system consists of the following components:</p><ul><li><p><strong>Core Kafka Authentication:</strong>&nbsp; It likely uses SASL (Simple Authentication and Security Layer) mechanisms supported by Kafka.</p></li><li><p><strong>ACLs:</strong> Access Control Lists for fine-grained permission management.</p></li><li><p><strong>Credential Generation:</strong> A custom component for creating and managing user credentials.</p></li><li><p><strong>Credential Assignment:</strong> A system to associate credentials with specific users or teams.</p></li><li><p><strong>Self-Service Portal:</strong> An interface allowing teams to request Kafka credentials and ACLs without direct intervention from the Kafka team.</p></li></ul><h2>Kafka Load Balancing</h2><p>Agoda, as an online travel booking platform, aims to offer its customers the most competitive and current prices for accommodations and services from a wide range of external suppliers, including hotels, restaurants, and transportation providers.&nbsp;</p><p>To achieve this, Agoda's supply system is designed to efficiently process and incorporate a vast number of real-time price updates received from these suppliers. A single supplier can provide 1.5 million price updates and offer details in just one minute. Any delays or failures in reflecting these updates can lead to incorrect pricing and booking failures.</p><p>Agoda uses Kafka to handle these incoming price updates. Kafka partitions help them achieve parallelism by distributing the workload across mple partitions and consumers.</p><p>See the diagram below:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d1829f4-6126-4fdc-88a3-797c6e496a39_1600x1021.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="929" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d1829f4-6126-4fdc-88a3-797c6e496a39_1600x1021.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>The Partitioner and Assignor Strategy</h3><p>Apache Kafka's message distribution and consumption are heavily influenced by two key strategies: the partitioner and the assignor.</p><ul><li><p>The partitioner strategy determines how incoming messages are allocated across partitions during production. Common approaches include round-robin distribution and sticky partitioning.&nbsp;</p></li><li><p>On the consumer side, the assignor strategy dictates how partitions are distributed among consumers within a consumer group. Examples include range assignments and round-robin assignments.</p></li></ul><p>See the diagram below for reference:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F609f6063-1832-4c3b-b88c-cc58203044ea_1600x1020.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="928" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F609f6063-1832-4c3b-b88c-cc58203044ea_1600x1020.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Traditionally, these strategies were designed with the assumption that all consumers have similar processing capabilities and that all messages require roughly the same amount of processing time.&nbsp;</p><p>However, Agoda's real-world scenario deviated from these assumptions, leading to significant load-balancing challenges in their Kafka implementation.</p><p>There were two primary challenges:</p><ul><li><p><strong>Hardware Heterogeneity:</strong> Agoda's use of a private cloud infrastructure with Kubernetes resulted in pods being deployed across servers with varying hardware specifications. Benchmark tests revealed substantial performance disparities between different hardware generations.</p></li><li><p><strong>Inconsistent Message Workloads:</strong> The processing requirements for different messages varied considerably. Some messages necessitated additional steps such as third-party API calls or database queries, leading to unpredictable processing times and latency fluctuations.</p></li></ul><p>These challenges ultimately resulted in an over-provisioning problem, where resources were inefficiently allocated to compensate for the load imbalances caused by hardware differences and varying message processing demands.</p><h3>Overprovisioning Problem at Agoda</h3><p>The over-provisioning involves allocating more resources than necessary to handle the expected peak workload efficiently.</p><p>To illustrate this, let's consider a scenario where Agoda's processor service employs Kafka consumers running on heterogeneous hardware:</p><ul><li><p>They have two high-performance workers, each capable of processing 20 messages per second.</p></li><li><p>Additionally, they have one slower worker that can only handle 10 messages per second.</p></li></ul><p>Theoretically, this setup should be able to process a total of 50 messages per second (20 + 20 + 10). However, when using a round-robin distribution strategy, each worker receives an equal share of the messages, regardless of their processing capabilities. If the incoming message rate consistently reaches 50 messages per second, the following issues arise:</p><ul><li><p>The two faster workers can comfortably handle their allocated share of approximately 16.7 messages per second each.</p></li><li><p>The slower worker, on the other hand, struggles to keep up with its assigned 16.7 messages per second, resulting in a growing lag over time.</p></li></ul><p>See the diagram below</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3a3077b-62a6-40e6-8cd7-3190349f0353_1600x1019.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="927" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3a3077b-62a6-40e6-8cd7-3190349f0353_1600x1019.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>To maintain acceptable latency and meet processing SLAs, Agoda would need to allocate additional resources to this setup.&nbsp;</p><p>In this example, they would have to scale out to five machines to effectively process 50 messages per second. This means that they are overprovisioning by two extra machines due to the inefficient distribution logic that fails to consider the varying processing capabilities of the hardware.</p><p>A similar scenario can occur when the processing workload for each message varies, even if the hardware is homogeneous.&nbsp;</p><p>In both cases, this leads to several negative consequences:</p><ul><li><p>Higher hardware costs due to the need for additional resources.</p></li><li><p>Inefficient utilization of resources, with some consumers being underutilized while others are overburdened.</p></li><li><p>Increased maintenance overhead to manage the overprovisioned infrastructure.</p></li></ul><p>The round-robin distribution strategy, while ensuring an equal distribution of messages across consumers, fails to account for the heterogeneity in hardware performance and message processing workload.</p><h3>Agoda&#8217;s Dynamic Lag-Aware Solution</h3><p>To solve this, Agoda adopted a dynamic, lag-aware approach to solving the Kafka load balancing challenges. They didn&#8217;t opt for static balancing solutions like weighted load balancing due to messages having non-uniform workloads.</p><p>They implemented two main strategies:</p><ul><li><p>Lag-aware Producer</p></li><li><p>Lag-aware Consumer</p></li></ul><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd47b39d7-ded5-4b77-b7af-00c10672a131_1600x1021.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="929" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd47b39d7-ded5-4b77-b7af-00c10672a131_1600x1021.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h4>Lag-Aware Producer</h4><p>A lag-aware producer is a dynamic approach to load balancing in Apache Kafka that adjusts message partitioning based on the current lag information of the target topic.&nbsp;</p><p>It works as follows:</p><ul><li><p>The producer maintains a cached copy of partition lag data to minimize the frequency of requests to Kafka brokers for this information.</p></li><li><p>The producer uses the lag data to intelligently distribute messages across partitions using a custom algorithm. The algorithm is designed to send fewer messages to partitions with high lag and more messages to partitions with low lag. They use algorithms like the same-queue length algorithm and outlier detection algorithm.</p></li><li><p>When the lags across partitions are balanced and stable, the lag-aware producer ensures an even distribution of messages.</p></li></ul><p>Let's consider an example scenario in Agoda's supply system, where an internal producer publishes task messages to a processor.&nbsp;</p><p>The target topic has 6 partitions with the following lag distribution:</p><ul><li><p>Partition 1: 110 messages</p></li><li><p>Partition 2: 150 messages</p></li><li><p>Partition 3: 80 messages</p></li><li><p>Partition 4: 400 messages</p></li><li><p>Partition 5: 120 messages</p></li><li><p>Partition 6: 380 messages</p></li></ul><p>In this situation, the lag-aware producer would identify that partitions 4 and 6 have significantly higher lag compared to the other partitions. As a result, it would adapt its partitioning strategy to send fewer messages to partitions 4 and 6 while directing more messages to the partitions with lower lag (partitions 1, 2, 3, and 5).</p><p>By dynamically adjusting the message distribution based on the current lag state, the lag-aware producer helps to rebalance the workload across partitions, preventing further lag accumulation on the already overloaded partitions.&nbsp;</p><h4>Lag-Aware Consumer</h4><p>Lag-aware consumers are a solution employed when multiple consumer groups are subscribed to the same Kafka topic, making lag-aware producers less effective.</p><p>The process works as follows:</p><ul><li><p>In a downstream service, such as Agoda's Processor, if a particular consumer instance detects that it has fallen significantly behind in processing messages (i.e., it has a high lag), it can voluntarily unsubscribe from the topic. This action triggers a rebalance operation.</p></li><li><p>During the rebalance, a custom partition Assigner, developed by Agoda, reassigns the partitions across all the remaining consumer instances. The redistribution takes into account each consumer's current lag and processing capacity, ensuring a more balanced workload.</p></li><li><p>To minimize the performance impact of rebalancing, Agoda leverages Kafka 2.4's incremental cooperative rebalance protocol. This protocol allows for more frequent partition reassignments without causing significant disruptions to the overall processing flow.</p></li></ul><p>Let's illustrate this with an example.</p><p>Suppose Agoda's Processor service has three consumer instances (workers) that are consuming messages from six partitions of a topic:</p><ul><li><p>Worker 1 is responsible for processing messages from Partitions 1 and 2</p></li><li><p>Worker 2 handles Partitions 3 and 4</p></li><li><p>Worker 3 processes messages from Partitions 5 and 6</p></li></ul><p>If Worker 3 happens to be running on older, slower hardware compared to the other workers, it may struggle to keep up with the message influx in Partitions 5 and 6, resulting in higher lag. In this situation, Worker 3 can proactively unsubscribe from the topic, triggering a rebalance event.</p><p>During the rebalance, the custom Assigner evaluates the current lag and processing capacity of each worker and redistributes the partitions accordingly. For example, it may assign Partition 5 to Worker 1 and Partition 6 to Worker 2, effectively relieving Worker 3 of its workload until the lag is reduced to an acceptable level.</p><h2>Conclusion</h2><p>In conclusion, Agoda's journey with Apache Kafka has been one of continuous growth, learning, and adaptation.&nbsp;</p><p>By implementing strategies such as the 2-step logging architecture, splitting Kafka clusters based on use cases, developing robust monitoring and auditing systems, and Kafka load balancing Agoda has successfully managed the challenges that come with handling 1.8 trillion events per day.&nbsp;</p><p>As Agoda continues to evolve and grow, its Kafka setup will undoubtedly play a crucial role in supporting the company's ever-expanding needs. The various solutions also provide great learning for other software developers in the wider community when it comes to adapting Kafka to their organizational needs.</p><p><strong>References:</strong></p><ul><li><p><a href="https://medium.com/agoda-engineering/how-agoda-manages-1-8-trillion-events-per-day-on-kafka-1d6c3f4a7ad1">How Agoda manages 1.8 trillion events per day on Kafka</a></p></li><li><p><a href="https://medium.com/agoda-engineering/how-we-solve-load-balancing-challenges-in-apache-kafka-8cd88fdad02b">How we solve load balancing challenges in Apache Kafka</a></p></li><li><p><a href="https://kafka.apache.org/documentation/#security_sasl">Kafka SASL</a></p></li></ul><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 15:30:48 GMT</pubDate>
</item>
<item>
<title>EP135: Big Data Pipeline Cheatsheet for AWS, Azure, and Google Cloud</title>
<link>https://blog.bytebytego.com/p/ep135-big-data-pipeline-cheatsheet</link>
<guid>https://blog.bytebytego.com/p/ep135-big-data-pipeline-cheatsheet</guid>
<content:encoded><![CDATA[
<div> Enterprise Ready Conference SaaS identity compliance encryption logging
<br />
总结: 企业就绪会议是一天在旧金山举办的活动，聚集了塑造企业SaaS未来的产品和工程领导者。演讲嘉宾包括OpenAI、Vanta、Checkr、Dropbox和Canva等具有直接企业建设经验的人。讨论的主题涉及高级身份管理、合规性、加密和日志记录等企业客户必需的重要但复杂功能。具有企业路线图任务的创始人、高管、产品经理或工程师可以从具有多年经验的行业领导者得到详细的见解。在AWS、Azure和Google Cloud上构建大数据管道的小抄、比较API体系结构风格、使用的每日十个关键数据结构、构建安全API的小抄等内容也都提供了实用的信息。 <div>
<h2><a href="https://bit.ly/WorkOS_092824">The Enterprise Ready Conference for engineering leaders (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/WorkOS_092824" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="854" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F327c28fa-4555-414e-83df-ab52767d1f10_2784x1632.png" title="" width="1456" /><div></div></div></a></figure></div><p>The Enterprise Ready Conference is a one-day event in SF, bringing together product and engineering leaders shaping the future of enterprise SaaS. <br /><br />The event features a curated list of speakers with direct experience building for the enterprise, including <strong>OpenAI</strong>, <strong>Vanta</strong>, <strong>Checkr</strong>, <strong>Dropbox</strong>, and <strong>Canva</strong>.<br /><br />Topics include advanced identity management, compliance, encryption, and logging &#8212; essential yet complex features that most enterprise customers require.<br /><br />If you are a founder, exec, PM, or engineer tasked with the enterprise roadmap, this conference is for you. You&#8217;ll get detailed insights from industry leaders that have years of experience navigating the same challenges you face today. And best of all, it&#8217;s completely free since it&#8217;s hosted by WorkOS.</p><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/WorkOS_092824"><span>Request an invite</span></a></p><div><hr /></div><p>This week&#8217;s system design interview:</p><ul><li><p>Big Data Pipeline Cheatsheet for AWS, Azure, and Google Cloud</p></li><li><p>A Cheatsheet on Comparing API Architectural Styles</p></li><li><p>10 Key Data Structures We Use Every Day</p></li><li><p>A Cheatsheet to Build Secure APIs</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2>Big Data Pipeline Cheatsheet for AWS, Azure, and Google Cloud</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F981ea45e-ee40-4bac-938f-1c1bf60c65e4_1280x1637.gif" title="No alt text provided for this image" /><div></div></div></a></figure></div><p>Each platform offers a comprehensive suite of services that cover the entire lifecycle:</p><ol><li><p>Ingestion: Collecting data from various sources</p></li><li><p>Data Lake: Storing raw data</p></li><li><p>Computation: Processing and analyzing data</p></li><li><p>Data Warehouse: Storing structured data</p></li><li><p>Presentation: Visualizing and reporting insights</p></li></ol><p>AWS uses services like Kinesis for data streaming, S3 for storage, EMR for processing, RedShift for warehousing, and QuickSight for visualization.<br /><br />Azure&#8217;s pipeline includes Event Hubs for ingestion, Data Lake Store for storage, Databricks for processing, Cosmos DB for warehousing, and Power BI for presentation.<br /><br />GCP offers PubSub for data streaming, Cloud Storage for data lakes, DataProc and DataFlow for processing, BigQuery for warehousing, and Data Studio for visualization.<br /><br />Over to you: What else would you add to the pipeline?</p><div><hr /></div><h2>A Cheatsheet on Comparing API Architectural Styles</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="1593" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe33842e0-2035-4f26-a15a-ed25b45d9d38_1280x1593.gif" title="graphical user interface" width="1280" /><div></div></div></a></figure></div><p>It covers the 6 most popular API architectural styles:</p><ol><li><p>SOAP</p></li><li><p>REST</p></li><li><p>GraphQL</p></li><li><p>gRPC</p></li><li><p>WebSocket</p></li><li><p>Webhook</p></li></ol><p>Over to you: Which other architectural style have you used?</p><div><hr /></div><h2><a href="https://bit.ly/Growthschool_102624">200+ hours of research on Advanced AI Tools for Technical Leaders &amp; Professionals (Sponsored)</a></h2><p>Imagine cutting hours of manual work, solving complex issues instantly, and having AI optimise your entire workflow&#8212;making you 10x more productive.</p><p>This <strong>FREE 3 hour Mini Crash Course on AI</strong> will help you save 16+ hours every week &amp; put your 50% of your work on autopilot.&nbsp;</p><p><strong>Save your seat now (usually $399 but free for first 100 readers)</strong></p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Growthschool_102624" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="819" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27b2fa7d-273b-4092-af9e-281d4c45be66_1600x900.png" width="1456" /><div></div></div></a></figure></div><p>You will learn 20+ AI tools &amp; use AI to:</p><ul><li><p>Apply data-backed insights to drive faster, smarter decision-making</p></li><li><p>Streamline reporting, content generation, and operational processes in seconds</p></li><li><p>Detect performance bottlenecks in real time to keep systems running smoothly and efficiently.</p></li><li><p>Dynamically balance load in event-driven systems by routing traffic and predicting spikes, so you never need to adjust manually.</p></li><li><p>Accelerates big data processing by automating insights and flagging anomalies instantly for faster problem-solving, and much more!</p></li></ul><p>By letting AI handle these critical tasks, you&#8217;ll save hours each week, unlock more time for strategic thinking &amp; innovation and spend more time with family.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Growthschool_102624"><span>Register for the Crash Course Now</span></a></p><div><hr /></div><h2>10 Key Data Structures We Use Every Day</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40ad6478-19cc-45e3-88c4-c9202078cf64_1280x1664.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><ul><li><p>list: keep your Twitter feeds </p></li><li><p>stack: support undo/redo of the word editor </p></li><li><p>queue: keep printer jobs, or send user actions in-game </p></li><li><p>hash table: cashing systems </p></li><li><p>Array: math operations </p></li><li><p>heap: task scheduling </p></li><li><p>tree: keep the HTML document, or for AI decision </p></li><li><p>suffix tree: for searching string in a document </p></li><li><p>graph: for tracking friendship, or path finding </p></li><li><p>r-tree: for finding the nearest neighbor </p></li><li><p>vertex buffer: for sending data to GPU for rendering </p></li></ul><p>Over to you: Which additional data structures have we overlooked?</p><div><hr /></div><h2>A Cheatsheet to Build Secure APIs</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1902" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ae8765f-6c8b-4fd9-91b9-9a546070f6db_1280x1902.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><p>An insecure API can compromise your entire application. Follow these strategies to mitigate the risk:</p><ol><li><p>Using HTTPS<br />Encrypts data in transit and protects against man-in-the-middle attacks.<br />This ensures that data hasn&#8217;t been tampered with during transmission.<br /></p></li><li><p>Rate Limiting and Throttling<br />Rate limiting prevents DoS attacks by limiting requests from a single IP or user.<br />The goal is to ensure fairness and prevent abuse.<br /></p></li><li><p>Validation of Inputs<br />Defends against injection attacks and unexpected data format.<br />Validate headers, inputs, and payload<br /></p></li><li><p>Authentication and Authorization<br />Don&#8217;t use basic auth for authentication. Instead, use a standard authentication approach like JWTs<br />Use a random key that is hard to guess as the JWT secret<br />Make token expiration short<br />For authorization, use OAuth<br /></p></li><li><p>Using Role-based Access Control<br />RBAC simplifies access management for APIs and reduces the risk of unauthorized actions.<br />Granular control over user permission based on roles.<br /></p></li><li><p>Monitoring<br />Monitoring the APIs is the key to detecting issues and threats early.<br />Use tools like Kibana, Cloudwatch, Datadog, and Slack for monitoring<br />Don&#8217;t log sensitive data like credit card info, passwords, credentials, etc.</p></li></ol><p>Over to you: What else would you do to build a secure API?</p><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 26 Oct 2024 15:31:17 GMT</pubDate>
</item>
<item>
<title>Event-Driven Architectural Patterns</title>
<link>https://blog.bytebytego.com/p/event-driven-architectural-patterns</link>
<guid>https://blog.bytebytego.com/p/event-driven-architectural-patterns</guid>
<content:encoded><![CDATA[
<div> 事件驱动架构, 事件, 状态变化, 响应, 异步, 分布式系统

事件驱动架构（EDA）是一种软件设计方法，强调事件的产生、检测、消费和响应。 在这种架构中，事件是系统内的状态变化或更新。 EDA在现代软件开发中特别有益，因为它可以解耦服务，增强可伸缩性，并提高响应速度。 通过允许系统异步地对事件做出响应，EDA支持实时处理，并使系统能够高效地处理大量数据。 这种方法在分布式系统和微服务架构中非常有用，不同的组件必须独立运行但又相互协调。 在今天的软件领域中，EDA的重要性不言而喻。 它提供了诸多优势，如提高容错能力、更好的资源利用和支持动态灵活的工作流程，可以让企业迅速适应不断变化的需求和市场条件。<br /><br />总结: 事件驱动架构是一种软件设计方法，通过产生、检测、消费和响应事件来实现。这种架构具有多种优势，包括提高容错能力、资源利用效率、支持动态灵活的工作流程。它在现代软件开发中发挥着重要作用，特别适合分布式系统和微服务架构。 <div>
<p>Event-driven architecture (EDA) is a software design approach emphasizing the production, detection, consumption, and reaction to events. In this architecture, events are state changes or updates within a system.&nbsp;</p><p>EDA is particularly beneficial in modern software development because it can decouple services, enhance scalability, and improve responsiveness.&nbsp;</p><p>By allowing systems to react to events asynchronously, EDA supports real-time processing and enables systems to handle high volumes of data efficiently. This approach is useful in distributed systems and microservices architectures, where different components must operate independently yet cohesively.</p><p>The importance of EDA in today's software landscape cannot be overstated. It offers significant advantages such as:</p><ul><li><p>Improved fault tolerance because systems can continue operating even if some components fail.&nbsp;</p></li><li><p>Better resource utilization by enabling services to scale independently based on demand.&nbsp;</p></li><li><p>Supports dynamic and flexible workflows, allowing businesses to adapt quickly to changing requirements and market conditions.</p></li></ul><p>In this article, we&#8217;ll explore various patterns used in event-driven architecture. By examining these patterns, the aim is to gather insights into how they can be applied to build robust, scalable, and responsive systems.&nbsp;</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0704e3e6-be46-4411-8c24-c72c77038d7e_1509x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1544" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0704e3e6-be46-4411-8c24-c72c77038d7e_1509x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2" fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/event-driven-architectural-patterns">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 15:31:13 GMT</pubDate>
</item>
<item>
<title>Uber Reduces Database Lock Time by 94% with Major MySQL Fleet Upgrade</title>
<link>https://blog.bytebytego.com/p/uber-reduces-database-lock-time-by</link>
<guid>https://blog.bytebytego.com/p/uber-reduces-database-lock-time-by</guid>
<content:encoded><![CDATA[
<div> Uber, MySQL, Upgrade, Automation, Performance<br />
<br />
总结: Uber决定将MySQL基础设施从版本5.7升级到8.0，带来了性能、安全和功能方面的显著优势。升级采用了侧-by-侧升级方法，通过4个阶段的过程实现了平稳的升级。遇到的挑战包括查询执行计划变更、不支持的查询和配置、排序和字符集变更以及客户端库不兼容等，但通过合作伙伴Percona的帮助和自动化处理等方式，成功解决问题。升级后，服务器端性能和客户端性能均得到显著改善，提高了响应速度和数据处理效率。Uber的经验表明，自动化、彻底测试、回滚机制和合作是完成大规模升级的关键。 <div>
<h2><a href="https://bit.ly/Datadog_102224">The Future of AI, LLMs, and Observability on Google Cloud (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Datadog_102224" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1080" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27568831-429b-4fc1-ab08-f8470bcb8982_1080x1080.png" width="1080" /><div></div></div></a></figure></div><p>Discover 7 key insights for technical leaders from Google&#8217;s Director of AI, Dr. Ali Arsanjani, and Datadog&#8217;s VP of Engineering, Sajid Mehmood. This ebook provides actionable insights around questions such as:</p><ul><li><p>How can organizations better approach AI and LLMs?</p></li><li><p>How can you build customer confidence in the output of LLMs and LLM-based applications?</p></li><li><p>How should you evolve your tooling as your maturity with LLMs grows?</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Datadog_102224"><span>Download the eBook</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the Uber Engineering Blog. All credit for the technical details goes to the Uber engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>MySQL serves as the backbone for Uber&#8217;s vast and complex operations. For many years, Uber relied upon MySQL version 5.7 to support business-critical features.</p><p>However, in 2023, they decided to upgrade from MySQL version 5.7 to version 8.</p><p>In this post, we&#8217;ll look at the need for this and the challenges Uber faced in such a large-scale upgrade. We will also investigate the solutions Uber used to achieve the upgrade without violating the Service-Level Objective (SLO).</p><h2>The Need for the Upgrade</h2><p>The decision to upgrade Uber's MySQL infrastructure from version 5.7 to 8.0 was driven by several critical factors.&nbsp;</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe031df61-7a3b-425d-944c-b6c9c70b22db_1600x1191.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1084" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe031df61-7a3b-425d-944c-b6c9c70b22db_1600x1191.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>First, MySQL 5.7 was reaching its end-of-life, meaning it would no longer receive security updates or bug fixes, leaving Uber's infrastructure vulnerable to potential security risks and operational instability. Upgrading to MySQL 8.0 mitigated these risks by ensuring ongoing support and security improvements.&nbsp;</p><p>Additionally, MySQL 8.0 offered significant performance and concurrency enhancements such as:</p><ul><li><p><strong>Improved indexing and resource utilization:</strong> This led to faster query execution and better concurrency handling, crucial for Uber&#8217;s high-traffic operations.</p></li><li><p><strong>Enhanced performance:</strong> These optimizations reduced latency and improved the overall user experience by supporting smoother transaction processing.</p></li></ul><p>Beyond performance, MySQL 8.0 introduced several new functionalities such as:</p><ul><li><p><strong>Window functions and enhanced JSON handling:</strong> These improvements allowed more efficient data querying and manipulation.</p></li><li><p><strong>Improved spatial data capabilities:</strong> This enabled more advanced processing of geographic data which is important for location-based services.</p></li><li><p><strong>"Dual passwords" for smoother password rotations:</strong> This feature allowed Uber to rotate passwords during security incidents without causing service disruptions, enhancing security protocols.</p></li><li><p><strong>Instant ADD Column functionality:</strong> This feature allowed schema changes to be made with minimal downtime, streamlining Uber's database management and ensuring high service availability.</p></li></ul><p>Overall, these performance, security, and operational benefits made the transition to MySQL 8.0 a critical move for Uber's data infrastructure.</p><div><hr /></div><h2><a href="https://bit.ly/Sentry_102224">Workshop: Implementing Clean Architecture in Next.js (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Sentry_102224" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="628" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefda4c9f-e136-478a-bf24-72c7dc9ea5ce_1200x628.png" title="" width="1200" /><div></div></div></a></figure></div><p>Lazar Nikolov and Sarah Guthals are hosting a free workshop on Implementing Clean Architecture in Next.js. It will dive deep into what clean architecture *actually* is, what problems it solves, and how to implement it in a Next.js application with Sentry.</p><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/Sentry_102224"><span>RSVP Here</span></a></p><div><hr /></div><h2>The Scale of The Upgrade</h2><p>Uber&#8217;s MySQL infrastructure is vast, operating at a scale that supports its global platform operations. Here are some stats about the overall scale that shows the critical role of MySQL in Uber&#8217;s services:</p><ul><li><p>The system is composed of over 2,100 MySQL clusters.</p></li><li><p>The clusters are spread across 19 production zones in different regions.&nbsp;</p></li><li><p>More than 16,000 nodes manage the massive volumes of data.&nbsp;</p></li><li><p>These clusters handle petabytes of data and serve around 3 million queries per second.</p></li></ul><p>Also, to ensure high availability and data redundancy, Uber employs a primary-secondary replication architecture. It works as follows:</p><ul><li><p><strong>Primary node:</strong> Responsible for handling all write traffic in each cluster.</p></li><li><p><strong>Secondary nodes:</strong> Replicate the data asynchronously from the primary node, ensuring redundancy and fault tolerance. These secondary nodes are distributed across multiple data centers to enhance data availability and support seamless failover in case of primary node failure.</p></li></ul><h2>Challenges with the Upgrade</h2><p>Several challenges had to be addressed during the upgrade of Uber&#8217;s MySQL fleet from version 5.7 to 8.0. Some of the major ones are as follows:</p><ul><li><p>Manual upgrades were not possible due to the sheer scale of Uber&#8217;s MySQL infrastructure. It was important to have a detailed upgrade strategy that could be executed efficiently across diverse environments.</p></li><li><p>Uber&#8217;s platform operates globally, meaning that downtime could significantly impact services. Maintaining SLOs throughout the upgrade was crucial.</p></li><li><p>It was important to ensure compatibility with Uber&#8217;s existing applications and services. Since upgrading from MySQL 5.7 to 8.0 introduced new features and syntax changes that could potentially break existing queries, extensive testing was needed.</p></li></ul><p>Uber conducted thorough regression checks and validation tests to ensure all existing systems and applications continued to work seamlessly with the upgraded database.&nbsp;</p><p>This process included testing in a staging environment before making production upgrades. By validating every aspect of the system, Uber was able to mitigate the risk of any unexpected issues after the upgrade.</p><p>Finally, Uber implemented automated rollback mechanisms to safeguard the upgrade process.&nbsp;</p><p>In the event of any failures or compatibility issues during the upgrade, these mechanisms could automatically revert the changes, ensuring the maintenance of service continuity and data integrity.</p><p>For instance, in the pre-maintenance stage, where the new MySQL 8.0 nodes operated as replicas, if performance issues or system degradation were detected, Uber could instantly roll back to MySQL 5.7 without any risk of data loss. The rollback capability was crucial for addressing any latency, resource consumption, or service degradation issues, allowing Uber to revert to a stable state until the issues were resolved.&nbsp;</p><p>However, once a MySQL 8.0 node was promoted to the primary status, rolling back to MySQL 5.7 became more complex because replication between the new and old versions was no longer possible. In other words, Uber had to ensure everything was functioning correctly before promoting the new nodes to avoid irreversible complications.</p><h2>Upgrade Strategy</h2><p>When upgrading its massive MySQL infrastructure from version 5.7 to 8.0, Uber had two possible strategies to choose from: side-by-side upgrade and in-place upgrade.</p><h3>In-Place Upgrade</h3><p>An in-place upgrade involves directly upgrading the existing MySQL installation to the new version (MySQL 8.0) on the same nodes.&nbsp;</p><p>The process typically requires stopping the MySQL service, upgrading the software, and restarting it. While this method can be simpler in terms of setup, it also comes with significant drawbacks:</p><ul><li><p><strong>Extended downtime:</strong> Since the MySQL service must be stopped during the upgrade, this approach leads to a noticeable period of downtime. For a global platform like Uber, even a brief service interruption can have a major impact.</p></li><li><p><strong>Limited rollback:</strong> If issues arise after the upgrade, rolling back to the previous version can be difficult. In-place upgrades provide less flexibility in case of failure, making it harder to revert to a stable state.</p></li><li><p><strong>Risk of data loss or degradation:</strong> Any problems encountered during the in-place upgrade might lead to data loss or degradation of system performance, with fewer opportunities to recover without downtime.</p></li></ul><p>Due to these limitations, Uber decided against the in-place upgrade method.</p><h3>Side-by-Side Upgrade</h3><p>Uber chose a side-by-side upgrade approach, which allowed for a smoother and less risky transition.&nbsp;</p><p>See the diagram below:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde0206-9381-47d1-9f6e-6bc89e7ff658_1600x1019.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="927" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde0206-9381-47d1-9f6e-6bc89e7ff658_1600x1019.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>In this method, the new MySQL 8.0 nodes were set up and operated alongside the existing MySQL 5.7 nodes.&nbsp;</p><p>This approach was more suitable for Uber&#8217;s infrastructure due to the following reasons:</p><ul><li><p><strong>Minimal downtime:</strong> With the side-by-side method, the old MySQL 5.7 nodes remained operational while the new MySQL 8.0 nodes were being deployed. This allowed Uber to gradually transfer traffic from the old nodes to the new ones, avoiding significant service disruptions.</p></li><li><p><strong>Easier rollback:</strong> If any issues occurred with the new MySQL 8.0 nodes, Uber could easily revert to the old MySQL 5.7 nodes. Since the old nodes were still running, the rollback process was simple and risk-free, reducing the chance of data loss or service degradation.</p></li><li><p><strong>Thorough testing:</strong> Running the two versions side-by-side allowed Uber to fully test the new MySQL 8.0 nodes with real production traffic before completing the migration. This ensured that problems were detected and addressed before fully switching to the new version.</p></li></ul><h3>Scaling the Upgrade Process with Automation</h3><p>To manage the complexity of upgrading such a large infrastructure, Uber implemented an automated workflow.&nbsp;</p><p>With more than 2,100 clusters and over 16,000 nodes, upgrading each node manually was an impossible task. Automation ensured that the process was scalable, efficient, and free from human error.</p><p>Two main aspects of this automation are:</p><ul><li><p><strong>Monitoring and alerts:</strong> The system was designed to automatically monitor each stage of the upgrade, notifying the engineering team if any problems occurred. This allowed Uber to handle the upgrade across thousands of nodes without risking service stability.</p></li><li><p><strong>Risk mitigation:</strong> The automated workflows minimized the risk of human error and allowed for quick intervention if any issues were detected during the upgrade process.</p></li></ul><h2>Four-Stage Upgrade Process for MySQL</h2><p>Uber&#8217;s MySQL upgrade from version 5.7 to 8.0 was carefully planned and executed in a four-stage process.&nbsp;</p><p>This approach ensured minimal service disruption and allowed Uber to transition its massive data infrastructure safely. Let&#8217;s break down the four stages in simple terms:</p><h3>1. Pre-Maintenance Stage</h3><p>In the pre-maintenance stage, new MySQL 8.0 nodes were added as replicas to the existing MySQL 5.7 clusters. A "node" here is a server running a MySQL instance.&nbsp;</p><p>By adding these MySQL 8.0 nodes as replicas, they could work alongside the old 5.7 nodes without disrupting any operations.</p><p>This setup ensured that the old system (MySQL 5.7) continued functioning normally while the new system (MySQL 8.0) was being integrated, allowing Uber to keep everything running smoothly.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8573c580-7302-4789-abf3-3ecaad478413_1600x1018.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="926" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8573c580-7302-4789-abf3-3ecaad478413_1600x1018.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>2. System Monitoring (Soak Period)</h3><p>After setting up the MySQL 8.0 nodes, Uber entered the system monitoring stage, also known as the "soak period." This stage lasted for about a week and was crucial for testing the new system under real-world conditions.</p><p>During this time, Uber monitored the MySQL 8.0 nodes as they handled real production traffic (read operations), checking for issues such as slow performance, errors, or increased resource usage.</p><p>This period was essential to detect potential problems before making the final switch to MySQL 8.0.</p><h3>3. Maintenance Stage</h3><p>Once the soak period confirmed that everything was working smoothly, Uber moved to the maintenance stage.&nbsp;</p><p>In this phase, the MySQL 8.0 node was promoted to primary status, meaning it now handled all write operations and became the main database for that cluster.</p><p>This promotion marked the point where MySQL 8.0 officially became the main database, while the MySQL 5.7 nodes were demoted or turned off for write traffic.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3649b9e7-4d83-43c5-8ab0-25d7ccf45f3b_1600x1017.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="925" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3649b9e7-4d83-43c5-8ab0-25d7ccf45f3b_1600x1017.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>4. Post-Maintenance Stage</h3><p>Finally, in the post-maintenance stage, Uber removed all the old MySQL 5.7 nodes that were no longer needed.&nbsp;</p><p>At this point, the new MySQL 8.0 nodes were fully operational, and all traffic (both read and write) was being handled by the new system.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9779c3e0-56ed-4211-9102-a861eb575036_1600x1016.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="925" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9779c3e0-56ed-4211-9102-a861eb575036_1600x1016.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>By completing this step, Uber successfully transitioned to the new version, ensuring that the system was upgraded without any data loss or significant service disruptions.</p><h2>Issues During Upgrade</h2><p>During the upgrade of Uber&#8217;s MySQL infrastructure to version 8.0, several issues were encountered that required careful handling and technical solutions to ensure the system continued to run smoothly.&nbsp;</p><p>Here&#8217;s a breakdown of the key problems and how they were addressed:</p><h3>Query Execution Plan Changes</h3><p>One of the major issues that Uber faced was related to changes in the query execution plans in MySQL 8.0.&nbsp;</p><p>A query execution plan is the path the database system uses to retrieve data. In some clusters, MySQL 8.0 chose different paths compared to version 5.7, leading to increased latencies (delays) and higher resource consumption.</p><p>These changes could slow down certain operations, affecting the performance of dashboards and other tools that relied on quick access to data. For instance, clusters powering key dashboards at Uber experienced noticeable slowdowns.</p><p>Uber worked with Percona, a database consulting company, to develop a patch that optimized the execution plans for the affected clusters. By applying this patch, Uber was able to restore performance and reduce resource consumption, bringing the system back to optimal operation.</p><h3>Unsupported Queries and Configurations</h3><p>MySQL 8.0 introduced new syntax rules and stricter configurations, which caused some queries that worked in MySQL 5.7 to fail after the upgrade.&nbsp;</p><p>Specifically, some clusters didn&#8217;t have the STRICT_TRANS_TABLES SQL mode enabled, which is a default setting in MySQL 8.0. This mode enforces stricter rules on handling invalid or missing data.</p><p>Uber had to carefully adjust configurations and rewrite certain queries to align with MySQL 8.0&#8217;s new syntax and rules. For example, they enabled the STRICT_TRANS_TABLES and ONLY_FULL_GROUP_BY modes, which made the system more robust but required changes to some of the legacy queries and applications.</p><h3>Collation and Character Set Changes</h3><p>MySQL 8.0 also brought changes to the default character set and collation. The character set controls how text is stored, and the collation determines how text is compared.&nbsp;</p><p>In MySQL 5.7, Uber had been using the utf8mb4_unicode_520_ci collation, but MySQL 8.0 switched to the new utf8mb4_0900_ai_ci collation.&nbsp;</p><p>This change in the default character set and collation caused issues with sorting and comparing text data across different clusters, particularly when dealing with different languages or special characters. The system needed consistency in collation settings to function correctly, but this shift created mismatches.</p><p>Uber had to align the collation settings across its systems to ensure all nodes used the same character set and collation. This required detailed configuration changes and testing to ensure compatibility and proper sorting behavior across all clusters.</p><h3>Client Library Incompatibility</h3><p>Many client libraries that Uber used to connect to the MySQL database were not initially compatible with MySQL 8.0. Client libraries are essential for applications to communicate with the database, and outdated versions of these libraries did not support some of the new features and functions introduced in MySQL 8.0.</p><p>Without updating these libraries, Uber&#8217;s applications couldn&#8217;t fully utilize the benefits of MySQL 8.0, and some applications experienced failures or errors when trying to connect to the upgraded database.</p><p>Uber upgraded these client libraries across its systems. This process involved rigorous testing in a staging environment to ensure that all client libraries worked properly with MySQL 8.0 before the full upgrade. Once the testing was complete, the libraries were deployed in production, ensuring a smooth transition.</p><h2>Improvements After The Upgrade</h2><p>The upgrade to MySQL 8.0 brought significant performance improvements to Uber&#8217;s infrastructure, both on the server side and client side.</p><p>Let&#8217;s look at both.</p><p><strong>Server-Side Performance:</strong></p><ul><li><p><strong>29% improvement in p99 latency for inserts:</strong> At high concurrency levels (i.e., when many operations were happening simultaneously), the latency for insert operations improved by 29%, allowing Uber to handle more data input efficiently.</p></li><li><p><strong>33% improvement in read latency:</strong> Queries that required reading data from the database saw a 33% reduction in latency, meaning data retrieval became much faster.</p></li><li><p><strong>47% improvement in update latency:</strong> Similarly, update operations were executed 47% faster, enhancing the overall responsiveness of the system under heavy loads.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4bac956-6a85-4b2a-8cf8-59afe8fa4301_805x477.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="431.37391304347824" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4bac956-6a85-4b2a-8cf8-59afe8fa4301_805x477.png" width="728" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://www.uber.com/en-IN/blog/upgrading-ubers-mysql-fleet/">Uber Engineering Blog</a></figcaption></figure></div><p><strong>Client-Side Performance:</strong></p><ul><li><p><strong>94% reduction in database lock time:</strong> The upgrade dramatically reduced the time the system spent waiting for locks on database resources, leading to more efficient transaction processing.</p></li><li><p><strong>78% reduction in query time for certain queries:</strong> Some queries saw a significant 78% reduction in execution time, allowing Uber&#8217;s applications to run more smoothly and respond quicker to user requests.</p></li></ul><h2>Conclusion</h2><p>Through careful planning, automation, and a phased rollout strategy, Uber successfully transitioned its vast data systems with minimal downtime and disruption.&nbsp;</p><p>The new version brought significant benefits in terms of performance, security, and functionality, helping Uber improve its operational efficiency and user experience.</p><p>Some key learnings are as follows:</p><ul><li><p><strong>Automation is Critical:</strong> Given the scale of Uber&#8217;s MySQL infrastructure, automating the upgrade process was essential to reduce human error and ensure efficiency.</p></li><li><p><strong>Thorough Testing:</strong> Extensive testing, including regression checks and system validation, was necessary to identify and resolve issues before the full production rollout, ensuring that existing applications remained compatible.</p></li><li><p><strong>Rollback Mechanisms:</strong> Building automated rollback mechanisms proved vital to maintain service continuity and prevent data loss in case of unexpected issues during the upgrade.</p></li><li><p><strong>Collaboration:</strong> Working with partners like Percona helped Uber quickly resolve specific issues, such as query execution plan changes and performance bottlenecks.</p></li></ul><p><strong>References:</strong></p><ul><li><p><a href="https://www.uber.com/en-IN/blog/upgrading-ubers-mysql-fleet/">Upgrading Uber&#8217;s MySQL Fleet to Version 8.0</a></p></li><li><p><a href="https://dev.mysql.com/blog-archive/the-complete-list-of-new-features-in-mysql-8-0/">MySQL 8 New Features</a></p></li><li><p><a href="https://www.uber.com/en-IN/blog/postgres-to-mysql-migration/">Why Uber Engineering switched from Postgres to MySQL</a></p></li></ul><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 15:31:00 GMT</pubDate>
</item>
<item>
<title>EP134: What do Amazon, Netflix, and Uber have in common?</title>
<link>https://blog.bytebytego.com/p/ep134-what-do-amazon-netflix-and</link>
<guid>https://blog.bytebytego.com/p/ep134-what-do-amazon-netflix-and</guid>
<content:encoded><![CDATA[
<div> Scalability, Paging, Segmentation, Git, Microservices
总结:<br /><br />
本文介绍了系统设计中的可扩展性、分页和分段、Git工作原理以及微服务常用技术栈等内容。可扩展性是通过设计无状态的服务、水平扩展、负载均衡、自动扩展、缓存、数据库复制、数据库分片、异步处理等策略来实现系统的伸缩性。分页和分段分别是内存管理方案，分别解决了物理内存的连续分配和程序的逻辑分段问题。Git是分布式版本控制系统，具有快速提交和本地备份的优势。微服务的技术栈包括前期API定义、开发阶段的技术选择、生产阶段的部署和监控等。这些内容为了理解和实践系统设计中的关键概念提供了重要参考。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Scalability Simply Explained in 10 Minutes (Youtube video)</p></li><li><p>What do Amazon, Netflix, and Uber have in common? </p></li><li><p>What are the differences between paging and segmentation?</p></li><li><p>How does Git Work? </p></li><li><p>What tech stack is commonly used for microservices?</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/ScyllaDB_101924">Your free ticket to P99 CONF is waiting &#8212; 60+ engineering talks on all things performance (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/ScyllaDB_101924" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F450afba8-3157-45b1-8444-775af9028a85_1600x840.png" width="1456" /><div></div></div></a></figure></div><p>P99 CONF is the technical conference for anyone who obsesses over high-performance, low-latency applications. Engineers from Disney/Hulu, Shopify, LinkedIn, Netflix, Google, Meta, Uber + more will share 60+ talks on topics like Rust, Go, Zig, distributed data systems, Kubernetes, and AI/ML.</p><p>Signing up also gets you 30-day access to the complete O&#8217;Reilly library, free books, and a chance to win 1 of 300 free swag packs!</p><p>Don't miss this chance to join 20K of your peers for an unprecedented opportunity to learn from experts like the creators of Postgres, ScyllaDB, KVM, and the Rust tokio framework &#8211; for free, from anywhere.</p><p>Oct 23-24 | Online</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/ScyllaDB_101924"><span>GET YOUR FREE TICKET</span></a></p><div><hr /></div><h2>Scalability Simply Explained in 10 Minutes</h2><div class="youtube-wrap" id="youtube2-EWS_CIxttVw"><div class="youtube-inner"></div></div><div><hr /></div><h2>What do Amazon, Netflix, and Uber have in common? </h2><p>They are extremely good at scaling their system whenever needed. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="1686" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51214062-f149-464a-be32-a40c537f4150_1280x1686.gif" title="graphical user interface" width="1280" /><div></div></div></a></figure></div><p>Here are 8 must-know strategies to scale your system. </p><ol><li><p>Stateless Services <br />Design stateless services because they don&#8217;t rely on server-specific data and are easier to scale. <br /></p></li><li><p>Horizontal Scaling <br />Add more servers so that the workload can be shared. <br /></p></li><li><p>Load Balancing <br />Use a load balancer to distribute incoming requests evenly across multiple servers. <br /></p></li><li><p>Auto Scaling <br />Implement auto-scaling policies to adjust resources based on real-time traffic. <br /></p></li><li><p>Caching <br />Use caching to reduce the load on the database and handle repetitive requests at scale. <br /></p></li><li><p>Database Replication <br />Replicate data across multiple nodes to scale the read operations while improving redundancy. <br /></p></li><li><p>Database Sharding <br />Distribute data across multiple instances to scale the writes as well as reads. <br /></p></li><li><p>Async Processing <br />Move time-consuming and resource-intensive tasks to background workers using async processing to scale out new requests. </p></li></ol><p>Over to you: Which other strategies have you used?</p><div><hr /></div><h2><strong>Latest articles</strong></h2><p>If you&#8217;re not a subscriber, here&#8217;s what you missed this month.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F111e893a-29b4-45c8-b8fd-d5bee6f21a86_1444x1600.png" width="1444" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/data-sharing-between-microservices">Data Sharing Between Microservices</a></p></li><li><p><a href="https://blog.bytebytego.com/p/cap-pacelc-acid-base-essential-concepts">CAP, PACELC, ACID, BASE - Essential Concepts for an Architect&#8217;s Toolkit</a></p></li><li><p><a href="https://blog.bytebytego.com/p/api-gateway">API Gateway</a></p></li><li><p><a href="https://blog.bytebytego.com/p/software-architecture-patterns">Software Architecture Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/the-saga-pattern">The Saga Pattern</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>What are the differences between paging and segmentation?</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1603" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1b9975a-d087-406a-9b42-e5f94c44289a_1280x1603.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><ul><li><p>Paging <br />Paging is a memory management scheme that eliminates the need for contiguous allocation of physical memory. The process's address space is divided into fixed-size blocks called pages, while physical memory is divided into fixed-size blocks called frames. <br /> <br />The address translation process works in 3 steps: </p><ul><li><p>Logical Address Space: The logical address (generated by the CPU) is divided into a page number and a page offset. </p></li><li><p>Page Table Lookup: The page number is used as an index into the page table to find the corresponding frame number. </p></li><li><p>Physical Address Formation: The frame number is combined with the page offset to form the physical address in memory. </p></li></ul><p><br />Advantages: </p><ul><li><p>Eliminates external fragmentation. </p></li><li><p>Simplifies memory allocation. </p></li><li><p>Supports efficient swapping and virtual memory. </p></li></ul></li></ul><p></p><ul><li><p>Segmentation <br />Segmentation is a memory management technique where the memory is divided into variable-sized segments based on the logical divisions of a program, such as functions, objects, or data arrays. <br /> <br />The address tranlation process works in 3 steps: </p><ul><li><p>Logical Address Space: The logical address consists of a segment number and an offset within that segment. </p></li><li><p>Segment Table Lookup: The segment number is used as an index into the segment table to find the base address of the segment. </p></li><li><p>Physical Address Formation: The base address is added to the offset to form the physical address in memory. </p><p></p></li></ul><p>Advantages: </p><ul><li><p>Provides logical separation of different parts of a program. </p></li><li><p>Facilitates protection and sharing of segments. </p></li><li><p>Simplifies management of growing data structures. </p></li></ul></li></ul><div><hr /></div><h2>How does Git Work? </h2><p>The diagram below shows the Git workflow. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram" class="sizing-normal" height="1937" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb21036-5927-4335-a510-ef010de1fe55_1280x1937.jpeg" title="diagram" width="1280" /><div></div></div></a></figure></div><p>Git is a distributed version control system. <br /> <br />Every developer maintains a local copy of the main repository and edits and commits to the local copy. <br /> <br />The commit is very fast because the operation doesn&#8217;t interact with the remote repository. <br /> <br />If the remote repository crashes, the files can be recovered from the local repositories. <br /> <br />Over to you: Which Git command do you use to resolve conflicting changes?</p><div><hr /></div><h2>What tech stack is commonly used for microservices?</h2><p>Below you will find a diagram showing the microservice tech stack, both for the development phase and for production.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1359" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e2a25f-9ff2-47b0-8c88-ef6412207357_1646x1536.jpeg" title="No alt text provided for this image" width="1456" /><div></div></div></a></figure></div><ul><li><p><strong>Pre-Production</strong></p><ul><li><p>Define API - This establishes a contract between frontend and backend. We can use Postman or OpenAPI for this.</p><p></p></li><li><p>Development - Node.js or react is popular for frontend development, and java/python/go for backend development. Also, we need to change the configurations in the API gateway according to API definitions.<br /></p></li><li><p>Continuous Integration - JUnit and Jenkins for automated testing. The code is packaged into a Docker image and deployed as microservices.<br /></p></li></ul></li><li><p><strong>Production</strong></p><ul><li><p>NGinx is a common choice for load balancers. Cloudflare provides CDN (Content Delivery Network).<br /></p></li><li><p>API Gateway - We can use spring boot for the gateway, and use Eureka/Zookeeper for service discovery.<br /></p></li><li><p>The microservices are deployed on clouds. We have options among AWS, Microsoft Azure, or Google GCP.<br /></p></li><li><p>Cache and Full-text Search - Redis is a common choice for caching key-value pairs. ElasticSearch is used for full-text search.<br /></p></li><li><p>Communications - For services to talk to each other, we can use messaging infra Kafka or RPC.<br /></p></li><li><p>Persistence - We can use MySQL or PostgreSQL for a relational database, and Amazon S3 for object store. We can also use Cassandra for the wide-column store if necessary.</p><p></p></li><li><p>Management &amp; Monitoring - To manage so many microservices, the common Ops tools include Prometheus, Elastic Stack, and Kubernetes.</p></li></ul></li></ul><p>Over to you: Did I miss anything? Please comment on what you think is necessary to learn microservices.</p><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 19 Oct 2024 15:30:49 GMT</pubDate>
</item>
<item>
<title>Data Sharing Between Microservices</title>
<link>https://blog.bytebytego.com/p/data-sharing-between-microservices</link>
<guid>https://blog.bytebytego.com/p/data-sharing-between-microservices</guid>
<content:encoded><![CDATA[
<div> 微服务架构、数据管理、数据共享、优缺点、方法<br />
<br />总结：<br />微服务架构是构建复杂、可扩展软件系统的流行架构风格。每个微服务专注于特定的业务能力，并可以独立进行开发、部署和扩展。其中一个基本原则是让每个服务拥有和管理自己的数据，以确保松散耦合和自治。虽然不鼓励在服务之间共享数据源（例如数据库），但在服务之间共享数据通常是必要和可接受的。文章探讨了在微服务之间共享数据的不同方式以及具体方法的优缺点。 <div>
<p>Microservices architecture has become popular for building complex, scalable software systems.</p><p>This architectural style structures an application as a collection of loosely coupled, independently deployable services. Each microservice is focused on a specific business capability and can be developed, deployed, and scaled independently.</p><p>While microservices offer numerous benefits, such as improved scalability, flexibility, and faster time to market, they also introduce significant challenges in terms of data management.</p><p>One of the fundamental principles of microservices architecture is that each service should own and manage its data. This principle is often expressed as "don't share databases between services&#8221; and it aims to ensure loose coupling and autonomy among services, allowing them to evolve independently.&nbsp;</p><p>However, it's crucial to distinguish between sharing a data source and sharing data itself. While sharing a data source (e.g., a database) between services is discouraged, sharing data between services is often necessary and acceptable.</p><p>In this post, we&#8217;ll look at different ways of sharing data between microservices and the various advantages and disadvantages of specific approaches.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F111e893a-29b4-45c8-b8fd-d5bee6f21a86_1444x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F111e893a-29b4-45c8-b8fd-d5bee6f21a86_1444x1600.png" width="1444" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/data-sharing-between-microservices">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 15:31:06 GMT</pubDate>
</item>
<item>
<title>How Uber Manages Petabytes of Real-Time Data</title>
<link>https://blog.bytebytego.com/p/how-uber-manages-petabytes-of-real</link>
<guid>https://blog.bytebytego.com/p/how-uber-manages-petabytes-of-real</guid>
<content:encoded><![CDATA[
<div> 实时数据基础设施, Kafka, Flink, Pinot, Presto, 自动化运营, 大规模数据处理, 高可用性, 快速发展, 定制化技术方案

要点一: Uber在实时数据基础设施中采用了开源技术如Kafka, Flink, Pinot, Presto等，为其业务提供了强大支持。
要点二: Uber通过定制化技术方案提高了开源技术的适配性和性能，如Kafka集群联邦化、FlinkSQL、Pinot新增支持等。
要点三: Uber注重快速系统开发，通过客户端标准化和CI/CD框架实现了系统功能的快速迭代和部署。
要点四: Uber强调了操作自动化，通过自动化工具和监控系统，实现了高效的集群管理和故障排查。
要点五: Uber通过自动化用户引导和数据审计工具，简化了用户使用数据系统的流程，提高了数据的可靠性和准确性。

<br /><br />总结: Uber的实时数据基础设施采用开源技术，并通过定制化技术方案，实现了快速系统开发和高效运营自动化。公司注重用户体验和数据可靠性，通过自动化引导和审计工具，简化用户操作流程，提高数据处理效率。Uber的系统架构为公司持续发展提供了坚实基础。 <div>
<h2><a href="https://bit.ly/FusionAuth_101224">Stop renting auth. Make it yours instead.(Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/FusionAuth_101224" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1456" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5b65d1-d667-4be1-b633-b22149b248d2_2400x2400.png" width="1456" /><div></div></div></a></figure></div><ul><li><p>Developer-Centric: True API first design, quick integration, built on standards, highly flexible &amp; customizable.</p></li><li><p>Hosting Flexibility: You host or we host - the choice is yours with no loss of features.</p></li><li><p>Unlimited: Unlimited IDPs, unlimited users, unlimited tenants, unlimited applications, always free.</p></li><li><p>Total Control: Deploy on any computer, anywhere in the world and integrate easily with any tech stack.</p></li><li><p>Scale Confidently: Lightning-fast performance for 10 users or 10 million users (or more).</p></li><li><p>Data Isolation: Single tenant by design means your data is physically isolated from everyone else&#8217;s.</p></li></ul><p>FusionAuth is a complete auth &amp; user platform that has 15M+ downloads and is trusted by industry leaders.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/FusionAuth_101224"><span>Start For Free</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the Uber Research Paper and Engineering Blog. All credit for the technical details goes to the Uber engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>Uber's real-time data infrastructure is a cornerstone of its business operations, processing massive amounts of data every day.&nbsp;</p><p>From drivers and riders to restaurants and back-end systems, Uber collects petabytes of data to power important features such as customer incentives, fraud detection, and predictions made by machine learning models.</p><p>To manage this vast flow of information, Uber relies on a sophisticated system that handles three key components:&nbsp;</p><ul><li><p>Messaging platforms&nbsp;</p></li><li><p>Stream processing</p></li><li><p>OLAP (OnLine Analytical Processing)</p></li></ul><p>Each element plays a crucial role in ensuring that data is processed and analyzed quickly, allowing Uber to respond to real-time events like ride requests, price changes, and more.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F498ebffc-29c3-46f5-8937-588f2e2a0460_1600x1019.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="927" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F498ebffc-29c3-46f5-8937-588f2e2a0460_1600x1019.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>However, maintaining this real-time infrastructure isn't easy.&nbsp;</p><p>As Uber continues to grow, so do the challenges. The company needs to scale its systems to handle more data, support new use cases, and accommodate a growing number of users. At the same time, they must ensure that the data is always fresh, latency is kept low, and the entire system remains highly available.</p><p>In short, Uber's real-time data system is essential to its ability to make split-second decisions, and the company has built an advanced architecture to keep everything running smoothly, even as demand grows.</p><h2>Critical Requirements of Uber&#8217;s Real-Time Data</h2><p>Uber's real-time data system has some critical requirements to ensure it operates smoothly and efficiently, especially given its massive scale.&nbsp;</p><p>These requirements handle everything from ride pricing to food delivery, ensuring users get the best possible experience.</p><ul><li><p><strong>Consistency: </strong>It is crucial for Uber's mission-critical services. The data needs to be consistent across all regions, with no room for loss or duplication. This is especially important for services that rely on accurate data, like financial dashboards.</p></li><li><p><strong>Availability: </strong>Uber&#8217;s system needs to be up and running almost all the time, with a 99.99% uptime guarantee. Services like dynamic pricing, which adjust ride prices in real time based on supply and demand, simply can't afford downtime.</p></li><li><p><strong>Data Freshness: </strong>Events, like ride requests or order placements, need to be processed within seconds so the system can make real-time decisions, whether it's matching riders with drivers or alerting restaurants of new orders. Some tools, such as the UberEats Restaurant Manager, depend on query latency being super low&#8212;ideally, under a second. This allows restaurants to get instant insights into things like orders, sales, and performance metrics.</p></li><li><p><strong>Scalability: </strong>As Uber grows, its scalability becomes even more important. The data system must be able to expand as more data is generated and new use cases arise, ensuring smooth performance no matter how much demand increases.</p></li><li><p><strong>Cost Efficiency:</strong> Finally, cost efficiency plays a big role. Since Uber operates on low margins, the company focuses on managing resources efficiently, including memory and storage. This helps keep operational costs down while maintaining top performance.</p></li></ul><p>In short, Uber's real-time data infrastructure is designed to be consistent, available, fast, scalable, and cost-effective, ensuring the company's services run smoothly at all times.</p><div><hr /></div><h2><a href="https://bit.ly/Paragon_101524">Tutorial: Build a RAG AI Application with external contextual data in 3 days (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Paragon_101524" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16225646-3d1a-43f1-8765-538e10ef7970_1600x840.png" title="" width="1456" /><div></div></div></a></figure></div><p>There are two parts to building any gen-AI application.</p><ol><li><p>Context ingestion &amp; retrieval</p></li><li><p>Taking action based on that context</p></li></ol><p>Most AI companies are racing to build a knowledge copilot to solve that first problem.</p><p>But building an AI copilot/feature that ingests, stores, and retrieves contextual data from dozens of data sources (ie. Google Drive, Notion, Slack, etc.) can get complex very quickly.</p><p>That said, with the right tech stack and architecture, your team could ship production-ready RAG systems and AI features much faster.</p><p>This RAG tutorial series covers the step-by-step process (repo included) of building a multi-tenant RAG AI chatbot application that:</p><ul><li><p>Ingests users&#8217; external data from Google Drive, Notion, and Slack (Paragon)</p></li><li><p>Chunks and stores this data in a vector store (Pinecone)</p></li><li><p>Retrieves relevant context from these systems to answer queries at runtime (Llama-index)</p></li></ul><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/Paragon_101524"><span>Read or Watch Tutorial</span></a></p><div><hr /></div><h2>Key Technologies Used By Uber</h2><p>Uber's real-time data infrastructure is powered by a combination of advanced open-source technologies, each customized to handle the company&#8217;s massive data needs.&nbsp;</p><p>The diagram below shows the overall landscape.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e74a5c9-a041-4657-a3e4-39017b238e76_1600x1017.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="925" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e74a5c9-a041-4657-a3e4-39017b238e76_1600x1017.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Let&#8217;s take a closer look at the key technologies Uber relies on, how they work, and the unique tweaks that make them fit Uber's requirements.</p><h3>Apache Kafka for Streaming Data</h3><p>Kafka is the backbone of Uber&#8217;s data streaming.&nbsp;</p><p>It handles trillions of messages and petabytes of data daily, helping to transport information from user apps (like driver and rider apps) and microservices. Kafka&#8217;s key role is to move this streaming data to batch and real-time systems.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35bab385-a2ed-4c4f-958d-66e20e5d269b_1600x813.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="740" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35bab385-a2ed-4c4f-958d-66e20e5d269b_1600x813.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>At Uber, Kafka was heavily customized to meet its large-scale needs. Some of the key features are as follows:</p><ul><li><p><strong>Cluster Federation:</strong> Uber created a federated Kafka cluster setup to improve scalability and reliability. With federation, data is distributed across smaller clusters (each with about 150 nodes), making it easier to manage and scale.</p></li><li><p><strong>Dead Letter Queues (DLQ):</strong> When messages fail (due to corruption or unexpected behavior), Uber's Kafka pushes them to a DLQ so they don&#8217;t block live traffic. This keeps data moving smoothly.</p></li><li><p><strong>Consumer Proxy: </strong>Kafka's client libraries were complex, and with so many programming languages in use at Uber, it became difficult to manage. Uber built a proxy layer to simplify client interactions. This layer consumes Kafka messages and forwards them to a user&#8217;s service, streamlining error handling and reducing client complexity.</p></li><li><p><strong>Cross-cluster Replication:</strong> For fault tolerance and redundancy, Uber developed uReplicator, which replicates Kafka messages across data centers. This ensures that data is available globally, even in case of failures.</p></li></ul><h3>Apache Flink for Stream Processing</h3><p>Apache Flink is another critical component of Uber&#8217;s infrastructure, used for processing data streams in real-time.&nbsp;</p><p>Flink can handle complex workloads, scale efficiently, and manage backlogs of millions of messages without slowing down.</p><p>Here&#8217;s how Uber improved Flink for their environment:</p><ul><li><p><strong>FlinkSQL:</strong> Uber built a SQL layer on top of Flink, known as FlinkSQL, to make stream processing more accessible to users. It translates SQL queries into Flink jobs, allowing engineers and non-engineers alike to build real-time applications without the need to know the underlying code.</p></li><li><p><strong>Resource Estimation and Auto-scaling:</strong> Flink jobs can vary in resource requirements. Uber built tools to estimate how much CPU and memory a job needs based on its type and adjust resources automatically as workloads change throughout the day.</p></li><li><p><strong>Failure Recovery:</strong> Flink jobs are continuously monitored, and if a job fails, an automated system restarts it or scales it up as needed.</p></li></ul><p>By implementing these changes, Uber has made Flink more reliable and easier to use at scale, allowing thousands of real-time data processing jobs to run efficiently.</p><p>See the diagram below that shows the Unified Flink Architecture at Uber.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9e8a845-940c-468d-a19c-f39f1a8cc4b4_1600x1017.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="925" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9e8a845-940c-468d-a19c-f39f1a8cc4b4_1600x1017.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Apache Pinot for Real-Time OLAP</h3><p>For quick, low-latency data analysis, Uber uses Apache Pinot.&nbsp;</p><p>Pinot allows real-time analytics on terabytes of data, making it ideal for Uber&#8217;s dashboards and operational insights, like tracking ride demand or Uber Eats orders in real-time.</p><p>Uber enhanced Pinot in several ways:</p><ul><li><p><strong>Upsert Support:</strong> Uber added the ability to handle upserts (updates + inserts), which is crucial for scenarios where data changes frequently, like correcting a ride fare or updating a delivery status. This feature ensures the latest data is always available for analysis.</p></li><li><p><strong>Full SQL Support with Presto:</strong> Although Pinot is great for real-time queries, it lacks certain advanced SQL features like joins. Uber integrated Pinot with Presto, a distributed SQL query engine, to bridge the gap. This combination allows users to run complex queries on Pinot&#8217;s real-time data with low latency.</p></li><li><p><strong>Peer-to-peer Segment Recovery:</strong> Originally, Pinot relied on external storage systems (like HDFS or S3) for backing up data segments, which created bottlenecks. Uber developed a peer-to-peer segment recovery system, allowing data replicas to serve as backups for each other. This improved both speed and reliability.</p></li></ul><h3>HDFS for Long-Term Storage</h3><p>Uber uses HDFS (Hadoop Distributed File System) as the foundation for its long-term data storage. HDFS stores data from Kafka streams and converts it into more efficient formats, like Parquet, for long-term use.</p><p>The data stored in HDFS is used for:</p><ul><li><p><strong>Backfilling:</strong> When systems need to reprocess historical data (for example, if a bug is fixed), HDFS provides the source for this backfill. The stored data is also used to train new machine-learning models or test new data pipelines.</p></li><li><p><strong>Checkpointing:</strong> For services like Flink and Pinot, HDFS is used to store checkpoints&#8212;snapshots of the system&#8217;s state at a particular point in time. This allows systems to recover quickly in case of failure.</p></li></ul><h3>Presto for Interactive Queries</h3><p>Presto is Uber&#8217;s go-to query engine for exploring large datasets in real-time.&nbsp;</p><p>It&#8217;s designed to provide fast, distributed SQL queries across multiple data sources, including Pinot, Hive, and others.</p><p>At Uber, Presto plays a critical role in:</p><ul><li><p><strong>Real-time Data Exploration:</strong> Presto allows engineers and data scientists to query real-time data in Pinot and other systems, providing insights on the fly. For example, an Uber Eats operations team might query real-time order data to monitor restaurant performance or customer trends.</p></li><li><p><strong>Optimization with Pinot:</strong> Uber has customized Presto to push as much query processing as possible to Pinot, reducing query latency and improving efficiency.</p></li></ul><h2>Use Cases</h2><p>Uber's real-time data infrastructure powers a variety of critical use cases, from surge pricing to real-time analytics for Uber Eats.&nbsp;</p><p>Let&#8217;s look at some of the most important use cases supported by Uber&#8217;s data infrastructure.</p><h3>1 - Surge Pricing</h3><p>One of Uber&#8217;s most well-known features is surge pricing, which adjusts fares based on real-time demand and supply conditions in a given area.&nbsp;</p><p>To make this happen, Uber uses a data pipeline powered by Kafka and Flink. Kafka ingests streaming data from ride requests and driver availability, while Flink processes this information in real-time to calculate pricing multipliers for each area.</p><p>The surge pricing pipeline prioritizes data freshness and availability over strict consistency, meaning the system focuses on making quick pricing decisions rather than ensuring every single message is perfectly consistent across all regions.&nbsp;</p><p>This approach allows Uber to respond to changes in demand within seconds, keeping the marketplace balanced while maximizing driver availability.</p><h3>2 - UberEats Restaurant Manager</h3><p>For restaurant partners, Uber provides a dashboard called the UberEats Restaurant Manager, which offers real-time insights into order trends, sales, and service performance.&nbsp;</p><p>This dashboard is powered by Apache Pinot, a real-time OLAP system designed to handle large datasets with low-latency queries.</p><p>Pinot enables fast querying by using pre-aggregated data, meaning that instead of running a complex query every time, certain metrics (like popular menu items or sales figures) are pre-calculated and stored, allowing for quick responses when the restaurant manager requests information.&nbsp;</p><p>This setup ensures that restaurant owners get real-time feedback, helping them make informed decisions about their business.</p><h3>3 - Real-Time Prediction Monitoring</h3><p>To ensure the quality of its machine-learning models, Uber has a system in place for real-time prediction monitoring. This system uses Flink to aggregate predictions from models and compare them against actual outcomes in real-time.</p><p>With thousands of machine learning models deployed across Uber&#8217;s services, monitoring their performance is critical.&nbsp;</p><p>The system processes millions of data points per second to detect any deviations or inaccuracies in model predictions. By continuously aggregating and analyzing this data, Uber ensures that its models are performing as expected and can quickly identify any issues that need attention.</p><h3>4 - Ops Automation</h3><p>During the pandemic, Uber needed a way to respond quickly to operational needs, such as limiting the number of customers at restaurants to comply with health regulations. The Ops Automation system for Uber Eats was designed to provide this flexibility.</p><p>This system uses Presto and Pinot to allow for ad hoc exploration of real-time data, enabling Uber&#8217;s operations team to run custom queries on current data about restaurant orders, couriers, and customer activity. For example, if a restaurant was nearing its capacity limit, the system could automatically trigger alerts or actions, such as temporarily pausing orders or notifying couriers to wait outside.</p><p>This combination of real-time data and flexible querying made it possible for Uber Eats to adapt to fast-changing regulations, helping restaurants stay open while keeping customers and couriers safe.</p><h2>Scaling Strategies</h2><p>To handle the massive scale of its operations, Uber has developed several key strategies for ensuring its data systems are highly available, reliable, and able to handle real-time and historical data processing.&nbsp;</p><p>Three of the most important strategies include the Active-Active Kafka setup, Active-Passive Kafka setup, and a backfill support solution for historical data processing. Let&#8217;s look at each in more detail:</p><h3>1 - Active-Active Kafka Setup</h3><p>For critical services like surge pricing, ensuring high availability and redundancy is a top priority at Uber. To meet this need, Uber uses an Active-Active Kafka setup across multiple regions. This means that Kafka clusters are deployed in different geographic regions, allowing data to be processed and synchronized across these regions in real-time.</p><p>The Active-Active setup ensures that if one region experiences a failure&#8212;whether due to a network issue or a server problem&#8212;Uber&#8217;s systems can continue to function smoothly from another region.&nbsp;</p><p>For example, surge pricing calculations, which depend on real-time supply and demand data, are too important to fail. If the primary region fails, another region can immediately take over and continue processing the data without missing a beat.</p><p>See the diagram below:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00f0c703-4ef5-4a6e-bc5e-82c3a6c86db6_1600x1141.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1038" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F00f0c703-4ef5-4a6e-bc5e-82c3a6c86db6_1600x1141.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>This setup requires careful synchronization of data between regions. Uber uses uReplicator, a tool they developed to replicate Kafka messages across clusters, ensuring the system remains redundant and reliable. Even if one region goes down, the data is preserved and can be quickly restored in the backup region, minimizing disruption to the service.</p><h3>2 - Active-Passive Kafka Setup</h3><p>In addition to Uber's Active-Active Kafka setup, the company also employs an Active-Passive Kafka setup for certain services where strong data consistency is critical. While the Active-Active approach prioritizes availability and redundancy, the Active-Passive strategy is designed for use cases that cannot tolerate any data loss and require more stringent consistency guarantees, such as payment processing or auditing.</p><p>In an Active-Passive Kafka setup, only one consumer (in a specific region, called the primary region) is allowed to process messages from Kafka at any given time.&nbsp;</p><p>If the primary region fails, the system fails over to a backup (passive) region, which then resumes processing from the same point where the primary left off. This ensures that no data is lost during the failover, preserving message order and maintaining data integrity.</p><p>See the diagram below that shows the Active-Passive setup.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bd81bc0-b086-4fa9-bde0-b16c1fe32634_1600x961.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="875" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bd81bc0-b086-4fa9-bde0-b16c1fe32634_1600x961.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The key challenge in Active-Passive setups is offset synchronization&#8212;ensuring that the consumer in the backup region starts processing from the same point as the primary region.&nbsp;</p><p>To achieve this, Uber developed a sophisticated offset management service. This service synchronizes offsets between the active and passive regions, using the uReplicator to replicate Kafka messages across clusters and periodically checkpoint the offset mappings between the regions.</p><h3>3 - Backfill Support with Kappa+ Architecture</h3><p>While real-time data processing is critical, there are times when Uber needs to reprocess historical data&#8212;whether for testing new data pipelines, training machine learning models, or fixing issues after a bug has been discovered. This is where Uber&#8217;s backfill support comes in, allowing them to reprocess data streams without causing significant delays or overhead.</p><p>Uber uses a solution called Kappa+ architecture, which builds on the idea of processing real-time and historical data streams using the same logic.&nbsp;</p><p>Kappa+ allows Uber to reprocess archived data (stored in systems like Hive or HDFS) while using the same stream-processing framework they use for real-time data&#8212;Apache Flink. This eliminates the need for separate systems to handle real-time and batch data, simplifying operations and making the system more efficient.</p><p>The backfill process also includes optimizations to handle the high throughput of historical data.&nbsp;</p><p>For example, Uber uses larger memory buffers and fine-tunes Flink jobs to handle out-of-order data or throttling. This ensures that even when processing large volumes of past data, the system performs smoothly without overwhelming resources.</p><h2>Key Lessons</h2><p>Uber's journey in building its real-time data infrastructure is packed with valuable lessons.</p><p>Here are the key ones to keep in mind:</p><h3>1 - Open-Source Adoption</h3><p>Uber&#8217;s decision to adopt open-source technologies was essential for building its infrastructure quickly and scaling effectively. Tools like Kafka, Flink, and Pinot provided solid foundations.</p><p>However, scaling them to meet Uber&#8217;s massive data needs required significant customizations.&nbsp;</p><p>For example, Uber added features like Kafka cluster federation, FlinkSQL, and Pinot upserts to handle their unique requirements for high availability, low-latency queries, and real-time data streaming.&nbsp;</p><p>While open-source adoption allowed Uber to innovate fast, they also had to invest heavily in adapting these technologies to fit their system&#8217;s scale and complexity.</p><h3>2 - Rapid System Development</h3><p>To keep pace with their evolving business needs, Uber focused on enabling rapid system development.&nbsp;</p><p>One way they achieved this was through client standardization. By ensuring a consistent interface for interacting with systems, Uber reduced the risk of breaking connections when making updates. Additionally, the use of thin client models (simplified client software that offloads much of the processing to centralized servers) allowed Uber to streamline updates and reduce maintenance overhead.</p><p>A strong CI/CD (Continuous Integration/Continuous Deployment) framework was also critical. This framework allowed Uber to test, integrate, and deploy updates automatically, reducing the risk of bugs and ensuring that new features could be quickly rolled out without affecting the system&#8217;s stability.</p><h3>3 - Operational Automation</h3><p>Managing Uber&#8217;s massive data infrastructure manually would be impossible, so operational automation became a key priority.&nbsp;</p><p>Uber emphasized automation for tasks like scaling infrastructure, managing clusters, and deploying new services. This minimized the need for manual intervention, helping the system scale efficiently even as the company grew.</p><p>In addition to automation, Uber implemented robust monitoring and alerting systems. These systems allow the engineering teams to keep track of the health of the infrastructure in real-time, alerting them to any issues so they can be addressed quickly before they impact critical services like surge pricing or Uber Eats operations.</p><h3>4 - User Onboarding and Debugging</h3><p>With so many engineers, data scientists, and operational teams interacting with Uber&#8217;s data infrastructure, automated onboarding was crucial.&nbsp;</p><p>Uber developed processes that allowed new users to quickly access data, create Kafka topics, and launch Flink jobs without needing deep technical knowledge.</p><p>Additionally, automated data discovery and auditing tools helped users find the data they needed and ensured that the data flowing through the system was accurate and reliable. By automating these processes, Uber reduced the workload for their core engineering teams and enabled more users to interact with the data systems independently.</p><h2>Conclusion</h2><p>Uber's real-time data infrastructure is a vital part of its business operations, supporting everything from surge pricing and UberEats dashboards to real-time machine learning predictions.&nbsp;</p><p>By leveraging a combination of open-source technologies like Apache Kafka, Flink, Pinot, and Presto, Uber has built a highly scalable and reliable system that processes trillions of messages and petabytes of data every day.</p><p>Key innovations, such as the Active-Active Kafka setup for high availability and the Kappa+ architecture for seamless backfill support, allow Uber to maintain real-time and historical data processing with minimal disruption. The infrastructure&#8217;s success also stems from Uber&#8217;s emphasis on customization, rapid development with thin client models, and extensive operational automation.</p><p>As Uber continues to scale, these technologies and strategies provide the foundation for further innovation, enabling it to respond to new challenges while maintaining the high-performance standards required to serve millions of users globally. Uber's journey highlights the importance of combining open-source solutions with tailored engineering efforts to meet the needs of a fast-growing, data-driven organization.</p><p><strong>References:</strong></p><ul><li><p><a href="https://arxiv.org/pdf/2104.00087">Real-time Data Infrastructure at Uber</a></p></li><li><p><a href="https://kafka.apache.org/documentation/#gettingStarted">Kafka Introduction</a></p></li><li><p><a href="https://docs.pinot.apache.org/basics/concepts/architecture">Pinot Architecture</a></p></li><li><p><a href="https://prestodb.io/docs/current/overview/concepts.html">What is Presto?</a></p></li></ul><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p><p></p>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 15:31:29 GMT</pubDate>
</item>
<item>
<title>EP133: API vs SDK</title>
<link>https://blog.bytebytego.com/p/ep133-api-vs-sdk</link>
<guid>https://blog.bytebytego.com/p/ep133-api-vs-sdk</guid>
<content:encoded><![CDATA[
<div> Kubernetes, API, SDK, Terraform, HTTP Status Code

总结:<br /><br />
本文介绍了Kubernetes的流行原因和基本概念。同时讨论了API和SDK的区别及应用情况，以及Terraform如何将代码转化为云端架构。此外，还介绍了HTTP状态码以及常用的部署策略。文章还提供了关于QA Wolf和New Relic AI Monitoring的赞助信息。整体而言，这篇文章覆盖了软件开发中的重要概念和工具，并推荐了一些解决方案来提高软件工程团队的效率和质量。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Why is Kubernetes Popular | What is Kubernetes? (Youtube video)</p></li><li><p>API vs SDK</p></li><li><p>How does Terraform turn Code into Cloud? </p></li><li><p>HTTP Status Code You Should Know</p></li><li><p>Top 5 Most-Used Deployment Strategies</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/QAWolf_101224">&#9986;&#65039;Cut your QA cycles down to minutes with QA Wolf (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/QAWolf_101224" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="736" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fb097e-150b-4ce0-a3c7-7d8d6416cabc_1473x745.png" title="" width="1456" /><div></div></div></a></figure></div><p>If slow QA processes bottleneck you or your software engineering team and you&#8217;re releasing slower because of it &#8212; you need to check out QA Wolf.</p><p>Their AI-native approach gets engineering teams to <a href="https://bit.ly/QAWolf_101224">80% automated end-to-end test coverage</a> and helps them <strong>ship 5x faster</strong> by reducing QA cycles from hours to minutes.</p><p><a href="https://bit.ly/QAWolf_101224">QA Wolf</a> takes testing off your plate. They can get you:</p><ul><li><p>Unlimited parallel test runs</p></li><li><p>24-hour maintenance and on-demand test creation</p></li><li><p>Human-verified bug reports sent directly to your team</p></li><li><p>Zero flakes guaranteed</p></li></ul><p>The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.</p><p>With QA Wolf, <a href="https://bit.ly/QAWolf_101224CaseStudy">Drata&#8217;s team of 80+ engineers</a> achieved 4x more test cases and <strong>86% faster QA cycles</strong>.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/QAWolf_101224"><span>Schedule a demo to learn more</span></a></p><div><hr /></div><h2>Why is Kubernetes Popular | What is Kubernetes?</h2><div class="youtube-wrap" id="youtube2-lv0DdVLZuHc"><div class="youtube-inner"></div></div><div><hr /></div><h2><a href="https://bit.ly/NewRelic_101224">Optimizing AI chatbot performance with New Relic AI Monitoring (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc715da2f-dc99-43b0-9993-ffe23a4552d4_1200x630.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="630" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc715da2f-dc99-43b0-9993-ffe23a4552d4_1200x630.jpeg" width="1200" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>If your chatbot is prone to errors or lags, it can frustrate your users, leading to poor experiences and potential loss of business. By using an observability tool like New Relic AI monitoring, you can observe key metrics such as response time, token usage, and error rates to ensure your chatbot performs optimally and delivers a smooth and efficient experience for your users.</p><p>Check out this guide to learn how to monitor and optimize the performance of a chatbot using New Relic AI Monitoring.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/NewRelic_101224"><span>Get started</span></a></p><div><hr /></div><h2>API vs SDK</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc76a04ea-9a5c-49f7-94b4-1ea2c06a6a69_1280x1664.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><p>API (Application Programming Interface) and SDK (Software Development Kit) are essential tools in the software development world, but they serve distinct purposes: <br /> <br />API: An API is a set of rules and protocols that allows different software applications and services to communicate with each other. </p><ol><li><p>It defines how software components should interact. </p></li><li><p>Facilitates data exchange and functionality access between software components. </p></li><li><p>Typically consists of endpoints, requests, and responses. </p></li></ol><p>SDK: An SDK is a comprehensive package of tools, libraries, sample code, and documentation that assists developers in building applications for a particular platform, framework, or hardware. </p><ol><li><p>Offers higher-level abstractions, simplifying development for a specific platform. </p></li><li><p>Tailored to specific platforms or frameworks, ensuring compatibility and optimal performance on that platform. </p></li><li><p>Offer access to advanced features and capabilities specific to the platform, which might be otherwise challenging to implement from scratch. </p></li></ol><p>The choice between APIs and SDKs depends on the development goals and requirements of the project. <br /> <br />Over to you: Which do you find yourself gravitating towards &#8211; APIs or SDKs &#8211; Every implementation has a unique story to tell. What's yours?</p><div><hr /></div><h2>How does Terraform turn Code into Cloud? </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1652" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0365c8a7-9e4d-4b44-9e3c-7243ef0708f5_1280x1652.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><p>There are multiple stages in a Terraform workflow: </p><ol><li><p>Write Infrastructure as Code <br /> <br />Define resources, providers, and configurations in Terraform configuration files. <br />Use variables, modules, and functions to make the code reusable and maintainable. <br />Integrate with Terraform community registries for ready-to-use modules. <br /></p></li><li><p>Terraform Plan <br />Preview the changes Terraform will make to the infrastructure by running &#8220;terraform plan&#8221;. It can be triggered as part of a CI/CD pipeline. <br />Terraform compares the desired state defined in the configuration file with the current state in the state file. <br /></p></li><li><p>Terraform Apply <br />Run &#8220;terraform apply&#8221; to create, update, or delete resources based on the plan. <br />Terraform makes API calls to the specified providers (AWS, Azure, GCP, Kubernetes, etc) to provision the resources. <br />The state file is updated to reflect the new state of the infrastructure. <br /></p></li><li><p>Infrastructure Ready <br />Terraform state file acts as a single source of truth for the current state of the infrastructure. <br />State file enables version control and collaboration between team members for future changes. </p></li></ol><p>Over to you: Have you used Terraform in your projects?</p><div><hr /></div><h2>HTTP Status Code You Should Know</h2><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37f0612c-440c-4d53-8c92-c77c8da9ff9c_1206x1166.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1166" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37f0612c-440c-4d53-8c92-c77c8da9ff9c_1206x1166.jpeg" width="1206" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The response codes for HTTP are divided into five categories:<br /><br />Informational (100-199)<br />Success (200-299)<br />Redirection (300-399)<br />Client Error (400-499)<br />Server Error (500-599)<br /><br />These codes are defined in RFC 9110. To save you from reading the entire document (which is about 200 pages), here is a summary of the most common ones:<br /><br />Over to you: HTTP status code 401 is for Unauthorized. Can you explain the difference between authentication and authorization, and which one does code 401 check for?</p><div><hr /></div><h2>Top 5 Most-Used Deployment Strategies</h2><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd35c1466-f8cc-47fe-bbe7-4315e39e093b_2925x3900.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1941" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd35c1466-f8cc-47fe-bbe7-4315e39e093b_2925x3900.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>- Big Bang Deployment </p><p>- Rolling Deployment </p><p>- Blue-Green Deployment </p><p>- Canary Deployment </p><p>- Feature Toggle </p><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 12 Oct 2024 15:31:15 GMT</pubDate>
</item>
<item>
<title>CAP, PACELC, ACID, BASE - Essential Concepts for an Architect’s Toolkit</title>
<link>https://blog.bytebytego.com/p/cap-pacelc-acid-base-essential-concepts</link>
<guid>https://blog.bytebytego.com/p/cap-pacelc-acid-base-essential-concepts</guid>
<content:encoded><![CDATA[
<div> 分布式系统、CAP定理、PACELC定理、ACID模型、BASE模型
<br />
CAP定理是分布式系统设计的基本原则，指导架构师在一致性、可用性和分区容忍之间做出权衡。PACELC定理对CAP进行了扩展，更细致地解释了在正常操作和网络分区中一致性和可用性之间的权衡。ACID和BASE模型则提供了设计事务性系统和处理分布式数据库一致性挑战的指导。通过仔细考虑CAP、PACELC、ACID和BASE模型的影响，架构师可以做出符合应用特定要求和约束的明智选择。 
<br /><br />总结: 
CAP定理指导分布式系统设计权衡一致性、可用性和分区容忍。PACELC定理扩展CAP，解释了一致性和可用性的权衡。ACID和BASE模型提供了处理事务和分布式数据库一致性挑战的指导。架构师应谨慎考虑这些模型的影响，并根据特定要求和约束做出明智选择。 <div>
<p>In today's world, distributed systems have become ubiquitous, powering everything from social media platforms and e-commerce websites to financial systems and healthcare applications.&nbsp;</p><p>As these systems grow in complexity and scale, it becomes increasingly important for software architects and developers to understand the inherent trade-offs and challenges associated with designing and building such systems.&nbsp;</p><p>One of the key challenges in distributed systems is ensuring data consistency, availability, and partition tolerance. These properties are often in tension with one another, and achieving all three simultaneously is impossible, as stated by the famous CAP theorem. This theorem has become a fundamental principle in distributed systems design, guiding architects in making informed decisions about the trade-offs between consistency, availability, and partition tolerance.</p><p>Building upon the CAP theorem, other frameworks and models have emerged to help reason about the trade-offs in distributed systems. The PACELC theorem extends the CAP theorem to provide a more nuanced understanding of the trade-offs between consistency and availability during normal operations and network partitions.</p><p>In addition to CAP and PACELC, the ACID (Atomicity, Consistency, Isolation, Durability) and BASE (Basically Available, Soft-state, Eventually Consistent) models provide guidance for designing transactional systems and dealing with the challenges of eventual consistency in distributed databases.</p><p>By carefully considering the implications of CAP, PACELC, ACID, and BASE, architects can make informed choices that align with the specific requirements and constraints of their applications.</p><p>In this article, we will dive deep into these concepts, exploring their definitions and implications. We will also discuss the limitations of these models and the factors to consider when choosing the right approach for a given use case.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F732e86b8-f57e-4e0a-941f-5c6e15c2a53d_1591x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1464" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F732e86b8-f57e-4e0a-941f-5c6e15c2a53d_1591x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>The CAP Theorem</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/cap-pacelc-acid-base-essential-concepts">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 15:32:49 GMT</pubDate>
</item>
<item>
<title>How TikTok Manages A 200K File MonoRepo with Sparo</title>
<link>https://blog.bytebytego.com/p/how-tiktok-manages-a-200k-file-monorepo</link>
<guid>https://blog.bytebytego.com/p/how-tiktok-manages-a-200k-file-monorepo</guid>
<content:encoded><![CDATA[
<div> Azure OpenAI, performance, Sparo, monorepo, Git

总结:<br /><br />
本文介绍了TikTok前端团队在面对日益增长的monorepo规模时遇到的Git操作速度慢的问题。他们开发了Sparo工具，利用Git的sparse checkout和partial clone功能来加快大型monorepo的Git操作。Sparo实现了checkout profiles和mirrored commands等增强功能，极大提高了开发效率。通过Sparo，TikTok团队看到了巨大的性能提升，克隆时间从23分钟减少到2分钟，其它操作时间也有显著改善。Sparo的成功应用，表明及时解决大型代码库的性能问题的重要性。随着越来越多公司面对庞大monorepo的性能挑战，类似Sparo这样的工具将变得越来越重要。 <div>
<h2><a href="https://bit.ly/Datadog_100824">Learn to monitor Azure OpenAI (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Datadog_100824" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="736" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93d2d74a-f006-44bb-b30c-b20326b18559_1694x856.png" width="1456" /><div></div></div></a></figure></div><p>Learn how you can track Azure OpenAI usage and performance, from monitoring your API requests and token usage, to tracking performance in aggregate or by language model.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Datadog_100824"><span>Download the cheatsheet</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the TikTok Developer Blog. All credit for the technical details goes to the TikTok engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>TikTok, the popular short-form video-sharing platform, has a large and rapidly growing codebase for its web front end. This front-end app is built using TypeScript and is organized as a monorepo - a single Git repository containing code for multiple projects and libraries.</p><p>As TikTok's frontend engineering team and feature set grew, so did the size and complexity of this monorepo.&nbsp;</p><p>Over time, it expanded to contain over 1,000 separate projects and more than 200,000 source files. While the monorepo architecture provided benefits like shared code and tooling, it also started to cause significant performance issues.</p><p>Developers began to notice slowness with common Git operations that are essential to their everyday workflows. These slowdowns were a major drain on productivity, wasting valuable engineering time and ruining the development experience.&nbsp;</p><p>The issues stemmed from the sheer size of the codebase that Git had to process for each operation. To address this problem, TikTok's frontend infrastructure team began exploring solutions. They experimented with Git's built-in performance features like partial clones, shallow clones, and Git LFS, but found that they were insufficient for the monorepo's scale and growth rate.</p><p>Ultimately, the team developed an in-house tool called Sparo to tackle the monorepo performance challenges. They also eventually made Sparo open-source.</p><p>In this article, we'll explore how Sparo works and the benefits it has provided for TikTok's front-end engineering team.</p><h2>The Problem of Git Slowness in Large Monorepos</h2><p>A monorepo, short for a monolithic repository, is a software development strategy where a single repository contains multiple projects, libraries, and services, often maintained by different teams. This contrasts with the multi-repo approach, where each project has its separate repository.&nbsp;</p><p>The diagram below shows the difference between monorepos, multi-repos, and monolith code bases.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5610638b-18c9-4029-8f58-ed3a2a38c2d6_1600x1016.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="925" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5610638b-18c9-4029-8f58-ed3a2a38c2d6_1600x1016.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Monorepos have gained popularity among large tech companies like Google, Facebook, and Microsoft for several reasons.</p><ul><li><p>Firstly, monorepos enable better code sharing and reuse across projects, reducing duplication and promoting standardization.&nbsp;</p></li><li><p>Secondly, they simplify dependency management, as projects within the monorepo can easily reference and use each other.&nbsp;</p></li><li><p>Thirdly, monorepos provide a unified view of the entire codebase, making it easier to perform cross-cutting changes and maintain a consistent build and test infrastructure.</p></li></ul><p>However, as monorepos grow in size and complexity, Git operations become increasingly slow for everyone.&nbsp;</p><p>This is exactly what happened at TikTok. Commands like git clone to download a copy of the repository, git checkout to switch between branches, git status to see current changes, and git commit to save new changes took much longer than they had previously.&nbsp;</p><p>Here are some interesting stats from their observations:</p><ul><li><p>Cloning the repository to start working on it could take over 40 minutes for developers with slower network connections. Even on fast connections, a full clone took over 20 minutes.&nbsp;</p></li><li><p>Checking out a different branch took over a minute and a half.&nbsp;</p></li><li><p>Just running git status to see the current state of the working copy took 7 seconds, interrupting the flow of developers.</p></li><li><p>Committing code changes was also painful at around 15 seconds per commit.</p></li></ul><h2>How Sparo Improves MonoRepo Performance?</h2><p>At its core, Sparo leverages two advanced features of Git to dramatically speed up common operations on large monorepos: sparse checkout and partial clone.</p><p>Let&#8217;s look at both in detail.</p><h3>Sparse Checkout</h3><p>Git's sparse checkout feature allows specifying a subset of files or directories to check out from a repository, rather than fetching the entire codebase. Sparo uses this to only check out the files needed to build a given application - namely, the project and its dependencies.</p><p>In a monorepo with hundreds or thousands of projects, the files needed for any one project are a relatively small and slow-growing subset compared to the repo as a whole.&nbsp;</p><p>By using sparse checkout to limit the working copy to just this subset, Sparo significantly reduces the amount of data that needs to be fetched and the number of files that Git operations need to process. This results in much faster checkout times.</p><p>See the diagram below that explains the concept of sparse checkout in Git in which a developer only has to work on the Android Client Development.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8da339-fed9-4b3f-95d0-884bc1be6b52_1600x1017.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="925" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b8da339-fed9-4b3f-95d0-884bc1be6b52_1600x1017.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Partial Clones</h3><p>While sparse checkout reduces the number of files in the working copy, a standard Git clone still fetches the contents of every file in the repo and their complete history.&nbsp;</p><p>For large repositories, this still means a sizable amount of data transfer and disk usage, even if much of it isn't needed locally.</p><p>Git's partial clone feature, enabled by passing the --filter=blob:none option to git clone, optimizes this by only fetching file contents on-demand as they are needed, and excluding objects that aren't reachable by any reference. This reduces the size of the initial clone and speeds up subsequent fetches.&nbsp;</p><p>See the diagram below for a visual representation of the same.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe38d59c1-e0c7-4086-91f8-2b75457ff297_799x414.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="414" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe38d59c1-e0c7-4086-91f8-2b75457ff297_799x414.png" width="799" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://github.blog/open-source/git/get-up-to-speed-with-partial-clone-and-shallow-clone/">GitHub Blog</a></figcaption></figure></div><p>Unlike a shallow clone, the full history is still available if needed, just not eagerly downloaded. Also, unlike Git LFS, partial clone works automatically for all files without a separate storage system.</p><h2>Additional Sparo Enhancements</h2><p>Sparo adds some additional enhancements beyond leveraging those two key Git features.</p><h3>Checkout Profiles</h3><p>Sparo introduces the concept of checkout profiles, which are predefined sets of directories to include in a sparse checkout. Profiles act as an easy starting point for new hires or contributors to discover what part of the codebase is relevant to a given team.</p><p>For example, a frontend team might define a profile that checks out their five most actively developed projects, the dependencies of those projects, and a few additional directories like docs and config.</p><p>These profiles are defined in a JSON file and checked into the repo, making it easy for developers to share profiles and quickly set up their working copy to match their team's standard environment.&nbsp;</p><p>See the example of the profile file:</p><pre><code>{
&nbsp;&nbsp;"selections": [
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"selector": "--to",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"argument": "project-a"
&nbsp;&nbsp;&nbsp;&nbsp;},
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"selector": "--to",&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"argument": "project-b"
&nbsp;&nbsp;&nbsp;&nbsp;},
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"selector": "folder",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"argument": "docs"
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;]
}</code></pre><p>This profile would help the developers checkout project-a, project-b, and the docs folder.</p><p>With the profile created, developers can use sparo checkout --profile to checkout the repository according to the profile. This checks out only the specified projects and folders, significantly reducing the data fetched and the time taken.</p><h3>Mirrored Commands</h3><p>To make adoption frictionless, Sparo provides its command line interface that aims to be a drop-in replacement for the standard Git CLI. In other words, Sparo is fully compatible with Git, so teams can incrementally adopt it while still interoperating with standard Git usage.</p><p>By mirroring the Git interface, Sparo minimizes the learning curve and allows it to be gradually adopted into existing workflows. Familiar commands like clone, checkout, status, add, commit, etc. are all provided with the same syntax as their Git equivalents.&nbsp;</p><p>Developers can use the Sparo versions of commands to take advantage of the optimized performance, while still falling back to the regular Git commands if needed for advanced cases.&nbsp;</p><p>Under the hood, the Sparo versions of these commands automatically set the appropriate configurations to enable partial clone and sparse checkout based on the active profile. They also allow collecting anonymous usage telemetry to power dashboards for monitoring the adoption and performance of the tool.</p><h2>Sparo Performance Gains</h2><p>The TikTok team saw dramatic performance improvements after adopting Sparo:</p><ul><li><p>clone time dropped from 23 minutes to just 2 minutes</p></li><li><p>checkout time went from 1.5 minutes to 30 seconds</p></li><li><p>status command time reduced from 7 seconds to 1 second</p></li><li><p>commit time improved from 15 seconds to 11 seconds</p></li></ul><p>These improvements have a huge impact on developer productivity and experience. Instead of waiting minutes for Git operations to complete, developers can iterate and commit changes quickly, maintaining their flow and focus.</p><h2>Conclusion</h2><p>As monorepos grow in size and complexity, maintaining good performance for common Git operations becomes increasingly challenging. TikTok's frontend team experienced this firsthand as their TypeScript monorepo surpassed 1,000 projects and 200,000 source files, leading to significant slowdowns in commands like clone, checkout, status, and commit.</p><p>To address this problem, the TikTok team developed Sparo, an open-source tool that leverages Git's sparse checkout and partial clone features to speed up Git operations on large monorepos.</p><p>The TikTok engineering team's journey with Sparo highlights the importance of proactively addressing performance issues in rapidly growing codebases.&nbsp;</p><p>Sparo's roadmap includes a telemetry plugin system for monitoring and support for additional front-end build systems. As more companies encounter performance challenges with huge monorepos, tools like Sparo can become increasingly important for maintaining developer productivity.</p><p><strong>References:</strong></p><ul><li><p><a href="https://developers.tiktok.com/blog/2024-sparo-faster-git-for-frontend-monorepos">Faster Git for Frontend Monorepos</a></p></li><li><p><a href="https://www.infoq.com/news/2024/09/tiktok-monorepo-sparo/">TikTok Releases Tool to Improve MonoRepo Performance</a></p></li><li><p><a href="https://tiktok.github.io/sparo/">Sparo GitHub</a></p></li><li><p><a href="https://github.blog/open-source/git/bring-your-monorepo-down-to-size-with-sparse-checkout/">Bring your monorepo down to size with sparse-checkout</a></p></li></ul><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong><br /></p>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 15:31:29 GMT</pubDate>
</item>
<item>
<title>EP132: Big O Notation 101: The Secret to Writing Efficient Algorithms</title>
<link>https://blog.bytebytego.com/p/ep132-big-o-notation-101-the-secret</link>
<guid>https://blog.bytebytego.com/p/ep132-big-o-notation-101-the-secret</guid>
<content:encoded><![CDATA[
<div> Rust, Big O Notation, Authentication Mechanisms, DDD, NoSQL Database  
总结:  
Rust被应用于解决低延迟挑战，并在技术会议中讨论。Big O Notation是编写高效算法的关键。认证机制包括SSH Keys、OAuth Tokens、SSL Certificates和Credentials。领域驱动设计强调通过领域建模驱动软件设计。NoSQL数据库包括MongoDB、Cassandra、Redis、Couchbase、Neo4j、Amazon DynamoDB、Apache Hbase、Elasticsearch和CouchDB，各自适用于不同场景。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Big O Notation 101: The Secret to Writing Efficient Algorithms </p></li><li><p>Top 4 Forms of Authentication Mechanisms </p></li><li><p>8 Key Concepts in DDD</p></li><li><p>Top 9 NoSQL Database Use Cases </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/ScyllaDB_100524">Free tickets to P99 CONF - 60+ low-latency engineering talks (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/ScyllaDB_100524" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22440194-878a-4cfc-a2f3-66a531d67f18_1600x840.png" width="1456" /><div></div></div></a></figure></div><p>P99 CONF is the technical conference for anyone who obsesses over high-performance, low-latency applications. Naturally, Rust is a core topic.<br /><br />How is Rust being applied to solve today&#8217;s low latency challenges &#8211; and where it could be heading next? That&#8217;s what experts like Carl Lerche (Tokio creator, Rust contributor) and Ashley Williams (Rust core team, Rust Foundation founder) will be exploring.<br /><br />Join 20K of your peers for an unprecedented opportunity to learn from engineers from Disney, Shopify, LinkedIn, Netflix, Google, Meta, Uber + more &#8211; for free, from anywhere. </p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/ScyllaDB_100524"><span>GET YOUR FREE TICKET</span></a></p><p>Bonus: Registrants get 30-day free access to the complete O&#8217;Reilly library and a chance to win 1 of 500 swag packs!</p><div><hr /></div><h2>Big O Notation 101: The Secret to Writing Efficient Algorithms </h2><p>From simple array operations to complex sorting algorithms, understanding the Big O Notation is critical for building high-performance software solutions. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90a6414b-bc2f-4f35-9110-bd1b54acdf74_1280x1664.gif" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><ol><li><p>O(1) <br />This is the constant time notation. The runtime remains steady regardless of input size. For example, accessing an element in an array by index and inserting/deleting an element in a hash table. <br /></p></li><li><p>O(n) <br />Linear time notation. The runtime grows in direct proportion to the input size. For example, finding the max or min element in an unsorted array. <br /></p></li><li><p>O(log n) <br />Logarithmic time notation. The runtime increases slowly as the input grows. For example, a binary search on a sorted array and operations on balanced binary search trees. <br /></p></li><li><p>O(n^2) <br />Quadratic time notation. The runtime grows exponentially with input size. For example, simple sorting algorithms like bubble sort, insertion sort, and selection sort. <br /></p></li><li><p>O(n^3) <br />Cubic time notation. The runtime escalates rapidly as the input size increases. For example, multiplying two dense matrices using the naive algorithm. <br /></p></li><li><p>O(n logn) <br />Linearithmic time notation. This is a blend of linear and logarithmic growth. For example, efficient sorting algorithms like merge sort, quick sort, and heap sort <br /></p></li><li><p>O(2^n) <br />Exponential time notation. The runtime doubles with each new input element. For example, recursive algorithms solve problems by dividing them into multiple subproblems. <br /></p></li><li><p>O(n!) <br />Factorial time notation. Runtime skyrockets with input size. For example, permutation-generation problems. <br /></p></li><li><p>O(sqrt(n)) <br />Square root time notation. Runtime increases relative to the input&#8217;s square root. For example, searching within a range such as the Sieve of Eratosthenes for finding all primes up to n. <br /></p></li></ol><p>Over to you: What else will you add to better understand the Big O Notation?</p><div><hr /></div><h2><a href="https://bit.ly/NewRelic_100524">How to monitor a Next.js application with app-based router (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/NewRelic_100524" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="420" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab7ea82-d92d-4f48-825c-b27a33f1754e_800x420.png" title="" width="800" /><div></div></div></a></figure></div><p>Next.js is a powerful JavaScript framework that offers optimized speed and performance for both development and runtime.&nbsp;In this blog post, you&#8217;ll learn how to set up application performance monitoring for the server side and browser monitoring for the frontend using the new App Router, giving you full-stack observability in your Next.js application.&nbsp;</p><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/NewRelic_100524"><span>Get started for free</span></a></p><div><hr /></div><h2>Top 4 Forms of Authentication Mechanisms </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram, map" class="sizing-normal" height="1700" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F969687e0-8f52-4a52-9ecc-40ab5e56aa2b_1280x1700.jpeg" title="diagram, map" width="1280" /><div></div></div></a></figure></div><ol><li><p>SSH Keys: <br />Cryptographic keys are used to access remote systems and servers securely <br /></p></li><li><p>OAuth Tokens: <br />Tokens that provide limited access to user data on third-party applications <br /></p></li><li><p>SSL Certificates: <br />Digital certificates ensure secure and encrypted communication between servers and clients <br /></p></li><li><p>Credentials: <br />User authentication information is used to verify and grant access to various systems and services <br /></p></li></ol><p>Over to you: How do you manage those security keys? Is it a good idea to put them in a GitHub repository?</p><div><hr /></div><h2>8 Key Concepts in DDD</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb19d22da-0681-4ffc-a95a-ab527452bc0f_1350x1536.gif" title="No alternative text description for this image" /><div></div></div></a></figure></div><ul><li><p>Domain Driven Design <br />Domain-driven design advocates driving the design of software through domain modeling. <br /> <br />Unified language is one of the key concepts of domain-driven design. A domain model is a bridge across the business domains. <br /></p></li><li><p>Business Entities <br />The use of models can assist in expressing business concepts and knowledge and in guiding further development of software, such as databases, APIs, etc. <br /></p></li><li><p>Model Boundaries <br />Loose boundaries among sets of domain models are used to model business correlations. <br /></p></li><li><p>Aggregation <br />An Aggregate is a cluster of related objects (entities and value objects) that are treated as a single unit for the purpose of data changes. <br /></p></li><li><p>Entities vs. Value Objects <br />In addition to aggregate roots and entities, there are some models that look like disposable, they don't have their own ID to identify them, but are more as part of some entity that expresses a collection of several fields. <br /></p></li><li><p>Operational Modeling <br />In domain-driven design, in order to manipulate these models, there are a number of objects that act as "operators". <br /></p></li><li><p>Layering the architecture <br />In order to better organize the various objects in a project, we need to simplify the complexity of complex projects by layering them like a computer network. <br /></p></li><li><p>Build the domain model <br />Many methods have been invented to extract domain models from business knowledge.</p></li></ul><div><hr /></div><h2>Top 9 NoSQL Database Use Cases </h2><p>Different databases excel in different areas and it&#8217;s important to choose the right database for the requirement. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4eeaa11-fb42-4cc2-bee4-a9b3cb4df08c_1280x1664.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><ol><li><p>MongoDB (Document Store) <br />Used for content management systems and catalog management. Features BSON format, schema-less design, supports horizontal scaling with sharding, and high availability with replication <br /></p></li><li><p>Cassandra (Wide-column Store) <br />Ideal for time-series data management and recommendation engines. Offers wide-column format, distributed architecture, and CQL for SQL-like querying. <br /></p></li><li><p>Redis (Key-Value Store) <br />Suited for Cache, Session Management, and Gaming Leaderboards. Provides in-memory storage, support for complex data structures, and persistence options with RDB and AOF. <br /></p></li><li><p>Couchbase (Document Store with Key-Value) <br />Used for content management systems and e-commerce platforms. Combines key-value and document-based operations with memory-first architecture and cross-data center replication. <br /></p></li><li><p>Neo4j (Graph DB) <br />Excellent for social networking and fraud detection. Features ACID compliance, index-free adjacency, Cypher Query Language, and HA cluster capabilities. <br /></p></li><li><p>Amazon DynamoDB (Key-Value and Document) <br />Perfect for serverless and IoT applications. Supports both key-value and complex document data, managed by AWS, with features like partition data across nodes and DynamoDB streams. <br /></p></li><li><p>Apache Hbase (Wide-Column Store) <br />Used for data warehouse and large-scale data processing. Modeled after Google&#8217;s Bigtable, offers Hadoop integration, auto-sharding, strong consistency, and region servers. <br /></p></li><li><p>Elasticsearch (Search Engine) <br />Ideal for full-text search and log and event data analysis. Built on Apache Lucene, document-oriented, with sharding and replication capabilities, and a RESTful interface. <br /></p></li><li><p>CouchDB (Document Store) <br />Suitable for mobile applications and CMS. Document-oriented, ensures data consistency without locking, supports eventual consistency, and uses a RESTful API. <br /></p></li></ol><p>Over to you: Which other NoSQL database would you add to the list?</p><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 05 Oct 2024 15:30:57 GMT</pubDate>
</item>
<item>
<title>API Gateway</title>
<link>https://blog.bytebytego.com/p/api-gateway</link>
<guid>https://blog.bytebytego.com/p/api-gateway</guid>
<content:encoded><![CDATA[
<div> API Gateway, modern software architectures, microservices-based systems, single entry point, API requests

总结:<br /><br />API Gateway在现代软件架构中起着至关重要的作用，特别是在基于微服务的系统中。它们作为所有API请求的单一入口点，提供统一的接口来访问各种服务和数据。通过将客户端应用程序与后端微服务解耦，API Gateway简化了API管理，并增强了系统的整体性能和安全性。API Gateway有助于在构建可扩展和可维护的系统时提升开发人员体验。通过使用API Gateway，组织可以更好地控制其API景观。随着软件系统复杂性的持续增长，API Gateway将在促进服务和客户端之间有效通信和集成方面发挥越来越重要的作用。 <div>
<p>API Gateways are essential components in modern software architectures, particularly microservices-based systems.</p><p>They act as a single entry point for all API requests, providing a unified interface for accessing various services and data. By decoupling client applications from backend microservices, API Gateways simplify API management and enhance the overall performance and security of the system.</p><p>API Gateways help enhance the developer experience while building scalable and maintainable systems. Organizations can achieve better control over their API landscape by using API Gateways.</p><p>As the complexity of software systems continues to grow, API Gateways will play an increasingly important role in enabling effective communication and integration between services and clients.</p><p>In this post, we&#8217;ll explore the various aspects of API Gateways in detail.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d572a6-c679-43d9-affa-54cc711a4b75_2250x2504.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1620" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d572a6-c679-43d9-affa-54cc711a4b75_2250x2504.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>What is an API Gateway</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/api-gateway">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 15:30:42 GMT</pubDate>
</item>
<item>
<title>The Trillion Message Kafka Setup at Walmart</title>
<link>https://blog.bytebytego.com/p/the-trillion-message-kafka-setup</link>
<guid>https://blog.bytebytego.com/p/the-trillion-message-kafka-setup</guid>
<content:encoded><![CDATA[
<div> Enterprise, Kafka, Walmart, MPS, Challenges
总结:<br /><br />这篇文章介绍了Walmart在大规模Apache Kafka部署中遇到的挑战以及他们设计的消息代理服务(MPS)的解决方案。其中主要挑战包括消费者再平衡、有毒消息和成本问题。MPS通过Reader线程、有界缓冲队列、Order Iterator、Writer线程、Offset Commit线程和Consumer Service REST API等组件实现了将Kafka消息消费与分区模型解耦的目的。这种设计使得Walmart能够避免常见问题，并有效提升处理消息的效率和可靠性，同时降低了成本。 <div>
<h2><a href="https://bit.ly/WorkOS_092824">The Enterprise Ready Conference for engineering leaders (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/WorkOS_092824" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="854" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F327c28fa-4555-414e-83df-ab52767d1f10_2784x1632.png" title="" width="1456" /><div></div></div></a></figure></div><p>The Enterprise Ready Conference is a one-day event in SF, bringing together product and engineering leaders shaping the future of enterprise SaaS. <br /><br />The event features a curated list of speakers with direct experience building for the enterprise, including <strong>OpenAI</strong>, <strong>Vanta</strong>, <strong>Checkr</strong>, <strong>Dropbox</strong>, and <strong>Canva</strong>.<br /><br />Topics include advanced identity management, compliance, encryption, and logging &#8212; essential yet complex features that most enterprise customers require.<br /><br />If you are a founder, exec, PM, or engineer tasked with the enterprise roadmap, this conference is for you. You&#8217;ll get detailed insights from industry leaders that have years of experience navigating the same challenges you face today. And best of all, it&#8217;s completely free since it&#8217;s hosted by WorkOS.</p><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/WorkOS_092824"><span>Request an invite</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the Walmart Global Tech Blog. All credit for the technical details goes to the Walmart engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>Walmart has a massive Apache Kafka deployment with 25K+ consumers across private and public cloud environments.</p><p>This deployment processes trillions of Kafka messages per day at 99.99% availability. It supports critical use cases such as:</p><ul><li><p>Movement of data</p></li><li><p>Event-driven microservices</p></li><li><p>Streaming analytics</p></li></ul><p>At Walmart's scale, the Kafka setup must be able to handle sudden traffic spikes. Also, consumer applications are written in multiple languages. Therefore, all consumer applications must adopt some best practices to maintain the same level of reliability and quality.</p><p>In this post, we&#8217;ll look at the main challenges of a Kafka setup at this scale. Then, we will look at how Walmart&#8217;s engineering team enhanced its setup to overcome these challenges and reliably process messages cost-efficiently.</p><h2>Challenges with Kafka at Walmart&#8217;s Scale</h2><p>Let&#8217;s start with understanding the main challenges that Walmart faced.</p><h3>1 - Consumer Rebalancing</h3><p>One of the most frequent problems was related to consumer rebalancing.&nbsp;</p><p>But what triggers consumer rebalancing in Kafka?</p><p>This can happen due to the changing number of consumer instances within a consumer group.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d8d97b8-fddf-4ad6-a420-b8178d7b6ea7_1600x923.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="840" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d8d97b8-fddf-4ad6-a420-b8178d7b6ea7_1600x923.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Several scenarios are possible such as:</p><ul><li><p>A consumer pod may enter or leave a consumer group. This can happen due to Kubernetes deployments, rolling restarts, or automatic scale-ins or scale-outs. Whenever it happens, Kafka needs to redistribute the partitions among the consumers.</p></li><li><p>The Kafka broker may believe that a consumer has failed. If the broker has not received a heartbeat from a consumer within the configured session timeout, it assumes that the consumer has died. This can happen if the consumer&#8217;s JVM exits or experiences a long stop-the-world garbage collection pause.</p></li><li><p>The Kafka broker may believe that a consumer is stuck and trigger rebalancing. If the consumer takes longer than a threshold to poll for the next batch of records, the broker marks it as stuck. This can happen when processing the previous batch takes too long.</p></li></ul><p>Consumer rebalancing is needed to ensure partitions are evenly distributed. However, rebalancing can cause disruption and increased latency, particularly due to the near real-time nature of the e-commerce landscape.</p><h3>2 - Poison Pill Messages</h3><p>A &#8220;poison pill&#8221; message in Kafka is a message that consistently causes a consumer to fail when attempting to process it. This can happen due to various reasons such as:</p><ul><li><p><strong>Malformed Data: </strong>The message payload may not be in an expected format. For example, invalid JSON or missing required fields. This may cause the consumer to throw an exception while processing.</p></li><li><p><strong>Unexpected Data: </strong>The message content might be syntactically valid but semantically incorrect. In other words, it might violate some business constraints.</p></li><li><p><strong>Bugs in the Consumer Code: </strong>If there&#8217;s a bug (like a null pointer exception) in the code that handles the message, the processing will fail.</p></li></ul><p>When the consumer encounters such a message, it will fail to process it and throw an exception. By default, the consumer will return to the broker to fetch the same batch of messages again. Since the poison pill message is still present in that batch, the consumer will again fail to process it, and this loop continues indefinitely.</p><p>As a result, the consumer gets stuck on this one bad message and is unable to make progress on other messages in the partition. This is similar to the &#8220;head-of-line blocking&#8221; problem in networking.</p><h3>3 - Cost Concerns</h3><p>There is a strong coupling between the number of partitions in a Kafka topic and the maximum number of consumers that can read from that topic in parallel. This coupling can lead to increased costs when trying to scale consumer applications to handle higher throughput.</p><p>For example, consider that you have a Kafka topic with 10 partitions and 10 consumer instances reading from this topic. Now, if the rate of incoming messages increases and the consumers are unable to keep up (i.e. consumer lag starts to increase), you might want to scale up your consumer application by adding more instances.</p><p>However, once you have 10 consumers (one for each partition) in a single group, adding more consumers to that group won&#8217;t help because Kafka will not assign more than one consumer from the same group to a partition. The only way to allow more consumers in a group is to increase the number of partitions in the topic.</p><p>However, increasing the number of partitions comes with its challenges and costs.</p><ul><li><p>Kafka has a recommended limit on the number of partitions per broker (for example, 4000 partitions per broker). If you keep increasing partitions, you may hit this limit and need to scale the Kafka brokers to larger instances, even if the brokers have sufficient resources to handle the current load. Scaling to larger broker instances is expensive.</p></li><li><p>Increasing partitions requires coordination among the Kafka team, the producer, and the consumer teams. In a large organization with thousands of Kafka pipelines, this coordination overhead is significant.</p></li><li><p>More partitions also mean more open file handles, increased memory usage, and more threads on the Kafka brokers. This can lead to higher resource utilization and costs.</p></li></ul><h2>Designing the Messaging Proxy Service (MPS)</h2><p>To overcome the challenges mentioned in the previous section, the Walmart engineering team designed a Message Proxy Service (MPS).</p><p>The diagram below shows a high-level view of MPS.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c3c9ab4-201f-4142-a92f-2a799a765f70_1600x995.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="905" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c3c9ab4-201f-4142-a92f-2a799a765f70_1600x995.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The MPS aims to decouple Kafka message consumption from the constraints imposed by Kafka&#8217;s partition-based model. It works like this:</p><ul><li><p>MPS acts as a proxy between the Kafka brokers and the actual message consumer applications. It reads messages from Kafka partitions and puts them into a separate in-memory queue.</p></li><li><p>Consumer applications don&#8217;t directly read from Kafka. Instead, they receive messages from MPS via HTTP/REST. This allows the consumer applications to scale independently of the number of Kafka partitions.</p></li><li><p>MPS ensures in-order message processing per key, handles consumer application failures, and manages offset commits back to Kafka.</p></li></ul><p>The diagram below shows the detailed design of the MPS with all its components</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d8ac999-99eb-4672-ae8b-0bf3193ad818_1600x1091.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="993" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d8ac999-99eb-4672-ae8b-0bf3193ad818_1600x1091.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Let&#8217;s now look at the various components of the MPS in more detail.</p><h3>Reader Thread</h3><p>This is a single thread that reads messages from Kafka.&nbsp;</p><p>It writes the messages from the Kafka broker into a bounded queue called the &#8220;PendingQueue&#8221;. If the PendingQueue reaches its maximum size, the reader thread will pause reading from Kafka.&nbsp;</p><p>This is a form of backpressure to prevent the queue from growing indefinitely if the writer threads cannot keep up.</p><h3>Bounded Buffer Queue (PendingQueue)</h3><p>This is a queue that sits between the reader thread and the writer threads. It has a maximum size to prevent it from consuming too much memory.</p><p>The PendingQueue allows the reader and write threads to work at different speeds. The reader can read messages as fast as Kafka can provide them, while the writers can process them at their own pace.</p><h3>Order Iterator</h3><p>This component ensures that messages with the same key are processed in the order they were received from Kafka.</p><p>It goes through the messages in the PendingQueue and skips any message if there is already an earlier message with the same key being processed. At any given time, at most one message per key is being handled by the writer threads.</p><h3>Writer Threads</h3><p>These are a pool of threads that take messages from the PendingQueue and send them to the consumer applications via HTTP POST requests.</p><p>If a POST request fails, the writer thread will retry the request a few times. If the retries are exhausted or if the consumer application returns certain HTTP codes, the writer thread will put the message into a Dead Letter Queue (DLQ).</p><p>The writer threads also help manage offsets. They update a shared data structure to keep track of processed offsets.</p><h3>Offset Commit Thread</h3><p>This is a separate thread that periodically wakes up (for example, every minute) and commits the Kafka offsets of processed messages using the Kafka consumer API.</p><p>It checks the shared data structure that is updated by the writer threads. Then, it commits the latest continuous offset for each partition. For example, if messages with offsets 1, 2, 3, and 5 have been processed for a partition, it will commit offset 3 (because 4 is missing).</p><p>By committing offsets periodically, MPS tells Kafka which messages it has processed successfully. If MPS crashes or is restarted, it will start consuming messages from the last committed offset, avoiding reprocessing messages that have already been handled.</p><h3>Consumer Service REST API</h3><p>This is the specification that the actual message consumer applications need to implement to receive messages from the MPS.</p><p>It defines the format of the HTTP POST request that the MPS writer threads will send (headers, body, etc.). It also specifies the meaning of different HTTP response codes that the consumer application can return.</p><p>See the table below that shows the API specification:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fb44f9c-6026-481a-9e6c-5a7c6034e4bb_1400x615.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="615" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fb44f9c-6026-481a-9e6c-5a7c6034e4bb_1400x615.png" width="1400" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://medium.com/walmartglobaltech/reliably-processing-trillions-of-kafka-messages-per-day-23494f553ef9">Walmart Tech Blog</a></figcaption></figure></div><h2>Implementation of MPS</h2><p>MPS was implemented as a Kafka Connect sink connector.</p><p>For reference, Kafka Connect is a framework for connecting Kafka with external systems such as databases, key-value stores, search indexes, and file systems. It provides a standard way of defining connectors that move data into and out of Kafka.</p><p>The diagram below shows a high-level view of Kafka Connect</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27f0fe55-d1e0-4e2b-92a4-6cfd2236fd75_1600x999.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="909" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27f0fe55-d1e0-4e2b-92a4-6cfd2236fd75_1600x999.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>By implementing MPS as a Kafka Connect sink connector, the developers were able to use several features provided by the Kafka Connect Framework such as:</p><ul><li><p><strong>Multi-tenancy:</strong> Kafka Connect allows running multiple connectors on a single cluster. This means that a single MPS deployment can serve multiple consumer applications (tenants).</p></li><li><p><strong>Dead Letter Queue (DLQ) Handling:</strong> Kafka Connect has built-in support for handling messages that cannot be processed.</p></li><li><p><strong>Offset Commits:</strong> Kafka Connect provides APIs for committing offsets. MPS uses these for its offset commit thread.</p></li><li><p><strong>Scalability: </strong>Kafka Connect is designed to be scalable and fault-tolerant. By building on top of Kafka Connect, MPS inherits these properties.</p></li></ul><p>Apart from this, the consumer services (applications that process the messages) are designed to be stateless. This means they don&#8217;t maintain any persistent state locally. Any state they need is either passed with the message or stored in an external database.</p><p>Being stateless allows these services to be easily scaled up or down in Kubernetes based on changes in message volume. If more messages need to be processed, Kubernetes can simply start more instances of the consumer service. If fewer messages are coming in, Kubernetes can terminate some instances to save resources.</p><p>Importantly, this scaling of the consumer services is independent of MPS and Kafka. MPS continues to read from Kafka and deliver messages to the consumer services regardless of how many instances of the consumer service are running.</p><h2>Additional Points To Consider</h2><p>Here are some additional points worth considering based on the MPS solution implemented by Walmart.</p><h3>1 - Rebalancing of the MPS</h3><p>MPS is also essentially a Kafka consumer. It reads messages from Kafka topics and makes them available to the application consumers via REST endpoints.&nbsp;</p><p>Like any other Kafka consumer, MPS would also be subject to rebalancing when the number of MPS instances changes. Based on the details, however, it seems that MPS is designed to handle rebalancing gracefully. The separation of the reader thread (which polls Kafka) and the writer threads (which send messages to the REST consumers) is the key here.</p><p>As long as MPS comes back up quickly after a rebalance, the REST consumers should be able to continue processing messages without substantial lag. The MPS design also includes a bounded buffer (the PendingQueue) between the reader thread and writer threads. This buffer helps to smoothen any temporary fluctuations in the rate at which MPS is reading from Kafka.</p><h3>2 - Choice of REST</h3><p>MPS calls REST APIs exposed by the consumer instances. Interestingly, the choice was REST and not something like gRPC.&nbsp;</p><p>This may be because of the simplicity of REST. Also, REST is widely supported by almost all languages and frameworks.&nbsp;</p><h3>3 - Potential Increase in Complexity</h3><p>While MPS solves several problems, it also introduces an additional layer to the system.&nbsp;</p><p>Instead of just having Kafka and the consumer applications, there is now a proxy service in the middle. This means more components to develop, deploy, monitor, and maintain.</p><h2>Conclusion</h2><p>The implementation of MPS helped Walmart achieve some key improvements.</p><ul><li><p>Most rebalances have now been eliminated except for rare restarts or network issues. With MPS, the reader thread consistently puts all polled messages into the PendingQueue within the allocated time. This prevents rebalances triggered by the Kafka broker thinking the consumer is stuck.&nbsp;</p></li><li><p>Poison pill messages are handled in a better way. With MPS, consumer services can detect poison pill messages and notify MPS using specific HTTP return codes (600 and 700).&nbsp;</p></li><li><p>MPS enables cost savings in multiple ways. Firstly, consumer services are now stateless and can scale quickly in Kubernetes based on demand. They don&#8217;t need to be scaled in advance. Secondly, Kafka clusters can be scaled based on throughput rather than the number of partitions.</p></li></ul><p><strong>References:</strong></p><ul><li><p><a href="https://medium.com/walmartglobaltech/reliably-processing-trillions-of-kafka-messages-per-day-23494f553ef9">Reliably Processing Trillions of Kafka Messages Per Day</a></p></li><li><p><a href="https://docs.confluent.io/platform/current/connect/index.html">Kafka Connect</a></p></li></ul><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p><p><br /></p>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 15:30:39 GMT</pubDate>
</item>
<item>
<title>EP131: How Uber Served 40 Million Reads with Integrated Redis Cache?</title>
<link>https://blog.bytebytego.com/p/ep131-how-uber-served-40-million</link>
<guid>https://blog.bytebytego.com/p/ep131-how-uber-served-40-million</guid>
<content:encoded><![CDATA[
<div> Kafka, Redis, AWS Lambda, distributed lock, Enterprise Ready Conference
总结:<br /><br />
Uber使用集成的Redis缓存为4000万次读请求提供服务，包括CacheFront读写、多区域缓存预热和Redis与Docstore分片。AWS Lambda之所以快速，主要依靠Function Invocation、Assignment Service、Firecracker MicroVM和Component Storage。分布式锁有6个主要用例，包括Leader Election、Task Scheduling、Resource Allocation、Microservices Coordination、Inventory Management和Session Management。企业Ready会议聚集了企业SaaS的产品和工程领导者，涵盖高级身份管理、合规性、加密和日志等议题。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Top Kafka Use Cases You Should Know (Youtube video)</p></li><li><p>How Uber Served 40 Million Reads with Integrated Redis Cache? </p></li><li><p>What makes AWS Lambda so fast? </p></li><li><p>Why do we need to use a distributed lock?</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/WorkOS_092824">The Enterprise Ready Conference for engineering leaders (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/WorkOS_092824" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="854" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F327c28fa-4555-414e-83df-ab52767d1f10_2784x1632.png" width="1456" /><div></div></div></a></figure></div><p>The Enterprise Ready Conference is a one-day event in SF, bringing together product and engineering leaders shaping the future of enterprise SaaS. <br /><br />The event features a curated list of speakers with direct experience building for the enterprise, including <strong>OpenAI</strong>, <strong>Vanta</strong>, <strong>Checkr</strong>, <strong>Dropbox</strong>, and <strong>Canva</strong>.<br /><br />Topics include advanced identity management, compliance, encryption, and logging &#8212; essential yet complex features that most enterprise customers require.<br /><br />If you are a founder, exec, PM, or engineer tasked with the enterprise roadmap, this conference is for you. You&#8217;ll get detailed insights from industry leaders that have years of experience navigating the same challenges you face today. And best of all, it&#8217;s completely free since it&#8217;s hosted by WorkOS.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/WorkOS_092824"><span>Request an invite</span></a></p><div><hr /></div><h2>Top Kafka Use Cases You Should Know</h2><div class="youtube-wrap" id="youtube2-Ajz6dBp_EB4"><div class="youtube-inner"></div></div><div><hr /></div><h2>How Uber Served 40 Million Reads with Integrated Redis Cache? </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="1591" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7449b2ef-b91c-4a06-a8da-ac41d4fabb34_1280x1591.gif" title="graphical user interface" width="1280" /><div></div></div></a></figure></div><p>There are 3 main parts of the implementation: </p><ol><li><p>CacheFront Read and Writes with CDC </p><ul><li><p>Uber built CacheFront - an integrated caching solution with Redis, Docstore, and MySQL. </p></li><li><p>Rather than the microservice, Docstore&#8217;s query engine communicates with Redis for read requests. </p></li><li><p>For cache hits, the query engine fetches data from Redis. For cache misses, the request goes to the storage engine and the database. </p></li><li><p>In the case of writes, Docstore&#8217;s CDC service (Flux) invalidates the records in Redis. It tails MySQL binlog events to trigger the invalidation. </p><p></p></li></ul></li><li><p>Multi-Region Cache Warming with Redis Streaming </p><ul><li><p>A region fail-over can result in cache misses and overload the database. </p></li><li><p>To handle this, Uber&#8217;s engineering team uses cross-region Redis replication. This is done by tailing the Redis write stream to replicate keys to the remote region. </p></li><li><p>In the remote region, the stream consumer issues read requests to the query engine that reads the database and updates the cache. <br /> </p></li></ul></li><li><p>Redis and Docstore Sharding </p><ul><li><p>All teams in Uber use Docstore and some generate a huge number of requests. </p></li><li><p>Both Redis and Docstore instances are sharded or partitioned to handle the load. But a single Redis cluster going down may create a hot DB shard. </p></li><li><p>To prevent this, they partitioned the Redis cluster using a scheme that was different from the DB sharding. This ensures that the load is evenly distributed. </p></li></ul></li></ol><p>Over to you: Would you have done something differently?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1440" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86fdda13-d57b-4d9c-b868-ac35d3c52569_1600x1582.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/software-architecture-patterns">Software Architecture Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/the-saga-pattern">The Saga Pattern</a></p></li><li><p><a href="https://blog.bytebytego.com/p/infrastructure-as-code">Infrastructure as Code</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-data">A Crash Course on Scaling the Data Layer</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-load-balancers">A Crash Course on Load Balancers for Scaling</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>What makes AWS Lambda so fast? </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1622" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe0c0849-e36d-4fff-9425-a9dd5e00de6f_1280x1622.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><p>There are 4 main pillars: </p><ol><li><p>Function Invocation <br />AWS Lambda supports synchronous and asynchronous invocation. <br /> <br />In synchronous invocation, the caller directly calls the Lambda function using AWS CLI, SDK, or other services. <br /> <br />In asynchronous invocation, the caller doesn&#8217;t wait for the function&#8217;s response. The request is authorized and an event is placed in an internal SQS queue. Pollers read messages from the queue and send them for processing. <br /></p></li><li><p>Assignment Service <br />The Assignment Service manages the execution environments. <br /> <br />The service is written in Rust for high performance and is divided into multiple partitions with a leader-follower approach for high availability. <br /> <br />The state of execution environments is written to an external journal log. <br /></p></li><li><p>Firecracker MicroVM <br />Firecracker is a lightweight virtual machine manager designed for running serverless workloads such as AWS Lambda and AWS Fargate. <br /> <br />It uses Linux&#8217;s Kernel-based virtual machine to create and manage secure, fast-booting microVMs. <br /></p></li><li><p>Component Storage <br />AWS Lambda also has to manage the state consisting of input data and function code. </p><p><br />To make it efficient, it uses multiple techniques: </p><ul><li><p>Chunking to store the container images more efficiently. </p></li><li><p>Using convergent encryption to secure the shared data. This involves appending additional data to the chunk to compute a more robust hash. </p></li><li><p>SnapStart feature to reduce cold start latency by pre-initializing the execution environment <br /></p></li></ul></li></ol><p>Over to you: Which other features do you think make AWS Lambda fast?</p><div><hr /></div><h2>Why do we need to use a distributed lock?</h2><p>A distributed lock is a mechanism that ensures mutual exclusion across a distributed system. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d97726d-333d-434e-b00d-7e1d56c19def_1329x1536.gif" title="No alt text provided for this image" width="1329" /><div></div></div></a></figure></div><p>Top 6 Use Cases for Distributed Locks</p><ol><li><p>Leader Election<br />Distributed locks can be used to ensure that only one node becomes the leader at any given time.</p><p></p></li><li><p>Task Scheduling<br />In a distributed task scheduler, distributed locks ensure that a scheduled task is executed by only one worker node, preventing duplicate execution.<br /></p></li><li><p>Resource Allocation<br />When managing shared resources like file systems, network sockets, or hardware devices, distributed locks ensure that only one process can access the resource at a time.<br /></p></li><li><p>Microservices Coordination<br />When multiple microservices need to perform coordinated operations, such as updating related data in different databases, distributed locks ensure that these operations are performed in a controlled and orderly manner.<br /></p></li><li><p>Inventory Management<br />In e-commerce platforms, distributed locks can manage inventory updates to ensure that stock levels are accurately maintained when multiple users attempt to purchase the same item simultaneously.<br /></p></li><li><p>Session Management<br />When handling user sessions in a distributed environment, distributed locks can ensure that a user session is only modified by one server at a time, preventing inconsistencies.</p></li></ol><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 28 Sep 2024 15:31:11 GMT</pubDate>
</item>
<item>
<title>Software Architecture Patterns</title>
<link>https://blog.bytebytego.com/p/software-architecture-patterns</link>
<guid>https://blog.bytebytego.com/p/software-architecture-patterns</guid>
<content:encoded><![CDATA[
<div> client-server, architectural patterns, productivity, code quality, communication
<br />
<br />
总结: 软件架构师经常在其职业生涯中不断遇到类似的目标和问题，而建筑模式提供了解决这些常见设计挑战的系统化方法。这些模式捕捉了各种系统和软件元素的核心设计结构，允许它们在不同项目和场景中得以重复使用。了解关于架构模式的关键好处包括提高生产力，改善代码质量，促进沟通，加快开发周期。其中，客户端-服务器模式是一种广泛使用的网络通信模型，在该模型中，客户端向服务器发送请求，并服务器响应请求。 <div>
<p>Software architects often encounter similar goals and problems repeatedly throughout their careers. These challenges can arise within a single project, across multiple projects within a company, or even throughout an architect's career.&nbsp;</p><p>Architectural patterns provide a systematic approach to solving these recurring design issues.</p><p>In essence, architectural patterns are reusable approaches to building software that address common design challenges. These patterns capture the core design structures of various systems and software elements, allowing them to be reused across different projects and scenarios.</p><p>Some key benefits of knowing about architectural patterns are as follows:</p><ul><li><p><strong>Increased productivity:</strong> Instead of reinventing the wheel for each project, architects can rely on established patterns to guide design decisions while saving time and effort. The productivity increase allows teams to focus on the unique aspects of their projects rather than reinventing solutions for recurring issues.</p></li><li><p><strong>Improved code quality:</strong> By following standard patterns, developers can produce higher-quality code that is more maintainable, scalable, and easier to understand.</p></li><li><p><strong>Better communication:</strong> Patterns provide a common language and vocabulary making it easy to discuss and communicate design decisions.</p></li><li><p><strong>Faster development cycles:</strong> With the help of established patterns, development teams can accelerate their development cycles.</p></li></ul><p>In this post, we&#8217;ll look at the most popular software architecture patterns used by developers and architects across organizations and systems.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86fdda13-d57b-4d9c-b868-ac35d3c52569_1600x1582.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1440" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86fdda13-d57b-4d9c-b868-ac35d3c52569_1600x1582.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>Client-Server Pattern</h2><p>Client-server architecture is a widely used model for network communication, where a client (user or application) sends requests to a server, and the server responds with the requested data or service. This architecture can be implemented on a single machine or across different machines connected through a network.</p><p>See the diagram below for a typical client-server setup.</p>
      <p>
          <a href="https://blog.bytebytego.com/p/software-architecture-patterns">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 26 Sep 2024 15:31:31 GMT</pubDate>
</item>
<item>
<title>How Uber Scaled Cassandra for Tens of Millions of Queries Per Second?</title>
<link>https://blog.bytebytego.com/p/how-uber-scaled-cassandra-for-tens</link>
<guid>https://blog.bytebytego.com/p/how-uber-scaled-cassandra-for-tens</guid>
<content:encoded><![CDATA[
<div> 关键词：QA Wolf, Uber, Cassandra, 问题，解决方案

总结：<br /><br />本文介绍了QA Wolf提供的AI-native方法能帮助软件工程团队提升5倍的发布速度，通过减少QA周期，将测试工作从团队的责任中解放出来。同时，文章还探讨了Uber的Cassandra数据库架构，以及团队面对的挑战和解决方案，如节点替换的可靠性、Cassandra轻量级事务的错误率和数据不一致等问题。通过一系列技术改进和优化，Uber的Cassandra服务在规模上取得了显著的成功。 <div>
<h2><a href="https://bit.ly/QAWolf_092424">&#9986;&#65039;Cut your QA cycles down to minutes with QA Wolf (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/QAWolf_092124https://bit.ly/QAWolf_092424" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="736" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fb097e-150b-4ce0-a3c7-7d8d6416cabc_1473x745.png" title="" width="1456" /><div></div></div></a></figure></div><p>If slow QA processes bottleneck you or your software engineering team and you&#8217;re releasing slower because of it &#8212; you need to check out QA Wolf.</p><p>Their AI-native approach gets engineering teams to <a href="https://bit.ly/QAWolf_092424">80% automated end-to-end test coverage</a> and helps them ship 5x faster by reducing QA cycles from hours to minutes.</p><p><a href="https://bit.ly/QAWolf_092424">QA Wolf</a> takes testing off your plate. They can get you:</p><ul><li><p>Unlimited parallel test runs</p></li><li><p>24-hour maintenance and on-demand test creation</p></li><li><p>Human-verified bug reports sent directly to your team</p></li><li><p>Zero flakes guaranteed</p></li></ul><p>The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.</p><p>With QA Wolf, <a href="https://bit.ly/QAWolf_092124CaseStudy">Drata&#8217;s team of 80+ engineers</a> achieved 4x more test cases and 86% faster QA cycles.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/QAWolf_092424"><span>Schedule a demo to learn more</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the Uber Engineering Blog. All credit for the technical details goes to the Uber engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>Uber&#8217;s app is a wonderful piece of engineering, enabling the movement of millions of people around the world and tens of millions of food and grocery deliveries.</p><p>One common thing across each trip and delivery is the need for low-latency and highly reliable database interactions. Uber made this a reality with their Cassandra database as a service platform.</p><p>Supporting a multitude of Uber&#8217;s mission-critical OLTP workloads for more than six years, the Cassandra database as a service has achieved some amazing stats:</p><ul><li><p>Tens of millions of queries per second</p></li><li><p>Petabytes of data</p></li><li><p>Tens of thousands of Cassandra nodes</p></li><li><p>Thousands of unique keyspaces</p></li><li><p>Hundreds of unique Cassandra clusters with over 400+ nodes per cluster</p></li><li><p>Multi-region support</p></li></ul><p>However, this scale wasn&#8217;t achieved overnight. Over the years, the Uber engineering team faced numerous operational challenges.&nbsp;</p><p>In this post, we&#8217;ll pull back the curtains from the architecture of Uber&#8217;s Cassandra setup and the multiple problems they solved to reach the desired scale.</p><h2>The Cassandra Team Responsibilities</h2><p>As mentioned, Uber runs Cassandra as a managed service. A dedicated team takes care of the platform's day-to-day operations. Here are the responsibilities of the team:</p><ul><li><p>Implement new features in Cassandra and contribute to the community. This also includes critical bug fixes.</p></li><li><p>Integrate Cassandra into Uber&#8217;s ecosystem. This involves changes to the control plane, configuration management tools, observability, and alert management platform at Uber.</p></li><li><p>Build the managed Cassandra solution as a one-stop shop for Uber&#8217;s application teams.</p></li><li><p>Ensure 4 nines of availability (99.99%) and 24/7 support for the application teams.</p></li><li><p>Guide on best practices and data modeling needs of the application teams.</p></li></ul><h2>Architecture of Uber&#8217;s Cassandra Setup</h2><p>The diagram below shows the overall Cassandra ecosystem at Uber.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14331c91-4223-4995-922d-2cec96438ea4_1600x994.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="905" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14331c91-4223-4995-922d-2cec96438ea4_1600x994.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>On a high level, the Cassandra cluster spans across regions with the data replicated between them.&nbsp;</p><p>Uber&#8217;s in-house stateful management system Odin handles the configuration and orchestration of thousands of clusters. Together this forms the Cassandra Managed Service that powers different types of workloads, ranging from read-skewed to mixed to write-skewed.</p><p>See the diagram below to understand Odin&#8217;s role:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44cd2499-06a0-47ef-866d-f4c377246531_1600x1264.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1150" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44cd2499-06a0-47ef-866d-f4c377246531_1600x1264.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Let&#8217;s now look at the major components of the Cassandra database as a service architecture:</p><h3>Cassandra Framework</h3><p>This is an in-house framework developed by Uber and is responsible for the lifecycle of running Cassandra in Uber&#8217;s production environment.</p><p>The framework is powered by Uber&#8217;s stateful control plane Odin. It adheres to Odin&#8217;s standards and abstracts the complexity of Cassandra&#8217;s various functionalities such as:</p><ul><li><p>Node selection and replacement</p></li><li><p>Rolling restarts and capacity adjustments</p></li><li><p>Decommissioning nodes</p></li><li><p>Starting and stopping nodes</p></li></ul><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b304636-0f78-4a19-8226-c3e7d1715bd8_1600x988.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="899" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b304636-0f78-4a19-8226-c3e7d1715bd8_1600x988.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Cassandra Client</h3><p>The Cassandra client is the interface between the applications and the Cassandra clusters.&nbsp;</p><p>Uber forked the Go and Java open-source Cassandra clients and adapted them to work in Uber&#8217;s ecosystem. These clients use the service discovery mechanism to find the initial nodes to connect. This way there&#8217;s no need to hardcode the Cassandra endpoints in the application layer.</p><p>Some of the enhancements made by Uber within these clients are around additional observability features such as:</p><ul><li><p>Capturing query fingerprints</p></li><li><p>DDL/DML features mapping</p></li></ul><h3>Service Discovery</h3><p>Service discovery is a critical piece of large-scale distributed systems. It helps discover service instances on the fly, preventing hard coding and unnecessary configurations.</p><p>See the diagram below that shows the concept of service discovery:&nbsp;</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bc4eea2-54fc-4395-b7be-c45d5c7c628e_1600x981.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="893" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bc4eea2-54fc-4395-b7be-c45d5c7c628e_1600x981.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>At Uber, each Cassandra cluster is uniquely identified and their nodes can be discovered in real time.</p><p>Whenever a Cassandra node changes its status (Up, Down, or Decommission), the framework notifies the service discovery about the change. Applications (or service consumers) use the service discovery as the first contact point to connect to a Cassandra node. As the nodes change their status, the service discovery adjusts the list.</p><h3>Challenges of Scaling Uber&#8217;s Cassandra Service</h3><p>Since the inception of Cassandra&#8217;s service at Uber, it has continued to grow every year and more critical use cases have been added.</p><p>The service was hit with significant reliability challenges. Let&#8217;s look at some of the most important ones in more detail.</p><h4>1 - Unreliable Node Replacement</h4><p>Node replacement is a critical part of any large-scale fleet. There are multiple reasons for node replacement:</p><ul><li><p>Hardware failures</p></li><li><p>Fleet optimization</p></li><li><p>Changes to the deployment topology</p></li><li><p>Disaster recovery</p></li></ul><p>Cassandra provides a graceful node replacement approach that decommissions the existing node and adds a new node. But there were a few hiccups such as:</p><ul><li><p>Node decommissioning getting stuck</p></li><li><p>Failing node addition</p></li><li><p>Data inconsistency</p></li></ul><p>Every node replacement does not face these issues, but even a small percentage has the potential to impact the entire fleet. It adds operational overhead to teams. For example, even a 95% success rate means 5 failures out of 100 node replacements. In case 500 nodes are replaced every day, 25 failures may easily engage 2 engineers just recovering from these failures.</p><p>It was important to fix the root problem and that was Cassandra not cleaning up hint files for orphan nodes.&nbsp;</p><p>Think of it like this: You have a big family message board at home. When a family member moves out, everyone keeps their old sticky notes about them on the board. When you decide to get a new message board, you carefully copy all these notes to the new board.</p><p>The same was the case with Cassandra. A legit node N1 may store hint files locally for its peer node that was part of the Cassandra ring. However, even when the peer node is not part of the ring, node N1 does not purge the hint files. When N1 decommissions, it transfers all the orphan hint files to its next successor. Over time, the hint files keep growing, resulting in terabytes of garbage hint files. Transferring such a big file could take multiple days.</p><p>The team made a few changes in Cassandra:</p><ul><li><p>Proactively purge the hint files belonging to orphan nodes.</p></li><li><p>Dynamically adjust the hint transfer rate limiter so the transfer can finish in hours instead of days.</p></li></ul><p>Another fix was related to the node decommission step erroring out due to parallel activity, such as rolling restart due to fleet upgrades. The control plane wasn&#8217;t able to probe Cassandra about the decommissioned state. To handle this, they improved Cassandra's bootstrap and decommission path by exposing the state so the control plane could get the current status and take necessary action.</p><p>See the snippet below that explains the approximate change:</p><pre><code>public boolean isDecommissionFailed()
{
  if (operationMode == Mode.<em>LEAVING</em> &amp;&amp; hasDecommissionFailed)
  {
      return true;
  }
  return false;
}

public boolean isBootstrapFailed()
{
  if (operationMode == Mode.<em>JOININ</em>G &amp;&amp; hasBootstrapFailed)
  {
      return true;
  }
  return false;
}</code></pre><p>Source: <a href="https://www.uber.com/en-IN/blog/how-uber-optimized-cassandra-operations-at-scale/">Uber&#8217;s Engineering Blog</a></p><p></p><p>After these changes, the node replacement became 99.99% reliable.</p><h3>2 - Cassandra&#8217;s Lightweight Transactions Error Rate</h3><p>Few business use cases relied on Cassandra&#8217;s Lightweight Transactions at scale. However, these cases suffered higher error rates every other week.</p><p>It was a general belief that Cassandra&#8217;s Lightweight Transactions were unreliable. One of the errors was due to pending range due to multiple simultaneous node replacements. It&#8217;s similar to a mismanaged library where too many librarians are replaced at once and tracking books becomes quite difficult.&nbsp;</p><p>When a new node N2 replaces an old node N1, the Gossip code path on N2 continues pointing to N1&#8217;s IP address, resulting in a DNS resolution error. Ultimately, N2 could not function as expected, and restarting it was the only option.&nbsp;</p><p>Uber&#8217;s engineering team improved the error handling inside the Gossip protocol, making Cassandra Lightweight Transactions more robust.</p><h3>3 - Data Inconsistency Issues</h3><p>Another problem was related to data inconsistency due to sluggish Cassandra repairs.</p><p>Cassandra repairs are an important activity for every Cassandra cluster to fix data inconsistencies. While there are open-source solutions to trigger the repair, Uber did not want a control-plane-based solution. In their view, the repair should be an integral part of Cassandra similar to compaction.</p><p>With this goal, the Uber Cassandra team implemented the repair orchestration inside Cassandra itself. See the diagram below:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74af6ca0-2a71-4dde-b0c2-ec7ca6cec090_1600x1127.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1026" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74af6ca0-2a71-4dde-b0c2-ec7ca6cec090_1600x1127.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>On a high level, they assigned a dedicated thread pool to the repair scheduler. The repair scheduler inside Cassandra maintains a new replicated table for the node status and repair history.</p><p>The scheduler picks the node that ran the repair first and continues orchestration to ensure each table and all ranges are repaired.&nbsp;</p><p>Since the implementation of a fully automated repair scheduler inside Cassandra, there has been no dependency on the control plane, which reduced the operational overhead significantly.</p><h2>Conclusion</h2><p>Uber&#8217;s Cassandra setup is a testament to the importance of incremental changes to build a large-scale fleet.&nbsp;</p><p>In this post, we&#8217;ve taken a deep look at the architecture of Uber&#8217;s Cassandra setup and the design of the managed service. We also looked at multiple challenges the engineering team overcame to make the Cassandra database service more reliable at scale.</p><p>Reference:&nbsp;</p><ul><li><p><a href="https://www.uber.com/en-IN/blog/how-uber-optimized-cassandra-operations-at-scale/">How Uber Optimized Cassandra Operations at Scale</a></p></li><li><p><a href="https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/architecture/archGossipAbout.html">Cassandra Gossip Protocol</a></p></li><li><p><a href="https://www.uber.com/en-IN/blog/odin-stateful-platform/">Odin: Uber&#8217;s Stateful Platform</a></p></li></ul>
]]></content:encoded>
<pubDate>Tue, 24 Sep 2024 15:31:22 GMT</pubDate>
</item>
<item>
<title>EP130: Design a System Like YouTube</title>
<link>https://blog.bytebytego.com/p/ep130-design-a-system-like-youtube</link>
<guid>https://blog.bytebytego.com/p/ep130-design-a-system-like-youtube</guid>
<content:encoded><![CDATA[
<div> QA Wolf, AI-native approach, engineering teams, faster QA cycles, 4x more test cases

<br />
<br />
总结: QA Wolf是一个采用AI技术的测试工具，能够帮助工程团队加快测试周期，实现4倍更多的测试用例。他们提供无限并行测试运行、24小时维护和按需测试创建、人工验证的错误报告直接发送给团队，以及零错误保证。通过QA Wolf，工程团队实现了更快的测试周期，无需手动端对端测试，避免了慢速的QA周期。 <div>
<h2><a href="https://bit.ly/QAWolf_092124">&#9986;&#65039;Cut your QA cycles down to minutes with QA Wolf (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/QAWolf_092124" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="736" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55fb097e-150b-4ce0-a3c7-7d8d6416cabc_1473x745.png" width="1456" /><div></div></div></a></figure></div><p>If slow QA processes bottleneck you or your software engineering team and you&#8217;re releasing slower because of it &#8212; you need to check out QA Wolf.</p><p>Their AI-native approach gets engineering teams to <a href="https://bit.ly/QAWolf_092124">80% automated end-to-end test coverage</a> and helps them ship 2x faster by reducing QA cycles from hours to minutes.</p><p><a href="https://bit.ly/QAWolf_092124">QA Wolf</a> takes testing off your plate. They can get you:</p><ul><li><p>Unlimited parallel test runs</p></li><li><p>24-hour maintenance and on-demand test creation</p></li><li><p>Human-verified bug reports sent directly to your team</p></li><li><p>Zero flakes guaranteed</p></li></ul><p>The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.</p><p>With QA Wolf, <a href="https://bit.ly/QAWolf_092124CaseStudy">Drata&#8217;s team of 80+ engineers</a> achieved 4x more test cases and 86% faster QA cycles.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/QAWolf_092124"><span>Schedule a demo to learn more</span></a></p><div><hr /></div><p>This week&#8217;s system design refresher:</p><ul><li><p>Introduction to Generative AI (Youtube video)</p></li><li><p>Iterative, Agile, Waterfall... What are the differences between these Software Development Life Cycle models? </p></li><li><p>How to Design a System Like YouTube? </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2>Introduction to Generative AI</h2><div class="youtube-wrap" id="youtube2-2p5OHDxR2l8"><div class="youtube-inner"></div></div><div><hr /></div><h2>Iterative, Agile, Waterfall... What are the differences between these Software Development Life Cycle models? </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe?" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1564" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce0f4be-6997-4f1c-818a-073a63492220_1280x1564.gif" width="1280" /><div></div></div></a></figure></div><p>The Software Development Life Cycle (SDLC) is a framework that outlines the process of developing software in a systematic way. Here are some of the most common ones: </p><ol><li><p>Waterfall Model: <br />- A linear and sequential approach. <br />- Divides the project into distinct phases: Requirements, Design, Implementation, Verification, and Maintenance. <br /></p></li><li><p>Agile Model: <br />- Development is done in small, manageable increments called sprints. <br />- Common Agile methodologies include Scrum, Kanban, and Extreme Programming (XP). <br /></p></li><li><p>V-Model (Validation and Verification Model): <br />- An extension of the Waterfall model. <br />- Each development phase is associated with a testing phase, forming a V shape. <br /></p></li><li><p>Iterative Model: <br />- Focuses on building a system incrementally. <br />- Each iteration builds upon the previous one until the final product is achieved. <br /></p></li><li><p>Spiral Model: <br />- Combines iterative development with systematic aspects of the Waterfall model. <br />- Each cycle involves planning, risk analysis, engineering, and evaluation. <br /></p></li><li><p>Big Bang Model: <br />- All coding is done with minimal planning, and the entire software is integrated and tested at once. <br /></p></li><li><p>RAD Model (Rapid Application Development): <br />- Emphasizes rapid prototyping and quick feedback. <br />- Focuses on quick development and delivery. <br /></p></li><li><p>Incremental Model: <br />- The product is designed, implemented, and tested incrementally until the product is finished. <br /></p></li></ol><p>Each of these models has its advantages and disadvantages, and the choice of which to use often depends on the specific requirements and constraints of the project at hand.</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe?" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1550" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ead610b-d0c7-41a6-aa16-4ad86534f65f_1503x1600.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/the-saga-pattern">The Saga Pattern</a></p></li><li><p><a href="https://blog.bytebytego.com/p/infrastructure-as-code">Infrastructure as Code</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-data">A Crash Course on Scaling the Data Layer</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-load-balancers">A Crash Course on Load Balancers for Scaling</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-api">A Crash Course on Scaling the API Layer</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>How to Design a System Like YouTube? </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe?" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5237f88e-8a2c-4e0b-8457-3dfabd2d6ca3_1329x1536.gif" width="1329" /><div></div></div></a></figure></div><p></p><p>Here&#8217;s a 9-step process: </p><ol><li><p>The user creates a video upload request and provides the video files along with the details about the video. </p></li><li><p>The raw video files are uploaded to an Object Storage (such as S3). </p></li><li><p>Also, the metadata is saved in a database as well as a cache for faster retrieval when needed. </p></li><li><p>The raw video files are sent for transcoding to a special transcoding server. Transcoding is the process of encoding the videos into compatible bitrates and formats for streaming. </p></li><li><p>The transcoded video is uploaded to another object storage. </p></li><li><p>The notification for transcoding completion is sent to a special service via a message queue. </p></li><li><p>The Transcoding Status Handler updates the metadata DB and cache with the latest details of the video. </p></li><li><p>The user raises a video streaming request that goes to a Content Delivery Network (CDN). </p></li><li><p>The CDN fetches the video from the object storage for streaming. It also caches the video locally for subsequent streaming requests. </p></li></ol><p>Over to you: What else would you add to make the YouTube-like system?</p><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 21 Sep 2024 15:31:08 GMT</pubDate>
</item>
<item>
<title>The Saga Pattern</title>
<link>https://blog.bytebytego.com/p/the-saga-pattern</link>
<guid>https://blog.bytebytego.com/p/the-saga-pattern</guid>
<content:encoded><![CDATA[
<div> Spring framework, transaction management, microservices, Saga pattern, multiple databases

<br /><br />总结:
在企业应用程序中，每个请求几乎都在数据库事务中执行。Spring框架等使用特殊的注解来简化事务管理，但在涉及多个数据库和消息代理的情况下，传统的事务处理方法变得复杂。微服务架构中涉及多个服务和数据库，传统事务管理方法不再适用，因此需要采用更复杂的Saga模式来管理事务。Saga模式为微服务架构提供了一种更有效的事务管理机制。 <div>
<p>In enterprise applications, nearly every request is executed within a database transaction.&nbsp;</p><p>Developers often use frameworks and libraries with declarative mechanisms to simplify transaction management.</p><p>The Spring framework, for example, uses a special annotation to arrange for method invocations to be automatically executed within a transaction. This annotation simplifies writing transactional business logic, making it easier to manage transactions in a monolithic application that accesses a single database.</p><p>However, while transaction management is relatively straightforward in a monolithic application accessing a single database, it becomes more complex in scenarios involving multiple databases and message brokers.&nbsp;</p><p>For example, in a microservice architecture, business transactions span multiple services, each with its database. This complexity makes the traditional transaction approach impractical. Instead, microservices-based applications must adopt alternative mechanisms to manage transactions effectively.</p><p>In this post, we&#8217;ll learn why microservices-based applications require a more sophisticated approach to transaction management, such as using the Saga pattern. We&#8217;ll also understand the different approaches to implementing the Saga pattern in an application.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ead610b-d0c7-41a6-aa16-4ad86534f65f_1503x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1550" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ead610b-d0c7-41a6-aa16-4ad86534f65f_1503x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>The Need for Saga Pattern</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/the-saga-pattern">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 19 Sep 2024 15:30:53 GMT</pubDate>
</item>
<item>
<title>How Netflix Warms Petabytes of Cache Data</title>
<link>https://blog.bytebytego.com/p/how-netflix-warms-petabytes-of-cache</link>
<guid>https://blog.bytebytego.com/p/how-netflix-warms-petabytes-of-cache</guid>
<content:encoded><![CDATA[
<div> Netflix, EVCache, Cache Warmer, Controller, Populator
总结:<br /><br />文章介绍了Netflix的EVCache系统及其缓存预热工具Cache Warmer的架构和实现。EVCache用于降低流媒体应用的延迟，支持多种用例，如查找缓存、临时数据存储、主要存储和高访问量数据。Cache Warmer包括Controller、Dumper和Populator三个组件，通过复制现有数据到新节点或替换节点来提高缓存效率。而Instance Warmer则用于快速填充替换或重新启动的节点。这种灵活的架构允许Netflix有效地扩展缓存数据，提高系统效率。 <div>
<h2><a href="https://bit.ly/ScyllaDB_091724">Free tickets to P99 CONF &#8212; 60+ low-latency engineering talks (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/ScyllaDB_091724" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7b8023b-68b7-4ba2-912c-1f27fc4bcd7a_1600x840.png" width="1456" /><div></div></div></a></figure></div><p>P99 CONF is the technical conference for anyone who obsesses over high-performance, low-latency applications. Engineers from Disney, Shopify, LinkedIn, Netflix, Google, Meta, Uber + more will be sharing 60+ talks on topics like Rust, Go, Zig, distributed data systems, Kubernetes, and AI/ML.</p><p>Join 20K of your peers for an unprecedented opportunity to learn from experts like Michael Stonebraker, Bryan Cantrill, Avi Kivity, Liz Rice &amp; Gunnar Morling &amp; more &#8211; for free, from anywhere.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/ScyllaDB_091724"><span>GET YOUR FREE TICKET</span></a></p><p>Bonus: Registrants are eligible to enter to win 1 of 300 free swag packs, get 30-day access to the complete O&#8217;Reilly library &amp; learning platform, plus free digital books.</p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the Netflix Tech Blog. All credit for the technical details goes to the Netflix engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>The goal of Netflix is to keep users streaming for as long as possible.&nbsp;</p><p>However, a user&#8217;s typical attention span is just 90 seconds. If the streaming application is not responding fast enough, there is a high chance that the user will drop off.</p><p>EVCache is one tool that helps Netflix reduce latency on their streaming app by supporting multiple use cases such as:</p><ul><li><p><strong>Lookaside Cache:</strong> When the application needs some data, it first tries the EVCache client and if the data is not in the cache, it goes to the backend service and the Cassandra database to fetch the data.</p></li><li><p><strong>Transient Data Store: </strong>Netflix uses EVCache to keep track of transient data such as playback session information. One application service might start the session while the other may update the session followed by a session closure at the end.</p></li><li><p><strong>Primary Store: </strong>Netflix runs large-scale pre-compute systems every night to compute a brand-new home page for every profile belonging to every user based on the watch history and recommendations. All of that data is written into the EVCache cluster from where the online services read the data and build the homepage.</p></li><li><p><strong>High Volume Data: </strong>Netflix has data that has a high volume of access and also needs to be highly available. For example, UI strings and translations are shown on Netflix's home page. A separate process asynchronously computes and publishes the UI strings to EVCache from where the application can read it with low latency and high availability.</p></li></ul><p>The diagram below shows the various use cases in more detail.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3335b763-3ba6-4dac-801f-c8f9974b1b54_1600x1549.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1410" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3335b763-3ba6-4dac-801f-c8f9974b1b54_1600x1549.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Netflix uses EVCache as a tier-1 cache. For reference, it is a distributed in-memory caching solution based on memcached that is integrated with Netflix OSS and AWS EC2 infrastructure.</p><p>At Netflix, EVCache holds petabytes of data comprising thousands of nodes and hundreds of clusters in production. These clusters are routinely scaled up due to the increasing growth of the Netflix user base and the user data generated.</p><p>The earlier process to make the cache bigger followed the below steps:</p><ul><li><p>Set up a new, empty cache replica (group of nodes).</p></li><li><p>Write data to both the old and new cache replica at the same time.</p></li><li><p>Wait for the data in the old cache to expire.</p></li><li><p>Then, switch to using just the new, bigger cache replica.</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/FusionAuth_091724">FusionAuth: Auth. Built for Devs, by Devs.&nbsp;(Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/FusionAuth_091724" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="382.2" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feba33b26-bc93-4e53-b974-efc289a53311_640x336.png" title="" width="728" /><div></div></div></a></figure></div><ul><li><p>Hosting Flexibility: You host or we host - the choice is yours with no loss of features.</p></li><li><p>Scale Confidently: Lightning-fast performance for 10 users or 10 million users (or more).</p></li><li><p>Developer-Centric: True API first design, quick integration, built on standards, highly flexible &amp; customizable.</p></li><li><p>Total Control: Deploy on any computer, anywhere in the world and integrate easily with any tech stack.</p></li><li><p>Data Isolation: Single tenant by design means your data is physically isolated from everyone else&#8217;s.</p></li><li><p>Unlimited: Unlimited IDPs, unlimited users, unlimited tenants, unlimited applications, always free.</p></li></ul><p>FusionAuth is a complete auth &amp; user platform that has&nbsp;10M+ downloads and is trusted by industry leaders!</p><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/FusionAuth_091724"><span>Start for Free</span></a></p><div><hr /></div><p>See the diagram below for reference:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59a71e37-8b1e-44aa-83c2-3fcbfd07725e_1600x994.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="905" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59a71e37-8b1e-44aa-83c2-3fcbfd07725e_1600x994.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>This approach worked, but it was expensive because they had to run two systems simultaneously for a while. Also, this solution had a few more problems:</p><ul><li><p>It didn&#8217;t work well for data that never expires.</p></li><li><p>It was inefficient for data that doesn&#8217;t change often.</p></li><li><p>When they replaced parts of the system, it could cause a slowdown because the new nodes weren&#8217;t warm, resulting in cache misses.</p></li></ul><p>To fix these problems, Netflix created a new tool called the cache warmer. This tool had two main features:</p><ul><li><p><strong>Replica Warmer: </strong>When Netflix adds new storage units (replicas), they need to fill them with data. The replica warmer quickly copies data from an existing storage unit to the new one. It does this without slowing down or disrupting the user experience for existing users.&nbsp;</p></li><li><p><strong>Instance Warmer: </strong>Sometimes, a storage unit (node) needs to be replaced or shuts down unexpectedly. The instance warmer quickly fills the new or restarted unit with data by copying data from another healthy storage unit.</p></li></ul><p>For Instance Warmer, they copy data to the new empty node from another replica in the system. This is done because the other nodes in the system have continued to operate and update data while this node was down or being replaced. By choosing a replica that doesn&#8217;t include this node, they ensure they&#8217;re getting the most up-to-date data from nodes that have been continuously operational.</p><div><hr /></div><h2>Cache Warmer Design</h2><p>The diagram below shows the architectural overview of the cache warming system built by Netflix.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b745a1a-e374-4d34-b607-b5d2b6c4c792_1600x994.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="905" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b745a1a-e374-4d34-b607-b5d2b6c4c792_1600x994.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The cache warming system consists of three main components:</p><ul><li><p>Controller</p></li><li><p>Dumper</p></li><li><p>Populator</p></li></ul><p>Let&#8217;s look at each component one by one.</p><h3>Controller</h3><p>The Controller is the manager of the entire cache warming process. It sets up the environment and creates a communication channel (SQS queue) between the Dumper and Populator.&nbsp;</p><p>Some of the key functions performed by the controller are as follows:</p><ul><li><p>The Controller selects the source replica. If the replica is not user-specified, it is usually the one with the most data.</p></li><li><p>Next, it creates an SQS queue for communication and initiates the data dump on the source nodes.</p></li><li><p>After that, the Controller creates a new Populator cluster.</p></li><li><p>Finally, it monitors the process and cleans up the resources after the job is done.</p></li></ul><h3>Dumper</h3><p>The Dumper is part of the EVCache sidecar, which is a separate service running alongside the main cache (memcached instance) on each node. Its job is to extract data from the existing cache.</p><p>The dumping process works as follows:</p><ul><li><p>The Dumper lists all keys using the memcached LRU Crawler utility</p></li><li><p>It saves keys into multiple &#8220;key-chunk&#8221; files.</p></li><li><p>For each key, it retrieves the value and additional data. This data is saved into &#8220;data-chunk&#8221; files.</p></li><li><p>It uploads the data chunks to S3 when they reach a certain size.</p></li><li><p>Lastly, it sends a message to the SQS queue with the S3 location of the data chunk.</p></li></ul><p>The Dumper supports configurable chunk sizes for flexibility. It is also capable of parallel processing of multiple key chunks.</p><h3>Populator</h3><p>The Populator is responsible for filling the new cache with data from the Dumper.</p><p>Here&#8217;s how it works:</p><ul><li><p>The Populator instance receives configuration (like SQS queue name) from the Controller.</p></li><li><p>It pulls messages from the SQS queue.</p></li><li><p>Next, it downloads data chunks from S3 based on these messages and inserts the data into the new cache replicas. It uses &#8220;add&#8221; operations to avoid overwriting newer data.</p></li></ul><p>The Populator starts working as soon as data is available and doesn&#8217;t wait for the full dump. It can auto-scale based on the amount of data available to process.</p><h2>Instance Warmer</h2><p>In a large EVCache deployment, individual nodes (servers) can sometimes be terminated or replaced due to hardware issues or other problems. This can cause:</p><ul><li><p>Latency spikes or slowdowns for applications</p></li><li><p>Drops in cache hit rates if multiple replicas are affected</p></li></ul><p>To minimize these issues, the Netflix engineering team also developed an Instance Warmer that can quickly fill up replaced or restarted nodes with data.</p><p>The diagram below shows the overall architecture of the Instance Warmer.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d139930-7597-4fb0-930b-05165b41a94c_1600x994.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="905" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d139930-7597-4fb0-930b-05165b41a94c_1600x994.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The Instance Warmer works as follows:</p><ul><li><p>EVCache nodes are modified to send a signal to the Controller when they restart.</p></li><li><p>The Controller receives the restart signal from a node and checks if the node has less than its expected share of data. If so, it triggers the warming process. However, it doesn&#8217;t use the replica containing the restarted node as the data source.</p></li><li><p>The Controller initiates dumping on all nodes in other replicas. It specifies which nodes need warming and which replica they belong to.</p></li><li><p>The Dumper only extracts data for keys that would hash to the specific nodes being warmed.</p></li><li><p>Lastly, the Populator consumes these targeted data chunks. It fills only the specific nodes that need warming in the affected replica.</p></li></ul><p>The instance warming process is more efficient than full replica warming because it deals with a smaller fraction of data on each node. Also, it is targeted to specific nodes rather than entire replicas.</p><h2>The Implementation Results</h2><p>Netflix is extensively using the cache warmer for scaling if the TTL is greater than a few hours. This has helped them handle the holiday traffic efficiently.</p><p>The chart below shows the warming up of two new replicas from one of the two existing replicas. Existing replicas had about 500 million items and 12 Terabytes of data. The warm-up took around 2 hours to complete.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c6f9000-4377-4e46-b013-5e3dfa340ad0_1400x338.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="338" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c6f9000-4377-4e46-b013-5e3dfa340ad0_1400x338.png" width="1400" /><div></div></div></a><figcaption class="image-caption">Source: <a href="https://netflixtechblog.com/cache-warming-agility-for-a-stateful-service-2d3b1da82642">Netflix Tech Blog</a></figcaption></figure></div><p>The Instance Warmer is also running in production and warming up a few instances every day. The chart below shows an instance getting replaced at around 5.27. It was warmed up in less than 15 minutes with about 2.2 GB of data and 15 million items.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8866ff8-3609-4147-b211-855bcbe377c5_1400x548.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="548" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8866ff8-3609-4147-b211-855bcbe377c5_1400x548.png" width="1400" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://netflixtechblog.com/cache-warming-agility-for-a-stateful-service-2d3b1da82642">Netflix Tech Blog</a></figcaption></figure></div><h2>Conclusion</h2><p>In this post, we&#8217;ve looked at the cache-warming architecture implemented by Netflix in detail.</p><p>This flexible cache-warming architecture has allowed Netflix to warm petabytes of cache data by copying it from existing replicas to one or more new replicas. Also, it has made it easy to warm specific nodes that were terminated or replaced due to hardware issues.</p><p>One of the key takeaways is the cache warmer following a loosely coupled design where the Dumper and Populator are integrated through SQS. This shows how loosely coupled systems provide flexibility and extensibility in the long run as Netflix continues to enhance its system for greater efficiency.</p><p><strong>References:</strong></p><ul><li><p><a href="https://netflixtechblog.com/announcing-evcache-distributed-in-memory-datastore-for-cloud-c26a698c27f7">Announcing EVCache: Distributed in-memory datastore for Cloud</a></p></li><li><p><a href="https://netflixtechblog.com/cache-warming-agility-for-a-stateful-service-2d3b1da82642">Cache warming: Agility for a Stateful Service</a></p></li><li><p><a href="https://youtu.be/Rzdxgx3RC0Q?si=bbLWislsVaY5i11u">Caching at Netflix: The Hidden Microservice</a></p></li></ul>
]]></content:encoded>
<pubDate>Tue, 17 Sep 2024 15:31:12 GMT</pubDate>
</item>
<item>
<title>EP129: The Ultimate Walkthrough of the Generative AI Landscape</title>
<link>https://blog.bytebytego.com/p/ep129-the-ultimate-walkthrough-of</link>
<guid>https://blog.bytebytego.com/p/ep129-the-ultimate-walkthrough-of</guid>
<content:encoded><![CDATA[
<div> Search, Generative AI, Relational Database Design, Soft Skills, Sponsor

搜索、生成式人工智能、关系数据库设计、软技能、赞助

<br /><br />总结:
本文重点介绍了搜索工作原理，生成式人工智能领域的最新发展，关系数据库设计要点，以及可以帮助开发者提升的软技能书籍推荐。此外，还介绍了赞助机会。搜索工作原理涵盖了生成式人工智能、基于关系数据库的设计原则，以及软技能书籍推荐。对于开发者来说，掌握这些知识将有助于提高技术能力和职业发展。赞助机会则为企业提供了推广产品和服务的机会。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>How Search Really Works (Youtube Video)</p></li><li><p>The Ultimate Walkthrough of the Generative AI Landscape </p></li><li><p>Cheatsheet on Relational Database Design </p></li><li><p>My Favorite 10 Soft Skill Books that Can Help You Become a Better Developer </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2>How Search Really Works</h2><div class="youtube-wrap" id="youtube2-TByRaraQqW4"><div class="youtube-inner"></div></div><div><hr /></div><h2>The Ultimate Walkthrough of the Generative AI Landscape </h2><p>Generative AI and LLMs are fast becoming a game-changer in the business world. Everyone wants to learn more about it. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b6b3d99-0c00-4d5b-8396-215f483e3a55_1280x1683.gif" title="graphical user interface, application" /><div></div></div></a></figure></div><p>The landscape covers the following points: </p><ol><li><p>What is GenAI? </p></li><li><p>Foundational Models and LLMs </p></li><li><p>&#8220;Attention is All You Need&#8221; and its impact </p></li><li><p>GenAI vs Traditional AI </p></li><li><p>How to train a foundation model? </p></li><li><p> The GenAI Development Stack (LLMs, Frameworks, Programming Languages, etc.) </p></li><li><p>GenAI Applications </p></li><li><p>Designing a simple GenAI application </p></li><li><p>The AI Engineer Job Role </p></li></ol><p>Over to you: What else would you add to the GenAI landscape?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a28aee2-6cef-4a3b-8bb2-5ee2d54059ed_1451x1600.png" width="1451" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/infrastructure-as-code">Infrastructure as Code</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-data">A Crash Course on Scaling the Data Layer</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-load-balancers">A Crash Course on Load Balancers for Scaling</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-api">A Crash Course on Scaling the API Layer</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-architectural-scalability">A Crash Course on Architectural Scalability</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>Cheatsheet on Relational Database Design </h2><p>A relational database is a type of database that organizes data into structured tables, also known as relations. These tables consist of rows (records) and columns (fields). </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1649" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F546c4b28-11db-4fb2-910f-26f2d1e3391f_1280x1649.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><p>Some key points to know about Relational Database Design </p><ol><li><p>SQL <br />SQL is the standard programming language used to interact with relational databases. It supports fundamental operations for data manipulation, data definition, and data control. <br /></p></li><li><p>Fundamental RDBMS Concepts <br />There are some fundamental RDBMS concepts such as table, row, column, primary key, foreign key, join, index, and view. <br /></p></li><li><p>Keys in Relational Databases <br />Different types of keys are as follows: <br />Primary Key: A column or combination of columns uniquely identifying each record in a table. <br />Surrogate Key: Artificial key generated by the database system or a globally unique identifier that has no inherent meaning to the data. <br />Foreign Key: A column or a combination of columns in one table that references the primary key of another table. <br /></p></li><li><p>Relation Types <br />Relationships between tables play a key role in defining how data is connected. Three main types of relationships are: </p><ul><li><p>One-to-One Relationship: A record in one table is associated with one record in another table. </p></li><li><p>One-to-Many Relationship: A record in one table is associated with multiple records in another table </p></li><li><p>Many-to-Many Relationship: Records in both tables can have multiple records in the other table. <br /></p></li></ul></li><li><p>Joins <br />Joins act as bridges, connecting different tables based on their relationship. They are extremely useful when you need to retrieve data from multiple tables. There are 3 main types of joins: </p><ul><li><p>Inner Join </p></li><li><p>Right Outer Join </p></li><li><p>Left Outer Join </p></li></ul><p></p><p>Over to you: What else should you know about relational database design?</p></li></ol><div><hr /></div><h2>My Favorite 10 Soft Skill Books that Can Help You Become a Better Developer </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc24bbd6-3a2c-43fc-af11-98ce80f5ce7c_1331x1536.gif" title="diagram" width="1331" /><div></div></div></a></figure></div><p>Productivity &amp; Personal Development </p><ol><li><p>Deep Work by Cal Newport </p></li><li><p>Atomic Habits by James Clear </p></li><li><p>The Effective Executive by Peter Drucker </p></li></ol><p>Communication Skills </p><ol><li><p>Crucial Conversations by Kerry Patterson et al. </p></li><li><p>How to Win Friends and Influence People by Dale Carnegie </p></li></ol><p>Leadership &amp; Team Dynamics </p><ol><li><p>Extreme Ownership by Jocko Willink and Leif Babin </p></li><li><p>The Five Dysfunctions of a Team by Patrick Lencioni </p></li><li><p>Start with Why by Simon Sinek </p></li></ol><p>Design &amp; Craftsmanship </p><ol><li><p>The Clean Coder by Robert Martin </p></li><li><p>The Design of Everyday Things by Dan Norman </p></li></ol><p>Over to you: What is your favorite book?</p><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 14 Sep 2024 15:30:52 GMT</pubDate>
</item>
<item>
<title>Infrastructure as Code</title>
<link>https://blog.bytebytego.com/p/infrastructure-as-code</link>
<guid>https://blog.bytebytego.com/p/infrastructure-as-code</guid>
<content:encoded><![CDATA[
<div> 关键词: 可伸缩基础设施管理, 可用性, 应用性能, 服务质量, 成本效率

总结:<br /><br />可伸缩基础设施管理对于现代企业至关重要，能够确保应用在任何时候都保持运行，并根据需求动态调整资源，以维持最佳性能水平。它提高了可用性、应用性能和服务质量，提高了客户满意度和忠诚度，并提供了成本优化的机会。然而，管理和配置可伸缩基础设施可能变得复杂，需要有效的方法来应对不断增长的代码库和潜在的错误风险。综上所述，有效的基础设施管理和配置是确保现代应用基础设施顺利运行的关键。 <div>
<p>It is difficult to imagine modern businesses operating without a scalable process to provision and manage the infrastructure.&nbsp;</p><p>Scalable infrastructure management is critical for businesses to adapt quickly to fluctuating workloads and user demands without compromising performance or incurring excessive costs. It offers several key benefits from multiple perspectives:</p><ul><li><p><strong>Availability:</strong> Scalable infrastructure management ensures that the application remains up and running at all times. It minimizes downtime and enables continuous service availability by dynamically adjusting resources based on demand.</p></li><li><p><strong>Application Performance:</strong> Scalable infrastructure ensures that application performance is not affected by changes in workload. It allows for the seamless addition or removal of resources to maintain optimal performance levels, even during peak periods.</p></li><li><p><strong>Better Quality of Service:</strong> By enhancing availability and application performance, scalable infrastructure management allows businesses to offer better service quality to their customers. This improved service quality can lead to higher customer satisfaction and loyalty.</p></li><li><p><strong>Cost Efficiency:</strong> Scalable infrastructure management provides opportunities for cost optimization by scaling down resources during idle times.&nbsp;</p></li></ul><p>Modern cloud-based infrastructure is particularly adept at scaling to support millions of users.&nbsp;</p><p>However, while it may seem effortless, managing scalability beyond simple autoscaling groups and load balancers can become complex.</p><p>As the codebase grows due to the involvement of numerous engineers, the potential for mistakes also increases. While minor issues like syntax errors or forgotten comments can be mitigated quickly, more serious mistakes such as leaked security keys, improper storage security settings, or open security groups can have disastrous consequences.</p><p>Therefore, it is crucial to formulate effective ways to manage and provision the infrastructure.</p><p>In this post, we&#8217;ll explore the most important ways of scaling the provisioning and management of modern application infrastructure.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a28aee2-6cef-4a3b-8bb2-5ee2d54059ed_1451x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a28aee2-6cef-4a3b-8bb2-5ee2d54059ed_1451x1600.png" width="1451" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>What is Non-Scalable Infrastructure?</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/infrastructure-as-code">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 12 Sep 2024 15:30:10 GMT</pubDate>
</item>
<item>
<title>How Shopify Manages its Petabyte Scale MySQL Database</title>
<link>https://blog.bytebytego.com/p/how-shopify-manages-its-petabyte</link>
<guid>https://blog.bytebytego.com/p/how-shopify-manages-its-petabyte</guid>
<content:encoded><![CDATA[
<div> Shard balancing, Database replication, Read consistency, Database backup, Restore process
<br /><br />总结:
Shopify在管理其大规模MySQL数据库时采取了一些重要的技术方法。他们通过零停机的分片平衡，确保资源利用效率和用户体验。同时，对数据库复制进行管理，以确保可用性和性能，并解决复制滞后的问题。为了快速有效地备份和恢复数据库，他们采用了PD快照功能，将RTO缩短到不到30分钟。这些方法体现了对用户体验和成本效益的重视。 <div>
<h2><a href="https://bit.ly/Speakeasy_091024">Generate Handwritten SDKs (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Speakeasy_091024" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1512" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2dab712-6e41-43fb-a866-3b42d2d092f4_4623x4800.png" width="1456" /><div></div></div></a></figure></div><p>Invest hundreds of hours your team doesn't have in maintaining SDKs by hand or generate crappy SDKs that leave a bad taste in your users' mouths. That's two bad options. Fortunately, you can now use Speakeasy to generate ergonomic type-safe SDKs in over 10 languages. We've worked with language experts to create a generator that gets the details right. With Speakeasy you can build SDKs that your team is proud of.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Speakeasy_091024"><span>Try for free</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the article originally published on the Shopify Engineering Blog. All credit for the details about Shopify&#8217;s architecture goes to their engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>Shopify has revolutionized the e-commerce landscape by empowering small business owners to establish and grow their online presence.&nbsp;</p><p>With millions of merchants relying on their platform globally, Shopify&#8217;s infrastructure has evolved to handle the ever-increasing demands of their user base.</p><p>At the heart of Shopify&#8217;s infrastructure lies their MySQL database, which has grown to an impressive petabyte scale. Managing a database of this magnitude presents significant challenges, especially when considering Shopify&#8217;s commitment to providing a zero-downtime service.&nbsp;</p><p>Their direct customers are business owners, who depend on their online stores to generate revenue and sustain their livelihoods. Any downtime or service disruption can have severe consequences for these merchants, potentially leading to lost sales and damaged customer relationships.</p><p>In this post, we will look at how Shopify manages its critical MySQL database in three major areas:</p><ul><li><p>Shard balancing with zero downtime</p></li><li><p>Maintaining read consistency with database replication</p></li><li><p>Database backup and restore</p></li></ul><p>Each area is critical for operating a database at Shopify&#8217;s scale. For us, it&#8217;s a great opportunity to derive some key learnings.</p><h2>Shard Balancing with Zero Downtime</h2><p>Shopify runs a large fleet of MySQL database instances.</p><p>These instances are internally known as shards and are hosted within pods.&nbsp;</p><p>Each shard can store the data for one or more shops. See the diagram below where the MySQL shard within pod 1 contains the data for shop ABC and FOO.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d572c9-166d-459a-bf02-bf8e00689ebd_1600x1149.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1046" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38d572c9-166d-459a-bf02-bf8e00689ebd_1600x1149.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>As traffic patterns for individual shops change, certain database shards become unbalanced in their resource utilization and load.&nbsp;</p><p>For example, if both shop ABC and shop FOO launch a mega sale simultaneously, it will result in a surge of traffic causing the database server to struggle. To deal with this, Shopify moves one of the shop&#8217;s data to another shard.&nbsp;</p><p>This process is known as shard balancing and it&#8217;s important for multiple reasons such as:</p><ul><li><p>Mitigating the risk of database failure</p></li><li><p>Improving the productivity of the infrastructure</p></li><li><p>Guaranteeing that buyers can always access their favorite shops (no downtime)</p></li></ul><p>An interesting takeaway from these reasons is how successful companies are focused on the customer experience even when dealing with largely technical concerns. A well-balanced shard is not directly visible to the end user. However, an unbalanced shard can indirectly impact the user experience negatively.</p><p>The second takeaway is a strong focus on cost. This is evident from the idea of improving the infrastructure&#8217;s productivity, which ultimately translates to savings.</p><p>Let&#8217;s now investigate how Shopify runs the shard rebalancing process.</p><h3>The Concept of Pods</h3><p>Shopify&#8217;s infrastructure is composed of many pods.&nbsp;</p><p>Each pod is an isolated instance of the core Shopify application and a MySQL database shard. There are other data stores such as Redis and Memcached but we are not concerned about them right now.</p><p>A pod houses the data for one or more shops. Web requests for shops arrive at the Nginx load balancer that consults a routing table and forwards the request to the correct pod based on the shop.</p><p>The concept of pods in Shopify&#8217;s case is quite similar to cells in a cell-based architecture.&nbsp;</p><p>Nginx acts as the cell router and the application layer is the same across all pods. It has access to a routing table that maps a shop to a particular shard. See the diagram below:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbda257b-189f-43e4-8207-2281d29f4aa4_1600x1149.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1046" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbda257b-189f-43e4-8207-2281d29f4aa4_1600x1149.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>However, there is also a slight difference from cell-based architecture. The data in each pod varies depending on the shops hosted in a pod&#8217;s database instance.</p><p>As discussed earlier, each pod consists of a shard or a partition of the data.&nbsp;</p><p>Shopify&#8217;s data model works well with this topology since &#8220;shop&#8221; is an identifier for most tables. The shop acts as a natural partition between data belonging to different customers. They can attach a <em>shop_id</em> field to all shop-owned tables and use it as a sharding key.&nbsp;</p><p>The trouble starts when multiple shops living on the same pod become too big, resulting in higher database usage for some shards and lower usage for others. There are two problems when this happens:</p><ul><li><p>The high-traffic shards are at an increased risk of failure due to over-utilization.</p></li><li><p>Shards with low database usage are not used productively resulting in a higher cost of operation.</p></li></ul><p>The graph below highlights the variation in database usage per shard that developed over time as merchants came on board and grew. Each line represents the database usage for a unique shard on a given day.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F848196aa-731e-45b7-b927-1446c3a77862_1600x1249.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1137" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F848196aa-731e-45b7-b927-1446c3a77862_1600x1249.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://shopify.engineering/mysql-database-shard-balancing-terabyte-scale">Shopify Engineering Blog</a></figcaption></figure></div><h3>Balancing the Shards</h3><p>Shopify faces two key challenges when it comes to rebalancing shards for optimal resource utilization:</p><ul><li><p>Which shops should live on which shards?</p></li><li><p>How to move a shop from one shard to another with minimal downtime?</p></li></ul><p>A simplistic approach of evenly distributing shops across shards is not effective due to the varying data sizes and resource requirements of each shop. Some shops may consume a disproportionate amount of resources, leading to an imbalanced shard utilization.</p><p>Instead, Shopify employs a data-driven approach to shard rebalancing.&nbsp;</p><p>They analyze historical database utilization and traffic data for each shard to identify usage patterns and classify shops based on their resource requirements. The analysis takes into account factors such as:</p><ul><li><p>Shop size</p></li><li><p>Time required to move the shop</p></li><li><p>Occurrence of flash sales</p></li><li><p>Other relevant metrics</p></li></ul><p>Nevertheless, this is an ongoing process that requires continuous optimization. Shopify also uses data analysis and machine learning algorithms to make better decisions.</p><h3>Moving the Shop</h3><p>Moving a shop from one shard to another is straightforward: select all records from all tables having the required <em>shop_id </em>and copy them to another MySQL shard.</p><p>However, there are three main constraints Shopify has to deal with:</p><ul><li><p><strong>Availability: </strong>The shop move must be performed online without <em>visible</em> downtime or interruption to the merchant&#8217;s storefront.&nbsp; In other words, customers should be able to interact with the storefront throughout the process.</p></li><li><p><strong>Data Integrity: </strong>No data must be lost or corrupted during the transition. Also, all writes to the source database during the migration should also get copied.</p></li><li><p><strong>Throughput: </strong>The shop move should be completed in a reasonable amount of time.</p></li></ul><p>As expected, availability is critical. Shopify doesn&#8217;t want any <em>visible</em> downtime. While there&#8217;s a possibility for some downtime, the end user should not feel the impact.</p><p>Also, data integrity is crucial. Imagine there was a sale that got wiped out because the shop was moving from one shard to another. This would be unacceptable for the business owner.</p><p>As you can notice, each technical requirement is driven by strong business drivers.</p><p>Let us now look at each step in the process:</p><h4>Phase One: Batch Copying and Tailing the Binlog</h4><p>To perform the data migration, Shopify uses Ghostferry. It&#8217;s an in-house tool written in Go.</p><p>Later on, Shopify made it open-source. At present, Ghostferry&#8217;s GitHub repository has around 690+ stars.</p><p>Let&#8217;s assume that Pod 1 has two shops - ABC and FOO. Both shops decided to run a sale and expect a surge of traffic. Based on Shopify&#8217;s rebalancing strategy, Shop ABC should be moved from Pod 1 to Pod 2 for better resource utilization.</p><p>The diagram below shows the initial state where the traffic for Shop ABC is served by Pod 1. However, the copy process has started.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F963cb584-e197-4817-89b6-04acf5031f2e_1600x923.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="840" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F963cb584-e197-4817-89b6-04acf5031f2e_1600x923.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Ghostferry uses two main components to copy over data:&nbsp;</p><ul><li><p>Batch copying</p></li><li><p>Tailing the binlog</p></li></ul><p>In <strong>batch copying</strong>, Ghostferry iterates over the tables on the source shard, selects the relevant rows based on the shop&#8217;s ID, and writes these rows to the target shard. Each batch of writes is performed within a separate MySQL transaction to ensure data consistency.</p><p>To ensure that the rows being migrated are not modified on the source shard, Ghostferry uses MySQL&#8217;s SELECT&#8230;FOR UPDATE clause. This statement implements locking reads, which means that the selected rows from the source shard are write-locked for the duration of the transaction.</p><p>Ghostferry also starts tailing MySQL&#8217;s binlog to track and replicate changes that occur on the source shard to the target shard. The binlog serves as a sink for events that describe the modifications made to a database, making it the authoritative source of truth.</p><p>In essence, both batch copying and tailing the binlog take place together.</p><h4>Phase Two: Entering Cutover</h4><p>The only opportunity for downtime is during the cutover. Therefore, the cutover is designed to be a short process.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47a92d14-c51e-4bc2-bb51-01d405852fb0_1600x845.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="769" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47a92d14-c51e-4bc2-bb51-01d405852fb0_1600x845.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Here&#8217;s what happens during the cutover phase:</p><ul><li><p>Ghostferry initiates the cutover phase when the queue of pending binlog events to be replayed on the target shard becomes small. The queue is considered small when the difference between the newly generated binlog events on the source shard and the events being replayed on the target shard is nearly real-time.</p></li><li><p>Once the cutover phase begins, all write operations on the source database are stopped. This ensures that no new binlog events are generated.&nbsp;</p></li><li><p>At this point, Ghostferry records the final binlog coordinate of the source database, which serves as the stopping coordinate.&nbsp;</p></li><li><p>Ghostferry then processes the remaining queue of binlog events until it reaches the stopping coordinate. When the stopping coordinate is reached, the copying process is complete, and the target shard is in sync with the source shard.</p></li></ul><h4>Phase Three: Switch Traffic and Prune Stale Data</h4><p>In the last phase, the shop mover process updates the routing table to associate the shop with its new pod.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a011703-6904-41aa-97c9-c40f6b903d4a_1600x845.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="769" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a011703-6904-41aa-97c9-c40f6b903d4a_1600x845.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The shop is now served from the new pod. However, the old pod still contains the shop data.</p><p>They perform a verification to ensure that the movement is successful. If no issues are identified during the verification process, stale data of shop ABC on the old pod is deleted.</p><h2>Read Consistency with Database Replication</h2><p>The second major learning point from data management at Shopify&#8217;s scale is related to database replication.</p><p>Read replicas are copies of a primary database that are used to handle read-only queries. They help distribute the read workload across multiple servers, reducing the load on the primary database server. This allows the primary servers to be used for time-sensitive read/write operations.</p><p>An interesting point to note here is that read replicas don&#8217;t handle all the reads. Time-sensitive reads still go to the primary servers.</p><p>Why is this the case?</p><p>The unavoidable reason is the existence of replication lag.&nbsp;</p><p>Any database replication process will have some lag. The implication is that applications reading from a replica might end up reading stale data. However, this may not be acceptable for some specific reads. For example, a customer updating the profile information and not seeing the updates reflected on the profile page.</p><p>Also, reads are not always atomic. There can be a scenario where related pieces of data are assembled from the results of multiple queries.&nbsp;</p><p>For example, consider the below sequence of events:</p><ul><li><p>The customer places an order for two items: Item A and Item B.</p></li><li><p>The order processing system sends a query to read replica 1 to check the inventory for Item A.</p></li><li><p>At the same time, it sends another query to read replica 2 to check the inventory for Item B.</p></li></ul><p>Imagine that between steps 2 and 3, the inventory for Item B gets updated on the master and the item is sold out. However, replica 2 has a higher replication lag compared to replica 1. This means that while replica 1 returns the updated inventory, replica 2 returns the outdated inventory for Item B.</p><p>This can create inconsistency within the application.</p><p>The diagram below shows this scenario:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb4207f9-0a8b-4d41-9cd1-31f3ed127e48_1600x1059.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="964" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb4207f9-0a8b-4d41-9cd1-31f3ed127e48_1600x1059.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>To use replication effectively, Shopify had to solve this issue:&nbsp;</p><p>There were two potential solutions Shopify considered but did not use:</p><ul><li><p><strong>Tight Consistency: </strong>One way was to enforce tight consistency to deal with variable lag. This means all replicas are guaranteed to be up to date with the primary server. However, this solution negates the benefits of using replica and also reduces the overall availability of write operations. Even if one of the replicas is down, the write operation can fail.</p></li><li><p><strong>Causal Consistency: </strong>Another approach was causal consistency based on a global transaction identifier (GTID). Each transaction in the primary server will have a GTID, which will be preserved during replication. The disadvantage of this approach was the need to implement special software on each replica that would report its GTID back to the proxy that makes the server selection.</p></li></ul><p>Finally, Shopify settled on a solution to implement monotonic read consistency. In this approach, successive reads should follow a consistent timeline even if the data read is not real-time.</p><p>This can be ensured by routing a series of related reads to the same server so that successive reads fetch a consistent state even if it&#8217;s not the latest state. See the diagram below for reference:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05f979f9-db16-4d9e-a1f5-e62ac378a7ca_1600x1059.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="964" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05f979f9-db16-4d9e-a1f5-e62ac378a7ca_1600x1059.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>To implement this technically, Shopify had to take care of two points:</p><ul><li><p>Determine if a request is related to another request.</p></li><li><p>Determine the server where the request should go.</p></li></ul><p>Any application that requires read consistency within a series of requests supplies a unique identifier common to those requests. This identifier is passed within query comments as a key-value pair.</p><p>The diagram below shows the complete process:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08235da7-536f-4cd0-a668-89f42d774a67_1600x1057.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="962" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08235da7-536f-4cd0-a668-89f42d774a67_1600x1057.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The identifier is a UUID that represents a series of related requests.&nbsp;</p><p>The UUID is labeled as <em>consistent_read_id </em>within the comments and goes through an extraction followed by a hashing process to determine the server that should receive all the requests that contain this identifier.</p><p>Shopify&#8217;s approach to consistent reads was simple to implement and had a low overhead in terms of processing. Its main drawback was that intermittent server outages can introduce read consistencies but this tradeoff was acceptable to them.</p><h2>Database Backup and Restore</h2><p>The last major learning point from Shopify&#8217;s data management is related to how they manage database backup and restore.</p><p>As mentioned earlier, Shopify runs a large fleet of MySQL servers. These servers are spread across three Google Cloud Platform (GCP) regions.</p><p>Initially, Shopify&#8217;s data backup process was as follows:</p><ul><li><p>Use Percona&#8217;s Xtrabackup utility</p></li><li><p>Store output in files</p></li><li><p>Archive them on Google Cloud Storage</p></li></ul><p>While the process was robust, it was time-consuming. Backing up a petabyte of data spread across multiple regions was too long. Also, the restore time for each shard was more than six hours. This meant Shopify had to accept a very high Recovery Time Objective (RTO).</p><p>To bring the RTO down to just 30 minutes, Shopify redesigned the backup and restore process. Since their MySQL servers ran on GCP&#8217;s VM using Persistent Disk (PD), they decided to leverage PD&#8217;s snapshot feature.</p><p>Let&#8217;s look at each step of the process in detail.</p><h3>Taking a Backup&nbsp;</h3><p>Shopify developed a new backup solution that uses GCP API to create persistent disk snapshots of their MySQL instances.</p><p>They deployed this backup tooling as a CronJob within their Kubernetes infrastructure. The CronJob is configured to run every 15 minutes across all clusters in all available regions. The tool creates snapshots of MySQL instances nearly 100 times a day across all shards, resulting in thousands of daily snapshots.</p><p>The diagram below shows the process:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d92ae68-1c4f-4c25-92fd-21bb105ec5bb_1600x1065.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="969" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d92ae68-1c4f-4c25-92fd-21bb105ec5bb_1600x1065.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Retaining Snapshots</h3><p>Since the backup process generated so many snapshots, Shopify also wanted to have a retention process to keep the important snapshots only. This was to keep the costs down.</p><p>They built another tool that implements the retention policy and deployed it using another CronJob on Kubernetes.</p><p>The diagram below shows the snapshot deletion process based on the retention policy.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8602b3da-a870-4c00-8b93-8cd6d022a1b0_1600x975.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="887" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8602b3da-a870-4c00-8b93-8cd6d022a1b0_1600x975.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Performing a Restore</h3><p>Having a very recent snapshot readily available enables Shopify to clone replicas with the most up-to-date data possible.</p><p>The process of restoring the backup is quite simple. It involves the following steps:</p><ul><li><p>Create new PDs using the latest snapshot as the source.</p></li><li><p>Start MySQL on top of the newly created disks.</p></li></ul><p>By exporting a snapshot to a new PD volume, Shopify can restore the database in a matter of minutes. This approach has reduced their RTO to less than 30 minutes, including the time needed to recover from any replication lag.</p><p>The diagram below shows the database restore process:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-imag" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1b0459d-e2e1-4011-99d4-c21e840db7ec_1600x973.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="885" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1b0459d-e2e1-4011-99d4-c21e840db7ec_1600x973.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>Conclusion</h2><p>Shopify&#8217;s database management techniques are a great example of how simple solutions can help organizations achieve the needed scale. Also, it shows that companies like Shopify have a strong focus on the user experience and cost while making any technical decision.</p><p>In this post, we&#8217;ve seen a glimpse of how Shopify manages its petabyte-scale MySQL database. Some of the key things we&#8217;ve covered are as follows:</p><ul><li><p>Shard balancing with zero downtime is important for efficient resource utilization and a good customer experience.</p></li><li><p>Database replication improves the availability and performance but also creates issues related to lag. A clear consistency model is needed to counter the effects of replication lag.</p></li><li><p>A quick and efficient database backup and restore process is important to minimize the recovery time.</p></li></ul><p><strong>References:</strong></p><ul><li><p><a href="https://shopify.engineering/mysql-database-shard-balancing-terabyte-scale">Shard Balancing: Moving Shops Confidently with Zero-downtime at Terabyte-scale</a></p></li><li><p><a href="https://shopify.engineering/read-consistency-database-replicas">Read Consistency with Database Replicas</a></p></li><li><p><a href="https://shopify.engineering/shopify-manages-petabyte-scale-mysql-backup-restore">How Shopify Manages Petabyte Scale MySQL Backup and Restore</a></p></li><li><p><a href="https://shopify.github.io/ghostferry/master/introduction.html">Introduction to Ghostferry</a></p></li></ul>
]]></content:encoded>
<pubDate>Tue, 10 Sep 2024 15:30:35 GMT</pubDate>
</item>
<item>
<title>EP128: The Ultimate Software Architect Knowledge Map</title>
<link>https://blog.bytebytego.com/p/ep128-the-ultimate-software-architect</link>
<guid>https://blog.bytebytego.com/p/ep128-the-ultimate-software-architect</guid>
<content:encoded><![CDATA[
<div> 系统设计，软件架构，PostgreSQL，技术知识图谱，软件开发者。  

总结：  
系统设计中，软件架构师需掌握多项技术知识，包括编程语言、工具、设计原则、架构原则、平台知识、数据分析、网络与安全、以及辅助技能等。同时，PostgreSQL在各个领域均有应用，支持诸如时序数据、机器学习、OLAP、地理空间等多种用例。另外，软件开发者需掌握团队协作工具、编程语言、API开发、Web服务器与云平台、认证与测试、数据库、CI/CD、数据结构与算法、系统设计、设计模式以及人工智能工具等知识和技能。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>The Ultimate Software Architect Knowledge Map </p></li><li><p>Is PostgreSQL eating the database world?</p></li><li><p>11 steps to go from Junior to Senior Developer</p></li><li><p>Tech Career Basic Requirements</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/QAWolf_090724">Cut your QA cycles down to minutes with QA Wolf (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/QAWolf_090724" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="736" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed9f31be-899a-44d6-a4fa-50f746791471_1473x745.png" width="1456" /><div></div></div></a></figure></div><p>If slow QA processes bottleneck you or your software engineering team and you&#8217;re releasing slower because of it &#8212; you need to check out QA Wolf.</p><p>Their AI-native approach gets engineering teams to <a href="https://bit.ly/QAWolf_090724">80% automated end-to-end test coverage</a> from hours to minutes.</p><p><a href="https://bit.ly/QAWolf_090724">QA Wolf</a> takes testing off your plate. They can get you:</p><ul><li><p>Unlimited parallel test runs</p></li><li><p>24-hour maintenance and on-demand test creation</p></li><li><p>Human-verified bug reports sent directly to your team</p></li><li><p>Zero flakes guaranteed</p></li></ul><p>The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.</p><p>With QA Wolf, <a href="https://bit.ly/QAWolf_072024CaseStudy">Drata&#8217;s team of 80+ engineers</a> achieved 4x more test cases and 86% faster QA cycles.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/QAWolf_090724"><span>Schedule a demo to learn more</span></a></p><div><hr /></div><h2>The Ultimate Software Architect Knowledge Map </h2><p>Becoming a Software Architect is a journey where you are always learning. But there are some things you must definitely strive to know. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1534" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5310e054-6284-4048-add2-8fea3efcba5e_1457x1536.gif" title="graphical user interface, application" width="1456" /><div></div></div></a></figure></div><ol><li><p>Master a Programming Language <br />Look to master 1-2 programming languages such as Java, Python, Golang, JavaScript, etc. <br /></p></li><li><p>Tools <br />Build proficiency with key tools such as GitHub, Jenkins, Jira, ELK, Sonar, etc. <br /></p></li><li><p>Design Principles <br />Learn about important design principles such as OOPS, Clean Code, TDD, DDD, CAP Theorem, MVC Pattern, ACID, and GOF. <br /></p></li><li><p>Architectural Principles <br />Become proficient in multiple architectural patterns such as Microservices, Publish-Subscribe, Layered, Event-Driven, Client-Server, Hexagonal, etc. <br /></p></li><li><p>Platform Knowledge <br />Get to know about several platforms such as containers, orchestration, cloud, serverless, CDN, API Gateways, Distributed Systems, and CI/CD <br /></p></li><li><p>Data Analytics <br />Build a solid knowledge of data and analytics components like SQL and NoSQL databases, data streaming solutions with Kafka, object storage, data migration, OLAP, and so on. <br /></p></li><li><p>Networking and Security <br />Learn about networking and security concepts such as DNS, TCP, TLS, HTTPS, Encryption, JWT, OAuth, and Credential Management. <br /></p></li><li><p>Supporting Skills <br />Apart from technical, software architects also need several supporting skills such as decision-making, technology knowledge, stakeholder management, communication, estimation, leadership, etc. <br /></p></li></ol><p>Over to you - What else would you add to the roadmap?</p><div><hr /></div><h2><a href="https://aigents.co/learn/roadmaps/intro">Tech Career Basic Requirements</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://aigents.co/learn/roadmaps/introhttps://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1296" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972c149b-444c-4f27-bad3-857d4059328a_1714x1526.png" title="" width="1456" /><div></div></div></a><figcaption class="image-caption">Source: <a href="https://aigents.co/learn/roadmaps/intro">Aigent</a></figcaption></figure></div><div><hr /></div><h2>Is PostgreSQL eating the database world?</h2><p>It seems that no matter what the use case, PostgreSQL supports it. When in doubt, you can simply use PostgreSQL.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F893bd2f0-1a81-48bc-bcd7-2a5e6fa4c2ee_1280x1591.gif" title="No alt text provided for this image" /><div></div></div></a></figure></div><ol><li><p>TimeSeries<br />PostgreSQL embraces Timescale, a powerful time-series database extension for efficient handling of time-stamped data.<br /></p></li><li><p>Machine Learning<br />With pgVector and PostgresML, Postgres can support machine learning capabilities and vector similarity searches.<br /></p></li><li><p>OLAP<br />Postgres can support OLAP with tools such as Hydra, Citus, and pg_analytics.<br /></p></li><li><p>Derived<br />Even derived databases such as DuckDB, FerretDB, CockroachDB, AlloyDB, YugaByte DB, Supabase, etc provide PostgreSQL.<br /></p></li><li><p>GeoSpatial<br />PostGIS extends PostgreSQL with geospatial capabilities, enabling you to easily store, query, and analyze geographic data.<br /></p></li><li><p>Search<br />Postgres extensions like pgroonga, ParadeDB, and ZomboDB provide full-text search, text indexing, and data parsing capabilities.<br /></p></li><li><p>Federated<br />Postgres seamlessly integrates with various data sources such as MongoDB, MySQL, Redis, Oracle, ParquetDB, SQLite, etc, enabling federated querying and data access. <br /></p></li><li><p>Graph<br />Apache AGE and EdgeDB are graph databases built on top of PostgreSQL. Also, pg_graphql is an extension that provides GraphQL support for Postgres.</p></li></ol><p>Over to you: Have you seen any other use cases of PostgreSQL?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd75e0fce-77e4-4a0b-8a00-d3bb6ac923bd_1449x1536.gif" title="No alternative text description for this image" width="1449" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-data">A Crash Course on Scaling the Data Layer</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-load-balancers">A Crash Course on Load Balancers for Scaling</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-api">A Crash Course on Scaling the API Layer</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-architectural-scalability">A Crash Course on Architectural Scalability</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservices-design">A Crash Course on Microservices Design Patterns</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>11 steps to go from Junior to Senior Developer</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1562" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300e1cb0-9691-4516-8342-b2586595b771_1280x1562.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><ol><li><p>Collaboration Tools <br />Software development is a social activity. Learn to use collaboration tools like Jira, Confluence, Slack, MS Teams, Zoom, etc. <br /></p></li><li><p>Programming Languages <br />Pick and master one or two programming languages. Choose from options like Java, Python, JavaScript, C#, Go, etc. <br /></p></li><li><p>API Development <br />Learn the ins and outs of API Development approaches such as REST, GraphQL, and gRPC. <br /></p></li><li><p>Web Servers and Hosting <br />Know about web servers as well as cloud platforms like AWS, Azure, GCP, and Kubernetes <br /></p></li><li><p>Authentication and Testing <br />Learn how to secure your applications with authentication techniques such as JWTs, OAuth2, etc. Also, master testing techniques like TDD, E2E Testing, and Performance Testing <br /></p></li><li><p>Databases <br />Learn to work with relational (Postgres, MySQL, and SQLite) and non-relational databases (MongoDB, Cassandra, and Redis). <br /></p></li><li><p>CI/CD <br />Pick tools like GitHub Actions, Jenkins, or CircleCI to learn about continuous integration and continuous delivery. <br /></p></li><li><p>Data Structures and Algorithms <br />Master the basics of DSA with topics like Big O Notation, Sorting, Trees, and Graphs. <br /></p></li><li><p>System Design <br />Learn System Design concepts such as Networking, Caching, CDNs, Microservices, Messaging, Load Balancing, Replication, Distributed Systems, etc. <br /></p></li><li><p>Design patterns <br />Master the application of design patterns such as dependency injection, factory, proxy, observers, and facade. <br /></p></li><li><p>AI Tools <br />To future-proof your career, learn to leverage AI tools like GitHub Copilot, ChatGPT, Langchain, and Prompt Engineering. </p></li></ol><p>Over to you: What else would you add to the roadmap?</p><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">hi@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 07 Sep 2024 15:30:55 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Scaling the Data Layer</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-data</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-data</guid>
<content:encoded><![CDATA[
<div> 关键词: scalability, data layer, horizontally scaling, techniques, advantages and disadvantages

总结:<br /><br />总结:本文讨论了系统的可扩展性与数据层密切相关，无论API或应用层如何努力扩展，都受到数据层扩展的限制。水平扩展数据层，即“横向扩展”，涉及将数据和负载分布到多个服务器或节点上，适合处理大量数据和高流量负载。然而，这种方法增加了复杂度，引起许多涉及事务和一致性的问题。文章探讨了水平扩展数据层的主要技术及其优缺点，并提供了例子。读者可根据需要选择合适的方法。 <div>
<p>The scalability of a system is heavily dependent on the data layer.&nbsp;</p><p>No matter how much effort is made to scale the API or the application layer, it is limited by the scalability of the data layer. Also, scaling the data layer is often the most difficult task during application design.</p><p>Horizontally scaling the data layer of an application, also known as &#8220;scaling out&#8221;, involves distributing the data and load across multiple servers or nodes.&nbsp;</p><p>This approach is particularly effective for handling large volumes of data and high traffic loads, but it also adds multiple orders of complexity. Since the data is distributed, many issues regarding transactions and consistency that don&#8217;t appear in monolithic databases become quite common.</p><p>Several techniques are available for horizontally scaling the data layer, each having pros and cons with specific nuances worth considering.</p><p>In this post, we&#8217;ll explore the major techniques for scaling the data layer horizontally along with examples. Also, we will understand the advantages and disadvantages of each technique to get a better idea of when to use a particular approach over another choice.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1543" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02b2ef61-cf3f-4c17-bc08-c370d9199f01_3679x3900.png" width="1456" /><div></div></div></a></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-data">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 05 Sep 2024 15:30:43 GMT</pubDate>
</item>
<item>
<title>Millions of Requests Per Hour: SoundCloud’s Microservices Evolution</title>
<link>https://blog.bytebytego.com/p/millions-of-requests-per-hour-soundclouds</link>
<guid>https://blog.bytebytego.com/p/millions-of-requests-per-hour-soundclouds</guid>
<content:encoded><![CDATA[
<div> BFFs, Value-Added Services, Domain Gateways, SoundCloud, microservices
总结:<br /><br />
SoundCloud在架构演变过程中，从单片式设计发展到三层架构。BFFs允许SoundCloud从相同的基础服务中为不同类型的客户端提供服务。随着SoundCloud的发展，基于DDD概念的Value-Added Services被引入，作为访问聚合的权威入口点。最后，Domain Gateways被用来处理与多个领域相关的上下文。SoundCloud正在沿着相同的道路演变其架构，以提高灵活性并将重复项降至最低。 <div>
<h2><a href="https://bit.ly/Datadog_090324">How to monitor containerized applications in Azure (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Datadog_090324" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1081" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F501f9581-41e9-4cd2-b984-e408d5469a75_1081x1081.png" width="1081" /><div></div></div></a></figure></div><p>In this eBook, you&#8217;ll learn how to deploy and monitor containerized applications using Azure and Datadog.</p><ul><li><p>Get best practices on monitoring containerized applications in Azure</p></li><li><p>Gain insight into how Azure Kubernetes Service works</p></li><li><p>Learn how to monitor Azure Kubernetes Service with Datadog's end-to-end observability platform</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Datadog_090324"><span>Download the eBook</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the SoundCloud Engineering Blog. All credit for the technical details goes to the SoundCloud engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>Just like in real life, BFFs can also be life savers when it comes to software projects.&nbsp;</p><p>SoundCloud found this to be the case when they wanted to evolve their service architecture to handle millions of requests per hour.</p><p>In case you&#8217;re not aware, SoundCloud is an online website to stream and listen to free music. They have over 320 million music tracks with the world&#8217;s largest online community of artists, bands, DJs, and audio creators.</p><p>Initially, SoundCloud&#8217;s web application followed what they called the <em>&#8220;eat-your-own-dog-food&#8221;</em> approach. A single monolithic API served the requirements for official applications and third-party integrations.</p><p>Naturally, when SoundCloud grew, this approach wasn&#8217;t sufficient for their scalability needs operationally or organizationally, resulting in a migration from the monolith-based architecture to microservices.</p><p>However, this was easier said than done. With the creation of new microservices, the clients that relied on the monolith had to call multiple services to get the data they needed. This made development more difficult for the clients, which included third-party applications relying on SoundCloud.</p><p>Since this wasn&#8217;t a tenable situation, SoundCloud had to create a way to make things easy for the client applications, while retaining the microservices architecture under the hood. In this post, we will learn how they achieved these goals with BFFs, Value-Added Services, and Domain Gateways.</p><h2>BFFs at SoundCloud</h2><p>The term BFF stands for Backends-for-Frontends. In simpler terms, think of a BFF as a dedicated API gateway for each device or interface type interacting with your application.</p><p>The diagram below shows a high-level view of BFF.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24ff4a8d-9810-41ad-aa98-897255b78bc5_1600x971.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="884" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24ff4a8d-9810-41ad-aa98-897255b78bc5_1600x971.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The SoundCloud engineering team operates dozens of BFFs, each serving a specific type of client. For example, a BFF named Mobile API serves Android and iOS clients. Next, there is a Web API BFF that handles the web front end and the widgets. Also, there are dedicated BFFs for public and partner APIs.</p><p>All external traffic coming into SoundCloud passes through one of the BFFs. Also, these BFFs handle multiple functionalities such as:</p><ul><li><p>Rate Limiting</p></li><li><p>Authentication</p></li><li><p>Header Sanitization</p></li><li><p>Cache-Control</p></li></ul><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bb9b89-2f1b-4a46-89e6-498d3d4ba73b_1600x1025.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="933" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bb9b89-2f1b-4a46-89e6-498d3d4ba73b_1600x1025.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>To facilitate the sharing of common logic across all the BFFs, all of them make use of an internal library that provides edge capabilities. Any changes to this library are rolled out automatically within hours.</p><p>SoundCloud follows the inner source model development philosophy for these BFFs.&nbsp;</p><p>According to this philosophy, individual teams can contribute to the BFF code base, and a core team reviews every change based on the principles discussed in the Collective. This Collective, organized by a Platform Lead, meets regularly to discuss issues and share knowledge.</p><div><hr /></div><h2><a href="https://bit.ly/MLOps_090324">A Virtual Conference to Rival the In-Person Ones<br />It's All About The Data (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/MLOps_090324" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="628" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62779dc9-007a-4373-923e-e21bdf3f9fd7_1200x628.png" title="" width="1200" /><div></div></div></a></figure></div><p>Data remains the key element for any AI or ML workload. It has too often been forgotten in the recent GenAI hype waves. On <strong>September 12th</strong> the MLOps Community is organizing a <strong>free</strong> <strong>virtual conference</strong> to&nbsp;address key challenges around data engineering for AI/ML.<br /><br />The speaker lineup is en fuego &#128293;&#128293;&nbsp;with over 40 speakers from notable companies like NVIDIA, Databricks, DuckDB and Lyft. Talk topics range from data ingestion to FinOps for AI/ML.&nbsp;</p><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/MLOps_090324"><span>Register Now Free</span></a></p><div><hr /></div><h3>Advantages of BFF</h3><p>BFFs provide multiple advantages. Let us look at a few major ones.</p><h4>1 - Autonomy</h4><p>Autonomy is perhaps the biggest value addition in using a BFF.</p><p>Separate APIs per client type means that we can optimize the API for whatever is most convenient for a particular client type.</p><p>For example, in the case of SoundCloud, the mobile clients preferred larger responses with a higher number of embedded entities as a way to minimize the number of requests. In contrast, the web front end prefers fine-grained responses.</p><p>BFFs take care of these varying demands for each client type.</p><h4>2 - Resilience and Lower Risk</h4><p>BFFs also reduce the overall risk of the application going down.</p><p>While a bad deployment might bring down an entire BFF in an availability zone, it doesn&#8217;t bring down the entire platform, which was a possibility with the monolithic API approach.</p><p>See the diagram below that represents a scenario where the mobile BFF going down does not mean the Web BFF is also down.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1057fe80-09d6-4d90-ac18-8c3ba88d80f1_1600x971.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="884" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1057fe80-09d6-4d90-ac18-8c3ba88d80f1_1600x971.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h4>3 - High Development Speed</h4><p>Autonomy and resilience work together to improve confidence, resulting in a higher development speed for new features.</p><p>At SoundCloud, the main BFFs are deployed multiple times a day due to contributions from all over the engineering division.</p><h3>Disadvantages</h3><p>All decisions in software development come with a set of trade-offs. The same is the case with using BFFs.&nbsp;</p><h4>1 - Complexity</h4><p>When the microservices powering the BFFs are very small, performing only CRUD operations with no business logic, the feature integration ends up in the BFF layer. In other words, the entire business logic goes into the BFF layer.</p><p>Moreover, there is a prevalent idea that the BFF is just an extension of the client and should be treated as the backend side of the client.&nbsp;</p><p>While the sentiment is justified due to the naming, it results in developers pushing complex client-side logic to BFF. For example, pushing functionalities like pagination to the server.</p><h4>2 - Duplication of Code</h4><p>While centralized API gateways also have a problem when business logic ends up getting migrated to the gateway, BFFs also suffer from duplication.</p><p>Business logic is duplicated across multiple BFFs. Over time, this duplication can diverge, resulting in inconsistent implementations that drift apart as more development happens.</p><p>For example, SoundCloud saw this issue with the authorization logic getting duplicated across multiple BFFs. This was because the authorization logic needed data from both Track and Playlist entities handled by different microservices and therefore, the integration logic had to be moved into the BFFs.</p><p>See the diagram below:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8c8a10-b5a1-4161-826c-71535171f045_1600x971.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="884" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8c8a10-b5a1-4161-826c-71535171f045_1600x971.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h4>3 - Proliferation of BFFs</h4><p>Even too many BFFs can be a bad thing.&nbsp;</p><p>While BFFs provide autonomy, they also introduce operational overhead. If the team starts to create BFFs for every minor use case, suddenly there is a lot of maintenance overhead.</p><p>Moreover, full autonomy is an illusion. BFFs are an intersection of two worlds and a strong collaboration is still needed between the frontend and backend engineers to build the right BFFs.</p><h2>Value-Added Services</h2><p>At SoundCloud, the BFF implementations became problematic over time due to multiple reasons:</p><ul><li><p>Increased complexity</p></li><li><p>Increased duplicate of code</p></li><li><p>Divergent authorization and business logic across BFFs</p></li></ul><p>This divergence was especially dangerous because the maintenance and synchronicity of the authorization logic are critical for a secure system.</p><p>These challenges led to modifications in the overall architecture of SoundCloud&#8217;s backend.&nbsp;</p><p>See the diagram below for a high-level view of SoundCloud&#8217;s service architecture:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd216a-274c-4b33-bb6d-ac860a379b69_1600x1551.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1411" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd216a-274c-4b33-bb6d-ac860a379b69_1600x1551.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>There were three major parts to this architecture:</p><ul><li><p><strong>Edge Services: </strong>Provides API gateway capabilities. This is essentially the place where the BFFs live.</p></li><li><p><strong>Value-Added Services: </strong>Services in this layer consume data from other services and process them in some way to build better experiences for the user. More on them in a bit.</p></li><li><p><strong>Foundational Services: </strong>These are the low-level microservices that act as the building blocks of the application.</p></li></ul><h3>Responsibility of VAS</h3><p>The Value-Added Services or VAS, built on the concepts of Domain-Driven Design (DDD) form the centrepoint of this new architecture. Specifically, DDD has four major concepts that are as follows:</p><ul><li><p><strong>Domain: </strong>A user or business concern that forms a boundary around service integrations.</p></li><li><p><strong>Entity: </strong>An object that has an independent identifier and lifecycle.</p></li><li><p><strong>Value Objects: </strong>Metadata related to a given entity</p></li><li><p><strong>Aggregates: </strong>Collection of one or more related entities.</p></li></ul><p>In the context of SoundCloud, the Value-Added Services sit between the BFFs and downstream foundational services, and synthesize the aggregates for the BFF. Their main responsibility is to serve core aggregates like Track and Playlist. Also, the VAS handled all context-specific logic related to track visibility and authorization rules.</p><p>For example, the Track VAS filters out all geo-blocked tracks in certain territories.</p><p>See the diagram below for reference:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73413b77-42f3-441c-9048-95566692428c_1600x964.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="877" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73413b77-42f3-441c-9048-95566692428c_1600x964.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>With this approach, BFFs no longer made calls to the individual microservices. The main advantage of this was that all the shared code now lived in a singular codebase and there was no need to duplicate calls to foundational services in multiple BFFs.</p><h3>VAS Migration Process</h3><p>To migrate the logic from the BFFs to VAS, the SoundCloud engineering team employed a 3-step process.&nbsp;</p><p>For example, here&#8217;s how this process was used to create a dedicated VAS for handling Playlists:</p><ul><li><p>First, logic was extracted from the BFFs to create a new Playlist VAS. This required analysis, investigation, and documentation.</p></li><li><p>Second, automatic tests were written to ensure that centralized logic matched the refactored services. Integration tests were added to verify the response format.</p></li><li><p>Lastly, around 50 playlist endpoints in the BFFs were migrated to use the Playlist VAS. Responses from both were carefully compared to ensure a healthy migration.</p></li></ul><h3>Challenges of VAS</h3><p>Despite their great utility, VAS also posed a couple of challenges:</p><ul><li><p>VAS suffered from a big fan out of calls to multiple services. With more features, the aggregates grew and so did the number of network calls to the foundational services, resulting in a big challenge around the size of the fanout.</p></li><li><p>BFFs often have different needs based on their application. For example, one track feature might only be needed on mobile, which makes it pointless to fetch the entire track aggregate from the Web API where that feature may not be needed.</p></li></ul><p>There was a need for centralized VAS endpoints to serve customized aggregates based on the specific needs of the BFF.</p><p>This was supported by the partial response feature in which API consumers can specify which part of the response they will consume using a FieldMask in the request.</p><h2>Domain Gateways</h2><p>The major recent evolution in SoundCloud&#8217;s service architecture came with the introduction of Domain Gateways.&nbsp;</p><p>This was needed because SoundCloud not only provides a consumer application to a music catalog but also provides tools for creators to upload and distribute their music.</p><p>In other words, Consumer and Creator are different domains and are owned by different teams. Implementing the concerns of both domains in a single VAS worked well for a time, but eventually created a large amount of coupling and complexity, resulting in decreased development speed.</p><p>To mitigate this, the SoundCloud engineering team introduced the concept of a Domain Gateway.</p><p>At its core, Domain Gateway is an implementation of a VAS to a specific business domain. See the diagram below to get a better idea:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9ca372d-ec26-46e5-b832-c92d7775c92f_1545x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1508" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9ca372d-ec26-46e5-b832-c92d7775c92f_1545x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>In this approach, the SoundCloud engineering team identified the different business domains that need to use a given entity or aggregate and created a Domain Gateway for each domain. Each gateway can then be maintained by different teams and represent different views on a given entity while relying on the same foundational microservices.</p><p>You can also think of the Domain Gateway as a facade that provides stability and acts as an anti-corruption layer.&nbsp;</p><p>It trades off a certain level of duplication in exchange for autonomy and increased scalability. However, it&#8217;s suitable when different domains have significantly different access patterns and feature sets.</p><h2>Conclusion</h2><p>The evolution of SoundCloud&#8217;s service architecture went from a standard monolithic design into a three-tier architecture. We&#8217;ve looked at the entire journey in detail.</p><p>Some key takeaways are as follows:</p><ul><li><p>BFFs played a key role in enabling SoundCloud to serve different types of clients from the same set of foundational services.</p></li><li><p>As SoundCloud grew, Value-Added Services, modeled on DDD concepts, were introduced to act as authoritative entry points for accessing aggregates.</p></li><li><p>Lastly, Domain Gateways were used to handle the context associated with multiple domains.</p></li></ul><p>At present, SoundCloud is evolving its architecture on the same path to improve flexibility and keep duplication to a minimum.</p><p><strong>References:</strong></p><ul><li><p><a href="https://developers.soundcloud.com/blog/service-architecture-1">Service Architecture at SoundCloud - Part 1: Backends for Frontends</a></p></li><li><p><a href="https://developers.soundcloud.com/blog/service-architecture-2">Service Architecture at SoundCloud - Part 2: Value-Added Services</a></p></li><li><p><a href="https://developers.soundcloud.com/blog/service-architecture-3">Service Architecture at SoundCloud - Part 3: Domain Gateways</a></p></li></ul><p><br /><br /><br /></p>
]]></content:encoded>
<pubDate>Tue, 03 Sep 2024 15:31:02 GMT</pubDate>
</item>
<item>
<title>EP127: 20 Popular Open Source Projects Started or Supported By Big Companies</title>
<link>https://blog.bytebytego.com/p/ep127-20-popular-open-source-projects</link>
<guid>https://blog.bytebytego.com/p/ep127-20-popular-open-source-projects</guid>
<content:encoded><![CDATA[
<div> HTTP, UML, Tools, Status Codes, NoSQL
<br />
HTTP 1、HTTP 2和HTTP 3之间的区别，20个由大公司发起或支持的流行开源项目，UML类图表的基本要素，将代码转化为美丽图表的6个工具，以及一些奇怪的HTTP状态码。NoSQL的重要性和相关课程也被推荐。NVIDIA最新的H200 SXM提供了更好的性能，并邀请大家测试。总结: 文章介绍了HTTP和UML的基本概念、相关工具以及一些奇怪的HTTP状态码，同时推荐了NoSQL相关课程和新产品。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>HTTP 1 Vs HTTP 2 Vs HTTP 3! (Youtube video)</p></li><li><p>20 Popular Open Source Projects Started or Supported By Big Companies </p></li><li><p>A Cheatsheet for UML Class Diagrams </p></li><li><p>Top 6 Tools to Turn Code into Beautiful Diagrams</p></li><li><p>5 HTTP Status Codes That Should Never Have Been Created </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/ScyllaDB_083124Updated">Free NoSQL Training: Live and Instructor Led (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/ScyllaDB_083124Updated" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42d26a69-0e51-4464-9c1e-6c314bc098ee_1600x840.png" width="1456" /><div></div></div></a></figure></div><p>Whether you&#8217;re just curious about NoSQL or looking to optimize your NoSQL performance, this event is a fast way to learn more and get your questions answered by experts.&nbsp;</p><p>You can choose from 6 courses across two tracks:</p><ul><li><p><strong>Essentials:</strong> NoSQL vs SQL architectures, data modeling fundamentals, and building a sample high-performance application with ScyllaDB.</p></li><li><p><strong>Advanced: </strong>Deep dives into application development practices, advanced data modeling, optimizing your database topology, monitoring for performance, and more.</p></li></ul><p>As the name suggests, it&#8217;s truly live (with no on-demand equivalent!) Bring your toughest questions. You can interact with speakers and connect with fellow attendees throughout the event.&nbsp;</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/ScyllaDB_083124Updated"><span>Register for Free</span></a></p><div><hr /></div><h2><strong>HTTP 1 Vs HTTP 2 Vs HTTP 3!</strong></h2><div class="youtube-wrap" id="youtube2-UMwQjFzTQXw"><div class="youtube-inner"></div></div><div><hr /></div><h2>20 Popular Open Source Projects Started or Supported By Big Companies </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1622" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde7879c-18c0-4c7c-8f35-56f661859421_1280x1622.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><ol><li><p>Google <br />- Kubernetes <br />- TensorFlow <br />- Go <br />- Angular </p></li><li><p>Meta <br />- React <br />- PyTorch <br />- GraphQL <br />- Cassandra </p></li><li><p>Microsoft <br />- VSCode <br />- TypeScript <br />- Playwright </p></li><li><p>Netflix <br />- Chaos Monkey <br />- Hystrix <br />- Zuul </p></li><li><p>LinkedIn <br />- Kafka <br />- Samza <br />- Pinot </p></li><li><p>RedHat <br />- Ansible <br />- OpenShift <br />- Ceph Storage </p></li></ol><p>Over to you: Which other project would you add to the list?</p><div><hr /></div><h2><a href="https://bit.ly/Nebius_083024">Are you ready to train your AI model on NVIDIA&#174; H200 GPU cluster with InfiniBand? (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Nebius_083024" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="Image" class="sizing-normal" height="1066" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a177836-5d21-4b16-947b-d5aa08d32778_1066x1066.jpeg" title="Image" width="1066" /><div></div></div></a></figure></div><p>NVIDIA claims that H200 SXM offer significant enhancements over the H100 SXM, delivering up to 45% better performance in generative AI and HPC tasks. Want to test it this autumn? Contact Nebius AI team. </p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Nebius_083024"><span>Learn more</span></a></p><div><hr /></div><h2>A Cheatsheet for UML Class Diagrams </h2><p>UML is a standard way to visualize the design of your system and class diagrams are used across the industry. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, diagram" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f236404-6ba9-4ba6-a25b-b3c76a7c073a_1281x1536.gif" title="graphical user interface, diagram" width="1281" /><div></div></div></a></figure></div><p>They consist of: </p><ol><li><p>Class <br />Acts as the blueprint that defines the properties and behavior of an object. <br /></p></li><li><p>Attributes <br />Attributes in a UML class diagram represent the data fields of the class. <br /></p></li><li><p>Methods <br />Methods in a UML class diagram represent the behavior that a class can perform. <br /></p></li><li><p>Interfaces <br />Defines a contract for classes that implement it. Includes a set of methods that the implementing classes must provide. <br /></p></li><li><p>Enumeration <br />A special data type that defines a set of named values such as product category or months in a year. <br /></p></li><li><p>Relationships <br />Determines how one class is related to another. Some common relationships are as follows: <br />- Association <br />- Aggregation <br />- Composition <br />- Inheritance <br />- Implementation <br /> </p></li></ol><p>Over to you: What other building blocks have you seen in UML class diagrams?</p><div><hr /></div><h2>Top 6 Tools to Turn Code into Beautiful Diagrams</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7011f6fb-599e-4dac-b783-55240a86f8e7_1280x1664.gif" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><ul><li><p>Diagrams</p></li><li><p>Go Diagrams</p></li><li><p>Mermaid</p></li><li><p>PlantUML</p></li><li><p>ASCII diagrams</p></li><li><p>Markmap</p></li></ul><p>Did we miss anything? What's your favorite?</p><div><hr /></div><h2>5 HTTP Status Codes That Should Never Have Been Created </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, text, application, email" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4462d605-d2ae-4643-add4-f8811cdd49ec_1289x1536.jpeg" title="graphical user interface, text, application, email" width="1289" /><div></div></div></a></figure></div><ul><li><p>451 Unavailable for Legal Reasons: Access denied due to legal issues. </p></li><li><p>218 This is Fine: Inspired by the meme, bypasses server error overrides. </p></li><li><p>420 Enhance Your Calm: Twitter&#8217;s old code for exceeding rate limits. Now changed to 429. </p></li><li><p>530 Site Frozen: Used by Pantheon for locked sites, often unpaid bills. </p></li><li><p>418 I'm a Teapot: A classic April Fool's joke indicating a server's limitations.</p></li></ul><p>Over to you: Have you come across any other weird HTTP Status Codes? </p><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong><br /></p>
]]></content:encoded>
<pubDate>Sat, 31 Aug 2024 15:30:54 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Load Balancers for Scaling</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-load-balancers</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-load-balancers</guid>
<content:encoded><![CDATA[
<div> 关键词: load balancers, system architecture, network traffic, multiple servers, horizontal scalability

总结:<br /><br />
load balancers在系统架构中扮演关键角色，像酒店前台接待员一样，分配来自网络的流量到多台服务器上。通过有效地分担工作负荷，load balancers帮助系统在高峰期保持性能稳定。这样系统可以水平扩展，保证了系统的可靠性和性能。 <div>
<p>Over the years, load balancers have become a crucial component of system architecture, acting as the front-line gatekeepers that distribute incoming network traffic across multiple servers.&nbsp;</p><p>Much like a hotel receptionist who greets guests, checks their documents, and directs them to specific rooms, load balancers manage the data flow within a system to ensure that the system is horizontally scalable.</p><p>By effectively distributing the workload across multiple servers, load balancers help maintain system performance, even during peak usage.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53ec7e8b-12a1-403f-aecb-cc370ee7e837_2250x2840.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1838" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53ec7e8b-12a1-403f-aecb-cc370ee7e837_2250x2840.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-load-balancers">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 29 Aug 2024 15:30:30 GMT</pubDate>
</item>
<item>
<title>EP126: The Ultimate Kafka 101 You Cannot Miss</title>
<link>https://blog.bytebytego.com/p/ep126-the-ultimate-kafka-101-you</link>
<guid>https://blog.bytebytego.com/p/ep126-the-ultimate-kafka-101-you</guid>
<content:encoded><![CDATA[
<div> AWS Services, Kafka, API Design, Internet Works, QA Wolf
<br />
AWS是云服务领域的领导者，不断提供新的服务以满足不断发展的需求。Kafka是分布式事件存储和流平台，被广泛应用于大数据流处理。API设计需要遵循领域模型，选择合适的HTTP方法和状态码，实现幂等性，版本控制，语义化路径，批量处理和查询语言设计等。而互联网工作原理简单易懂，QA Wolf提供快速测试服务，可以提高测试效率，减少错误。总结：AWS提供多样化服务，Kafka用于大数据处理，API设计需遵循一定规则，互联网工作原理简单，QA Wolf可提高测试效率。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>How the Internet Works in 9 Minutes (Youtube video)</p></li><li><p>AWS Services Cheat Sheet </p></li><li><p>The Ultimate Kafka 101 You Cannot Miss </p></li><li><p>8 Tips for Efficient API Design</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/QAWolf_082424">&#9986;&#65039;Cut your QA cycles down to minutes with automated testing (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/QAWolf_082424" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="736" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F321732b3-caf3-420e-9e7b-6a330fbf281d_1473x745.png" width="1456" /><div></div></div></a></figure></div><p>Are slow test cycles bottlenecking your dev teams&#8217; release velocity? With QA Wolf, your organization can run entire test suites in minutes for faster feedback and developer confidence to ship.</p><p><a href="https://bit.ly/QAWolf_082424">QA Wolf</a> takes testing off your plate. They can get you:</p><ul><li><p><a href="https://bit.ly/QAWolf_082424">80% automated test coverage in weeks</a>&#8212;not years</p></li><li><p><a href="https://bit.ly/QAWolf_082424">Unlimited parallel test runs</a></p></li><li><p>24-hour maintenance and on-demand test creation</p></li><li><p>Zero flakes, guaranteed</p></li></ul><p>The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.</p><p>With QA Wolf, <a href="QAWolf_082424CaseStudy">D</a><a href="https://bit.ly/QAWolf_072024CaseStudy">rata&#8217;s team of 80+ engineers</a> achieved 4x more test cases and 86% faster QA cycles.</p><p>&#127775;Rated 4.8/5 on G2.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/QAWolf_082424"><span>Schedule a demo to learn more</span></a></p><div><hr /></div><h2>How the Internet Works in 9 Minutes</h2><div class="youtube-wrap" id="youtube2-sMHzfigUxz4"><div class="youtube-inner"></div></div><div><hr /></div><h2>AWS Services Cheat Sheet </h2><p>AWS grew from an in-house project to the market leader in cloud services, offering so many different services that even experts can find it a lot to take in. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff416d0da-2f7a-46df-9709-f7859c5e5613_1415x1536.gif" title="No alternative text description for this image" width="1415" /><div></div></div></a></figure></div><p>The platform not only caters to foundational cloud needs but also stays at the forefront of emerging technologies such as machine learning and IoT, establishing itself as a bedrock for cutting-edge innovation. AWS continuously refines its array of services, ensuring advanced capabilities for security, scalability, and operational efficiency are available. <br /> <br />For those navigating the complex array of options, this AWS Services Guide is a helpful visual aid. <br /> <br />It simplifies the exploration of AWS's expansive landscape, making it accessible for users to identify and leverage the right tools for their cloud-based endeavors. <br /> <br />Over to you: What improvements would you like to see in AWS services based on your usage?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e469dc6-af33-408b-acc5-e11f57fd9aca_1437x1600.png" width="1437" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-api">A Crash Course on Scaling the API Layer</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-architectural-scalability">A Crash Course on Architectural Scalability</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservices-design">A Crash Course on Microservices Design Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-domain-driven-design">A Crash Course on Domain-Driven Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/tidying-code">"Tidying" Code</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>The Ultimate Kafka 101 You Cannot Miss </h2><p>Kafka is super-popular but can be overwhelming in the beginning. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9d00da63-c92c-46c4-8bb7-86f0adf769c6_1354x1536.gif" title="graphical user interface, application" width="1354" /><div></div></div></a></figure></div><p>Here are 8 simple steps that can help you understand the fundamentals of Kafka.</p><ol><li><p>What is Kafka? <br />Kafka is a distributed event store and a streaming platform. It began as an internal project at LinkedIn and now powers some of the largest data pipelines in the world in orgs like Netflix, Uber, etc. <br /></p></li><li><p>Kafka Messages <br />Message is the basic unit of data in Kafka. It&#8217;s like a record in a table consisting of headers, key, and value. <br /></p></li><li><p>Kafka Topics and Partitions <br />Every message goes to a particular Topic. Think of the topic as a folder on your computer. Topics also have multiple partitions. <br /></p></li><li><p>Advantages of Kafka <br />Kafka can handle multiple producers and consumers, while providing disk-based data retention and high scalability. <br /></p></li><li><p>Kafka Producer <br />Producers in Kafka create new messages, batch them, and send them to a Kafka topic. They also take care of balancing messages across different partitions. <br /></p></li><li><p>Kafka Consumer <br />Kafka consumers work together as a consumer group to read messages from the broker. <br /></p></li><li><p>Kafka Cluster <br />A Kafka cluster consists of several brokers where each partition is replicated across multiple brokers to ensure high availability and redundancy. <br /></p></li><li><p>Use Cases of Kafka <br />Kafka can be used for log analysis, data streaming, change data capture, and system monitoring. <br /></p></li></ol><p>Over to you: What else would you add to get a better understanding of Kafka?</p><div><hr /></div><h2>8 Tips for Efficient API Design</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43fc8687-6891-4cbf-acf6-d5447cd1e592_1350x1536.gif" title="graphical user interface" width="1350" /><div></div></div></a></figure></div><ul><li><p>Domain Model Driven <br /> When designing the path structure of a RESTful API, we can refer to the domain model. <br /></p></li><li><p>Choose Proper HTTP Methods <br />Defining a few basic HTTP Methods can simplify the API design. For example, PATCH can often be a problem for teams. <br /></p></li><li><p>Implement Idempotence Properly <br />Designing for idempotence in advance can improve the robustness of an API. GET method is idempotent, but POST needs to be designed properly to be idempotent. <br /></p></li><li><p>Choose Proper HTTP Status Codes <br />Define a limited number of HTTP status codes to use to simplify application development. <br /></p></li><li><p>Versioning <br />Designing the version number for the API in advance can simplify upgrade work. <br /></p></li><li><p>Semantic Paths <br />Using semantic paths makes APIs easier to understand, so that users can find the correct APIs in the documentation. <br /></p></li><li><p>Batch Processing <br />Use batch/bulk as a keyword and place it at the end of the path. <br /></p></li><li><p>Query Language <br />Designing a set of query rules makes the API more flexible. For example, pagination, sorting, filtering etc.</p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 24 Aug 2024 15:30:51 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Scaling the API Layer</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-api</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-api</guid>
<content:encoded><![CDATA[
<div> API、通信、后端服务、功能和数据、扩展性
<br />
API层是现代互联网应用程序中客户端和后端服务之间通信的基础。它作为客户端访问应用程序提供的功能和数据的主要接口。API层在应用程序架构中起着关键作用，对应用程序的扩展性至关重要。提升API层的扩展性对于处理负载和流量峰值、提供更好的用户体验以及优化成本和资源利用至关重要。需要理解关键概念，使用测试过的扩展策略和最佳实践来实现API层的扩展。<br /><br />总结:API层是应用程序的关键接口，扩展性对于处理负载和提供用户体验至关重要，需要理解关键概念，使用扩展策略和最佳实践来实现API层的扩展。 <div>
<p>The API (Application Programming Interface) layer serves as the backbone for communication between clients and the backend services in modern internet-based applications.</p><p>It acts as the primary interface through which clients, such as web or mobile applications, access the functionality and data provided by the application. The API Layer of any application has several key responsibilities such as:</p><ul><li><p>Process incoming requests from clients based on the defined API contract.</p></li><li><p>Enforce security mechanisms and protocols by authenticating and authorizing clients based on their credentials or access tokens.</p></li><li><p>Orchestrate interactions between various backend services and aggregate the responses received from them.</p></li><li><p>Handle responses by formatting and returning the result to the client.</p></li></ul><p>Due to the central role APIs play in the application architecture, they become critical for the application scalability.</p><p>The scalability of the API layer is crucial due to the following reasons:</p><ul><li><p><strong>Handling Load and Traffic Spikes: </strong>As applications become popular, they encounter increased traffic and sudden spikes in user demand. A scalable API can manage the increased load efficiently.</p></li><li><p><strong>Better User Experience:</strong> The bar for user expectation has gone up. Most users these days expect fast and responsive applications. A scalable API ensures that the application can support a high number of users without compromising performance.</p></li><li><p><strong>Cost and Resource Optimization: </strong>Scalable APIs unlock the path to better resource utilization. Rather than provisioning the infrastructure upfront for the highest demand level, instances are added and removed based on demand, resulting in reduced operational costs.</p></li></ul><p>In this article, we&#8217;ll learn the key concepts a developer must understand for API scalability. We will also look at some tried and tested strategies for scaling the API layer with basic code examples for clarity. Lastly, we will also look at some best practices that can help with scaling the API layer.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e469dc6-af33-408b-acc5-e11f57fd9aca_1437x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e469dc6-af33-408b-acc5-e11f57fd9aca_1437x1600.png" width="1437" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-scaling-the-api">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 22 Aug 2024 15:30:14 GMT</pubDate>
</item>
<item>
<title>Trillions of Indexes: How Uber’s LedgerStore Supports Such Massive Scale</title>
<link>https://blog.bytebytego.com/p/trillions-of-indexes-how-ubers-ledgerstore</link>
<guid>https://blog.bytebytego.com/p/trillions-of-indexes-how-ubers-ledgerstore</guid>
<content:encoded><![CDATA[
<div> LedgerStore, Indexing Architecture, Trillion Entries Migration, Data Backfill, Validation <br />
LedgerStore是Uber的自定义存储解决方案，用于管理财务交易数据，支持三种主要的索引类型，包括强一致性索引、最终一致性索引和时间范围索引。Uber成功将1.2PB的压缩数据和超过1万亿条记录迁移至LedgerStore，通过进行阴影验证和离线验证，解决了数据回填过程中的挑战。迁移至LedgerStore带来了显著的成本节约和性能改进，是Uber处理大规模数据操作的成功案例。 <br /><br />总结: <br /> LedgerStore是Uber的关键存储解决方案，支持不同类型的索引，迁移1万亿条记录的成功案例展示了数据回填和验证过程中的挑战和解决方案。包括成本节约和性能改进在内，这个迁移案例展示了Uber处理大规模数据操作的能力。 <div>
<h2><a href="https://bit.ly/Astronomer_082024">Try Fully Managed Apache Airflow and get certified for FREE (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://www.astronomer.io/try-astro/?utm_source=bytebytego-newsletter&amp;utm_medium=newsletter&amp;utm_campaign=bytebytego-newsletter-free-cert-rialhttps://bit.ly/Astronomer_082024" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29240e69-a578-49dc-aa08-187f03c43096_1600x840.png" title="" width="1456" /><div></div></div></a></figure></div><p>Run Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new sign ups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).</p><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/Astronomer_082024"><span>Get Started &#8212;&gt;</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the Uber Engineering Blog. All credit for the technical details goes to the Uber engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>Ledgers are the source of truth of any financial event. By their very nature, ledgers are immutable. Also, we usually want to access data stored in these ledgers in various combinations.</p><p>With billions of trips and deliveries, Uber performs tens of billions of financial transactions. Merchants, riders, and customers are involved in these financial transactions. Money flows from the ones spending to the ones earning.&nbsp;</p><p>To manage this mountain of financial transaction data, the LedgerStore is an extremely critical storage solution for Uber. The myriad access patterns for the data stored in LedgerStore also create the need for a huge number of indexes.</p><p></p><p>In this post, we&#8217;ll look at how Uber implemented the indexing architecture for LedgerStore to handle trillions of indexes and how they migrated a trillion entries of Uber&#8217;s Ledger Data from DynamoDB to LedgerStore.</p><h2>What is LedgerStore?</h2><p>LedgerStore is Uber&#8217;s custom-built storage solution for managing financial transactions. Think of it as a giant, super-secure digital ledger book that keeps track of every financial event at Uber, from ride payments to food delivery charges.</p><p>What makes LedgerStore special is its ability to handle an enormous amount of data. We&#8217;re talking about trillions of entries.</p><p>Two main features supported by LedgerStore are:</p><ul><li><p><strong>Immutability: </strong>LedgerStore is designed to be immutable, which means once a record is written, it cannot be changed. This ensures the integrity of financial data.</p></li><li><p><strong>Indexes: </strong>LedgerStore allows quick look-up of information using various types of indexes. For example, if someone needs to check all transactions for a particular user or all payments made on a specific date, LedgerStore can retrieve this information efficiently.</p></li></ul><p>Ultimately, LedgerStore helps Uber manage its financial data more effectively, reducing costs compared to previous solutions.</p><h2>Types of Indexes</h2><p>LedgerStore supports three main types of indexes:</p><ul><li><p>Strongly consistent indexes</p></li><li><p>Eventually consistent indexes</p></li><li><p>Time-range indexes</p></li></ul><p>Let&#8217;s look at each of them in more detail.</p><h3>Strongly Consistent Indexes</h3><p>These indexes provide immediate read-your-write guarantees, crucial for scenarios like credit card authorization flows. For example, when a rider starts an Uber trip, a credit card hold is placed, which must be immediately visible to prevent duplicate charges.</p><p>See the diagram below that shows the credit-card payment flow for an Uber trip supported by strongly consistent indexes.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f08af7a-0a4d-4690-bd83-fbb99992f861_1600x983.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="895" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f08af7a-0a4d-4690-bd83-fbb99992f861_1600x983.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>If the index is not strongly consistent, the hold may take a while to be visible upon reading. This can result in duplicate charges on the user&#8217;s credit card.</p><p>Strongly consistent indexes at Uber are built using the two-phase commit approach. Here&#8217;s how this approach works in the write path and read path.</p><h4>1 - The Write Path</h4><p>The write path consists of the following steps:</p><ul><li><p>When a new record needs to be inserted, the system first writes an &#8220;index intent&#8221; to the index table. It could even be multiple indexes.</p></li><li><p>This intent signifies that a new record is about to be written. If the index intent write fails, the whole insert operation fails.</p></li><li><p>After the index intent is successfully written, the actual record is written to the main data store.</p></li><li><p>If the record write is also successful, the system commits the index. This is done asynchronously to avoid affecting the end-user insert latency.</p></li></ul><p>The diagram below shows this entire process.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57aa8987-01ef-489b-8fff-59074076d1b8_1600x983.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="895" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57aa8987-01ef-489b-8fff-59074076d1b8_1600x983.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>There is one special case to consider here: if the index intent write succeeds, but the record write fails, the index intent has to be rolled back to prevent the accumulation of unused intents. This part is handled during the read path.</p><h4>2 - The Read Path</h4><p>The below steps happen during the read path:</p><ul><li><p>If a committed index entry is found, the response data is returned to the client.</p></li><li><p>If an index entry is found but with a &#8220;pending&#8221; status, the system must resolve its state. This is done by checking the main data store for the corresponding record.</p></li><li><p>If the record exists, the index is asynchronously committed and the record is returned to the user.</p></li><li><p>If the record doesn&#8217;t exist, the index intent is deleted or rolled back and the read operation does not return a result for the query.</p></li></ul><p>The diagram below shows the process steps in more detail.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2403ad00-8922-4ef7-b953-37f1a582f496_1600x1540.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1401" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2403ad00-8922-4ef7-b953-37f1a582f496_1600x1540.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><div><hr /></div><h2><a href="https://bit.ly/TechForProduct_082024">Reserve Your Seat Now! | Upcoming Cohort on Aug 26th, 2024 (Sponsored)&nbsp;</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://maven.com/tech-for-product/tech-fundamentals?utm_campaign=MTM3MTk5&amp;utm_medium=clp_share_link&amp;utm_source=instructorhttps://bit.ly/TechForProduct_082024" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb960f66c-6691-44ce-b3ae-17b44e6fe1b8_1600x840.png" title="" width="1456" /><div></div></div></a></figure></div><p><strong>Build confidence without getting lost in technical jargon.</strong></p><p>This cohort is designed to help you build a foundational understanding of software applications. You won&#8217;t just memorize a bunch of terms - you&#8217;ll actually understand how software products are designed and deployed to handle millions of users.</p><p>And our community will be here to give you the support, guidance, and accountability you&#8217;ll need to finally stick with it.</p><p>After only 4 weeks, you&#8217;ll look back and think.. &#8220;WOW! I can&#8217;t believe I did that.&#8221;</p><p><strong>Now imagine if you could:</strong></p><p>&#9989; Master technical concepts without getting lost in an internet maze.</p><p>&#9989; Stop asking engineers to dumb down concepts when talking to you.</p><p>&#9989; Predict risks, anticipate issues, and avoid endless back-and-forth.</p><p>&#9989; Improve your communication with engineers, users, and technical stakeholders.</p><p>Grab your spot now with an exclusive 25% off discount for ByteByteGo Readers. See you there!</p><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/TechForProduct_082024"><span>Register Now!</span></a></p><div><hr /></div><h3>Eventually Consistent Indexes</h3><p>These indexes are designed for scenarios where real-time consistency is not critical, and a slight delay in index updates is acceptable. They offer better performance and availability at the cost of immediate consistency.</p><p>From a technical implementation point of view, the eventually consistent indexes are generated using the Materialized Views feature of Uber&#8217;s Docstore.&nbsp;</p><p>Materialized views are pre-computed result sets stored as a table, based on a query against the base table(s). The materialized view is updated asynchronously when the base table changes.</p><p>When a write occurs to the main data store, it doesn&#8217;t immediately update the index. Instead, a separate process periodically scans for changes and updates the materialized view. The consistency window is configurable and determines how frequently the background process runs to update the indexes.</p><p>In Uber&#8217;s case, the Payment History Display screen uses the eventually consistent indexes.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F007126e2-dc2a-4b12-a60c-0c1e28934d56_1600x1547.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1408" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F007126e2-dc2a-4b12-a60c-0c1e28934d56_1600x1547.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://www.uber.com/en-IN/blog/how-ledgerstore-supports-trillions-of-indexes/">Uber Engineering Blog</a></figcaption></figure></div><h3>Time-range Indexes</h3><p>Time-range indexes are a crucial component of LedgerStore, designed to query data within specific time ranges efficiently.</p><p>These indexes are important for operations like offloading older ledger data to cold storage or sealing data for compliance purposes. The main characteristic of these indexes is their ability to handle deterministic time-range batch queries that are significantly larger in scope compared to other index types.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2af67e1-9a70-4855-bca6-fa3c76b016f1_1600x1078.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="981" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2af67e1-9a70-4855-bca6-fa3c76b016f1_1600x1078.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Earlier, the time-range indexes were implemented using a dual-table design approach in DynamoDB. However, it was operationally complex.</p><p>The migration of LedgerStore to Uber&#8217;s Docstore paved the path for a simpler implementation of the time-range index. Here&#8217;s a detailed look at the Docstore implementation for the time-range indexes:</p><ul><li><p><strong>Single Table Design: </strong>Only one table is used for time-range indexes in Docstore.</p></li><li><p><strong>Partitioning Strategy: </strong>Index entries are partitioned based on full timestamp value. This ensures a uniform distribution of writes across partitions, eliminating the chances of hot partitions and write throttling.</p></li><li><p><strong>Sorted Data Storage: </strong>Data is stored in a sorted manner based on the primary key (partition + sort keys).&nbsp;</p></li><li><p><strong>Read Operation: </strong>Reads involve a prefix scanning of each shard of the table up to a certain time granularity. For example, to read 30 minutes of data, the operation might be broken down into three 10-minute interval scans. Each scan is bounded by start and end timestamps. After scanning, a scatter-gather operation is performed, followed by sort merging across shards to obtain all time-range index entries in the given window, in a sorted fashion.</p></li></ul><p>For clarity, consider a query to fetch all ledger entries between &#8220;2024-08-09 10:00:00&#8221; and &#8220;2024-08-09 10:30:00&#8221;. The query would be broken down into three 10-minute scans:</p><ul><li><p>2024-08-09 10:00:00 to 2024-08-09 10:10:00</p></li><li><p>2024-08-09 10:10:00 to 2024-08-09 10:20:00</p></li><li><p>2024-08-09 10:20:00 to 2024-08-09 10:30:00</p></li></ul><p>Each of these scans would be executed across all shards in parallel. The results would then be merged and sorted to provide the final result set.</p><p>The diagram below shows the overall process:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F916dc967-4d72-4c7f-9df1-18b3a7e225df_1600x1045.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="951" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F916dc967-4d72-4c7f-9df1-18b3a7e225df_1600x1045.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>Index Lifecycle Management</h2><p>Index lifecycle management is another component of LedgerStore&#8217;s architecture that handles the design, modification, and decommissioning of indexes.</p><p>Let&#8217;s look at the main parts of the index lifecycle management system.</p><h2>Index Lifecycle State Machine</h2><p>This component orchestrates the entire lifecycle of an index:</p><ul><li><p>Creating the index table</p></li><li><p>Backfilling it with historical index entries</p></li><li><p>Validating the entries for completeness</p></li><li><p>Swapping the old index with the new one for read/write operations</p></li><li><p>Decommissioning the old index</p></li></ul><p>The state machine ensures that each step is completed correctly before moving to the next, maintaining data integrity throughout the process.</p><p>The diagram below shows all the steps:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9e87701-86c6-4ab9-9174-fdbf0cf3558c_1600x914.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="832" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9e87701-86c6-4ab9-9174-fdbf0cf3558c_1600x914.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Historical Index Data Backfill</h3><p>When new indexes are defined or existing ones are modified, it&#8217;s essential to backfill historical data to ensure completeness.</p><p>The historical index data backfill component performs the following tasks:</p><ul><li><p>Builds indexes from historical data stored in cold storage.</p></li><li><p>Backfills the data to the storage layer in a scalable manner.</p></li><li><p>Uses configurable rate-limiting and batching to manage the process efficiently.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bdd9cd9-5ac9-4a1b-a519-da6b28f57e8f_1600x942.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="857" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bdd9cd9-5ac9-4a1b-a519-da6b28f57e8f_1600x942.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Index Validation</h3><p>After indexes are backfilled, they need to be verified for completeness. This is done through an offline job that:</p><ul><li><p>Computes order-independent checksums at a certain time-window granularity.</p></li><li><p>Compares these checksums across the source of truth data and the index table.</p></li></ul><p>From a technical point of view, the component uses a time-window approach i.e. computing checksums for every 1-hour block of data. Even if a single entry is missed, the aggregate checksum for that time window will lead to a mismatch.&nbsp;</p><p>For example, If checksums are computed for 1-hour blocks, and an entry from 2:30 PM is missing, the checksum for the 2:00 PM - 3:00 PM block will not match.</p><h2>Migration of Uber&#8217;s Payment Data to LedgerStore</h2><p>Now that we have understood about LedgerStore&#8217;s indexing architecture and capabilities, let&#8217;s look at the massive migration of Uber&#8217;s payment data to LedgerStore.</p><p>Uber&#8217;s payment platform, Gulfstream, was initially launched in 2017 using Amazon DynamoDB for storage. However, as Uber&#8217;s operations grew, this setup became increasingly expensive and complex.&nbsp;</p><p>By 2021, Gulfstream was using a combination of three storage systems:</p><ul><li><p>DynamoDB for the most recent 12 weeks of data. This was the hot data.</p></li><li><p>TerraBlob (Uber&#8217;s internal blob store like AWS S3) for older or cold data.</p></li><li><p>LedgerStore (LSG) where new data was being written and where they wanted to migrate all data.</p></li></ul><p>The primary reasons for migrating to LedgerStore were as follows:</p><ul><li><p><strong>Cost savings: </strong>Moving to LedgerStore promised significant recurring cost savings compared to DynamoDB.</p></li><li><p><strong>Simplification: </strong>Consolidating from three storage systems to one would simplify the code and design of the Gulfstream services.</p></li><li><p><strong>Improved Performance: </strong>LedgerStore offered shorter indexing lag and reduced latency due to being on-premise.</p></li><li><p><strong>Purpose-Built Design: </strong>LedgerStore was specifically designed for storing payment-style data, offering features like verifiable immutability and tiered storage for cost management.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f7e5a5e-1849-4d6f-9fb1-f1d719d029a9_1600x895.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="814" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f7e5a5e-1849-4d6f-9fb1-f1d719d029a9_1600x895.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The migration was a massive undertaking. Some statistics are as follows:</p><ul><li><p>1.2 Petabytes of compressed data</p></li><li><p>Over 1 trillion entries</p></li><li><p>0.5 PB of uncompressed data for secondary indexes.</p></li></ul><p>For reference, storing this data on typical 1 TB hard drives requires a total of 1200 hard drives just for the compressed data.</p><h3>Checks</h3><p>One of the key goals of the migration was to ensure that the backfill was correct and acceptable. Also, the current traffic requirements needed to be fulfilled.</p><p>Key validation methods adopted were as follows:</p><h4>1 - Shadow Validation</h4><p>This ensured that the new LedgerStore system could handle current traffic patterns without disruption.</p><p>Here&#8217;s how it worked:</p><ul><li><p>The system would compare responses from the existing DynamoDB-based system with what the LedgerStore would return for the same queries. This allowed the team to catch any discrepancies in real time.&nbsp;</p></li><li><p>An ambitious goal was to ensure 99.99% completeness and correctness with an upper bound of 99.9999%.</p></li><li><p>To achieve six nines of confidence, the team needed to compare at least 100 million records. At a rate of 1000 comparisons per second, this would take more than a day to collect sufficient data.</p></li><li><p>During shadow validation, production traffic was duplicated on LedgerStore. This helped the team verify the LedgerStore&#8217;s ability to handle the production load.</p></li></ul><h4>2 - Offline Validation</h4><p>While shadow validation was effective for current traffic patterns, it couldn&#8217;t provide strong guarantees about rarely-accessed historical data. This is where offline validation came into play.&nbsp;</p><p>Here&#8217;s how it worked:</p><ul><li><p>Offline validation involved comparing complete data dumps from DynamoDB with the data in LedgerStore. The largest offline validation job compared 760 billion records, involving 70 TB of compressed data.</p></li><li><p>The team used Apache Spark for these massive comparison jobs, leveraging distributed shuffle-as-a-service for Spark.</p></li></ul><h3>Backfill Issues</h3><p>The process of migrating Uber&#8217;s massive ledger data from DynamoDB to LedgerStore involved several backfill challenges that had to be solved:</p><ul><li><p><strong>Scalability: </strong>The engineering team learned that starting small and gradually increasing the scale was crucial. Blindly pushing beyond the system&#8217;s limit could create a DDoS attack on their systems.</p></li><li><p><strong>Incremental Backfills: </strong>Given the enormous volume of data, attempting to backfill all at once would generate 10 times the normal traffic load. The solution was to break the backfill into smaller, manageable batches that could be completed within minutes.</p></li><li><p><strong>Rate Control: </strong>To ensure consistent behavior during backfill, the team implemented rate control using Guava&#8217;s RateLimiter in Java/Scala.&nbsp; The team also developed a system to dynamically adjust the backfill rate based on the current system load. For this, they used an additive increase/multiplicative decrease approach similar to TCP congestion control.</p></li><li><p><strong>Data File Size Management: </strong>The team found that managing the size of data files was important. They aimed to keep the file sizes around 1 GB, with flexibility between 100 MB and 10 GB. This approach helped avoid issues like MultiPart limitations in various tools and prevented problems associated with having too many small files.</p></li><li><p><strong>Fault Tolerance: </strong>Data quality issues and data corruption were inevitable. The team&#8217;s solution was to monitor statistics. If the failure rate was high, they would manually stop the backfill, fix the issue, and continue. For less frequent issues, they let the backfill continue while addressing problems in parallel.</p></li><li><p><strong>Logging Challenges: </strong>Excessive logging during backfill could overload the logging infrastructure. The solution was to use a rate limiter for logging. For example, they might log only once every 30 seconds for routine operations while logging all errors if they occurred infrequently.</p></li></ul><h2>Conclusion</h2><p>The impact of Uber&#8217;s ledger data migration to LedgerStore has been amazing, with over 2 trillion unique indexes successfully transferred without a single data inconsistency detected in over six months of production use.&nbsp;</p><p>This migration, involving 1.2 PB of compressed data and over 1 trillion entries, showcases Uber&#8217;s ability to handle massive-scale data operations without disrupting critical financial processes. It also provides great learning points for the readers.</p><p>The cost savings from this migration have been substantial, with estimated yearly savings exceeding $6 million due to reduced spend on DynamoDB. Performance improvements have been notable, with LedgerStore offering shorter indexing lag and better network latency due to its on-premise deployment within Uber&#8217;s data centers.</p><p><strong>References:</strong></p><ul><li><p><a href="https://www.uber.com/en-IN/blog/how-ledgerstore-supports-trillions-of-indexes/">How LedgerStore Supports Trillions of Indexes at Uber</a></p></li><li><p><a href="https://www.uber.com/en-IN/blog/migrating-from-dynamodb-to-ledgerstore/">Migrating a Trillion Entries of Uber&#8217;s Ledger Data from DynamoDB to LedgerStore</a></p></li><li><p><a href="https://www.uber.com/en-IN/blog/dynamodb-to-docstore-migration">How Uber Migrated Financial Data from DynamoDB to Docstore</a></p></li></ul>
]]></content:encoded>
<pubDate>Tue, 20 Aug 2024 15:31:12 GMT</pubDate>
</item>
<item>
<title>EP125: How does Garbage Collection work?</title>
<link>https://blog.bytebytego.com/p/ep125-how-does-garbage-collection</link>
<guid>https://blog.bytebytego.com/p/ep125-how-does-garbage-collection</guid>
<content:encoded><![CDATA[
<div> Linux Performance Tools, Garbage Collection, Fault-Tolerant Systems, System Design Tradeoffs, WorkOS
<br />
<br />
总结:本文介绍了Linux性能工具、垃圾回收、设计容错系统的重要性和原则、系统设计中不可忽视的权衡，以及WorkOS的用户管理解决方案。Garbage Collection是一种自动内存管理特性，Java、Python和Go等编程语言都有不同的垃圾收集器。设计容错系统的六个原则包括复制、冗余、负载均衡、故障转移机制、优雅降级以及监控和警报。系统设计中必须考虑的权衡包括垂直vs水平扩展、SQLvsNoSQL、批处理vs流处理等。WorkOS提供的用户管理解决方案适用于企业级应用。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Linux Performance Tools! (Youtube video)</p></li><li><p>How does Garbage Collection work? </p></li><li><p>A Cheat Sheet for Designing Fault-Tolerant Systems</p></li><li><p>10 System Design Tradeoffs You Cannot Ignore </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/WorkOS_081724">WorkOS: Modern Identity Platform for B2B SaaS (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/WorkOS_081724" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="837" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2caec8f-a0c8-45ac-8339-477ffb093e8b_2560x1472.png" width="1456" /><div></div></div></a></figure></div><p>Start selling to enterprises with just a few lines of code.</p><p>&#8594; WorkOS provides a complete user management solution along with SSO, SCIM,&nbsp;Audit Logs, &amp; Fine-Grained Authorization.</p><p>&#8594; Unlike other auth providers that rely on user-centric models, WorkOS is designed for B2B SaaS with an org modeling approach.</p><p>&#8594; The APIs are flexible, easy-to-use, and modular. Pick and choose what you need and integrate in minutes.</p><p>&#8594; User management is free up to 1 million MAUs&nbsp;and comes standard with RBAC, bot protection, MFA, &amp; more.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/WorkOS_081724"><span>Get Started Today</span></a></p><div><hr /></div><h2>Linux Performance Tools!</h2><div class="youtube-wrap" id="youtube2-iJ_eIsA5E1U"><div class="youtube-inner"></div></div><div><hr /></div><h2>How does Garbage Collection work? </h2><p>Garbage collection is an automatic memory management feature used in programming languages to reclaim memory no longer used by the program. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F303b80e4-5cd9-41e6-87e3-d76510cd60bd_1536x1536.gif" title="No alternative text description for this image" /><div></div></div></a></figure></div><ul><li><p>Java <br />Java provides several garbage collectors, each suited for different use cases: <br /> <br />1. Serial Garbage Collector: Best for single-threaded environments or small applications. <br /> <br />2. Parallel Garbage Collector: Also known as the "Throughput Collector." <br /> <br />3. CMS (Concurrent Mark-Sweep) Garbage Collector: Low-latency collector aiming to minimize pause times. <br /> <br />4. G1 (Garbage-First) Garbage Collector: Aims to balance throughput and latency. <br /> <br />5. Z Garbage Collector (ZGC): A low-latency garbage collector designed for applications that require large heap sizes and minimal pause times. <br /></p></li><li><p>Python <br />Python's garbage collection is based on reference counting and a cyclic garbage collector: <br /> <br />1. Reference Counting: Each object has a reference count; when it reaches zero, the memory is freed. <br /> <br />2. Cyclic Garbage Collector: Handles circular references that can't be resolved by reference counting. <br /></p></li><li><p>GoLang <br />Concurrent Mark-and-Sweep Garbage Collector: Go's garbage collector operates concurrently with the application, minimizing stop-the-world pauses.</p></li></ul><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1760" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b1af0d4-fcea-4dc4-9296-dbd752b536b3_2250x2720.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-architectural-scalability">A Crash Course on Architectural Scalability</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservices-design">A Crash Course on Microservices Design Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-domain-driven-design">A Crash Course on Domain-Driven Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/tidying-code">"Tidying" Code</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-relational-database">A Crash Course on Relational Database Design</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>A Cheat Sheet for Designing Fault-Tolerant Systems</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram" class="sizing-normal" height="1657" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1568f10-1350-4435-a06f-60d31d4ccbbf_1280x1657.gif" title="diagram" width="1280" /><div></div></div></a></figure></div><p>Designing fault-tolerant systems is crucial for ensuring high availability and reliability in various applications. Here are six top principles of designing fault-tolerant systems: </p><ol><li><p>Replication <br />Replication involves creating multiple copies of data or services across different nodes or locations. </p></li><li><p>Redundancy <br />Redundancy refers to having additional components or systems that can take over in case of a failure. </p></li><li><p>Load Balancing <br />Load balancing distributes incoming network traffic across multiple servers to ensure no single server becomes a point of failure. </p></li><li><p>Failover Mechanisms <br />Failover mechanisms automatically switch to a standby system or component when the primary one fails. </p></li><li><p>Graceful Degradation <br />Graceful degradation ensures that a system continues to operate at reduced functionality rather than completely failing when some components fail.</p></li><li><p>Monitoring and Alerting <br />Continuously monitor the system's health and performance, and set up alerts for any anomalies or failures.</p></li></ol><div><hr /></div><h2>10 System Design Tradeoffs You Cannot Ignore </h2><p>If you don&#8217;t know trade-offs, you DON'T KNOW system design. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="998" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c3e3af8-5a12-4036-8a93-ef17c5461d8b_800x998.gif" title="graphical user interface" width="800" /><div></div></div></a></figure></div><ol><li><p>Vertical vs Horizontal Scaling <br />Vertical scaling is adding more resources (CPU, RAM) to an existing server.</p><p>Horizontal scaling means adding more servers to the pool. <br /></p></li><li><p>SQL vs NoSQL <br />SQL databases organize data into tables of rows and columns. </p><p>NoSQL is ideal for applications that need a flexible schema. <br /></p></li><li><p>Batch vs Stream Processing <br />Batch processing involves collecting data and processing it all at once. For example, daily billing processes. </p><p>Stream processing processes data in real time. For example, fraud detection processes. <br /></p></li><li><p>Normalization vs Denormalization <br />Normalization splits data into related tables to ensure that each piece of information is stored only once. </p><p>Denormalization combines data into fewer tables for better query performance. <br /></p></li><li><p>Consistency vs Availability <br />Consistency is the assurance of getting the most recent data every single time. </p><p>Availability is about ensuring that the system is always up and running, even if some parts are having problems. <br /></p></li><li><p>Strong vs Eventual Consistency <br />Strong consistency is when data updates are immediately reflected. </p><p>Eventual consistency is when data updates are delayed before being available across nodes. <br /></p></li><li><p>REST vs GraphQL <br />With REST endpoints, you gather data by accessing multiple endpoints. </p><p>With GraphQL, you get more efficient data fetching with specific queries but the design cost is higher. <br /></p></li><li><p>Stateful vs Stateless <br />A stateful system remembers past interactions. </p><p>A stateless system does not keep track of past interactions. <br /></p></li><li><p>Read-Through vs Write-Through Cache <br />A read-through cache loads data from the database in case of a cache miss. </p><p>A write-through cache simultaneously writes data updates to the cache and storage. <br /></p></li><li><p>Sync vs Async Processing <br />In synchronous processing, tasks are performed one after another. </p><p>In asynchronous processing, tasks can run in the background. New tasks can be started without waiting for a new task. <br /></p></li></ol><p>Over to you: Which other tradeoffs have you encountered?</p><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 17 Aug 2024 15:30:37 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Architectural Scalability</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-architectural-scalability</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-architectural-scalability</guid>
<content:encoded><![CDATA[
<div> 关键词: 软件应用，全球范围，可伸缩性，应对突发流量，建设架构
总结:<br /><br />
软件应用在今天的互连世界中拥有全球用户，需要具备可伸缩的架构以适应突发流量的挑战。可伸缩性能够动态调整应用程序以适应不断变化的工作负载需求，确保性能和可用性不受影响。设计和建设具有可伸缩性的架构是开发团队的重要任务，以应对突发用户需求带来的压力，同时确保良好的用户体验。这篇文章深入探讨了可伸缩性的真正含义和各种技术原则，为后续深入研究应用架构各层和组件的可伸缩性做好准备。 <div>
<p>In today's interconnected world, software applications have a global reach, serving users from diverse geographical locations.&nbsp;</p><p>With the rapid growth of social media and viral content, a single tweet or post can lead to a sudden and massive surge in traffic to an application. The importance of building applications with scalable architecture from the ground up has never been higher.&nbsp;</p><p>Being prepared for unexpected traffic spikes is indispensable for development teams building applications. A sudden increase in user demand may be just around the corner. Not being prepared for it can put immense pressure on the application's infrastructure. It not only causes performance degradation but can also, in some cases, result in a complete system failure.&nbsp;</p><p>To mitigate these risks and ensure a good user experience, teams must proactively design and build scalable architectures.</p><p>But what makes scalability such a desirable characteristic?</p><p>Scalability allows the application to dynamically adapt to changing workload requirements without compromising performance or availability.</p><p>In this post, we&#8217;ll understand the true meaning of scalability from different perspectives followed by the various techniques and principles that can help you scale the application&#8217;s architecture. Also, in subsequent posts in the coming weeks, we&#8217;ll take deeper dives into the scalability of each layer and component of a typical architecture.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b1af0d4-fcea-4dc4-9296-dbd752b536b3_2250x2720.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1760" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b1af0d4-fcea-4dc4-9296-dbd752b536b3_2250x2720.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>What is Scalability?</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-architectural-scalability">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 15 Aug 2024 15:30:52 GMT</pubDate>
</item>
<item>
<title>Counting Billions of Content Usage at Canva</title>
<link>https://blog.bytebytego.com/p/counting-billions-of-content-usage</link>
<guid>https://blog.bytebytego.com/p/counting-billions-of-content-usage</guid>
<content:encoded><![CDATA[
<div> MySQL, DynamoDB, Snowflake, OLAP, ELT
总结:<br /><br />本文介绍了Canva工程团队在实现计数服务的过程中的各种架构设计和经验教训。最初他们使用MySQL数据库设计计数服务，但由于数据量增长和性能限制，转向DynamoDB并最终使用Snowflake的OLAP解决方案。新架构消除了以往的技术瓶颈，提高了数据处理的速度和精度。尽管面临一些挑战，如数据处理复杂性、数据卸载和基础架构复杂性，但Canva成功地实现了计数服务的高效和可靠运行。整个过程体现出简单性、迭代改进、综合可观察性、鲁棒的基础架构等重要设计原则和关键点。 <div>
<h2><a href="https://bit.ly/ScyllaDB_081324">Hands-on Rust Developer Workshop: Build a Low-Latency Social Media App (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/ScyllaDB_081324" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b3316b7-c138-42fd-9e2b-59ea511f109d_1600x840.png" width="1456" /><div></div></div></a></figure></div><p>During this free interactive workshop oriented for developers, engineers, and architects, you will learn how to:&nbsp;</p><ul><li><p>Create and compile a sample social media app with Rust</p></li><li><p>Connect the application to ScyllaDB (NoSQL data store) and Redpanda (streaming data)</p></li><li><p>Negotiate tradeoffs related to data modeling and querying</p></li><li><p>Manage and monitor the database for consistently low latencies</p></li></ul><p>If you&#8217;re an application developer with an interest in Rust, Tokio, and event-driven architectures this workshop is for you! This is a great way to discover the NoSQL strategies used by top teams and apply them in a guided, supportive environment.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/ScyllaDB_081324"><span>Register for Free</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the Canva Engineering Blog. All credit for the technical details goes to the Canva engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>What if incorrect counting results in incorrect payments?&nbsp;</p><p>Either the company making the payment or the users receiving that payment based on the incorrect count lose money. Both scenarios are problematic from the business perspective.</p><p>This is exactly the situation that Canva faced when they launched the Creators Program.</p><p>As you might already know, Canva is a tool that makes design accessible to everyone worldwide. One of the main ways they make this possible is through the Canva Creators Program.</p><p>In the three years since its launch, the use of content from this program has doubled every 18 months. They process billions of content uses monthly based on which the creators are paid. This includes the use of templates, images, videos, and more.</p><p>It is a critical requirement for Canva to count the usage data of this content accurately since the payments made to the creators depend on this data. However, it also presents some big challenges:</p><ul><li><p><strong>Accuracy:</strong> The count has to be correct to maintain creator trust and ensure fair pay.</p></li><li><p><strong>Scalability:</strong> The system needs to handle rapidly growing amounts of data</p></li><li><p><strong>Operability:</strong> As data increases, so does the complexity of maintenance and problem-solving.</p></li></ul><p>In this post, we will look at the various architectures Canva&#8217;s engineering team experimented with to implement a robust counting service and the lessons they learned in the process.</p><div><hr /></div><h2>The Initial Counting Service Design</h2><p>Canva's original design for the content usage counting service was built on a MySQL database, a familiar and widely-used technology stack.</p><p>This initial design comprised several key components:&nbsp;</p><ul><li><p>A MySQL database for storing all usage data (including raw events, deduplicated usage, and aggregated counts).</p></li><li><p>Separate worker services handling different pipeline stages (data collection, deduplication, and aggregation).</p></li><li><p>The persistence of multiple layers of reusable intermediary output at various stages.&nbsp;</p></li></ul><p>The process flow for the solution could be broken down into three main steps:</p><ul><li><p><strong>Data Collection: </strong>Usage events from various sources (web, and mobile apps) were collected and stored in MySQL.</p></li><li><p><strong>Deduplication: </strong>A worker service identifies duplicated usage events and matches them with a specific set of rules.&nbsp;</p></li><li><p><strong>Aggregation: </strong>Another worker scanned the updated deduplication table, incrementing counters in a separate table.</p></li></ul><p>The diagram below shows the architecture on a high level.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F506f7f4d-63de-40ee-9e46-b72fe51b001f_1474x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1580" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F506f7f4d-63de-40ee-9e46-b72fe51b001f_1474x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>This architecture employed a single-threaded sequential process for deduplication, using a pointer to track the latest scanned record. While this approach made it easier to reason about and verify data processing, especially during troubleshooting or incident recovery, it faced significant scalability challenges.&nbsp;</p><p>The system required at least one database round trip per usage record, resulting in O(N) database queries for N records, which became increasingly problematic as data volume grew.&nbsp;</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe308b14f-4b1f-4d97-ba32-ea57116388ab_1600x973.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="885" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe308b14f-4b1f-4d97-ba32-ea57116388ab_1600x973.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The initial MySQL-based architecture prioritized simplicity and familiarity over scalability. It allowed for quick implementation but created substantial challenges as the system expanded.&nbsp;</p><ul><li><p>The heavy reliance on MySQL for both storage and processing created a bottleneck, failing to leverage the strengths of distributed systems or parallel processing.&nbsp;</p></li><li><p>MySQL RDS does not horizontally scale through partitioning by itself. Every time they needed more storage, they doubled the RDS instance size. This happened every 8-10 months, resulting in significant operational overhead.&nbsp;</p></li><li><p>Once the MySQL RDS instance reached several TBs, maintaining it became costly.</p></li><li><p>Lastly, finding and fixing issues in case of incidents was difficult because engineers had to look into databases and manually fix the incorrect data.</p></li></ul><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1820" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee02bbe9-ee3c-4028-a05e-36e5274202a3_2250x2812.png" title="" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservices-design">A Crash Course on Microservices Design Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-domain-driven-design">A Crash Course on Domain-Driven Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/tidying-code">"Tidying" Code</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-relational-database">A Crash Course on Relational Database Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-distributed-systems">A Crash Course on Distributed Systems</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>Migration to DynamoDB</h2><p>Faced with the scalability limitations of the MySQL-based counting service, the team initially looked to DynamoDB as a potential solution.</p><p>This decision was primarily driven by DynamoDB's reputation for handling large-scale, high-throughput workloads - a perfect fit for Canva's rapidly growing data needs.&nbsp;</p><p>The migration process began with moving raw usage events from the data collection stage to DynamoDB, which provided immediate relief to the storage constraints. This initial success prompted the team to consider moving the entirety of their data to DynamoDB. It was a move that would have necessitated a substantial rewrite of their codebase.</p><p>However, after careful evaluation, Canva decided against a full migration to DynamoDB.&nbsp;</p><p>While DynamoDB could have effectively addressed the storage scalability issues, it wouldn't have solved the fundamental problem of processing scalability. The team found it challenging to eliminate the need for frequent database round trips, which was a key bottleneck in their existing system.&nbsp;</p><p>This reveals a crucial lesson in system design: sometimes, what appears to be a storage problem is a processing problem in disguise.</p><p>Canva's approach clearly shows the importance of thoroughly analyzing the root causes of system limitations before committing to major architectural changes. It also highlights the complexity of scaling data-intensive applications, where the interplay between storage and processing capabilities can be subtle and non-obvious.</p><h2>The OLAP-based Counting Service</h2><p>Canva's latest architecture for the content usage counting service shows a shift from traditional OLTP databases to an OLAP-based solution, specifically using Snowflake.&nbsp;</p><p>The change came after realizing that previous attempts with MySQL and DynamoDB couldn't adequately address their scalability and processing needs. The new architecture altered how Canva processed and stored the usage data, adopting an ELT (Extract, Load, Transform) approach.</p><p>The diagram below shows the new architecture:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1de81af8-487c-4113-b105-bd3130824ff2_1474x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1580" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1de81af8-487c-4113-b105-bd3130824ff2_1474x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>In the extraction phase, Canva pulled raw usage data from various sources, including web browsers and mobile apps. This data was then loaded into Snowflake using a reliable data replication pipeline provided by Canva&#8217;s data platform team. The reliability of this data replication was crucial, as it formed the foundation for all subsequent processing.</p><p>The transformation phase used Snowflake's powerful computational capabilities. It also utilized DBT (Data Build Tool) to define complex transformations.&nbsp;</p><p>These transformations were written as SQL-like queries, allowing for end-to-end calculations directly on the source data. For example, one transformation aggregated usages per brand using a SQL query that selected data from a previous step named 'daily_template_usages' and grouped it by &#8216;day_id&#8217; and &#8216;template_brand&#8217;.</p><p>The SQL below shows the aggregate query.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b40bcc3-48b4-4af7-a0c5-96d476135214_1600x582.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="530" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b40bcc3-48b4-4af7-a0c5-96d476135214_1600x582.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://www.canva.dev/blog/engineering/scaling-to-count-billions">Canva Engineering Blog</a></figcaption></figure></div><p>The main steps in the transformation process were as follows:</p><ul><li><p>Extraction of source event data in JSON format from DynamoDB, which was not optimal for data warehouse query processing. Therefore, some JSON properties were projected into separate table columns within Snowflake for optimization.</p></li><li><p>Deduplicate and aggregate the usage events.</p></li></ul><p>A key aspect of this new architecture was the elimination of intermediary outputs. Instead of persisting data at various pipeline stages, Canva materialized intermediate transformation outputs as SQL Views.&nbsp;</p><h3>The Advantage of OLAP Database</h3><p>The separation of storage and computing in an OLAP database like Snowflake was a game-changer for Canva. It enabled them to scale computational resources independently.&nbsp;</p><p>As a result, they could now aggregate billions of usage records within minutes, a task that previously took over a day. This improvement was largely due to most of the computation being done in memory, which is several orders of magnitude faster than the database round trips required in their previous architecture.</p><p>There were several improvements such as:</p><ul><li><p>The pipeline latency was reduced from over a day to under an hour.</p></li><li><p>Incident handling became more manageable. Most issues could be resolved by simply re-running the pipeline end-to-end, without manual database interventions.</p></li><li><p>Reduction in over 50% of the stored data and elimination of thousands of lines of deduplication and aggregation calculation code. The logic rewritten in SQL was simpler compared to the previous code.</p></li><li><p>The number of incidents dropped to once every few months or fewer.</p></li></ul><h3>Challenges of the New Solution</h3><p>Despite the advantages, the solution also introduced new challenges such as:</p><ul><li><p><strong>Transformation Complexity:</strong> The data transformation jobs, written in an SQL-like language and deployed as a standalone service, introduced new deployment and compatibility considerations.</p></li><li><p><strong>Data Unloading:</strong> Canva needed to build a reliable pipeline to unload data from Snowflake into databases used by other services. This involved using S3 as intermediary storage and integrating with SQS for durability. Optimizing the ingestion query and carefully tuning rate limits was crucial to prevent service database throttling.</p></li><li><p><strong>Infrastructure Complexity:</strong> The new architecture increased infrastructure complexity due to data replication integration and standalone services running transformation jobs. This required additional effort to maintain observability across different toolsets.</p></li></ul><h2>Conclusion</h2><p>Canva&#8217;s journey of implementing the counting service for the Creators Program is full of learning for software developers and architects.&nbsp;</p><p>Some of the key points to take away are as follows:</p><ul><li><p>Simplicity is crucial for designing reliable services. In other words, reducing code and data complexity is the key. For example, minimizing intermediary output in various counting pipeline stages using OLAP and ELT helped simplify the system.</p></li><li><p>It&#8217;s good to start small and be pragmatic. The initial MySQL-based design served its purpose for the first two years and allowed timely delivery of the functionality to the users. Once scalability became a problem, they looked at alternative solutions.</p></li><li><p>Comprehensive observability, while requiring extra overhead, helps identify and resolve problems. Close monitoring from day one of every part of the pipeline was crucial to make the right decisions.</p></li><li><p>Recognizing when solutions are no longer adequate and being open to change is important. In Canva&#8217;s case, this was demonstrated by the willingness to pivot from familiar MySQL to a more suitable solution using OLAP database.</p></li><li><p>Big changes happen in increments. For example, the journey from MySQL to DynamoDB and finally to an OLAP solution showed the power of iterative improvements.</p></li><li><p>Reliable infrastructure is important to build a stable platform.</p></li></ul><p><strong>References:</strong></p><ul><li><p><a href="https://www.canva.dev/blog/engineering/scaling-to-count-billions">Scaling to Count Billions</a></p></li><li><p><a href="https://aws.amazon.com/dynamodb/">AWS DynamoDB</a></p></li><li><p><a href="https://docs.snowflake.com/">Snowflake Documentation</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong><br /></p>
]]></content:encoded>
<pubDate>Tue, 13 Aug 2024 15:30:12 GMT</pubDate>
</item>
<item>
<title>EP124: How does SSH work?</title>
<link>https://blog.bytebytego.com/p/ep124-how-does-ssh-work</link>
<guid>https://blog.bytebytego.com/p/ep124-how-does-ssh-work</guid>
<content:encoded><![CDATA[
<div> SSH, 学校, Large Language Models, 系统设计, Nginx

系统设计中介绍了SSH的工作原理，以及8个常见问题及解决方案。学校应该添加一些新的必修课程，如LLM和系统设计。Nginx之所以受欢迎是因为其高性能和多功能性。想要推广产品可以通过广告位在技术专业人士面前展示。总结: 系统设计中介绍了SSH的工作原理和解决方案，学校应该增加新的课程，Nginx因为高性能和多功能性而受欢迎，想要推广产品可以通过广告位。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>How does SSH work?</p></li><li><p>List of Subjects That Should be Mandatory in Schools</p></li><li><p>8 Common System Design Problems and Solutions</p></li><li><p>Why is Nginx so popular? </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/NewRelic_081024">New Relic launched the industry's&nbsp;first fully-integrated, AI-driven Digital Experience Monitoring (DEM) solution (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/NewRelic_081024" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="819" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cac714b-6a9b-46ec-8311-f8659d0041cb_1920x1080.png" width="1456" /><div></div></div></a></figure></div><p>New Relic DEM makes it easy for businesses to quickly identify and resolve user friction points across mobile, web, and AI-powered applications.</p><p>New Relic&#8217;s advanced DEM capabilities include:</p><ul><li><p>Actionable user journeys without event duplication</p></li><li><p>The most comprehensive list of mobile platforms</p></li><li><p>Superior insights without the high costs</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/NewRelic_081024"><span>Get started</span></a></p><div><hr /></div><h2>How does SSH work?</h2><p>SSH (Secure Shell) is a network protocol used to securely connect to remote machines over an unsecured network. It encrypts the connection and provides various mechanisms for authentication and data transfer. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, text, application" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed998b2e-fbc8-4c3c-b339-eca5abd85ce3_1289x1536.gif" title="graphical user interface, text, application" width="1289" /><div></div></div></a></figure></div><p>SSH has two versions: SSH-1 and SSH-2. SSH-2 was standardized by the IETF. <br />It has three main layers: Transport Layer, Authentication Layer, and Connection Layer. </p><ol><li><p>Transport Layer <br />The Transport Layer provides encryption, integrity, and data protection to ensure secure communication between the client and server. <br /></p></li><li><p>Authentication Layer <br />The Authentication Layer verifies the identity of the client to ensure that only authorized users can access the server. <br /></p></li><li><p>Connection Layer <br />The Connection Layer multiplexes the encrypted and authenticated communication into multiple logical channels.</p></li></ol><div><hr /></div><h2>An interesting list of subjects that should be mandatory in schools</h2><p>While academics are essential, it's crucial to acknowledge that many elements in this diagram would have been beneficial to learn earlier. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="990" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98ced99c-1e91-4926-ae09-d81779bd454c_800x990.jpeg" title="No alt text provided for this image" width="800" /><div></div></div></a></figure></div><p>Over to you: What else should be on the list? What are the top 3 skills you wish schools would teach? <br /> <br />Credit: Instagram accounts on startup_rules</p><div><hr /></div><h2><a href="https://bit.ly/TheAIEdge_081024">The Train, Fine-Tune, and Deploy Large Language Models Bootcamp (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/TheAIEdge_081024" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d55f42a-137d-40e2-9d51-4a8966975ea7_1600x840.png" title="" width="1456" /><div></div></div></a></figure></div><p>In the past few years, we saw a revolution with the advent of Large Language Models. Rarely has a discovery changed the world of Machine Learning that much, and the hype around LLM is real! That is something that very few experts predicted, and preparing for the future is essential. This boot camp is meant to train the new generation of engineers who will continue leading this revolution. You will learn:</p><ul><li><p>The Transformer Architecture</p></li><li><p>Training LLMs to Follow Instruction</p></li><li><p>How to Scale Model Training</p></li><li><p>How to Fine-Tune LLMs</p></li><li><p>How to Deploy LLMs</p></li><li><p>How to Build the Application Layer</p></li></ul><p class="button-wrapper"><a class="button primary button-wrapper" href="https://bit.ly/TheAIEdge_081024"><span>Enroll 20% Discount</span></a></p><div><hr /></div><h2>8 Common System Design Problems and Solutions</h2><p>Do you know those 8 common problems in large-scale production systems and their solutions? Time to test your skills! </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ccb1c38-5e69-4284-a34c-d7be7b458f8f_1280x1600.gif" title="graphical user interface" width="1280" /><div></div></div></a></figure></div><ol><li><p>Read-Heavy System <br />Use caching to make the reads faster. </p></li><li><p>High-Write Traffic <br />Use async workers to process the writes <br />Use databases powered by LSM-Trees </p></li><li><p>Single Point of Failure <br />Implement redundancy and failover mechanisms for critical components like databases. </p></li><li><p>High Availability <br />Use load balancing to ensure that requests go to healthy server instances. <br />Use database replication to improve durability and availability. </p></li><li><p>High Latency <br />Use a content delivery network to reduce latency </p></li><li><p>Handling Large Files <br />Use block storage and object storage to handle large files and complex data. </p></li><li><p>Monitoring and Alerting <br />Use a centralized logging system using something like the ELK stack. </p></li><li><p>Slower Database Queries <br />Use proper indexes to optimize queries. <br />Use sharding to scale the database horizontally. </p></li></ol><p>Over to you: What other common problems and solutions have you seen?</p><div><hr /></div><h2>Why is Nginx so popular? </h2><p>Nginx is a high-performance web server and reverse proxy. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa0a6754-63e9-4377-b7fc-c29018e4fcf5_1311x1536.gif" title="No alternative text description for this image" width="1311" /><div></div></div></a></figure></div><p>It follows a master-worker process model that contributes to its stability, scalability, and efficient resource utilization. <br /> <br />The master process is responsible for reading the configuration and managing worker processes. Worker processes handle incoming connections using an event-driven non-blocking I/O model. <br /> <br />Due to its architecture, Nginx excels in supporting multiple features such as:</p><ol><li><p>High-Performance Web Server </p></li><li><p>Reverse Proxy and Load Balancing </p></li><li><p>Content Cache </p></li><li><p>SSL Termination </p></li></ol><p>Over to you: Do you know any other features supported by Nginx?</p><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 10 Aug 2024 15:30:59 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Microservices Design Patterns</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-microservices-design</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-microservices-design</guid>
<content:encoded><![CDATA[
<div> 灵活性 测试性 可扩展性 数据一致性 安全性 

设计模式为微服务架构中常见问题提供了有效解决方案。适用适当的设计模式可以有效解决数据一致性和最终一致性、安全性、可扩展性和数据库性能等挑战。常见的设计模式包括API网关模式、数据库每个服务模式和CQRS模式等。微服务架构通过拆分单块应用为独立部署的服务，提高了系统的灵活性、测试性和可扩展性。然而，微服务架构的实施也伴随着挑战，如数据一致性和最终一致性、安全性、可扩展性和数据库性能。通过合适的设计模式，可以有效应对这些问题。 

<br /><br />总结:设计模式为微服务架构中的关键问题提供了有效解决方案，包括数据一致性、安全性、可扩展性和数据库性能。微服务架构通过拆分单块应用为独立部署的服务，提高了系统的灵活性、测试性和可扩展性。 <div>
<p>Microservices architecture has gained popularity for its ability to improve the flexibility, testability, and scalability of software systems.&nbsp;</p><p>By breaking down a monolithic application into smaller, independently deployable services, microservices enable teams to develop, deploy, and scale each service independently.</p><p>However, the implementation of microservices architecture comes with its own set of challenges such as:</p><ul><li><p><strong>Data Consistency and Eventual Consistency:</strong> In a microservices architecture, data is often distributed across multiple nodes, which can be located in different data centers or even different geographic regions. At any given point in time, there can be discrepancies in the state of data between various nodes. This phenomenon is known as eventual consistency.</p></li><li><p><strong>Security: </strong>Microservices architecture introduces a larger attack surface for malicious actors compared to monolithic systems. It&#8217;s crucial to establish appropriate security mechanisms while building microservices. Design patterns such as the API Gateway pattern can help.</p></li><li><p><strong>Scalability and Database Performance: </strong>Microservices are known for their scalability. However, while it is relatively easy to scale the application layer by adding more instances, databases can become performance bottlenecks if not designed for scalability. Patterns such as Database per Service and CQRS help solve this challenge.</p></li></ul><p>Design patterns provide proven solutions to common problems encountered in a microservices architecture. By applying an appropriate design pattern, these problems can be effectively addressed.</p><p>In this post, we&#8217;ll explore the most popular microservices design patterns along with their benefits and adoption challenges.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1820" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee02bbe9-ee3c-4028-a05e-36e5274202a3_2250x2812.png" width="1456" /><div></div></div></a></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-microservices-design">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 08 Aug 2024 15:30:37 GMT</pubDate>
</item>
<item>
<title>How Facebook Syncs Time Across Millions of Servers</title>
<link>https://blog.bytebytego.com/p/how-facebook-syncs-time-across-millions</link>
<guid>https://blog.bytebytego.com/p/how-facebook-syncs-time-across-millions</guid>
<content:encoded><![CDATA[
<div> Facebook, 时间同步, NTP, PTP, 精确度
<br />
Facebook 在其基础设施中采用 Precision Time Protocol (PTP) 来确保精准可靠的时间同步，通过重新设计和重建各种组件，将PTP的可能性推向极限。PTP架构由PTP机架、PTP网络和PTP客户端组成，通过GNSS天线、时间设备、振荡器、网络接口卡等关键组件提供时钟服务。PTP网络利用透明时钟、避免边界钟等技术分发时钟。PTP客户端软件负责在需要准确时间的服务器上运行，使用ptp4l、fbclock、内核时间戳等组件。Facebook的PTP解决方案为我们展示了PTP的应用和优势。总结: <br />Facebook采用PTP确保时间同步，通过重新设计和重建各种组件，将PTP的可能性推向极限。PTP架构由PTP机架、PTP网络和PTP客户端组成，提供时钟服务，利用透明时钟、避免边界钟等技术分发时钟，采用ptp4l、fbclock、内核时间戳等组件运行在客户端。 <div>
<h2><a href="https://bit.ly/FusionAuth_080624">FusionAuth: Auth. Built for devs, by devs. (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/FusionAuth_080624" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="373.8" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ba85cc8-6394-4f42-be7b-d4192540c533_640x336.png" width="712" /><div></div></div></a></figure></div><ul><li><p>Hosting Flexibility: You host or we host - the choice is yours with no loss of features</p></li><li><p>Scale Confidently: Lightning-fast performance for 10 users or 10 million users (or more)</p></li><li><p>Developer-Centric: True API first design, quick integration, built on standards, highly flexible &amp; customizable</p></li><li><p>Total Control: Deploy on any computer, anywhere in the world and integrate easily with any tech stack</p></li><li><p>Data Isolation: Single tenant by design means your data is physically isolated from everyone else's</p></li><li><p>Unlimited: Unlimited IDPs, unlimited users, unlimited tenants, unlimited applications, always free</p></li></ul><p>FusionAuth is a complete auth &amp; user platform that has&nbsp;10M+ downloads and is trusted by industry leaders!</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/FusionAuth_080624"><span>Start for Free</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the Facebook/Meta Engineering Blog. All credit for the technical details goes to the Facebook engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>A clock showing the wrong time is worse than a faulty clock.</p><p>This is the challenge Facebook had to deal with while operating millions of servers connected to the Internet and each other.&nbsp;</p><p>All of these devices have onboard clocks that are expected to be accurate. However, many onboard clocks contain inaccurate internal oscillators, which cause seconds of inaccuracy per day and need to be periodically corrected.&nbsp;</p><p>Think of these internal oscillators as the &#8220;heartbeat&#8221; of the clock. Just like how an irregular heartbeat can affect a person&#8217;s health, an inaccurate oscillator can cause the clock to gain or lose time.</p><p>Incorrect time can lead to issues with varying degrees of impact. It could be missing a simple reminder or failing a spacecraft launch.</p><p>As Facebook&#8217;s infrastructure has grown, time precision has become extremely important. For example, knowing the accurate time difference between two random servers in a data center is critical to preserving the order of transactions across these servers.</p><p>In this post, we&#8217;ll learn how Facebook achieved time precision across its millions of servers with NTP and later with PTP.</p><div><hr /></div><h2>Network Time Protocol</h2><p>Facebook started with Network Time Protocol (NTP) to keep the devices in sync.</p><p>NTP is a way for computers to synchronize their clocks over a network. It helps ensure that all devices on the network have the same, accurate time.</p><p>Having synchronized clocks is critical for many tasks, such as:</p><ul><li><p>Scheduling events and meetings</p></li><li><p>Logging and tracking activities</p></li><li><p>Ensuring a proper sequence of transactions</p></li><li><p>Coordinating actions between different systems</p></li></ul><p>NTP uses a hierarchical system of time servers where the most accurate servers are at the top. There is a 3-step process to how NTP works:</p><ul><li><p>Computers on the network periodically request the current time from these servers.&nbsp;</p></li><li><p>The servers respond with their current time taking into account the network delay.&nbsp;</p></li><li><p>The requesting computer adjusts its clock based on the information received from the server.</p></li></ul><p>The diagram below shows the hierarchical system of servers used by NTP.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f38b4e-9b69-4bbb-a38c-836fc06b7245_1562x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1491" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f38b4e-9b69-4bbb-a38c-836fc06b7245_1562x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Facebook built an NTP service at scale. They used <strong>chrony</strong>, a modern NTP server implementation. While they used the <strong>ntpd </strong>service initially, testing revealed that <strong>chrony</strong> was far more accurate and scalable.</p><p>Chrony was a fairly new daemon at the time, but it offered a chance to bring the precision down to nanoseconds. Also, from a resource consumption perspective, chrony consumed less RAM compared to ntpd.</p><p>See the diagram below that shows the difference of around 1MB when it came to RAM consumption between chrony and ntpd.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F210d00e2-c251-45df-8f90-684d6a831d22_1600x900.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="819" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F210d00e2-c251-45df-8f90-684d6a831d22_1600x900.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/">Facebook Engineering Blog</a></figcaption></figure></div><p>They designed the NTP service in four layers based on the hierarchical structure of NTP.</p><ul><li><p>Stratum 0 is a layer of satellites with precise atomic clocks from a global navigation satellite system (GNSS), such as GPS, GLONASS, or Galileo.</p></li><li><p>Stratum 1 is a Facebook atomic clock synchronizing with a GNSS.</p></li><li><p>Stratum 2 is a pool of NTP servers synchronizing to the Stratum 1 devices.&nbsp;</p></li><li><p>Lastly, Stratum 3 is a tier of servers configured for a larger scale.</p></li></ul><p>The diagram below shows the layers of Facebook&#8217;s NTP service.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F666832a9-c43e-4cda-a44c-fef945fbb401_1536x864.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="819" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F666832a9-c43e-4cda-a44c-fef945fbb401_1536x864.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/">Facebook Engineering Blog</a></figcaption></figure></div><p>There are a couple of interesting concepts to note over here:&nbsp;</p><ul><li><p>Leap second</p></li><li><p>Smearing</p></li></ul><p>The Earth&#8217;s rotation is not consistent and can vary slightly over time. Therefore, clocks are kept in sync with the Earth&#8217;s rotation by occasionally adding or removing a second. This is called a leap second.</p><p>While adding or removing a leap second is hardly noticeable to humans, it can cause server issues. Servers expect time to move forward continuously, and a sudden change of a second can cause them to miss important tasks.&nbsp;</p><p>To mitigate the impact of leap seconds on servers, a technique called &#8220;smearing&#8221; is used.&nbsp;</p><p>Instead of adding or removing a full second at once, the time is gradually adjusted in small increments over several hours. It&#8217;s similar to masking a train&#8217;s delay by spreading the adjustment across multiple stations.</p><p>In the case of Facebook&#8217;s NTP service, the Leap-second smearing happens at Stratum 2. The Stratum 3 servers receive smeared time and are ignorant of leap seconds.</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1838" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F813befc8-72c0-45fc-9568-b9bc5ca9a69d_2250x2840.png" title="" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-domain-driven-design">A Crash Course on Domain-Driven Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/tidying-code">"Tidying" Code</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-relational-database">A Crash Course on Relational Database Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-distributed-systems">A Crash Course on Distributed Systems</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-scaling">A Crash Course in Database Scaling Strategies</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>The Arrival of Precision Time Protocol</h2><p>NTP adoption was quite successful for Facebook. It helped them improve accuracy from 10 milliseconds to 100 microseconds.&nbsp;</p><p>However, as Facebook wanted to scale to more advanced systems and build the metaverse, they wanted even greater levels of accuracy.&nbsp;</p><p>Therefore, in late 2022, Facebook moved from NTP to Precision Time Protocol (PTP).</p><p>There were some problems with NTP, which are as follows:</p><ul><li><p><strong>NTP and Asynchronous Systems:</strong> Systems using NTP are asynchronous, meaning they work independently without a shared global clock. These systems periodically check in with each other to ensure synchronization. However, as the system grows larger, more check-ins are required, which can slow down the network.</p></li><li><p><strong>NTP and Timekeeping Methods: </strong>NTP is susceptible to variance and latency due to its timekeeping methods of using physical clocks. In other words, NTP is like a microwave clock that keeps time on-device. If there&#8217;s a time change (e.g., daylight savings), the clock needs to be manually adjusted.</p></li></ul><p>In contrast, PTP works like a smartphone clock that updates its time automatically. When there&#8217;s a time change or the phone moves to a new time zone, the clock updates itself by referencing the time over a network.</p><p>While NTP provided millisecond-level synchronization, PTP networks could hope to achieve nanosecond-level precision.</p><h3>What makes PTP More Effective?</h3><p>As discussed earlier, a special computer called a Stratum acts as a time reference for other computers on a network. When a computer needs to know the current time, it sends a request to the Stratum, which replies with the current time. This process is known as sync messaging.</p><p>When the Stratum sends the current time to another computer, the information travels across the network, resulting in some latency. Several factors can increase this latency, such as:</p><ul><li><p>The speed at which signals travel through the fiber optic cables.</p></li><li><p>The time it takes for the network devices to convert signals.</p></li><li><p>The quality of network equipment, like routers and switches.</p></li><li><p>The time it takes for software and drivers to process the time information.</p></li></ul><p>Due to latency, the time received by the other computer is no longer accurate when it arrives at the receiving computer.</p><p>The obvious solution is to measure the latency and add it to the time received by the other computer to get a more accurate time. However, measuring latency is challenging because each computer has its clock, and there is no universal clock to compare against.</p><p>To measure latency, two assumptions about consistency and symmetry are made:</p><ul><li><p>The latency a packet experiences while traveling across the network is consistent.</p></li><li><p>The latency from the Stratum to the other computer is equal to the latency from the other computer back to the Stratum. In other words, the network is symmetric.</p></li></ul><p>Therefore, it follows that the accuracy of time synchronization can be improved by maximizing consistency and symmetry in the network.</p><p>PTP is a solution that helps achieve this.</p><ul><li><p>PTP uses hardware timestamping to improve consistency. This means that timestamps are added to the time information at the hardware level, reducing the impact of software and driver delays.</p></li><li><p>PTP also uses transparent clocks, which are special devices that measure and compensate for the time the information spends passing through network equipment.</p></li></ul><h2>The Need for PTP</h2><p>Let&#8217;s look at a practical case where PTP is needed.</p><p>Imagine you&#8217;re using Facebook and you post a new status update. When you try to view your post, there&#8217;s a chance that your request to see the post is handled by a different server than the one that originally processed your post.</p><p>If the server handling your view request doesn&#8217;t have the latest updated data, you might not see your post. This is annoying for users and goes against the promise that interacting with a distributed system like Facebook should work the same as interacting with a single server that has all the data.</p><p>In the old solution, Facebook used to send your view request to multiple servers and wait for a majority of them to agree on the data before showing it to you. But this takes up extra computing resources and adds delay because of the back-and-forth communication over the network.</p><p>By using PTP to keep precise time synchronization across all its servers, they can simply have the view request wait until the server has caught up to the timestamp of your original post. There is no need for multiple requests and responses.</p><p>The diagram below shows this scenario.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa69c52f-d90f-42eb-b83a-1bad27555321_1600x1088.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="990" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa69c52f-d90f-42eb-b83a-1bad27555321_1600x1088.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>However, this only works if all the server clocks are closely synchronized. Also, the difference between a server&#8217;s clock and the reference time needs to be known.&nbsp;</p><p>PTP provides this tight synchronization. It could synchronize time about 100 times more precisely than NTP, which was necessary for Facebook&#8217;s requirement.</p><p>This was just one example. There were several additional use cases where PTP excelled such as:</p><ul><li><p>Event tracing</p></li><li><p>Cache invalidations</p></li><li><p>Privacy violation detection improvements</p></li><li><p>Latency compensation in the metaverse</p></li></ul><h2>The PTP Architecture</h2><p>Facebook&#8217;s PTP architecture consists of three main components:</p><ul><li><p>The PTP Rack</p></li><li><p>The PTP Network</p></li><li><p>The PTP Client</p></li></ul><p>See the diagram below for a high-level view of the architecture:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7339a422-51ef-4ca7-a1cd-8db48d09805f_1600x1128.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1026" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7339a422-51ef-4ca7-a1cd-8db48d09805f_1600x1128.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://engineering.fb.com/2022/11/21/production-engineering/precision-time-protocol-at-meta/">Facebook Engineering Blog</a></figcaption></figure></div><p>Let&#8217;s look at each component and understand how they work together to provide precise timekeeping.</p><h3>The PTP Rack</h3><p>The PTP rack houses the hardware and software that serve time to clients.</p><p>It consists of critical components such as:</p><ol><li><p><strong>GNSS Antenna: </strong>This is where time originates on Earth. The antenna receives time signals from GPS, Galileo, and other satellite constellations. Facebook uses a GNSS-over-fiber technology to distribute the signal, which is more reliable and easier to install than traditional coaxial cables.</p></li><li><p><strong>Time Appliance: </strong>This is the heart of the timing infrastructure. It disciplines the time received from the GNSS antenna using atomic clocks for improved accuracy and stability. Facebook has developed a new Time appliance that can support up to 1 million clients without compromising accuracy.</p></li><li><p><strong>Oscillatord: </strong>This is a software component that configures and monitors the Time appliance, including the GNSS receiver and atomic clocks. It exports data that helps decide if the Time Appliance should serve clients or be taken offline.</p></li><li><p><strong>Network Card (NIC): </strong>It&#8217;s the interface between the Time Appliance and the network. It timestamps PTP packets using its clock, which is synchronized with the Time Appliance for nanosecond-level accuracy.</p></li><li><p><strong>Ptp4u: </strong>This is Facebook&#8217;s custom-built PTP server software that can handle over 1 million clients per server, far more than existing solutions. It runs on the Time Appliance and sends PTP messages to the clients.</p></li></ol><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9fbd1b-dc1e-43ed-b576-8b8d242cefa0_1600x920.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="837" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab9fbd1b-dc1e-43ed-b576-8b8d242cefa0_1600x920.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">The GNSS Antenna in a Datacenter (Source: <a href="https://engineering.fb.com/2022/11/21/production-engineering/precision-time-protocol-at-meta">Facebook Engineering Blog</a>)</figcaption></figure></div><h3>The PTP Network</h3><p>The PTP network is responsible for distributing time from the PTP rack to clients. Facebook uses PTP over a standard IP network with a few key enhancements:</p><ol><li><p><strong>PTP Unicast: </strong>They use unicast PTP instead of multicast for simpler network configuration and better scalability. Clients request time from servers, and servers grant requests and send PTP messages.</p></li><li><p><strong>Transparent Clocks:</strong> Each network switch between the client and server acts as a transparent clock. It measures the time each PTP packet spends in the switch and records it in the packet. This allows clients to accurately account for network delays.</p></li><li><p><strong>Boundary Clock Avoidance: </strong>&nbsp;Facebook avoided using boundary clocks, which act as both clients and servers, to reduce complexity. They rely solely on transparent clocks in the network switches.</p></li></ol><p>A typical PTP unicast flow consists of the following steps:</p><ul><li><p><strong>Client Initiates Negotiation: </strong>The PTP client starts the process by requesting unicast transmission from the PTP server. It sends three types of requests:</p><ul><li><p><strong>Sync Grant Request: </strong>The client asks the server to send a specified number of Sync and Follow-Up messages per second, containing the current time, for a certain duration. This helps the client adjust its clock to match the server&#8217;s clock.</p></li><li><p><strong>Announce Grant Request: </strong>The client requests the server to send a specific number of Announce messages per second, containing the server&#8217;s status, for a certain duration. This helps the client make sure that the server hasn&#8217;t stopped or gone haywire.</p></li><li><p><strong>Delay Response Grant Request: </strong>The client informs the server that it will send Delay Request messages, and asks the server to respond with Delay Response packets for a specified duration. This helps the client account for any delay in communication.</p></li></ul></li><li><p><strong>Server Grants Requests:</strong> The PTP server needs to grant these requests and send corresponding grant responses to the client.</p></li><li><p><strong>Server Sends PTP Messages: </strong>Once the requests are granted, the server starts sending the requested PTP messages</p></li><li><p><strong>Client Sends Delay Requests: </strong>The client sends Delay Request messages to the server at the agreed-upon interval to determine the network path delay.</p></li><li><p><strong>Client Refreshes Grant: </strong>The client needs to periodically refresh the grant by repeating the negotiation process before the current grant expires.</p></li></ul><p>The diagram below shows the PTP Exchange Process</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9ce734-68f3-4059-99ef-aaa32ae024f8_1600x961.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="875" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9ce734-68f3-4059-99ef-aaa32ae024f8_1600x961.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>The PTP Client</h3><p>The PTP client software runs on each server that needs accurate time. Facebook uses a few different components:</p><ol><li><p><strong>ptp4I: </strong>An open-source PTP client that receives PTP messages from the server and disciplines the NIC hardware clock. Facebook has made several enhancements to ptp4I to handle its scale and unique requirements.</p></li><li><p><strong>fbclock: </strong>This is Facebook&#8217;s custom API that provides PTP time to applications. Instead of a single timestamp, it gives a &#8220;window of uncertainty&#8221; - a time range that is guaranteed to contain the true time with a high degree of certainty.</p></li><li><p><strong>Kernel Timestamping: </strong>The Linux kernel on each server timestamps incoming and outgoing PTP packets in hardware for maximum accuracy. This relies on NIC driver support and careful configuration.</p></li></ol><h2>Conclusion</h2><p>To conclude, Facebook&#8217;s adoption of Precision Time Protocol (PTP) across its infrastructure is a significant step forward in ensuring precise and reliable timekeeping at an unprecedented scale. By redesigning and rebuilding various components, Facebook has pushed the boundaries of what&#8217;s possible with PTP.</p><p>Also, the open-source nature of most of the work helps us learn from the PTP solution implemented by them.</p><p><strong>References:</strong></p><ul><li><p><a href="https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/">Building a more accurate time service at Facebook scale</a></p></li><li><p><a href="https://engineering.fb.com/2022/11/21/production-engineering/future-computing-ptp">PTP: Timing accuracy and precision for the future of computing</a></p></li><li><p><a href="https://engineering.fb.com/2022/11/21/production-engineering/precision-time-protocol-at-meta/">How Precision Time Protocol is being deployed at Meta</a></p></li></ul><p></p>
]]></content:encoded>
<pubDate>Tue, 06 Aug 2024 15:30:43 GMT</pubDate>
</item>
<item>
<title>EP123: What is a Load Balancer?</title>
<link>https://blog.bytebytego.com/p/ep123-what-is-a-load-balancer</link>
<guid>https://blog.bytebytego.com/p/ep123-what-is-a-load-balancer</guid>
<content:encoded><![CDATA[
<div> credit card, load balancer, k8s design patterns, QA Wolf, Kubernetes

credit card被称为银行中最赚钱的产品，通过信用卡支付流程经济学图解说明了VISA/Mastercard如何盈利。Load Balancer是分配网络或应用流量到多个服务器的设备或软件应用。k8s设计模式分为基础、结构和行为等几种模式，包括健康探测、预测需求、批处理和有状态服务等。QA Wolf提供AI原生解决方案，在短时间内进行高容量、高速度的测试覆盖，减少QA周期。总结: credit card通过支付流程实现盈利，Load Balancer用于流量分配，k8s设计模式包括基础、结构和行为几种模式，QA Wolf提供高效的自动化测试解决方案。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>25 Computer Papers You Should Read (Youtube video)</p></li><li><p>Why is the credit card called &#8220;the most profitable product in banks&#8221;? </p></li><li><p>What is a Load Balancer? </p></li><li><p>Top 10 k8s Design Patterns </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><strong><a href="https://bit.ly/QAWolf_080324">&#128075;Goodbye low test coverage and slow QA cycles (Sponsored)</a></strong></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/QAWolf_080324" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="736" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb179cdc8-c1f6-4481-860e-bfaf78db67c8_1473x745.png" width="1456" /><div></div></div></a></figure></div><p>Bugs sneak out when less than 80% of user flows are tested before shipping. However, getting that kind of coverage (and staying there) is hard and pricey for any team.</p><p><a href="https://bit.ly/QAWolf_080324">QA Wolf&#8217;s</a> AI-native solution provides high-volume, high-speed test coverage for web and mobile app, reducing your organization&#8217;s QA cycle to minutes.</p><p>They can get you:</p><ul><li><p><a href="https://bit.ly/QAWolf_080324">80% automated E2E test coverage in weeks</a>&#8212;not years</p></li><li><p><a href="https://bit.ly/QAWolf_080324">Unlimited parallel test runs</a></p></li><li><p>24-hour maintenance and on-demand test creation</p></li><li><p>Zero flakes, guaranteed</p></li></ul><p>The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.</p><p>With QA Wolf, <a href="https://bit.ly/QAWolf_080324CaseStudy">Drata&#8217;s team of 80+ engineer</a>s achieved 4x more test cases and 86% faster QA cycles.</p><p>&#127775; Rated 4.8/5 on G2</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/QAWolf_080324"><span>Schedule a demo to learn more</span></a></p><div><hr /></div><h2>25 Computer Papers You Should Read<strong>!</strong></h2><div class="youtube-wrap" id="youtube2-_kynGl5hr9U"><div class="youtube-inner"></div></div><div><hr /></div><h2>Why is the credit card called &#8220;the most profitable product in banks&#8221;? </h2><p>How does VISA/Mastercard make money? </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1456" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72563e98-8a4c-4257-b1fa-9369bd43adf3_1536x1536.gif" title="No alternative text description for this image" width="1456" /><div></div></div></a></figure></div><p>The diagram below shows the economics of the credit card payment flow. <br /> <br />1. The cardholder pays a merchant $100 to buy a product. <br /> <br />2. The merchant benefits from the use of the credit card with higher sales volume, and needs to compensate the issuer and the card network for providing the payment service. The acquiring bank sets a fee with the merchant, called the &#8220;&#119846;&#119838;&#119851;&#119836;&#119841;&#119834;&#119847;&#119853; &#119837;&#119842;&#119852;&#119836;&#119848;&#119854;&#119847;&#119853; &#119839;&#119838;&#119838;.&#8221; <br /> <br />3 - 4. The acquiring bank keeps $0.25 as the &#119834;&#119836;&#119850;&#119854;&#119842;&#119851;&#119842;&#119847;&#119840; &#119846;&#119834;&#119851;&#119844;&#119854;&#119849;, and $1.75 is paid to the issuing bank as the &#119842;&#119847;&#119853;&#119838;&#119851;&#119836;&#119841;&#119834;&#119847;&#119840;&#119838; &#119839;&#119838;&#119838;. The merchant discount fee should cover the interchange fee. <br /> <br />The interchange fee is set by the card network because it is less efficient for each issuing bank to negotiate fees with each merchant. <br /> <br />5. The card network sets up the &#119847;&#119838;&#119853;&#119856;&#119848;&#119851;&#119844; &#119834;&#119852;&#119852;&#119838;&#119852;&#119852;&#119846;&#119838;&#119847;&#119853;&#119852; &#119834;&#119847;&#119837; &#119839;&#119838;&#119838;&#119852; with each bank, which pays the card network for its services every month. For example, VISA charges a 0.11% assessment, plus a $0.0195 usage fee, for every swipe. <br /> <br />6. The cardholder pays the issuing bank for its services. <br /> <br />Why should the issuing bank be compensated? </p><ul><li><p>The issuer pays the merchant even if the cardholder fails to pay the issuer.</p></li><li><p>The issuer pays the merchant before the cardholder pays the issuer. </p></li><li><p>The issuer has other operating costs, including managing customer accounts, providing statements, fraud detection, risk management, clearing &amp; settlement, etc. </p></li></ul><p>Over to you: Does the card network charge the same interchange fee for big merchants as for small merchants? </p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1838" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F813befc8-72c0-45fc-9568-b9bc5ca9a69d_2250x2840.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-domain-driven-design">A Crash Course on Domain-Driven Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/tidying-code">"Tidying" Code</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-relational-database">A Crash Course on Relational Database Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-distributed-systems">A Crash Course on Distributed Systems</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-scaling">A Crash Course in Database Scaling Strategies</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>What is a Load Balancer? </h2><p>A load balancer is a device or software application that distributes network or application traffic across multiple servers. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4ec75d1-6996-40d7-971e-ba67a59a204a_1536x1536.gif" title="graphical user interface" /><div></div></div></a></figure></div><ul><li><p>What Does a Load Balancer Do? <br />1. Distributes Traffic <br />2. Ensures Availability and Reliability <br />3. Improves Performance <br />4. Scales Applications <br /></p></li><li><p>Types of Load Balancers <br />1. Hardware Load Balancers: These are physical devices designed to distribute traffic across servers. <br /> <br />2. Software Load Balancers: These are applications that can be installed on standard hardware or virtual machines. <br /> <br />3. Cloud-based Load Balancers: Provided by cloud service providers, these load balancers are integrated into the cloud infrastructure. Examples include AWS Elastic Load Balancer, Google Cloud Load Balancing, and Azure Load Balancer. <br /> <br />4. Layer 4 Load Balancers (Transport Layer): Operate at the transport layer (OSI Layer 4) and make forwarding decisions based on IP address and TCP/UDP ports. <br /> <br />5. Layer 7 Load Balancers (Application Layer): Operate at the application layer (OSI Layer 7) . <br /> <br />6. Global Server Load Balancing (GSLB): Distributes traffic across multiple geographical locations to improve redundancy and performance on a global scale.</p></li></ul><div><hr /></div><h2>Top 10 k8s Design Patterns </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F820fba4c-56d0-4a18-8203-accc2c405d03_1536x1536.gif" title="graphical user interface" /><div></div></div></a></figure></div><ul><li><p>Foundational Patterns <br />These patterns are the fundamental principles for applications to be automated on k8s, regardless of the application's nature. <br /> <br />1. Health Probe Pattern <br />This pattern requires that every container must implement observable APIs for the platform to manage the application. <br /> <br />2. Predictable Demands Pattern <br />This pattern requires that we should declare application requirements and runtime dependencies. Every container should declare its resource profile. <br /> <br />3. Automated Placement Pattern <br />This pattern describes the principles of Kubernetes&#8217; scheduling algorithm. <br /></p></li><li><p>Structural Patterns <br />These patterns focus on structuring and organizing containers in a Pod. <br /> <br />4. Init Container Pattern <br />This pattern has a separate life cycle for initialization-releated tasks. <br /> <br />5. Sidecar Pattern <br />This pattern extends a container&#8217;s functionalities without changing it. <br /></p></li><li><p>Behavioral Patterns <br />These patterns describe the life cycle management of a Pod. Depending on the type of the workload, it can run as a service or a batch job. <br /> <br />6. Batch Job Pattern <br />This pattern is used to manage isolated atomic units of work. <br /> <br />7. Stateful Service Pattern <br />This pattern creates distributed stateful applications. <br /> <br />8. Service Discovery Pattern <br />This pattern describes how clients discover the services. <br /></p></li><li><p>Higher-Level Patterns <br />These patterns focus on higher-level application management. <br /> <br />9. Controller Pattern <br />This pattern monitors the current state and reconciles with the declared target state. <br /> <br />10. Operator Pattern <br />This pattern defines operational knowledge in an algorithmic and automated form. </p></li></ul><p>Reference: developers.redhat. com/blog/2020/05/11/top-10-must-know-kubernetes-design-patterns</p><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong><br /> </p>
]]></content:encoded>
<pubDate>Sat, 03 Aug 2024 15:31:05 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Domain-Driven Design</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-domain-driven-design</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-domain-driven-design</guid>
<content:encoded><![CDATA[
<div> 领域驱动设计，模型，业务逻辑，软件开发，可维护，可扩展<br />
领域驱动设计是一种软件开发方法，强调将主要关注点放在核心领域上，并基于该领域的模型构建复杂设计。通过建立技术和领域专家之间的合作关系，可以更好地理解和呈现业务概念、规则和流程。在当今微服务和云计算架构盛行的时代，没有清晰和明确定义的领域模型指导设计的系统容易变成“大球形泥块”。领域驱动设计帮助我们建立与核心领域和业务逻辑相一致的、更具可维护性和可扩展性的系统。 <br /><br />总结:领域驱动设计强调将主要关注点放在核心领域上，重视领域模型的建立，促进技术和领域专家的合作，有助于构建更具可维护性和可扩展性的系统。 <div>
<p>Developing software for complex domains is a challenging task.&nbsp;</p><p>As the complexity of the problem domain grows, it becomes increasingly difficult to create software that accurately represents the business concepts, rules, and processes. Poorly designed software can quickly turn into an incomprehensible tangle of code that is difficult to understand, maintain, and extend.</p><p>Domain-Driven Design (DDD) offers a solution to this problem.</p><p>DDD is an approach to software development that tackles domain complexity by emphasizing the importance of modeling the core domain and business logic and using those models as the foundation for software design.</p><p>At its heart, Domain-Driven Design is about:</p><ul><li><p>Placing the primary focus on the core domain.</p></li><li><p>Basing complex designs on a model of the domain</p></li><li><p>Building collaboration between technical and domain experts.</p></li></ul><p>The need for Domain-Driven Design has become more pressing in recent years. Architectures based on microservices and cloud computing have resulted in systems composed of numerous small components that interact in intricate ways. Without a clear and well-defined model of the domain guiding their design, such systems can quickly become a "big ball of mud".&nbsp;</p><p>In this article, we&#8217;ll understand the basics of Domain-Driven Design and its key concepts that can help us build more maintainable and extensible systems that are aligned with the core domain and business logic.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F813befc8-72c0-45fc-9568-b9bc5ca9a69d_2250x2840.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1838" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F813befc8-72c0-45fc-9568-b9bc5ca9a69d_2250x2840.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-domain-driven-design">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 01 Aug 2024 15:31:02 GMT</pubDate>
</item>
<item>
<title>EP122: API Gateway 101</title>
<link>https://blog.bytebytego.com/p/ep122-api-gateway-101</link>
<guid>https://blog.bytebytego.com/p/ep122-api-gateway-101</guid>
<content:encoded><![CDATA[
<div> Session JWT API Gateway Pub-Sub ETL

<br /><br />总结:
本文介绍了系统设计中的一些重要概念和模式。首先讲解了Session和JWT之间的差异，接着介绍了9种用于数据和通信流的架构模式，包括API Gateway、Pub-Sub、ETL等。文章还详细解释了为什么CDN如此受欢迎，以及全栈开发的技术栈。最后，介绍了ScyllaDB，并推荐了相关资源。整体来说，本文提供了丰富的系统设计知识，对于想深入了解系统设计的读者是一份有价值的参考资料。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Session Vs JWT: The Differences You May Not Know! (Youtube video)</p></li><li><p>Top 9 Architectural Patterns for Data and Communication Flow </p></li><li><p>API Gateway 101 </p></li><li><p>Why are Content Delivery Networks (CDN) so Popular? </p></li><li><p>A Roadmap for Full-Stack Development</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/ScyllaDB_072724">Learn ScyllaDB from Discord Engineer Bo Ingram [FREE BOOK] (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/ScyllaDB_072724" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b65ca93-39bc-4a25-8aa2-a6be2e8eebac_1600x840.png" width="1456" /><div></div></div></a></figure></div><p>Get practical tips for building low latency, high throughput apps with ScyllaDB</p><p><em>ScyllaDB in Action</em> is a practical guide to everything you need to know about ScyllaDB, from your first queries to running it in a production environment. It&#8217;s written by Discord Staff Engineer Bo Ingram, author of the famous blog on how Discord migrated trillions of messages from Cassandra to ScyllaDB.&nbsp;</p><p>This early access preview covers:</p><ul><li><p>Working with ScyllaDB vs. working with other databases</p></li><li><p>How ScyllaDB runs in practice</p></li><li><p>What matters most in real-world deployments</p></li><li><p>How to launch your first cluster, add tables, and run queries</p></li><li><p>How to perform effective data modeling in ScyllaDB</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/ScyllaDB_072724"><span>Free Book Download</span></a></p><div><hr /></div><h2>Session Vs JWT: The Differences You May Not Know!</h2><div class="youtube-wrap" id="youtube2-fyTxwIa-1U0"><div class="youtube-inner"></div></div><div><hr /></div><h2>Top 9 Architectural Patterns for Data and Communication Flow </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8725cbb8-4c30-413c-b623-4ca9e235f6d0_1280x1664.gif" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><ul><li><p>Peer-to-Peer <br />The Peer-to-Peer pattern involves direct communication between two components without the need for a central coordinator. <br /></p></li><li><p>API Gateway <br />An API Gateway acts as a single entry point for all client requests to the backend services of an application. <br /></p></li><li><p>Pub-Sub <br />The Pub-Sub pattern decouples the producers of messages (publishers) from the consumers of messages (subscribers) through a message broker. <br /> </p></li><li><p>Request-Response <br />This is one of the most fundamental integration patterns, where a client sends a request to a server and waits for a response. <br /></p></li><li><p>Event Sourcing <br />Event Sourcing involves storing the state changes of an application as a sequence of events. <br /></p></li><li><p>ETL <br />ETL is a data integration pattern used to gather data from multiple sources, transform it into a structured format, and load it into a destination database. <br /></p></li><li><p>Batching <br />Batching involves accumulating data over a period or until a certain threshold is met before processing it as a single group. <br /></p></li><li><p>Streaming Processing <br />Streaming Processing allows for the continuous ingestion, processing, and analysis of data streams in real-time. <br /></p></li><li><p>Orchestration <br />Orchestration involves a central coordinator (an orchestrator) managing the interactions between distributed components or services to achieve a workflow or business process.</p></li></ul><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="802" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9e76434-5699-479f-8731-c287002e1cd4_1600x881.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/tidying-code">"Tidying" Code</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-relational-database">A Crash Course on Relational Database Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-distributed-systems">A Crash Course on Distributed Systems</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-scaling">A Crash Course in Database Scaling Strategies</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-sharding">A Crash Course in Database Sharding</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>API Gateway 101 </h2><p>An API gateway is a server that acts as an API front-end, receiving API requests, enforcing throttling and security policies, passing requests to the back-end service, and then returning the appropriate result to the client. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="785.87" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdaa3f829-4026-4c5e-bc42-59417bf64f86_800x883.gif" title="graphical user interface, application" width="712" /><div></div></div></a></figure></div><p>It is essentially a middleman between the client and the server, managing and optimizing API traffic. <br /> <br />Key Functions of an API Gateway:</p><ul><li><p>Request Routing: Directs incoming API requests to the appropriate backend service. </p></li><li><p>Load Balancing: Distributes requests across multiple servers to ensure no single server is overwhelmed. </p></li><li><p>Security: Implements security measures like authentication, authorization, and data encryption. </p></li><li><p>Rate Limiting and Throttling: Controls the number of requests a client can make within a certain period. </p></li><li><p>API Composition: Combines multiple backend API requests into a single frontend request to optimize performance. </p></li><li><p>Caching: Stores responses temporarily to reduce the need for repeated processing.</p></li></ul><div><hr /></div><h2>Why are Content Delivery Networks (CDN) so Popular? </h2><p>The CDN market is expected to reach nearly $38 billion by 2028. Companies like Akamai, Cloudflare, and Amazon CloudFront are investing heavily in this area. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram" class="sizing-normal" height="1563" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F057a34e5-b671-414a-a54c-c68a1e96a3de_1280x1563.gif" title="diagram" width="1280" /><div></div></div></a></figure></div><p>There are several factors behind the popularity of CDNs </p><ol><li><p>The Impact of CDN <br />CDNs improve performance, increase availability, and enhance bandwidth costs. With the use of CDN, there is a significant reduction in latency. </p><p></p></li><li><p>CDN Request Flow <br />After DNS resolution, the user&#8217;s device sends the content request to the CDN edge server. <br />The edge server checks its local cache for the content. If found, the edge server serves the content to the user. <br />If not found, the edge server forwards the request to the origin server. <br />After receiving the content from the origin server, the edge server stores a copy in its cache and delivers it to the user. <br /></p></li><li><p>The Architecture of CDN <br />There are multiple components in a CDN&#8217;s architecture: <br /> <br />Origin Server: This is the primary source of content. <br />Edge Servers: They cache and server content to the users and are distributed across the world. <br />DNS: The DNS resolves the domain name to the IP address of the nearest edge server <br />Control Plane: Responsible for configuring and managing the edge servers. </p><p></p></li><li><p>CDN Request Routing <br />GSLB: Routes user requests to the server based on factors like geographic proximity, server load, network conditions <br />Anycast DNS: Allows multiple servers to share the same IP address. It helps route incoming traffic to the nearest data center. <br />Internet Exchange Points: CDN providers establish a presence at major IXPs, allowing them to exchange traffic directly with ISPs and other networks. <br /></p></li><li><p>Best Practices <br />Some key best practices to optimize CDN performance are related to security aspects, caching optimizations, and content optimizations. <br /></p></li></ol><p>Over to you: What do you think makes CDNs popular?</p><div><hr /></div><h2>A Roadmap for Full-Stack Development</h2><p>A full-stack developer needs to be proficient in a wide range of technologies and tools across different areas of software development. Here&#8217;s a comprehensive look at the technical stacks required for a full-stack developer. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="826.265" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6138c1f4-4c94-47b4-b3b0-9e6764e368e9_800x913.gif" title="graphical user interface, application" width="724" /><div></div></div></a></figure></div><ol><li><p>Frontend Development <br />Frontend development involves creating the user interface and user experience of a web application. </p></li><li><p>Backend Development <br />Backend development involves managing the server-side logic, databases, and integration of various services. </p></li><li><p>Database Development <br />Database development involves managing data storage, retrieval, and manipulation. </p></li><li><p>Mobile Development <br />Mobile development involves creating applications for mobile devices. </p></li><li><p>Cloud Computing <br />Cloud computing involves deploying and managing applications on cloud platforms. </p></li><li><p>UI/UX Design <br />UI/UX design involves designing the user interface and experience of applications. </p></li><li><p>Infrastructure and DevOps <br />Infrastructure and DevOps involve managing the infrastructure, deployment, and continuous integration/continuous delivery (CI/CD) of applications.</p></li></ol><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:sponsorship@bytebytego.com">sponsorship@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 27 Jul 2024 15:30:25 GMT</pubDate>
</item>
<item>
<title>"Tidying" Code</title>
<link>https://blog.bytebytego.com/p/tidying-code</link>
<guid>https://blog.bytebytego.com/p/tidying-code</guid>
<content:encoded><![CDATA[
<div> 敏捷，重构，Kent Beck，TDD，书籍

敏捷开发领域著名人物Kent Beck最新著作《Tidy First》介绍了代码重构的重要性。文章介绍了Kent在软件行业的重要贡献，包括签署敏捷宣言、推动测试驱动开发等实践。他曾在多家知名公司工作，并且积极撰写有关软件设计和开发的新闻。文章还描述了作者与Kent交流讨论书籍内容的经历。Kent提及他19年前就开始构思《Tidy First》，强调代码整洁对软件开发的重要性。总结：文章介绍了敏捷软件开发领域的重要人物Kent Beck及其新书《Tidy First》，强调了代码重构在软件开发中的重要性，突出了作者对此书的热切期盼。 <div>
<p>I&#8217;ve found my new favorite book on refactoring code. It&#8217;s &#8220;<a href="https://www.amazon.com/Tidy-First-Personal-Exercise-Empirical/dp/1098151240/">Tidy First?</a>&#8221; by Kent Beck.</p><p>Kent Beck is a renowned figure in the software industry. One of Kent&#8217;s notable contributions is his role as a signatory of the Agile Manifesto that was created in 2001 and laid the foundation for the agile software development methodology.</p><p>Kent has also been a key figure in the industry&#8217;s rediscovery of test-driven development (TDD). TDD is a software development practice that emphasizes writing tests before writing the code.</p><p>Throughout his career, Kent has worked at several prominent companies, including Facebook, Apple Computer, and Gusto. In addition to his professional work, Kent is also an active writer. He publishes a newsletter titled &#8220;<a href="https://tidyfirst.substack.com/bytebytego">Software Design: Tidy First?</a>&#8221; which explores various aspects of software design and development. For readers interested in subscribing, here is a <a href="https://tidyfirst.substack.com/bytebytego">20% discount offer</a>.</p><p>Recently, I had the opportunity to meet Kent and talk about his book &#8220;Tidy First&#8221;.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6ea57ee-22dc-43a6-ada7-a4a60275e5d1_1200x1600.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6ea57ee-22dc-43a6-ada7-a4a60275e5d1_1200x1600.jpeg" width="1200" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>Story of Tidy First&nbsp;</h2><p>I came to learn from Kent that the idea of the book Tidy First came to him around 19 years ago.</p>
      <p>
          <a href="https://blog.bytebytego.com/p/tidying-code">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 25 Jul 2024 15:31:02 GMT</pubDate>
</item>
<item>
<title>How Stripe Scaled to 5 Million Database Queries Per Second</title>
<link>https://blog.bytebytego.com/p/how-stripe-scaled-to-5-million-database</link>
<guid>https://blog.bytebytego.com/p/how-stripe-scaled-to-5-million-database</guid>
<content:encoded><![CDATA[
<div> Stripe, DocDB, Data Movement Platform, Sharding, 数据迁移
<br />
<br />
总结:Stripe的DocDB和Data Movement Platform对实现99.999%的可用性及零停机数据迁移起到了关键作用。DocDB扩展了MongoDB社区版，通过分片来横向扩展数据库，并使用Data Movement Platform实现跨分片在线迁移。迁移过程包括注册、数据导入、异步复制、检查正确性、流量切换和注销，均经过仔细规划和实施。 Data Movement Platform不仅支持数据迁移，还用于拆分DocDB分片、优化数据库基础架构等。Stripe的基础设施架构展现了其技术实力和可靠性。 <div>
<h2><a href="https://bit.ly/WorkOS_072324">WorkOS: modern identity platform for B2B SaaS (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/WorkOS_072324" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="763" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fcf8b91-ac0f-4e95-9215-449469ef555f_1600x838.png" width="1456" /><div></div></div></a></figure></div><p>Start selling to enterprises with just a few lines of code.</p><p>&#8594; WorkOS provides a complete User Management solution along with SSO, SCIM, RBAC, &amp; FGA.</p><p>&#8594; Unlike other auth providers that rely on user-centric models, WorkOS is designed for B2B SaaS with an org modeling approach.</p><p>&#8594; The APIs are flexible, easy-to-use, and modular. Pick and choose what you need and integrate in minutes.</p><p>&#8594; Best of all, User Management is free up to 1 million MAUs&nbsp;and comes standard with RBAC, bot protection, impersonation, MFA, &amp; more.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/WorkOS_072324"><span>Get started today</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the Stripe Engineering Blog. All credit for the architectural details goes to Stripe&#8217;s engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>As of 2023, only 19 countries had a GDP surpassing $1 trillion. Also, in 2023, Stripe alone processed $1 trillion in total payment value.</p><p>To make the achievement even more remarkable, they managed these numbers while supporting 5 million database queries per second at five-nines (99.999%) of availability.</p><p>What was behind the success of Stripe&#8217;s infrastructure?</p><p>The secret lies in the horizontal scaling capabilities of their database.</p><p>Stripe&#8217;s database infrastructure team built an internal database-as-a-service (DBaaS) offering called DocDB. It was created as an extension of MongoDB&#8217;s community edition because of MongoDB&#8217;s flexibility and ability to handle a massive volume of real-time data at scale.</p><p>In this post, we&#8217;ll explore how DocDB works and the various features it provides that allow Stripe to operate at such an incredible scale.</p><div><hr /></div><h2>Why the Need for DocDB?</h2><p>The first question while looking at DocDB is this: what forced Stripe to build a DBaaS offering?</p><p>Stripe launched in 2011. At the time, they chose MongoDB as the online database because its schema-less approach made it more productive for developers than standard relational databases. MongoDB also supports horizontal scaling through its robust sharding architecture, which is shown in the diagram below:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3fab4a6-83e8-49a8-986d-d272cda54583_1600x1002.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="912" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3fab4a6-83e8-49a8-986d-d272cda54583_1600x1002.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>However, to unlock the best developer experience, Stripe needed a database service that could work like a product for the development teams. MongoDB Atlas didn&#8217;t exist in 2011 and they couldn&#8217;t find an off-the-shelf DBaaS that met key requirements such as:</p><ul><li><p>Maintain the highest standards of availability, durability, and performance.</p></li><li><p>Expose a minimal set of database functions to prevent issues from suboptimal client queries. For example, queries running on a large collection based on a specific field that does not have a corresponding index. It also includes unbounded queries on large result sets and complex aggregations.</p></li><li><p>Support horizontal scalability with sharding.</p></li><li><p>First-class support for multi-tenancy with quotas</p></li><li><p>Strong security with authorization policies.</p></li></ul><p>The solution was to build DocDB with MongoDB as the underlying storage engine. The DocDB deployment was also highly customized to provide low latency and diverse access. Some interesting stats related to DocDB are as follows:</p><ul><li><p>It supports over 10,000 distinct query types that run over petabytes of important financial data.</p></li><li><p>The data is spread across 5000+ collections.</p></li><li><p>The collections are distributed over 2000 database shards.</p></li></ul><p>At the heart of DocDB is the Data Movement Platform. It was originally built as a horizontal scaling solution to overcome the vertical scaling limits of MongoDB.</p><p>The Data Movement Platform made it possible to transition from running a small number of database shards (each storing tens of terabytes of data) to thousands of database shards (each with a fraction of the original data).</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F890ed9c7-5215-492d-bad2-9b637361036e_1600x1088.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="990" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F890ed9c7-5215-492d-bad2-9b637361036e_1600x1088.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The platform performs multiple functions such as:</p><ul><li><p>Handling maintenance tasks like database engine upgrades.</p></li><li><p>Transitioning databases from a multitenant arrangement to single tenancy for large users. In a multitenant database arrangement, multiple users or applications share the same resources. In contrast, single tenancy means that a user or application has dedicated database resources for better isolation, customization, and performance.</p></li><li><p>Supporting migrations with zero downtime and no impact on the clients.</p></li><li><p>Splitting database shards during traffic surges and consolidating thousands of databases through bin packing when traffic is low.</p></li></ul><p>For reference, bin packing is an optimization problem where the goal is to pack a set of objects (in this case, data) into a minimum number of bins (database shards) of a fixed capacity. The objective is to minimize the number of bins used while ensuring that the total size or weight of the objects in each bin does not exceed its capacity.</p><h2>How Applications Access DocDB?</h2><p>DocDB leverages sharding to achieve horizontal scalability for its database infrastructure. With thousands of database shards distributed across Stripe&#8217;s product applications, sharding enables efficient data distribution and parallel processing.</p><p>However, the use of database sharding introduces a challenge for applications when determining the appropriate destination shard for their queries.&nbsp;</p><p>To address this issue, Stripe&#8217;s database infrastructure team developed a fleet of database proxy servers implemented in Golang. These proxy servers handle the task of routing queries to the correct shard.</p><p>The diagram shows DocDB&#8217;s high-level infrastructure overview.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07434131-eaae-43b4-8805-b2808307eeac_1600x1160.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1056" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07434131-eaae-43b4-8805-b2808307eeac_1600x1160.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>When an application sends a query to a database proxy server, it performs the following steps:</p><ul><li><p>Parsing the query</p></li><li><p>Routing it to one or more shards</p></li><li><p>Combining the results received from the shards</p></li><li><p>Returning the final result to the application</p></li></ul><p>But how do database proxy servers make the routing decisions?</p><p>The database proxy servers rely on the chunk metadata service to make routing decisions.&nbsp;</p><p>A chunk represents a small subset of data within a larger collection. Each shard contains a fraction of the total data, and these fractions are referred to as <em>chunks</em>.</p><p>For example, consider that Stripe has a large collection called &#8220;Transactions&#8221; that contains millions of documents representing financial transactions. To scale this collection horizontally, they might split the data into chunks based on a sharding key, such as customer ID or the transaction timestamp. Each chunk would then be assigned to a specific database shard.</p><p>The chunk metadata service manages the mapping between these chunks and their corresponding shards. It keeps track of which chunk resides on which shard, allowing the proxy servers to route queries and requests to the appropriate shard.</p><h3>Data Organization in DocDB</h3><p>At Stripe, product teams use an in-house tool called the <em>document database control plane </em>to create and manage their databases. When a team provisions a new database using this tool, they are creating a &#8220;logical database.&#8221;</p><p>A logical database is like a virtual container holding one or more data collections known as DocDB collections. Each DocDB collection contains related documents that serve a specific purpose for the product team.</p><p>Even though a logical database appears as a single entity to the product team, the data within the collections is spread across multiple physical databases behind the scenes. These physical databases are the actual databases running on Stripe&#8217;s infrastructure.</p><p>The diagram below shows this arrangement:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa593f9c1-4f71-4e6a-8b38-e2775f458eee_1600x1162.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1057" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa593f9c1-4f71-4e6a-8b38-e2775f458eee_1600x1162.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Each physical database contains a small portion (or &#8220;chunk&#8221;) of the data from the DocDB collection and is deployed on a shard. The shard consists of a primary database node and several secondary database nodes. These nodes work together as a replica set.&nbsp;</p><p>The primary node handles all the write operations and replicates the data to the secondary nodes. If the primary node fails, one of the secondary nodes automatically takes over as the new primary, ensuring continuous operation and availability.&nbsp;</p><p>The diagram below shows a different representation of the database hierarchy</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd19ab03f-2b27-4da6-bcd1-5760ac535dba_2050x1194.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="848" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd19ab03f-2b27-4da6-bcd1-5760ac535dba_2050x1194.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://stripe.com/blog/how-stripes-document-databases-supported-99.999-uptime-with-zero-downtime-data-migrations">Stripe Engineering Blog</a></figcaption></figure></div><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7e87813-2921-4379-a1c9-c101092fda5d_1241x1600.png" title="" width="1241" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-relational-database">A Crash Course on Relational Database Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-distributed-systems">A Crash Course on Distributed Systems</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-scaling">A Crash Course in Database Scaling Strategies</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-sharding">A Crash Course in Database Sharding</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservice-communication">A Crash Course on Microservice Communication Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture">A Crash Course on Cell-based Architecture</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>The Data Movement Platform</h2><p>What&#8217;s the most important ability required to build a DBaaS platform that is horizontally scalable and highly elastic?</p><p>It&#8217;s the ability to migrate data across database shards with zero downtime and no impact on the client.&nbsp;</p><p>Stripe achieved this ability with their Data Movement Platform. The platform had a few important requirements such as:</p><ul><li><p>Ensure that the data getting migrated is consistent and complete across both the source and target shards.</p></li><li><p>Prevent a situation of prolonged downtime. Millions of businesses rely 24/7 on Stripe to accept payments from their customers.</p></li><li><p>Support the migration of an arbitrary number of chunks from any number of sources to target shards. Moreover, the migration should take place at a high throughput.</p></li><li><p>Prevent any performance impact on the source shard during the migration process.</p></li></ul><p>The diagram below shows the architecture of the Data Movement Platform:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e2f3b3e-67f8-4dac-b334-a72e1c142f23_1545x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1508" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e2f3b3e-67f8-4dac-b334-a72e1c142f23_1545x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The heart of the platform is the Coordinator component, which is responsible for orchestrating the various steps involved in online data migrations.</p><h3>Step 1: Chunk Migration Registration</h3><p>The first step is registering a request to migrate database chunks from source shards to target shards.</p><p>Once the request is created, indexes are built on the target shards.&nbsp;</p><p>An index is a data structure that improves the speed of data retrieval operations on a database table or collection. Building the index first on the target shard has some advantages:50</p><ul><li><p><strong>Query Performance: </strong>By creating indexes on the target shards before data migration, Stripe ensures that the target shards are ready to handle queries efficiently as soon as the data is available. Without pre-built indexes, queries on the newly migrated data would have to perform full collection scans, leading to slower query response times until the indexes are created.</p></li><li><p><strong>Consistency: </strong>Building indexes on the target shards before data migration helps maintain consistency between the source and target shards. If indexes were created after the data migration, it would result in inconsistent query behavior for some time.</p></li><li><p><strong>Seamless Transition: </strong>Having the indexes ready on the target shards minimizes the impact on the applications and users querying the data.&nbsp;</p></li></ul><h3>Step 2: Bulk Data Import</h3><p>The next step involves using a snapshot of the chunks on the source shard at a specific point in time. This snapshot is used to load the data onto one or more target database shards.</p><p>The service performing the bulk data import accepts data filters, allowing for the selective import of chunks that satisfy the specified filtering criteria. This step initially appeared straightforward. However, Stripe&#8217;s infra team encountered throughput limitations when they tried to bulk load data onto a DocDB shard.</p><p>Efforts to address the issue by batching writes and adjusting DocDB engine parameters were not successful.</p><p>A significant breakthrough came when the team explored methods to optimize the insertion order by using the fact that DocDB organizes its data using a B-tree data structure. By sorting the data based on the most common index attributes in the collections and inserting it in sorted order, the write proximity was enhanced, resulting in a 10X boost in write throughput.</p><h3>Step 3: Async Replication</h3><p>After the bulk data import step is completed, the next step ensures that any subsequent writes or mutations that occur on the source shard after time T are replicated to the target shard.</p><p>This is where async replication comes into play.</p><p>Stripe&#8217;s async replication systems rely on the Change Data Capture (CDC) mechanism to capture and replicate the mutations from the source shards to the target shards.&nbsp;</p><p>Here&#8217;s how it works:</p><ul><li><p><strong>Operations Log (Oplog):</strong> Each DocDB shard maintains a special collection called the Oplog, which records all the operations that modify data on the shard. Wherever a write operation occurs on the source shard, it is logged in the Oplog.</p></li><li><p><strong>Oplog Transport: </strong>Oplog entries from each DocDB shard are transported to Kafka which acts as a message broker, allowing Oplog events to be consumed by downstream systems. Additionally, these events are archived in a cloud object storage service like Amazon S3 for durability and long-term storage.</p></li><li><p><strong>Replication Service: </strong>Stripe built a dedicated service to handle the replication of mutations from the source shards to the target shards. This service consumes Oplog events from Kafka and S3 and applies the corresponding writes to the target shards. By relying on Oplog events from the CDC systems, the replication service avoids impacting the performance of user queries on the source shard. It doesn&#8217;t consume read throughput on the source shard, which would otherwise be available for serving user queries.</p></li><li><p><strong>Bidirectional Replication: </strong>Mutations are replicated bidirectionally, meaning that writes are replicated from the source shards to the target shards and vice versa. This is done to provide flexibility in case there is a need to revert traffic to the source shards if any issues arise when directing traffic to the target shards.</p></li></ul><h3>Step 4: Correctness Check</h3><p>After the replication sync between the source and target shard, the Coordinator conducts a comprehensive check for data completeness and correctness.</p><p>This is done by comparing point-in-time snapshots. It was a deliberate design choice to avoid impacting the shard&#8217;s throughput.</p><h3>Step 5: Traffic Switch</h3><p>The next step is to switch the traffic of incoming requests from the source shard to the target shard.</p><p>The Coordinator orchestrates the traffic switch after the data is imported to the target shard and the mutations are replicated. The process consists of three steps:</p><ul><li><p>Stop the traffic on the source shard for a brief period</p></li><li><p>Update the routes in the chunk metadata service</p></li><li><p>Make the proxy server redirect reads and writes to the target shards.</p></li></ul><p>The traffic switch protocol uses the concept of versioned gating.&nbsp;</p><p>To support this, the infra team added a custom patch to MongoDB that allows a shard to enforce a version number check before serving a request. Each proxy server annotates requests to the DocDB shard with a version token number. The shard first checks the version token number and serves the request only if the token number is newer than the earlier one.</p><p>The diagram below shows the detailed process flow for the traffic switch protocol:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77bbaf29-d872-4580-a3ae-63b3aa50ef38_1600x1023.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="931" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77bbaf29-d872-4580-a3ae-63b3aa50ef38_1600x1023.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Here&#8217;s how the process works:</p><ul><li><p>The version token number is stored in a document in a special collection in DocDB. They increase the token number on the source DocDB shard. This allows all reads and writes on the chunk on the source shard to be rejected.</p></li><li><p>Then, they wait for the replication service to replicate any outstanding writes on the source.</p></li><li><p>Once the replication is over, they update the route for the chunk to point to the target shard. Also, the version token number in the chunk metadata service is updated.</p></li><li><p>The proxy servers fetch the updated routes for the chunk along with the most up-to-date token number from the chunk metadata service. Using the updated routes, the proxy servers route the traffic to the target shard.</p></li></ul><p>The entire traffic switch protocol takes less than two seconds to execute. Any failed reads and writes to the source shard succeed on retries that go to the target shard.</p><h3>Step 6: Chunk Migration Deregistration</h3><p>Finally, the migration process is concluded by marking the migration as complete in the chunk metadata service.</p><p>Also, the chunk data is dropped from the source shard to reclaim the resources.</p><h2>Conclusion</h2><p>Stripe&#8217;s custom-built database-as-a-service, DocDb, and its Data Movement Platform have been instrumental in achieving 99.999% uptime while enabling zero-downtime data migrations.</p><p>Some key takeaways are as follows:</p><ul><li><p>DocDB extends the MongoDB Community edition to provide a highly available and scalable database solution.</p></li><li><p>Sharding is employed to horizontally scale the database, with data distributed across thousands of shards.</p></li><li><p>The Data Movement Platform enables online migrations across shards while ensuring data consistency, availability, and adaptability.</p></li><li><p>The six-step migration process consists of chunk migration registration, bulk data import, async replication, correctness checks, traffic switching, and chunk migration deregistration.</p></li><li><p>The Data Movement Platform is used for various purposes such as splitting DocDB shards for size and throughput, bin-packing underutilized databases, and upgrading the database infrastructure fleet.</p></li></ul><p><strong>References:</strong></p><ul><li><p><a href="https://stripe.com/blog/how-stripes-document-databases-supported-99.999-uptime-with-zero-downtime-data-migrations">How Stripe&#8217;s Document Databases Supported 99.999% uptime with zero downtime data migrations?</a></p></li><li><p><a href="https://en.wikipedia.org/wiki/Bin_packing_problem">Bin Packing Problem</a></p></li><li><p><a href="https://www.mongodb.com/resources/products/capabilities/sharding">What is MongoDB Sharding?</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p><p><br /><br /><br /><br /><br /><br /></p><p></p>
]]></content:encoded>
<pubDate>Tue, 23 Jul 2024 15:30:54 GMT</pubDate>
</item>
<item>
<title>EP121: 9 Essential Components of a Production Microservice Application</title>
<link>https://blog.bytebytego.com/p/ep121-9-essential-components-of-a</link>
<guid>https://blog.bytebytego.com/p/ep121-9-essential-components-of-a</guid>
<content:encoded><![CDATA[
<div> Linux Crash Course, Production Microservice Application, Design Patterns, Software Development Life Cycle, Sponsorship  
总结：<br /><br />本文介绍了Linux系统设计面试，并深入讲解了文件权限的理解。接着详细介绍了生产微服务应用的9个关键组件，包括API网关、服务注册表、服务层、授权服务器、数据存储、分布式缓存、异步微服务通信、度量可视化以及日志聚合和可视化。接着是软件开发生命周期的不同模型介绍，如瀑布模型、敏捷模型、V模型、迭代模型、螺旋模型等。最后介绍了设计模式的小抄，包括工厂模式、建造者模式、原型模式、单例模式等。最后，文章提及了赞助机会，可以将产品推广给100万以上的技术专业人士。 <div>
<p>This week&#8217;s system design interview:</p><ul><li><p>Linux Crash Course - Understanding File Permissions (Youtube video)</p></li><li><p>9 Essential Components of a Production Microservice Application </p></li><li><p>Iterative, Agile, Waterfall, Spiral Model, RAD Model... What are the differences?</p></li><li><p>Design Patterns Cheat Sheet - Part 1 and Part 2 </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/QAWolf_072024">&#9986;&#65039;Cut your QA cycles down to minutes with automated testing (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/QAWolf_072024" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="736" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F090fca3b-6480-4321-b93f-c3a80486dd1a_1473x745.png" title="" width="1456" /><div></div></div></a></figure></div><p>Are slow test cycles bottlenecking your dev teams&#8217; release velocity? With QA Wolf, your organization can run entire test suites in minutes for faster feedback and developer confidence to ship.</p><p><a href="https://bit.ly/QAWolf_072024">QA Wolf</a> takes testing off your plate. They can get you:</p><ul><li><p><a href="https://bit.ly/QAWolf_071324Head">80% automated test coverage in weeks</a>&#8212;not years</p></li><li><p><a href="https://bit.ly/QAWolf_072024">Unlimited parallel test runs</a></p></li><li><p>24-hour maintenance and on-demand test creation</p></li><li><p>Zero flakes, guaranteed</p></li></ul><p>The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.</p><p>With QA Wolf, <a href="https://bit.ly/QAWolf_072024CaseStudy">Drata&#8217;s team of 80+ engineers</a> achieved 4x more test cases and 86% faster QA cycles.</p><p><strong>&#127775;</strong>Rated 4.8/5 on G2</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/QAWolf_072024"><span>Schedule a demo to learn more</span></a></p><div><hr /></div><h2>Linux Crash Course - Understanding File Permissions</h2><div class="youtube-wrap" id="youtube2-4N4Q576i3zA"><div class="youtube-inner"></div></div><div><hr /></div><h2>9 Essential Components of a Production Microservice Application </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ccf7c0-51e8-4243-953d-37ccd28df10e_1329x1536.gif" title="diagram" width="1329" /><div></div></div></a></figure></div><ol><li><p>API Gateway <br />The gateway provides a unified entry point for client applications. It handles routing, filtering, and load balancing. <br /></p></li><li><p>Service Registry <br />The service registry contains the details of all the services. The gateway discovers the service using the registry. For example, Consul, Eureka, Zookeeper, etc. <br /></p></li><li><p>Service Layer <br />Each microservices serves a specific business function and can run on multiple instances. These services can be built using frameworks like Spring Boot, NestJS, etc. <br /></p></li><li><p>Authorization Server <br />Used to secure the microservices and manage identity and access control. Tools like Keycloak, Azure AD, and Okta can help over here. <br /></p></li><li><p>Data Storage <br />Databases like PostgreSQL and MySQL can store application data generated by the services. <br /></p></li><li><p>Distributed Caching <br />Caching is a great approach for boosting the application performance. Options include caching solutions like Redis, Couchbase, Memcached, etc. <br /></p></li><li><p>Async Microservices Communication <br />Use platforms such as Kafka and RabbitMQ to support async communication between microservices. <br /></p></li><li><p>Metrics Visualization <br />Microservices can be configured to publish metrics to Prometheus and tools like Grafana can help visualize the metrics. <br /></p></li><li><p>Log Aggregation and Visualization <br />Logs generated by the services are aggregated using Logstash, stored in Elasticsearch, and visualized with Kibana. <br /></p></li></ol><p>Over to you: What else would you add to your production microservice architecture?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7e87813-2921-4379-a1c9-c101092fda5d_1241x1600.png" width="1241" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-relational-database">A Crash Course on Relational Database Design</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-distributed-systems">A Crash Course on Distributed Systems</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-scaling">A Crash Course in Database Scaling Strategies</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-sharding">A Crash Course in Database Sharding</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservice-communication">A Crash Course on Microservice Communication Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture">A Crash Course on Cell-based Architecture</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>Iterative, Agile, Waterfall, Spiral Model, RAD Model... What are the differences?</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F853579e5-3dc8-4746-9da3-4f25d7be44a2_1514x1536.gif" title="No alternative text description for this image" /><div></div></div></a></figure></div><p>The Software Development Life Cycle (SDLC) is a framework that outlines the process of developing software in a systematic way. Here are some of the most common ones: </p><ol><li><p>Waterfall Model: <br />- A linear and sequential approach. <br />- Divides the project into distinct phases: Requirements, Design, Implementation, Verification, and Maintenance. <br /></p></li><li><p>Agile Model: <br />- Development is done in small, manageable increments called sprints. <br />- Common Agile methodologies include Scrum, Kanban, and Extreme Programming (XP). <br /></p></li><li><p>V-Model (Validation and Verification Model): <br />- An extension of the Waterfall model. <br />- Each development phase is associated with a testing phase, forming a V shape. <br /></p></li><li><p>Iterative Model: <br />- Focuses on building a system incrementally. <br />- Each iteration builds upon the previous one until the final product is achieved. <br /></p></li><li><p>Spiral Model: <br />- Combines iterative development with systematic aspects of the Waterfall model. <br />- Each cycle involves planning, risk analysis, engineering, and evaluation. <br /></p></li><li><p>Big Bang Model: <br />- All coding is done with minimal planning, and the entire software is integrated and tested at once. <br /></p></li><li><p>RAD Model (Rapid Application Development): <br />- Emphasizes rapid prototyping and quick feedback. <br />- Focuses on quick development and delivery. <br /></p></li><li><p>Incremental Model: <br />- The product is designed, implemented, and tested incrementally until the product is finished. </p></li></ol><p>Each of these models has its advantages and disadvantages, and the choice of which to use often depends on the specific requirements and constraints of the project at hand.</p><div><hr /></div><h2>Design Patterns Cheat Sheet - Part 1 and Part 2 </h2><p>The cheat sheet briefly explains each pattern and how to use it. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram" class="sizing-normal" height="2002" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e6d06c7-9947-4525-a5f0-3b92f7d76691_1280x2002.jpeg" title="diagram" width="1280" /><div></div></div></a></figure></div><p>What's included? </p><ul><li><p>Factory </p></li><li><p>Builder </p></li><li><p>Prototype </p></li><li><p>Singleton </p></li><li><p>Chain of Responsibility </p></li><li><p>And many more!</p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 1,000,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 20 Jul 2024 15:30:46 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Relational Database Design</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-relational-database</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-relational-database</guid>
<content:encoded><![CDATA[
<div> 关键词：关系数据库，数据管理，数据库设计，规范化，索引

总结:<br /><br />关系数据库在今天的数据驱动世界中扮演着至关重要的角色，它提供了一个结构化的框架，用于存储和检索数据，并基于实体之间定义的关系进行操作。有效的数据库设计是优化性能，确保数据完整性，促进高效数据检索的关键。数据库设计原则如规范化、索引、连接和关系，在创建结构良好、高效的数据库中扮演着重要的角色。通过掌握关系数据库的关键概念、管理系统以及有效数据库设计的原则，可以帮助企业和组织更好地利用数据获取有意义的见解。 <div>
<p>In today's data-driven world, efficient storage and management of information are critical requirements for businesses and organizations of all sizes.</p><p>Relational databases provide a robust framework for storing and retrieving data based on well-defined relationships between entities. They offer a structured approach to data management, enabling users to:</p><ul><li><p>Define tables&nbsp;</p></li><li><p>Establish relationships</p></li><li><p>Perform complex queries to extract meaningful insights from the stored information</p></li></ul><p>However, just using a relational database is not enough to gain its benefits.&nbsp;</p><p>Effective database design is crucial for optimizing performance, ensuring data integrity, and facilitating efficient data retrieval. The principles of database design, such as normalization, indexing, joins, and relationships, play a vital role in creating a well-structured and performant database.</p><p>In this post, we&#8217;ll look at the fundamentals of relational databases, exploring their key concepts, management systems, and the principles that underpin effective database design.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7e87813-2921-4379-a1c9-c101092fda5d_1241x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7e87813-2921-4379-a1c9-c101092fda5d_1241x1600.png" width="1241" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-relational-database">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 18 Jul 2024 15:31:12 GMT</pubDate>
</item>
<item>
<title>Where to get started with GenAI</title>
<link>https://blog.bytebytego.com/p/where-to-get-started-with-genai</link>
<guid>https://blog.bytebytego.com/p/where-to-get-started-with-genai</guid>
<content:encoded><![CDATA[
<div> Generative AI, Terminologies, Model APIs, Building Applications, RAG  
总结:  
理解人工智能术语，包括AI、机器学习、自然语言处理、深度学习等。学习使用模型API构建应用程序。建立AI模型，例如RAG和微调模型，以满足特定领域需求。探索GenAI应用程序，如内容创作、客户支持、商业金融和教育。集成LLM提供的外部信息源，通过RAG技术生成更准确的回应。通过微调模型，优化AI模型，提高在特定领域任务中的性能。 <div>
<h2>Introduction to Generative AI</h2><p>The world of Generative AI (GenAI) is moving at a breakneck pace.&nbsp;</p><p>New models, techniques, and applications emerge every day, pushing the boundaries of what's possible with artificial intelligence.&nbsp;</p><p>Considering this fast-evolving landscape, developers and technology professionals need to keep their skills sharp and stay ahead of the curve.</p><p>To help you get started with GenAI, <a href="https://www.linkedin.com/in/pvergadia/">Priyanka Vergadia</a> and I have put together a concise guide covering essential steps, including:</p><ul><li><p>Understanding the terminologies</p></li><li><p>Using the Model APIs</p></li><li><p>Building applications using the AI Models</p></li><li><p>Making models your own:</p><ul><li><p>RAG&nbsp;</p></li><li><p>Fine-Tuning AI models</p></li></ul></li></ul><p>Here&#8217;s a sneak peek at all the cool topics we will cover.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c132d99-646b-4593-913c-9f136388e088_1600x1493.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1359" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c132d99-646b-4593-913c-9f136388e088_1600x1493.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Let&#8217;s start with the first step.</p><p><em>Also, don't forget to follow <a href="https://www.linkedin.com/in/pvergadia/">Priyanka Vergadia&#8217;s LinkedIn</a>, which is a must-read for anyone working on cloud and GenAI.</em></p><div><hr /></div><h2>Understanding the GenAI Terminologies</h2><p>One of the biggest obstacles to getting started with GenAI is not understanding the basic terminologies.</p><p>Let&#8217;s cover the most important things to know about.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a64c9fb-afaf-46d7-aabc-ca34adb17649_1600x1200.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1092" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a64c9fb-afaf-46d7-aabc-ca34adb17649_1600x1200.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Artificial Intelligence</h3><p>AI refers to the development of computer systems that can perform tasks that typically require human intelligence. It is a discipline like Physics.</p><p>It encompasses various subfields, such as Machine Learning, Natural Language Processing, Computer Vision, etc.</p><p>AI systems can be narrow (focused on specific tasks) or general (able to perform a wide range of tasks).</p><h3>Machine Learning</h3><p>Machine Learning is a subset of AI that focuses on enabling computers to learn and improve from experience without being explicitly programmed.</p><p>It involves training models on data to recognize patterns, make predictions, or take actions. There are three main types of ML:</p><ul><li><p>Supervised Learning</p></li><li><p>Unsupervised Learning</p></li><li><p>Reinforcement Learning.</p></li></ul><p>Lastly, there is Deep Learning, which uses artificial neural networks and is a subfield of Machine Learning.</p><p>The diagram below shows the key difference between a typical machine learning workflow and Deep Learning.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5caed1b-24bb-4d93-9ebf-59b9725f6b1a_1600x1200.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1092" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5caed1b-24bb-4d93-9ebf-59b9725f6b1a_1600x1200.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Natural Language Processing (NLP)</h3><p>NLP is a subfield of AI that focuses on enabling computers to understand, interpret, and generate human language.</p><p>It involves tasks such as text classification, sentiment analysis, entity recognition, machine translation, and text generation.</p><p>Deep learning models, particularly Transformer models, have revolutionized NLP in recent years.</p><h3>Transformer Models</h3><p>Transformer models are a type of deep learning model architecture introduced in the famous paper &#8220;Attention is All You Need&#8221; in 2017.</p><p>They rely on self-attention mechanisms to process and generate sequential data, such as text.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9de37a4-8d3b-422f-b74e-09de5d9ee5b5_1600x1384.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1259" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9de37a4-8d3b-422f-b74e-09de5d9ee5b5_1600x1384.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Transformers have become the foundation for state-of-the-art models in NLP, such as BERT, GPT, and T5. They have also been adapted for other domains, like computer vision and audio processing.</p><h3>GenAI</h3><p>GenAI, short for Generative Artificial Intelligence, refers to AI systems that can generate new content, such as text, images, or music. It can be considered a subset of Deep Learning.</p><p>GenAI models can generate novel and coherent outputs that resemble the training data. They use machine learning models, particularly deep learning models, to learn patterns and representations from existing data.</p><p>NLP is a key area of focus within GenAI, as it deals with generating and understanding human language. Transformer models have become the backbone of many GenAI systems, particularly language models.&nbsp;</p><p>The ability of Transformers to learn rich representations and generate coherent text has made them well-suited for GenAI applications. For reference, a transformer model is a type of neural network that excels at understanding the context of sequential data, such as text or speech, and generating new data. It uses a mechanism called &#8220;attention&#8221; to weigh the importance of different parts of the input sequence and better understand the overall context.</p><p>There are various types of GenAI Models:</p><ol><li><p>Language models that specialize in processing and generating text data. Examples include Google&#8217;s Gemini, GPT-4, Claude Opus, Llama3</p></li><li><p>Multimodal Models that can handle multiple modalities, like text, images, and audio. Examples include DALL-E, Midjourney, Stable Diffusion.</p></li><li><p>Audio Models that can generate and process speech, music, and other audio data. Examples: Google&#8217;s Imagen, Wavenet.</p></li></ol><h3>Prompt Engineering</h3><p>Prompt engineering is the practice of designing effective prompts to get desired outputs from GenAI models. It involves understanding the model&#8217;s capabilities, limitations, and biases.&nbsp;</p><p>Effective prompts provide clear instructions, relevant examples, and context to guide the model&#8217;s output.</p><p>Prompt engineering is a crucial skill for getting the most out of GenAI models.</p><h2>Using the Model APIs</h2><p>Most Generative AI (GenAI) models are accessible through REST APIs, which allow developers to integrate these powerful models seamlessly into their applications.&nbsp;</p><p>To get started, you'll need to obtain API access from the desired platform, such as Google&#8217;s Vertex AI, OpenAI, Anthropic, or Hugging Face.&nbsp;</p><p>Each platform has its process for granting API access, typically involving&nbsp;</p><ul><li><p>Signing up for an account</p></li><li><p>Creating an API key</p></li><li><p>Completing a verification or approval process.</p></li></ul><p>Once you have your API key, you can authenticate your requests to the GenAI model endpoints.&nbsp;</p><p>Authentication usually involves providing the API key in the request headers or as a parameter. It's crucial to keep your API key secure and avoid sharing it publicly.</p><p>It&#8217;s also important to follow best practices to ensure reliability and efficiency. Here are a couple of important best practices:</p><ul><li><p>Handle API errors gracefully by checking the response status code.</p></li><li><p>Optimize API usage by carefully selecting the model parameters, such as the maximum number of tokens. This is necessary to balance the desired output quality with costs.</p></li><li><p>When making API requests, be mindful of the rate limits imposed by the platform. Rate limits determine the maximum number of requests you can make within a specific time frame. Exceeding the rate limits may result in API errors or temporary access restrictions.&nbsp;</p></li><li><p>Use frameworks and libraries like Langchain to simplify the API interactions. These frameworks offer high-level abstractions and utilities for working with GenAI model APIs.</p></li></ul><h2>Building Application using the AI Model</h2><p>There are several use cases for GenAI-powered applications across various domains:</p><ul><li><p><strong>Content Creation and Marketing:</strong> GenAI applications can help create outlines for articles, ad copy generation, and product descriptions.</p></li><li><p><strong>Customer Support: </strong>AI-powered chatbots can understand user queries and provide accurate, context-aware responses.</p></li><li><p><strong>Business and Finance: </strong>GenAI applications can help generate financial reports, summaries, or analyses based on company data.</p></li><li><p><strong>Education and Learning: </strong>GenAI applications can generate customized learning material and explanations based on a student&#8217;s learning style.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bd2b569-35e8-4965-8e35-8ec55480c2ee_1600x1384.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1259" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8bd2b569-35e8-4965-8e35-8ec55480c2ee_1600x1384.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Let&#8217;s say we want to build a chatbot application that uses an LLM to provide personalized book recommendations based on user preferences.</p><p>Here are the high-level steps involved.</p><h3>1 - Choose an LLM Provider</h3><p>Research and compare different LLM providers, such as Google AI, Open AI, or a Hugging Face.&nbsp;</p><p>Before choosing, you can consider multiple factors such as pricing, availability, API documentation, and community support.</p><h3>2 - Set up the Development Environment</h3><p>Typically, the LLM providers give access to their LLM via APIs.&nbsp;</p><p>You must sign up for an API key from the chosen provider and install the necessary libraries and frameworks.</p><p>For example, if you build your application using Python, you should set up a Python project and configure the API credentials according to the best practices.</p><h3>3 - Design the Chatbot Conversation Flow</h3><p>Plan out the conversation flow for the book recommendation chatbot. Define the key questions the chatbot will ask users to gather preferences, such as favorite genres, authors, or book themes.</p><p>Determine the structure and format of the chatbot&#8217;s responses, including the recommended books and any additional information to provide.</p><h3>4 - Implement the Chatbot Application</h3><p>Use a web framework like Flask or Django to build the chatbot application.&nbsp;</p><p>Create a user interface for the chatbot, either as a web page or a messaging interface. Implement the necessary routes and views to handle user interactions and generate chatbot responses.</p><h3>5 - Integrate the LLM</h3><p>Most LLM providers have released libraries to talk to their model APIs. Initialize the model with the appropriate parameters, such as the model name, version, and temperature.</p><p>Define the prompts and instructions for the LLM to generate personalized book recommendations based on user preferences.</p><p>For example, you can create prompts like: &#8220;Recommend a science fiction book for a user who enjoys fast-paced plots and space exploration.&#8221;</p><p>Pass the user&#8217;s preferences and the prompts to the LLM using the API and retrieve the generated book recommendations.</p><h3>6 - Process and Display the Recommendations</h3><p>Process the LLM-generated book recommendations to extract the relevant information, such as book titles, authors, and descriptions.</p><p>Display the recommended books in a clear and visually appealing format. Provide options for users to interact with the recommendations, such as saving them for later or requesting more details about a specific book.</p><h3>7 - Refine and Expand</h3><p>Test the chatbot application with various user preferences and prompts to ensure it generates relevant and diverse book recommendations.</p><p>Gather user feedback and iterate on the chatbot&#8217;s conversation flow, prompts, and recommendation formatting based on suggestions.</p><p>Integrate additional features, such as providing book reviews, suggesting similar authors, and so on, to expand the chatbot's capabilities.</p><h3>8 - Deploy and Monitor</h3><p>Deploy the chatbot application to a hosting platform or cloud service provider, making it accessible to users via a web URL.</p><p>Set up monitoring and analytics to track user interactions, chatbot performance, and any errors or issues.</p><p>Regularly update the LLM prompts and application logic based on user feedback and new book releases.</p><h2>Making Models Your Own</h2><p>There is significant interest in making models more adaptable and customizable to suit the specific needs of the domain.&nbsp;</p><p>Let&#8217;s look at the main techniques to achieve this goal.</p><h3>Retrieval-Augmented Generation (RAG)</h3><p>RAG is a technique that helps improve the accuracy and relevance of the generated responses based on your use case.&nbsp;</p><p>It allows your LLM to have external information sources like your databases, documents, and even the Internet in real time. This way the LLM can get the most up-to-date and relevant information to answer the queries specific to your business.</p><p>Here&#8217;s a high-level overview of how a RAG system works:</p><ul><li><p>The user poses a question to the RAG system.</p></li><li><p>The retrieval component searches the knowledge corpus using the question as a query and retrieves the most relevant passages or documents.</p></li><li><p>The retrieved passages go through the augmentation step where this information is fed as input to the large language model. This step is crucial as it augments the model&#8217;s knowledge with relevant context from external sources.</p></li><li><p>The language model processes the input and generates an answer by combining the information from the retrieved passages and its base knowledge.</p></li><li><p>The generated answer is returned to the user.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c1eac76-dd82-4059-8ba7-dd240c61135d_1600x998.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="908" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c1eac76-dd82-4059-8ba7-dd240c61135d_1600x998.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>RAG has shown promising results in improving the accuracy and relevance of generated responses, especially in scenarios where the answer requires synthesizing information from multiple sources. It leverages the strengths of both information retrieval and language generation to provide better answers.</p><h3>Fine-Tuning AI Models</h3><p>Fine-tuning a base model on domain-specific data is a powerful technique to improve the performance and accuracy of AI models for specific tasks or industries.</p><p>Let&#8217;s understand how it&#8217;s done.</p><h4>1 - Understanding Base Models</h4><p>Base models, also known as pre-trained models, are AI models that have been trained on large, general-purpose datasets.</p><p>These models have learned general knowledge and patterns from the training data, making them versatile and applicable to a wide range of tasks.</p><p>Examples of base models include Google&#8217;s BERT and GPT, which have been trained on massive amounts of text or image data.</p><h4>2 - The Need for Fine-Tuning</h4><p>While base models are powerful, they may not always perform optimally for specific domains or tasks.&nbsp;</p><p>The reasons for fine-tuning a foundation model are as follows:</p><ul><li><p>Adding a specific task (such as code generation or content generation) to the foundation model.</p></li><li><p>Generating responses based on your company&#8217;s proprietary dataset.</p></li><li><p>Adapting to the unique vocabularies, writing styles, or data distribution that might differ in your specific use case.</p></li><li><p>Reducing hallucination, which is output that is not factually correct or reasonable.</p></li></ul><p>Fine-tuning allows us to adapt the base model to better understand and generate content specific to a particular domain.</p><h4>3 - Fine-Tuning Process</h4><p>The fine-tuning process consists of several steps such as:</p><ul><li><p><strong>Data Preparation: </strong>Collect a dataset that is representative of the target domain or task while ensuring that it is of sufficient size and quality. Preprocess the data to match the input requirements of the base model.</p></li><li><p><strong>Model Initialization</strong>: Start with the pre-trained base model that is most suitable for the target task. Load the pre-trained weights of the base model.</p></li><li><p><strong>Training: </strong>Feed the domain-specific dataset into the modified base model and train the model using techniques like transfer learning. Fine-tune the model&#8217;s parameters by backpropagating the errors and updating the weights based on the domain-specific data.</p></li><li><p><strong>Evaluation and Iteration: </strong>Evaluate the fine-tuned model&#8217;s performance on a validation set from the domain-specific data. Based on the metrics, iterate on the fine-tuning process.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11dc86ba-d135-46f7-a4b0-c0ce14cd49a7_1600x1120.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1019" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11dc86ba-d135-46f7-a4b0-c0ce14cd49a7_1600x1120.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h4>4 - Benefits of Fine-Tuning</h4><p>There are significant benefits to fine-tuning:</p><ul><li><p>It allows the model to capture the nuances and characteristics of the target domain, leading to better accuracy and performance on domain-specific tasks.</p></li><li><p>Starting with a pre-trained base model, fine-tuning requires less training data and computational resources than training a model from scratch.</p></li><li><p>Fine-tuning enables the model to leverage the knowledge learned from the general-purpose training data and adapt it to the specific domain.</p></li></ul><h2>Conclusion</h2><p>In conclusion, getting started with Generative AI is an exciting journey that opens up a world of possibilities for developers and businesses alike.&nbsp;</p><p>By understanding the key concepts, exploring the available models and APIs, and following best practices, you can harness GenAI's power to build innovative applications and solve complex problems.</p><p>Whether you're interested in natural language processing, image generation, or audio synthesis, there are numerous GenAI models and platforms to choose from. You can create highly accurate and efficient AI solutions tailored to your specific needs by leveraging pre-trained models and fine-tuning them on domain-specific data.</p><p></p>
]]></content:encoded>
<pubDate>Tue, 16 Jul 2024 15:30:47 GMT</pubDate>
</item>
<item>
<title>EP120: What do version numbers mean?</title>
<link>https://blog.bytebytego.com/p/ep120-what-do-version-numbers-mean</link>
<guid>https://blog.bytebytego.com/p/ep120-what-do-version-numbers-mean</guid>
<content:encoded><![CDATA[
<div> 并发性，并行性，版本号，网络安全，Kubernetes
总结:<br /><br />本文介绍了并发性和并行性的区别，版本号的含义，网络安全的基础知识，以及Kubernetes容器编排系统的概述。版本号采用语义化版本控制，包括主版本号、次版本号和补丁号，用于表示软件更新的含义。网络安全涉及CIA三要素、常见威胁和基本防御机制，如防火墙、杀毒软件和加密等。Kubernetes是一个容器编排系统，由控制平面和节点组成，用于容器的部署和管理，具有高可用和容错性。控制平面包括API服务器、调度器、控制器管理器和etcd，节点包括Pods、Kubelet和Kube Proxy。这些信息涵盖了本文的主要内容。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Concurrency Vs Parallelism!  (Youtube video)</p></li><li><p>What do version numbers mean? </p></li><li><p>Looking for a Job? This Free AI Tool Can Get You More Interviews!</p></li><li><p>Cybersecurity 101 in one picture</p></li><li><p>What is k8s (Kubernetes)? </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/QAWolf_071324Head">&#9986;&#65039;Cut your QA cycles down to minutes with automated testing (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/QAWolf_071324Head" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="736" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F090fca3b-6480-4321-b93f-c3a80486dd1a_1473x745.png" width="1456" /><div></div></div></a></figure></div><p>Are slow test cycles bottlenecking your dev teams&#8217; release velocity? With QA Wolf, your organization can run entire test suites in minutes for faster feedback and developer confidence to ship.</p><p><a href="https://bit.ly/QAWolf_071324Head">QA Wolf</a> takes testing off your plate. They can get you:</p><ul><li><p><a href="https://bit.ly/QAWolf_071324Head">80% automated test coverage in weeks</a>&#8212;not years</p></li><li><p><a href="https://bit.ly/QAWolf_071324Head">Unlimited parallel test runs</a></p></li><li><p>24-hour maintenance and on-demand test creation</p></li><li><p>Zero flakes, guaranteed</p></li></ul><p>The benefit? No more manual E2E testing. No more slow QA cycles. No more bugs reaching production.</p><p>With QA Wolf, <a href="https://bit.ly/QAWolf_071624CaseStudy">Drata&#8217;s team of 80+ engineers</a> achieved 4x more test cases and 86% faster QA cycles.</p><p><strong>&#127775;</strong>Rated 4.8/5 on G2</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/QAWolf_071324Head"><span>Schedule a demo to learn more</span></a></p><div><hr /></div><h2>Concurrency Vs Parallelism!</h2><div class="youtube-wrap" id="youtube2-RlM9AfWf1WU"><div class="youtube-inner"></div></div><div><hr /></div><h2>What do version numbers mean? </h2><p>Semantic Versioning (SemVer) is a versioning scheme for software that aims to convey meaning about the underlying changes in a release. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram" class="sizing-normal" height="988" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefaeb917-d62e-4f04-b3bc-0644d50fa948_800x988.gif" title="diagram" width="800" /><div></div></div></a></figure></div><ul><li><p>SemVer uses a three-part version number: MAJOR.MINOR.PATCH. </p><ul><li><p>MAJOR version: Incremented when there are incompatible API changes. </p></li><li><p>MINOR version: Incremented when functionality is added in a backward-compatible manner. </p></li><li><p>PATCH version: Incremented when backward-compatible bug fixes are made. </p></li></ul></li><li><p>Example Workflow <br />1 - Initial Development Phase <br />Start with version 0.1.0. </p><p><br />2 - First Stable Release <br />Reach a stable release: 1.0.0. <br /><br />3 - Subsequent Changes <br />Patch Release: A bug fix is needed for 1.0.0. Update to 1.0.1. <br /><br />Minor Release: A new, backward-compatible feature is added to 1.0.3. Update to 1.1.0. <br /><br />Major Release: A significant change that is not backward-compatible is introduced in 1.2.2. Update to 2.0.0. <br /><br />4 - Special Versions and Pre-releases <br />Pre-release Versions: 1.0.0-alpha, 1.0.0-beta, 1.0.0-rc.1. <br />Build Metadata: 1.0.0+20130313144700. </p></li></ul><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1812" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50d6ff61-09a2-493d-84e1-59c2568fd611_2250x2800.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-distributed-systems">A Crash Course on Distributed Systems</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-scaling">A Crash Course in Database Scaling Strategies</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-sharding">A Crash Course in Database Sharding</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservice-communication">A Crash Course on Microservice Communication Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture">A Crash Course on Cell-based Architecture</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>Looking for a Job? This Free AI Tool Can Get You More Interviews!</h2><p>If you're looking for a job, you should check this out.</p><p>My good friend<a href="https://www.linkedin.com/in/ericcheng26/"> </a>Eric Cheng and his team have built a fantastic AI tool called Jobright.ai that makes your job search much easier.</p><p>Here are some great features I wish I had when I was job hunting</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://jobright.ai/?utm_source=1043&amp;utm_campaign=Alex" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="819" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ef17784-e58c-41d4-902f-9117cfcb72fa_1999x1125.png" width="1456" /><div></div></div></a></figure></div><p class="button-wrapper"><a class="button primary" href="https://jobright.ai/?utm_source=1043&amp;utm_campaign=Alex"><span>Try Jobright for Free</span></a></p><ul><li><p>Aggregates all jobs in one place (8 million in total / 400K new jobs daily)</p></li><li><p>Match you with the right opportunities where you are a standout candidate</p></li><li><p>Create customized resumes for each job in seconds.</p></li><li><p>Automatically suggest past colleagues or alumni who can help with referrals.</p></li><li><p>It's completely free right now</p></li></ul><div><hr /></div><h2>Cybersecurity 101 in one picture</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="Image preview" class="sizing-normal" height="800" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4cd9bbc-d7ec-4da1-87aa-577e43f4a5d2_800x800.gif" title="Image preview" width="800" /><div></div></div></a></figure></div><ol><li><p>Introduction to Cybersecurity</p></li><li><p>The CIA Triad</p></li><li><p>Common Cybersecurity Threats</p></li><li><p>Basic Defense Mechanisms<br />To combat these threats, several basic defense mechanisms are employed:</p><ul><li><p>Firewalls: Network security devices that monitor and control incoming and outgoing network traffic.</p></li><li><p>Antivirus Software: Programs designed to detect and remove malware.</p></li><li><p>Encryption: The process of converting information into a code to prevent unauthorized access.</p></li></ul></li><li><p>Cybersecurity Frameworks</p></li></ol><div><hr /></div><h2>What is k8s (Kubernetes)? </h2><p>k8s is a container orchestration system. It is used for container deployment and management. Its design is greatly impacted by Google&#8217;s internal system Borg. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1092" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9321e6a8-602a-4c4c-9fa0-10c5921c92b2_800x1092.gif" title="graphical user interface, application" width="800" /><div></div></div></a></figure></div><p>A k8s cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. </p><p>The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance and high availability. </p><ul><li><p>Control Plane Components <br /> <br />1. API Server <br />The API server talks to all the components in the k8s cluster. All the operations on pods are executed by talking to the API server. <br /> <br />2. Scheduler <br />The scheduler watches the workloads on pods and assigns loads on newly created pods. <br /> <br />3. Controller Manager <br />The controller manager runs the controllers, including Node Controller, Job Controller, EndpointSlice Controller, and ServiceAccount Controller. <br /> <br />4. etcd <br />etcd is a key-value store used as Kubernetes' backing store for all cluster data.\</p></li><li><p>Nodes </p><p><br />1. Pods <br />A pod is a group of containers and is the smallest unit that k8s administers. Pods have a single IP address applied to every container within the pod. <br /> <br />2. Kubelet <br />An agent that runs on each node in the cluster. It ensures containers are running in a Pod. <br /> <br />3. Kube Proxy <br />kube-proxy is a network proxy that runs on each node in your cluster. It routes traffic coming into a node from the service. It forwards requests for work to the correct containers.</p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 13 Jul 2024 15:30:35 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Distributed Systems</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-distributed-systems</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-distributed-systems</guid>
<content:encoded><![CDATA[
<div> 分布式系统、节点、网络、协调、失败
总结:<br /><br />分布式系统是由多个节点组成的计算机集合，通过网络协作执行特定任务或提供服务。这些节点在物理上分离，并通过网络传递消息进行通信。分布式系统具有统一的外部视角，用户无需关心系统内部结构。节点需要协调并就行一致操作，但也可能出现独立故障及网络消息丢失等问题。分布式系统具有很高的灵活性，可以不断加入或移除节点。通过多个计算机协同工作，分布式系统能提供流畅且响应迅速的用户体验。 <div>
<p>A distributed system is a collection of computers, also known as nodes, that collaborate to perform a specific task or provide a service.</p><p>These nodes are physically separate and communicate with each other by passing messages over a network. Distributed systems can span geographical boundaries, enabling them to utilize resources from different locations.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb145d610-0804-4a50-b74b-bf13cac6fb8f_1600x1005.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="915" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb145d610-0804-4a50-b74b-bf13cac6fb8f_1600x1005.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Distributed systems have several characteristics that distinguish them from traditional centralized systems:</p><ul><li><p>The computers in a distributed system are physically separate and connected via a network. They do not share a memory or a common clock.</p></li><li><p>From an external perspective, a distributed system appears as a single, unified entity to the end user.</p></li><li><p>Distributed systems offer the flexibility to add or remove computers from the system.</p></li><li><p>The nodes in a distributed system need to coordinate and agree with each other to perform actions consistently.</p></li><li><p>Nodes in a distributed system can fail independently, and messages can be lost or delayed over the network.</p></li></ul><p>Distributed systems are ubiquitous in our daily lives. Examples include large web applications like Google Search, online banking systems, multiplayer games, etc. These systems leverage the power of multiple computers working together to provide a seamless and responsive user experience.</p><p>In this post, we&#8217;ll explore the benefits and challenges of distributed systems. We will also discuss common approaches and techniques used to address these challenges and ensure the reliable operation of distributed systems.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50d6ff61-09a2-493d-84e1-59c2568fd611_2250x2800.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1812" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50d6ff61-09a2-493d-84e1-59c2568fd611_2250x2800.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><div><hr /></div><h2>Understanding Distributed Systems&nbsp;</h2><p>The term &#8220;distributed systems&#8221; can sometimes confuse developers.&nbsp;</p>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-distributed-systems">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 11 Jul 2024 15:30:43 GMT</pubDate>
</item>
<item>
<title>EP119: What do Amazon, Netflix, and Uber have in common?</title>
<link>https://blog.bytebytego.com/p/ep119-what-do-amazon-netflix-and</link>
<guid>https://blog.bytebytego.com/p/ep119-what-do-amazon-netflix-and</guid>
<content:encoded><![CDATA[
<div> scaling, strategies, database, testing, functionality
<br />
要点: 
- 介绍了8种必须掌握的系统扩展策略，包括无状态服务、水平扩展、负载均衡、自动扩展、缓存、数据库复制、数据库分片和异步处理；
- 讨论了Figma如何实现100倍Postgres扩展，包括垂直扩展、复制、垂直分区和水平分区；
- 探讨了测试系统功能的最佳方式，包括单元测试、集成测试、系统测试、负载测试、错误测试和测试自动化；
- 强调了New Relic和NVIDIA发布的观测集成，使公司能够监控构建在NVIDIA NIM上的AI应用程序的健康和性能；
- 引入了赞助机会，以让产品直接面向50万以上的技术专业人士。 

总结: 
本文介绍了系统设计中的必备策略，包括扩展策略、数据库扩展实例、系统功能测试方式、AI监控新特性以及赞助机会。文章通过具体案例和实践经验，为读者提供了丰富的系统设计知识和实用技巧。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>7 Must-know Strategies to Scale Your Database (Youtube video)</p></li><li><p>What do Amazon, Netflix, and Uber have in common?</p></li><li><p>100X Postgres Scaling at Figma</p></li><li><p>Best ways to test system functionality</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/NewRelic_070624">Monitor AI Applications Built with NVIDIA NIM (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/NewRelic_070624" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="675" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77f05e92-21dd-4034-b054-7fac7e375d6e_1200x675.png" width="1200" /><div></div></div></a></figure></div><p>New Relic and NVIDIA released the first observability integration making it easy for companies to monitor the health and performance of their AI applications built with NVIDIA NIM.</p><p>Key features and use cases for AI monitoring include:</p><ul><li><p>Full AI stack integration</p></li><li><p>Deep trace insights for every response&nbsp;</p></li><li><p>Model inventory&nbsp;</p></li><li><p>Deep GPU insights</p></li><li><p>Enhanced data security</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/NewRelic_070624"><span>Get started</span></a></p><div><hr /></div><h2>7 Must-know Strategies to Scale Your Database</h2><div class="youtube-wrap" id="youtube2-_1IKwnbscQU"><div class="youtube-inner"></div></div><div><hr /></div><h2>What do Amazon, Netflix, and Uber have in common?</h2><p>They are extremely good at scaling their system whenever needed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1683" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2497aa7-95ab-45b0-a8f7-0af03b663342_1280x1683.gif" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><p>Here are 8 must-know strategies to scale your system.</p><ol><li><p>Stateless Services<br />Design stateless services because they don&#8217;t rely on server-specific data and are easier to scale.</p></li><li><p>Horizontal Scaling<br />Add more servers so that the workload can be shared.</p></li><li><p>Load Balancing<br />Use a load balancer to distribute incoming requests evenly across multiple servers.</p></li><li><p>Auto Scaling<br />Implement auto-scaling policies to adjust resources based on real-time traffic.</p></li><li><p>Caching<br />Use caching to reduce the load on the database and handle repetitive requests at scale.</p></li><li><p>Database Replication<br />Replicate data across multiple nodes to scale the read operations while improving redundancy. </p></li><li><p>Database Sharding<br />Distribute data across multiple instances to scale the writes as well as reads.</p></li><li><p>Async Processing<br />Move time-consuming and resource-intensive tasks to background workers using async processing to scale out new requests.</p></li></ol><p>Over to you: Which other strategies have you used?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cff25a5-b061-4fd4-8831-fab148e8926b_1401x1600.png" width="1401" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-scaling">A Crash Course in Database Scaling Strategies</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-sharding">A Crash Course in Database Sharding</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservice-communication">A Crash Course on Microservice Communication Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture">A Crash Course on Cell-based Architecture</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-content-delivery">A Crash Course on Content-Delivery Networks (CDN)</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe?"><span>Subscribe now</span></a></p><div><hr /></div><h2>100X Postgres Scaling at Figma</h2><p>With 3 million monthly users, Figma&#8217;s user base has increased by 200% since 2018. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram" class="sizing-normal" height="1638" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110264f5-e3b5-40bd-85bf-d7975ff345f9_1280x1638.gif" title="diagram" width="1280" /><div></div></div></a></figure></div><p>As a result, its Postgres database witnessed a whopping 100X growth. </p><ol><li><p>Vertical Scaling and Replication <br />Figma used a single, large Amazon RDS database. <br /> <br />As a first step, they upgraded to the largest instance available (from r5.12xlarge to r5.24xlarge). <br /> <br />They also created multiple read replicas to scale read traffic and added PgBouncer as a connection pooler to limit the impact of a growing number of connections. <br /></p></li><li><p>Vertical Partitioning <br />The next step was vertical partitioning. <br /> <br />They migrated high-traffic tables like &#8220;Figma Files&#8221; and &#8220;Organizations&#8221; into their separate databases. <br /> <br />Multiple PgBouncer instances were used to manage the connections for these separate databases. <br /></p></li><li><p>Horizontal Partitioning <br />Over time, some tables crossed several terabytes of data and billions of rows. <br /> <br />Postgres Vacuum became an issue and max IOPS exceeded the limits of Amazon RDS at the time. <br /> <br />To solve this, Figma implemented horizontal partitioning by splitting large tables across multiple physical databases. <br /> <br />A new DBProxy service was built to handle routing and query execution. </p></li></ol><p>Over to you - Would you have done something differently?</p><div><hr /></div><h2>Best ways to test system functionality</h2><p>Testing system functionality is a crucial step in software development and engineering processes.<br /><br />It ensures that a system or software application performs as expected, meets user requirements, and operates reliably.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4886cb0-e840-4d4a-84c7-8d9543d98101_1280x1664.gif" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><p>Here we delve into the best ways:</p><ol><li><p>Unit Testing: Ensures individual code components work correctly in isolation.</p></li><li><p>Integration Testing: Verifies that different system parts function seamlessly together.</p></li><li><p>System Testing: Assesses the entire system's compliance with user requirements and performance.</p></li><li><p>Load Testing: Tests a system's ability to handle high workloads and identifies performance issues.</p></li><li><p>Error Testing: Evaluates how the software handles invalid inputs and error conditions.</p></li><li><p>Test Automation: Automates test case execution for efficiency, repeatability, and error reduction.</p></li></ol><p>Over to you: </p><ul><li><p>How do you approach testing system functionality in your software development or engineering projects?</p></li><li><p>What's your company's release process look like?</p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 06 Jul 2024 15:30:46 GMT</pubDate>
</item>
<item>
<title>A Crash Course in Database Scaling Strategies</title>
<link>https://blog.bytebytego.com/p/a-crash-course-in-database-scaling</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-in-database-scaling</guid>
<content:encoded><![CDATA[
<div> 数据库、应用开发、数据库扩展、性能优化、索引<br />
<br />总结：<br />数据库是现代应用开发的重要组成部分，通过索引等技术来改进数据库的可扩展性和性能。数据库扩展是通过采用各种技术和策略来适应数据增长和维持性能，以确保应用程序的正常运行。选择合适的数据库扩展策略至关重要，否则可能会带来更多问题。 <div>
<p>Databases form the backbone of modern application development. They play a vital role in storing, managing, and retrieving data, enabling applications and services to function effectively.</p><p>As applications gain popularity and attract a growing user base, databases face the challenge of handling ever-increasing data volumes, concurrent users, and complex queries.</p><p>It becomes critical to scale databases effectively to ensure optimal performance and a good user experience.&nbsp;</p><p>Database scaling is the process of adapting and expanding the database infrastructure to accommodate growth and maintain performance under increased load. It involves employing various techniques and strategies to distribute data efficiently, optimize query execution, and utilize hardware resources judiciously.</p><p>Organizations and developers must understand and implement the right database scaling strategy. Choosing the wrong strategies for a particular situation can result in more harm than good.</p><p>In this post, we will cover the most popular database scaling strategies in detail, discussing their benefits and trade-offs.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cff25a5-b061-4fd4-8831-fab148e8926b_1401x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cff25a5-b061-4fd4-8831-fab148e8926b_1401x1600.png" width="1401" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><div><hr /></div><h2>Indexing</h2><p>Indexing is one of the foundational techniques to enhance the scalability and performance of databases.&nbsp;</p>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-in-database-scaling">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 04 Jul 2024 15:31:12 GMT</pubDate>
</item>
<item>
<title>EP118: What are the differences among database locks?</title>
<link>https://blog.bytebytego.com/p/ep118-what-are-the-differences-among</link>
<guid>https://blog.bytebytego.com/p/ep118-what-are-the-differences-among</guid>
<content:encoded><![CDATA[
<div> 数据库锁、API设计中的分页、架构模式（MVC、MVP、MVVM、MVVM-C、VIPER）、浏览器输入URL的过程、扫描二维码支付

总结:<br /><br />
数据库锁是用来保证数据完整性和一致性的机制，常见类型包括Shared Lock、Exclusive Lock等。分页在API设计中是为了高效处理大型数据集，常用技术有Offset-based Pagination、Cursor-based Pagination等。架构模式MVC、MVP、MVVM、MVVM-C、VIPER各有特点，都包含视图、模型和控制器/Presenter/ViewModel。浏览器输入URL时，经过DNS查询、建立TCP连接、发送HTTP请求等步骤。扫描二维码支付过程分为商家生成QR码和消费者扫描支付。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>What are the differences among database locks? </p></li><li><p>How do we Perform Pagination in API Design? </p></li><li><p>What distinguishes MVC, MVP, MVVM, MVVM-C, and VIPER architecture patterns from each other?</p></li><li><p>What happens when you type a URL into your browser?</p></li><li><p>How do you pay from your digital wallet, such as Paypal, Venmo, Paytm, by scanning the QR code? </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/Paragon_062924">Ship every native SaaS integration your users need (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Paragon_062924" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="756" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d88922-60bc-483c-86ca-b5916742f99a_7747x4021.png" width="1456" /><div></div></div></a></figure></div><p>Is product asking your team to build native product integrations with 3rd party services (ie. Salesforce, Slack, etc.)?</p><p>Save 70% of the engineering effort with Paragon, so you can stay focused on your core competencies.</p><p>Offload the plumbing around integrations with:</p><ul><li><p>Fully managed authentication (OAuth token refresh)</p></li><li><p>Webhook triggers &amp; API abstractions</p></li><li><p>Built-in error and 3rd party rate limit handling</p></li><li><p>100+ pre-built integrations &amp; custom integration builder</p></li><li><p>Enterprise-ready serverless infrastructure</p></li></ul><p>See how 100+ SaaS companies orchestrate ingestion jobs, real-time sync, and event-driven automations with 3rd party SaaS apps in weeks, not months.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Paragon_062924"><span>Learn more</span></a></p><div><hr /></div><h2>What are the differences among database locks? </h2><p>In database management, locks are mechanisms that prevent concurrent access to data to ensure data integrity and consistency. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c5f5c6-b7d8-46ac-895f-06edd3d7fb7a_1280x1664.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><p>Here are the common types of locks used in databases: </p><ol><li><p>Shared Lock (S Lock) <br />It allows multiple transactions to read a resource simultaneously but not modify it. Other transactions can also acquire a shared lock on the same resource. <br /></p></li><li><p>Exclusive Lock (X Lock) <br />It allows a transaction to both read and modify a resource. No other transaction can acquire any type of lock on the same resource while an exclusive lock is held. <br /></p></li><li><p>Update Lock (U Lock) <br />It is used to prevent a deadlock scenario when a transaction intends to update a resource. <br /></p></li><li><p>Schema Lock <br />It is used to protect the structure of database objects. <br /></p></li><li><p>Bulk Update Lock (BU Lock) <br />It is used during bulk insert operations to improve performance by reducing the number of locks required. <br /></p></li><li><p>Key-Range Lock <br />It is used in indexed data to prevent phantom reads (inserting new rows into a range that a transaction has already read). <br /></p></li><li><p>Row-Level Lock <br />It locks a specific row in a table, allowing other rows to be accessed concurrently. <br /></p></li><li><p>Page-Level Lock <br />It locks a specific page (a fixed-size block of data) in the database. <br /></p></li><li><p>Table-Level Lock <br />It locks an entire table. This is simple to implement but can reduce concurrency significantly.</p></li></ol><div><hr /></div><h2>How do we Perform Pagination in API Design? </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ec91193-f890-4942-bb23-21f706d9524a_1306x1536.gif" title="No alternative text description for this image" /><div></div></div></a></figure></div><p>Pagination is crucial in API design to handle large datasets efficiently and improve performance. Here are six popular pagination techniques: </p><ul><li><p>Offset-based Pagination: <br />This technique uses an offset and a limit parameter to define the starting point and the number of records to return. <br />- Example: GET /orders?offset=0&amp;limit=3 <br />- Pros: Simple to implement and understand. <br />- Cons: Can become inefficient for large offsets, as it requires scanning and skipping rows. <br /></p></li><li><p>Cursor-based Pagination: <br />This technique uses a cursor (a unique identifier) to mark the position in the dataset. Typically, the cursor is an encoded string that points to a specific record. </p><ul><li><p>Example: GET /orders?cursor=xxx <br />- Pros: More efficient for large datasets, as it doesn't require scanning skipped records. <br />- Cons: Slightly more complex to implement and understand. <br /></p></li></ul></li><li><p>Page-based Pagination: <br />This technique specifies the page number and the size of each page. </p><ul><li><p>Example: GET /items?page=2&amp;size=3 <br />- Pros: Easy to implement and use. <br />- Cons: Similar performance issues as offset-based pagination for large page numbers. <br /></p></li></ul></li><li><p>Keyset-based Pagination: <br />This technique uses a key to filter the dataset, often the primary key or another indexed column. </p><ul><li><p>Example: GET /items?after_id=102&amp;limit=3 <br />- Pros: Efficient for large datasets and avoids performance issues with large offsets. <br />- Cons: Requires a unique and indexed key, and can be complex to implement. <br /></p></li></ul></li><li><p>Time-based Pagination: <br />This technique uses a timestamp or date to paginate through records.</p><ul><li><p>Example: GET /items?start_time=xxx&amp;end_time=yyy <br />- Pros: Useful for datasets ordered by time, ensures no records are missed if new ones are added. <br />- Cons: Requires a reliable and consistent timestamp. <br /></p></li></ul></li><li><p>Hybrid Pagination: <br />This technique combines multiple pagination techniques to leverage their strengths. <br />Example: Combining cursor and time-based pagination for efficient scrolling through time-ordered records. </p><ul><li><p>Example: GET /items?cursor=abc&amp;start_time=xxx&amp;end_time=yyy <br />- Pros: Can offer the best performance and flexibility for complex datasets. <br />- Cons: More complex to implement and requires careful design.</p></li></ul></li></ul><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F571827ac-b3da-44a3-886a-8ca9770bb443_1337x1600.png" width="1337" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-database-sharding">A Crash Course in Database Sharding</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservice-communication">A Crash Course on Microservice Communication Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture">A Crash Course on Cell-based Architecture</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-content-delivery">A Crash Course on Content-Delivery Networks (CDN)</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-rest-apis">A Crash Course on REST APIs</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>What distinguishes MVC, MVP, MVVM, MVVM-C, and VIPER architecture patterns from each other?<br /></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1755" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c35aada-289b-4ac5-8c23-8b55065a5d60_1280x1755.jpeg" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><p>These architecture patterns are among the most commonly used in app development, whether on iOS or Android platforms. Developers have introduced them to overcome the limitations of earlier patterns. So, how do they differ?</p><ul><li><p>MVC, the oldest pattern, dates back almost 50 years</p></li><li><p>Every pattern has a "view" (V) responsible for displaying content and receiving user input</p></li><li><p>Most patterns include a "model" (M) to manage business data</p></li><li><p>"Controller," "presenter," and "view-model" are translators that mediate between the view and the model ("entity" in the VIPER pattern)</p></li><li><p>These translators can be quite complex to write, so various patterns have been proposed to make them more maintainable</p></li></ul><div><hr /></div><h2>What happens when you type a URL into your browser?</h2><p>The diagram below illustrates the steps.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1178" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cf6a7cb-3aec-4db8-9bdc-fda0dc70c9fe_1898x1536.jpeg" title="No alt text provided for this image" width="1456" /><div></div></div></a></figure></div><ol><li><p>Bob enters a URL into the browser and hits Enter. In this example, the URL is composed of 4 parts:<br />&#128313; scheme - &#119945;&#119957;&#119957;&#119953;://. This tells the browser to send a connection to the server using HTTP.<br />&#128313; domain - &#119942;&#119961;&#119938;&#119950;&#119953;&#119949;&#119942;.&#119940;&#119952;&#119950;. This is the domain name of the site.<br />&#128313; path - &#119953;&#119955;&#119952;&#119941;&#119958;&#119940;&#119957;/&#119942;&#119949;&#119942;&#119940;&#119957;&#119955;&#119946;&#119940;. It is the path on the server to the requested resource: phone.<br />&#128313; resource - &#119953;&#119945;&#119952;&#119951;&#119942;. It is the name of the resource Bob wants to visit.</p><p></p></li><li><p>The browser looks up the IP address for the domain with a domain name system (DNS) lookup. To make the lookup process fast, data is cached at different layers: browser cache, OS cache, local network cache, and ISP cache. <br /><br />2.1 If the IP address cannot be found at any of the caches, the browser goes to DNS servers to do a recursive DNS lookup until the IP address is found (this will be covered in another post). </p><p></p></li><li><p>Now that we have the IP address of the server, the browser establishes a TCP connection with the server.<br /></p></li><li><p>The browser sends an HTTP request to the server. The request looks like this:<br /><br />&#120334;&#120332;&#120347; /&#120369;&#120361;&#120368;&#120367;&#120358; &#120335;&#120347;&#120347;&#120343;/1.1<br />&#120335;&#120368;&#120372;&#120373;: &#120358;&#120377;&#120354;&#120366;&#120369;&#120365;&#120358;.&#120356;&#120368;&#120366;</p><p></p></li><li><p>The server processes the request and sends back the response. For a successful response (the status code is 200). The HTML response might look like this: <br /><br />&#120335;&#120347;&#120347;&#120343;/1.1 200 &#120342;&#120338;<br />&#120331;&#120354;&#120373;&#120358;: &#120346;&#120374;&#120367;, 30 &#120337;&#120354;&#120367; 2022 00:01:01 &#120334;&#120340;&#120347;<br />&#120346;&#120358;&#120371;&#120375;&#120358;&#120371;: &#120328;&#120369;&#120354;&#120356;&#120361;&#120358;<br />&#120330;&#120368;&#120367;&#120373;&#120358;&#120367;&#120373;-&#120347;&#120378;&#120369;&#120358;: &#120373;&#120358;&#120377;&#120373;/&#120361;&#120373;&#120366;&#120365;; &#120356;&#120361;&#120354;&#120371;&#120372;&#120358;&#120373;=&#120374;&#120373;&#120359;-8<br /><br />&lt;!&#120331;&#120342;&#120330;&#120347;&#120352;&#120343;&#120332; &#120361;&#120373;&#120366;&#120365;&gt;<br />&lt;&#120361;&#120373;&#120366;&#120365; &#120365;&#120354;&#120367;&#120360;="&#120358;&#120367;"&gt;<br />&#120335;&#120358;&#120365;&#120365;&#120368; &#120376;&#120368;&#120371;&#120365;&#120357;<br />&lt;/&#120361;&#120373;&#120366;&#120365;&gt;<br /></p></li><li><p>The browser renders the HTML content.</p></li></ol><div><hr /></div><h2>How do you pay from your digital wallet, such as Paypal, Venmo, Paytm, by scanning the QR code? </h2><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31897073-e7f9-422a-9dbf-2f6dc18e6a6d_1280x1780.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram" class="sizing-normal" height="1780" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31897073-e7f9-422a-9dbf-2f6dc18e6a6d_1280x1780.jpeg" title="diagram" width="1280" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>To understand the process involved, we need to divide the &#8220;scan to pay&#8221; process into two sub-processes: </p><ol><li><p>Merchant generates a QR code and displays it on the screen </p></li><li><p>Consumer scans the QR code and pays </p></li></ol><p>Here are the steps for generating the QR code: </p><ol><li><p>When you want to pay for your shopping, the cashier tallies up all the goods and calculates the total amount due, for example, $123.45. The checkout has an order ID of SN129803. The cashier clicks the &#8220;checkout&#8221; button. </p></li><li><p>The cashier&#8217;s computer sends the order ID and the amount to PSP. </p></li><li><p>The PSP saves this information to the database and generates a QR code URL. </p></li><li><p>PSP&#8217;s Payment Gateway service reads the QR code URL. </p></li><li><p>The payment gateway returns the QR code URL to the merchant&#8217;s computer. </p></li><li><p>The merchant&#8217;s computer sends the QR code URL (or image) to the checkout counter. </p></li><li><p>The checkout counter displays the QR code. </p></li></ol><p>These 7 steps complete in less than a second. Now it&#8217;s the consumer&#8217;s turn to pay from their digital wallet by scanning the QR code: </p><ol><li><p>The consumer opens their digital wallet app to scan the QR code. </p></li><li><p>After confirming the amount is correct, the client clicks the &#8220;pay&#8221; button. </p></li><li><p>The digital wallet App notifies the PSP that the consumer has paid the given QR code. </p></li><li><p>The PSP payment gateway marks this QR code as paid and returns a success message to the consumer&#8217;s digital wallet App. </p></li><li><p>The PSP payment gateway notifies the merchant that the consumer has paid the given QR code. </p></li></ol><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong><br /></p>
]]></content:encoded>
<pubDate>Sat, 29 Jun 2024 15:30:59 GMT</pubDate>
</item>
<item>
<title>A Crash Course in Database Sharding</title>
<link>https://blog.bytebytego.com/p/a-crash-course-in-database-sharding</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-in-database-sharding</guid>
<content:encoded><![CDATA[
<div> 数据库，横向扩展，性能，挑战，分片
<br />
分片是一种解决数据库横向扩展挑战的技术，将数据库划分为更小、更易管理的单元称为分片。当应用程序接收到更多流量和数据时，数据库可能成为性能瓶颈，影响用户体验。通过分片，可以解决数据库横向扩展的挑战，提高性能，确保应用程序的可伸缩性。不同的公司已经成功实施了分片技术来扩展他们的数据库，有效应对业务增长的需求。
<br /><br />总结: 数据库的横向扩展面临着性能挑战，分片技术能够有效解决这一问题，提升性能，并确保应用程序的可伸缩性。通过对数据库进行分片，可以更好地应对增加的流量和数据量，确保用户体验。 <div>
<p>As an application grows in popularity, it attracts more active users and incorporates additional features. This growth leads to a daily increase in data generation, which is a positive indicator from a business perspective.&nbsp;</p><p>However, it can also pose challenges to the application's architecture, particularly in terms of database scalability.</p><p>The database is a critical component of any application, but it is also one of the most difficult components to scale horizontally. When an application receives increased traffic and data volume, the database can become a performance bottleneck, impacting the user experience.</p><p>Sharding is a technique that addresses the challenges of horizontal database scaling. It involves partitioning the database into smaller, more manageable units called shards.</p><p>In this post, we&#8217;ll cover the fundamentals of database sharding, exploring its various approaches, technical considerations, and real-world case studies showcasing how companies have implemented sharding to scale their databases.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F571827ac-b3da-44a3-886a-8ca9770bb443_1337x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F571827ac-b3da-44a3-886a-8ca9770bb443_1337x1600.png" width="1337" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>What is Sharding?</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-in-database-sharding">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 27 Jun 2024 15:30:37 GMT</pubDate>
</item>
<item>
<title>How Netflix Manages 238 Million Memberships?</title>
<link>https://blog.bytebytego.com/p/how-netflix-manages-238-million-memberships</link>
<guid>https://blog.bytebytego.com/p/how-netflix-manages-238-million-memberships</guid>
<content:encoded><![CDATA[
<div> 关键词: DevOps, Netflix, membership platform, architecture, technical stack

总结:<br /><br />文章介绍了Netflix的会员平台架构及技术堆栈。该平台负责管理用户的订阅生命周期，经历从简单的库到可处理百万请求的强大架构的演变。采用微服务架构，使用CockroachDB和Cassandra等数据库存储会员数据，通过CDC追踪历史数据变更。开发堆栈中使用Java和Spring Boot，涉及Kafka、Spark和Flink等工具，保证高可用性和数据一致性。运维方面强调可观测性和监控，使用日志、仪表盘、分布式跟踪等工具快速识别和解决问题。总体而言，Netflix的会员平台是其成功的关键组成部分，为亿万用户提供持续流畅的观影体验。 <div>
<h2><a href="https://bit.ly/Datadog_062524">Guide to Accelerating DevOps Transformation (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Datadog_062524" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1356" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F25b41cc2-06b7-4572-8996-94c1353483f2_2482x2312.png" width="1456" /><div></div></div></a></figure></div><p>The DevOps model can engender faster development cycles and enhanced agility in responding to market needs. <br /><br />Datadog's DevOps Kit gives you the resources to create and strengthen cultures of observability, collaboration, and data sharing within organizations&#8212;key pillars of the DevOps movement.</p><p><strong>Gain instant access to:</strong></p><ul><li><p><strong>2 eBooks</strong> detailing methods for building and enabling effective development and operations teams</p></li><li><p><strong>A Solutions Brief</strong> describing ways to get full visibility into your DevOps tools</p></li><li><p><strong>A Technical Talk Video</strong> detailing the benefits of adopting a data-driven DevOps mindset</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Datadog_062524"><span>Get the kit</span></a></p></li></ul><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the articles/presentations made by the Netflix engineering team. All credit for the architectural details goes to the Netflix engineering team. The links to the original articles are present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>As a subscription-based streaming service, Netflix's primary revenue source is its membership business. With a staggering 238 million members worldwide, managing memberships efficiently is crucial for the company's success and continued growth.&nbsp;</p><p>The membership platform at Netflix plays a vital role in handling the entire lifecycle of a user's subscription.</p><p>The membership lifecycle consists of various stages and scenarios:</p><ul><li><p><strong>Signup: </strong>A user may begin their Netflix journey by signing up for the service, either directly or through partner channels such as T-Mobile and others.</p></li><li><p><strong>Plan Changes:</strong> Existing members can modify their subscription plans according to their preferences and needs.</p></li><li><p><strong>Renewal: </strong>As a subscription service, Netflix automatically attempts to renew a user&#8217;s plan using the payment method associated with the account.</p></li><li><p><strong>Payment Issues: </strong>In case of problems with the payment gateway or insufficient funds, a user&#8217;s account may be put on hold or granted a grace period to resolve the issue.</p></li><li><p><strong>Membership Pause or Cancellation: </strong>Users have the option to temporarily pause their membership or permanently cancel their subscriptions.</p></li></ul><p>In the following sections, we will explore the architectural decisions made by the Netflix engineering team to support the various capabilities and scalability of its membership platform.</p><div><hr /></div><h2>The High-Level Architecture of Netflix Membership Platform</h2><p>Before diving into the details of Netflix's membership platform, let's take a step back and examine how the company's original pricing architecture was designed.&nbsp;</p><p>In the early days, Netflix's pricing model was relatively straightforward, with only a handful of plans to manage and basic functionality to support.</p><p>To meet these initial requirements, Netflix employed a lightweight, in-memory library. This approach proved to be quite efficient, as the limited scope of the pricing system allowed for a simple and streamlined design.</p><p>The diagram below illustrates this basic architecture:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37775e14-0fb5-4edf-a82a-33c6d77fb195_1600x916.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="834" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37775e14-0fb5-4edf-a82a-33c6d77fb195_1600x916.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>As Netflix expanded its global presence and diversified its offerings, the lightweight, in-memory library that initially served the pricing architecture became insufficient.&nbsp;</p><p>The growing complexity and scope of the pricing catalog, coupled with its increasing importance across multiple applications, led to operational challenges. The library's size and dependencies made it difficult to maintain and scale, necessitating a transition to a more robust and scalable architecture.</p><p>The diagram below shows the high-level architecture of Netflix&#8217;s modern membership platform:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38f27d43-db0e-4f28-b763-21b2260f5fa0_1600x1060.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="965" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38f27d43-db0e-4f28-b763-21b2260f5fa0_1600x1060.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The membership platform consists of a dozen microservices and is designed to support four nines (99.99%) availability.&nbsp;</p><p>This high availability requirement originates from the platform's critical role in various user-facing flows. If any of the services experience downtime, it can directly impact the user experience.</p><p>The platform supports several key functionalities:</p><ul><li><p>When a user hits the play button on Netflix, a direct call is made to the membership systems to determine the quality of service associated with their plan. Factors such as the allowed concurrent streams and supported devices for the user are considered. This flow handles the highest traffic volume, as Netflix serves billions of streaming requests every day.</p></li><li><p>The membership flow is triggered when a user accesses their account page. Actions like changing plans, managing extra members, and cancellations directly interact with the membership services.</p></li><li><p>The platform serves as the authoritative source for the total membership count at any given point in time. It emits events and writes to a persistent store, which is consumed by downstream analytics systems within and outside Netflix.</p></li></ul><p>The key points about the architecture diagram are as follows:</p><ul><li><p>The membership platform manages the membership plan and pricing catalog globally, with variations across different regions. The Plan Pricing Catalog Service handles rule management based on location-specific offerings.</p></li><li><p>Two CockroachDB databases are utilized to store plan pricing and code redemption information. The Member Pricing Service supports member actions, such as changing plans or adding extra members.</p></li><li><p>A dedicated microservice handles partner interactions, including bundle activations, signups, and integration with platforms like Apple's App Store.</p></li><li><p>Membership data is stored in Cassandra databases, which support the Subscription Service and History Tracking Service.</p></li><li><p>The platform not only caters to the 238 million active memberships but also focuses on former members and rejoin experiences.</p></li><li><p>The data generated by the platform is shipped to downstream consumers for deriving insights on signups and revenue projections.</p></li></ul><p>Netflix&#8217;s choice of using CockroachDB and Cassandra is interesting.&nbsp;</p><p>While CockroachDB provides strong consistency, making it suitable for critical data such as the plan pricing information, Cassandra is a highly scalable NoSQL database for handling large volumes of membership data.</p><p>Also, the 99.99% availability indicates a strong focus on resilience and fault tolerance. Netflix is anyways famous for its comprehensive chaos engineering practices to proactively test the system&#8217;s resilience.</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffeb22a8b-a5fc-455d-b09b-f8e810d81cd9_1600x890.pnghttps%3A%2F%2Fblog.bytebytego.com%2Fsubscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="810" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffeb22a8b-a5fc-455d-b09b-f8e810d81cd9_1600x890.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservice-communication">A Crash Course on Microservice Communication Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture">A Crash Course on Cell-based Architecture</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-content-delivery">A Crash Course on Content-Delivery Networks (CDN)</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-rest-apis">A Crash Course on REST APIs</a></p></li><li><p><a href="https://blog.bytebytego.com/p/api-security-best-practices">API Security Best Practices</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>The Signup Process Flow</h2><p>Once users embark on their Netflix journey, they encounter plan selection options.</p><p>Rendering the plan selection page accurately is of utmost importance due to the geographical variations in currency, pricing, and available plans. Netflix's membership platform ensures that users are presented with the appropriate options based on their location and device type.</p><p>The diagram below shows the detailed steps involved in the Netflix signup process and the services that are triggered during the flow:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F721d4ccf-6623-4484-a0d8-7872e1cbe8e3_1600x1284.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1168" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F721d4ccf-6623-4484-a0d8-7872e1cbe8e3_1600x1284.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Here&#8217;s a look at each step in more detail.</p><ol><li><p>The journey begins with users selecting a plan through Netflix's growth engineering apps. The plan details are retrieved from the Membership Plan Catalog service, which is backed by CockroachDB.&nbsp;</p></li><li><p>The Membership Plan Catalog Service loads and reads the plans based on predefined region and device type rules.</p></li><li><p>The retrieved plans are then presented to the user, allowing them to make an informed decision based on their preferences and budget.</p></li><li><p>Once the user chooses a plan, the flow progresses to the payment confirmation screen. Here, users provide their payment details and confirm their subscription.</p></li><li><p>Upon confirmation, the user clicks the "Start Membership" button, triggering the Membership State Service. This service persists the relevant information, such as the selected plan, price tier, and country, into the Cassandra database.</p></li><li><p>The Membership State Service also notifies the Billing Engineering Apps about the payment.&nbsp;</p></li><li><p>The Billing Engineering Apps generate an invoice based on the signup data obtained from the Membership Pricing Service.</p></li><li><p>The membership data is simultaneously written to the Membership History Service, ensuring a comprehensive record of the user's subscription history.</p></li><li><p>Events are published to signal the activation of the membership. These events trigger messaging pipelines responsible for sending welcome emails to the user and informing downstream systems for analytics purposes.</p></li></ol><h2>How Member History Is Tracked?</h2><p>In the early stages of Netflix's membership platform, member history and data were tracked through application-level events.&nbsp;</p><p>While this approach sufficed initially, it became evident that a more granular and persistent data tracking solution was necessary as Netflix expanded and the complexity of member data increased.</p><p>To address this need, Netflix developed a robust solution based on the Change Data Capture (CDC) pattern.&nbsp;</p><p>For reference, CDC is a design pattern that directly captures changes made to a database and propagates those changes to downstream systems for further processing or analysis.</p><p>The diagram below shows how the CDC process works:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19620f0c-746b-4aa7-bd31-d0e6e7329acb_1600x1002.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="445.97802197802196" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19620f0c-746b-4aa7-bd31-d0e6e7329acb_1600x1002.png" width="712" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Adopting a CDC-like approach ensures that all delta changes made to the membership data sources are recorded in an append-only log system, which is backed by a Cassandra database.</p><p>The diagram below shows the flow of historical data in Netflix&#8217;s membership platform:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10297f35-e1ce-4889-906b-3fbd2e84ae6f_1600x1218.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1108" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10297f35-e1ce-4889-906b-3fbd2e84ae6f_1600x1218.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Let&#8217;s walk through the steps in this process:</p><ol><li><p>Suppose there is an update request to modify the billing partner for a member. The request is received by the Membership Subscription Service.</p></li><li><p>The Membership Subscription Service processes the request and updates the relevant information in the Cassandra database.</p></li><li><p>In addition to updating the primary database, the updated data for the user is appended to the Membership History Service. This service is responsible for maintaining a historical record of all changes made to membership data.</p></li><li><p>The Membership History Service takes the appended data and inserts it into the Member History Table. This table serves as a persistent store for the historical data.</p></li><li><p>Finally, an event is emitted to notify downstream systems about the membership update. This allows other services and processes to react to the change and perform any necessary actions.</p></li></ol><p>There are multiple benefits to this design:</p><ul><li><p><strong>Detailed Debugging:</strong> By maintaining a comprehensive history of membership data changes, the system enables detailed debugging and troubleshooting. Developers can trace the sequence of events and understand how the data evolved.</p></li><li><p><strong>Event Replay and Reconciliation:</strong> The append-only nature of the log system allows for the ability to replay events. In case of data corruption or inconsistencies, the system can reconcile the data by replaying the events from a known good state.</p></li><li><p><strong>Customer Service Analysis:</strong> The historical data captured by the Member History Service makes customer service analysis easy.&nbsp;</p></li></ul><h2>Technical Footprint of the Netflix Membership Platform</h2><p>The technical landscape of the Netflix membership platform can be broadly categorized into two main areas: development and operations/monitoring.&nbsp;</p><p>Let's look at each area in detail.</p><h3>Development Stack</h3><p>The development stack of Netflix&#8217;s membership platform can be described by the following key points:</p><ul><li><p>Netflix's architecture is optimized for handling high read requests per second (RPS) to support its massive user base.</p></li><li><p>The membership platform comprises over 12 microservices that communicate using gRPC at the HTTP layer. On a typical day, the platform can handle an impressive 3-4 million requests per second. To support this high volume, Netflix employs techniques like client-side caching at the gRPC level and in-memory caching of entire records to prevent CockroachDB from becoming a single point of failure.</p></li><li><p>The primary programming language used in the membership platform is Java with Spring Boot. However, in certain rewrite scenarios, Netflix is gradually transitioning to Kotlin.</p></li><li><p>Kafka plays a key role in message passing and interfacing with other teams, such as messaging and downstream analytics. This ensures smooth communication and data flow across different systems.</p></li><li><p>Netflix utilizes Spark and Flink for offline reconciliation tasks on their big data. These reconciliation jobs are crucial for maintaining data consistency and alignment between various systems of record within the membership platform, such as subscriptions and member history databases. The accuracy of data also extends to external systems, ensuring a consistent state across the entire ecosystem.</p></li><li><p>To ensure data consistency in online systems, Netflix employs lightweight transactions and uses databases like Cassandra. This approach guarantees the integrity and reliability of data across different services.</p></li></ul><h3>Operations and Monitoring</h3><p>Netflix places a strong emphasis on observability and monitoring to ensure the smooth operation of its membership platform:</p><ul><li><p>Extensive logging, dashboards, and distributed tracing mechanisms enable rapid error detection and resolution. In the complex microservice landscape of Netflix, these tools are essential for identifying and troubleshooting issues.</p></li><li><p>Production alerts are set up to track operational metrics and guarantee optimal service levels.&nbsp;</p></li><li><p>Operational data is leveraged to fuel machine learning models that enhance anomaly detection and enable automated issue resolution processes. All of this is done to try and maintain an uninterrupted streaming experience for the users.</p></li><li><p>Netflix utilizes tools like Kibana and Elasticsearch to create dashboards and analyze log data. In case of a spike in error rates, these dashboards allow the team to quickly identify the specific endpoint causing the issue and take corrective action.</p></li></ul><h2>Conclusion</h2><p>In conclusion, Netflix's membership platform is a critical component of the company's success, enabling it to manage the entire lifecycle of a user's subscription. The platform has evolved from a simple, lightweight library to a robust, scalable architecture that can handle millions of requests per second</p><p>Some key takeaways to remember are as follows:</p><ul><li><p>The membership platform is responsible for managing user signups, plan changes, renewals, and cancellations.</p></li><li><p>It utilizes a microservices architecture and makes use of databases like CockroachDB and Cassandra for storing membership data.</p></li><li><p>The platform captures and stores historical changes to membership data using CDC for debugging, event replay, and analytics.</p></li></ul><p><strong>References:</strong></p><ul><li><p><a href="https://youtu.be/fCQKek_J3lQ?si=ibssD2zAHtbW7aG9">Managing 238M memberships at Netflix</a></p></li><li><p><a href="https://www.infoq.com/articles/managing-memberships-netflix/">Netflix&#8217;s Membership Platform</a></p></li><li><p><a href="https://www.infoq.com/presentations/netflix-scalability/">Presentation on Managing 238M memberships at Netflix</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p><p><br /></p>
]]></content:encoded>
<pubDate>Tue, 25 Jun 2024 15:31:15 GMT</pubDate>
</item>
<item>
<title>EP117: What makes HTTP2 faster than HTTP1?</title>
<link>https://blog.bytebytego.com/p/ep117-what-makes-http2-faster-than</link>
<guid>https://blog.bytebytego.com/p/ep117-what-makes-http2-faster-than</guid>
<content:encoded><![CDATA[
<div> Kafka, RabbitMQ, Messaging Middleware, Pulsar, HTTP2 <br />
总结:<br />
HTTP2相比HTTP1更快的原因在于其具有以下关键特点：二进制帧层使消息编码为二进制格式，实现更高效的处理；多路复用允许客户端和服务器在传输过程中交错发送请求和响应，提高效率；流优先级可定制请求或流的相对权重，确保高优先级请求得到更快响应；服务器推送允许服务器在响应请求时向客户端一并发送附加资源；HPACK头部压缩算法可减小多个请求的头部大小，节省带宽。这些特点使HTTP2实现更快速的传输，但在特定技术场景下也可能存在缓慢情况，因此开发人员需要对其进行测试和优化。Idempotency在各种场景中至关重要，特别是在可能重试或多次执行操作的情况下。Netflix利用缓存技术来保持用户关注度并增强用户体验。包括EVCache的不同用例，如Lookaside Cache、Transient Data Store、Primary Store和High Volume Data。日志解析常用命令包括GREP、CUT、SED、AWK、SORT、UNIQ，结合使用可以快速从日志文件中提取有用信息。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Kafka vs. RabbitMQ vs. Messaging Middleware vs. Pulsar (Youtube video)</p></li><li><p>What makes HTTP2 faster than HTTP1? </p></li><li><p>Top 6 Cases to Apply Idempotency</p></li><li><p>4 Ways Netflix Uses Caching to Hold User Attention</p></li><li><p>Log Parsing Cheat Sheet</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/Postman_062224Headline">Collaborating on APIs Is Easier with Postman (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Postman_062224CTA" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ca59bbc-bd0c-4825-a4f5-b896c84aa121_1600x840.png" width="1456" /><div></div></div></a></figure></div><p><a href="https://bit.ly/Postman_062224Headline">API Collaboration</a> improves developer productivity by empowering producers and consumers to share, discover, and reuse high-quality API assets.</p><p>Postman revolutionizes the experience of collaborative API development with Postman <a href="https://bit.ly/Postman_062224Collections">Collections</a> and <a href="https://bit.ly/Postman_062224Workspaces">Workspaces</a>. Used together, they enable API design, testing, and documentation, while providing a shared canvas for collaborating on API assets.&nbsp;</p><p>Learn how companies like <a href="https://bit.ly/Postman_062224Cvent">Cvent</a>, <a href="https://bit.ly/Postman_062224Visma">Visma</a>, <a href="https://bit.ly/Postman_062224BuiltTech">Built Technologies</a>, and <a href="https://bit.ly/Postman_062224Amadeus">Amadeus</a> use Postman to collaborate more easily and deliver better APIs faster.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Postman_062224CTA"><span>Learn More</span></a></p><div><hr /></div><h2><strong>Kafka vs. RabbitMQ vs. Messaging Middleware vs. Pulsar</strong></h2><div class="youtube-wrap" id="youtube2-x4k1XEjNzYQ"><div class="youtube-inner"></div></div><div><hr /></div><h2>What makes HTTP2 faster than HTTP1? </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="1727" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69df47d1-db12-4ad8-8dfa-033d391baa0e_1280x1727.gif" title="graphical user interface" width="1280" /><div></div></div></a></figure></div><p>The key features of HTTP2 play a big role in this. Let&#8217;s look at them: </p><ol><li><p>Binary Framing Layer <br />HTTP2 encodes the messages into binary format. <br /> <br />This allows the messages into smaller units called frames, which are then sent over the TCP connection, resulting in more efficient processing. <br /></p></li><li><p>Multiplexing <br />The Binary Framing allows full request and response multiplexing. <br /> <br />Clients and servers can interleave frames during transmissions and reassemble them on the other side. <br /></p></li><li><p>Stream Prioritization <br />With stream prioritization, developers can customize the relative weight of requests or streams to make the server send more frames for higher-priority requests. <br /></p></li><li><p>Server Push <br />Since HTTP2 allows multiple concurrent responses to a client&#8217;s request, a server can send additional resources along with the requested page to the client. <br /></p></li><li><p>HPACK Header Compression <br />HTTP2 uses a special compression algorithm called HPACK to make the headers smaller for multiple requests, thereby saving bandwidth. </p></li></ol><p>Of course, despite these features, HTTP2 can also be slow depending on the exact technical scenario. Therefore, developers need to test and optimize things to maximize the benefits of HTTP2. <br /> <br />Over to you: Have you used HTTP2 in your application?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribehttps://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1652" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5db07039-ccc9-4fb2-afc3-d9a3b1093d6a_3438x3900.jpeg" title="" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-microservice-communication">A Crash Course on Microservice Communication Patterns</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture">A Crash Course on Cell-based Architecture</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-content-delivery">A Crash Course on Content-Delivery Networks (CDN)</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-rest-apis">A Crash Course on REST APIs</a></p></li><li><p><a href="https://blog.bytebytego.com/p/api-security-best-practices">API Security Best Practices</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>Top 6 Cases to Apply Idempotency</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="1568" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7da1d252-88b8-4d74-b026-1626cbbb8d59_1280x1568.gif" title="graphical user interface" width="1280" /><div></div></div></a></figure></div><p>Idempotency is essential in various scenarios, particularly where operations might be retried or executed multiple times. Here are the top 6 use cases where idempotency is crucial: </p><ol><li><p>RESTful API Requests <br />We need to ensure that retrying an API request does not lead to multiple executions of the same operation. Implement idempotent methods (like PUT and DELETE) to maintain consistent resource states. <br /></p></li><li><p>Payment Processing <br />We need to ensure that customers are not charged multiple times due to retries or network issues. Payment gateways often need to retry transactions; idempotency ensures only one charge is made. <br /></p></li><li><p>Order Management Systems <br />We need to ensure that submitting an order multiple times results in only one order being placed. We design a safe mechanism to prevent duplicate inventory deductions or updates. <br /></p></li><li><p>Database Operations <br />We need to ensure that reapplying a transaction does not change the database state beyond the initial application. <br /></p></li><li><p>User Account Management <br />We need to ensure that retrying a registration request does not create multiple user accounts. Also, we need to ensure that multiple password reset requests result in a single reset action. <br /></p></li><li><p>Distributed Systems and Messaging <br />We need to ensure that reprocessing messages from a queue does not result in duplicate processing. We Implement handlers that can process the same message multiple times without side effects.</p></li></ol><div><hr /></div><h2>4 Ways Netflix Uses Caching to Hold User Attention</h2><p>The goal of Netflix is to keep you streaming for as long as possible. But a user&#8217;s typical attention span is just 90 seconds.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94417c7a-9711-4103-8677-9fc64c645490_1344x1536.gif" title="No alt text provided for this image" width="1344" /><div></div></div></a></figure></div><p>They use EVCache (a distributed key-value store) to reduce latency so that the users don&#8217;t lose interest.<br /><br />However, EVCache has multiple use cases at Netflix.</p><ol><li><p>Lookaside Cache<br />When the application needs some data, it first tries the EVCache client and if the data is not in the cache, it goes to the backend service and the Cassandra database to fetch the data.<br /><br />The service also keeps the cache updated for future requests.<br /></p></li><li><p>Transient Data Store<br />Netflix uses EVCache to keep track of transient data such as playback session information. <br /><br />One application service might start the session while the other may update the session followed by a session closure at the very end.<br /></p></li><li><p>Primary Store<br />Netflix runs large-scale pre-compute systems every night to compute a brand-new home page for every profile of every user based on watch history and recommendations. <br /><br />All of that data is written into the EVCache cluster from where the online services read the data and build the homepage.<br /></p></li><li><p>High Volume Data<br />Netflix has data that has a high volume of access and also needs to be highly available. For example, UI strings and translations that are shown on the Netflix home page. <br /><br />A separate process asynchronously computes and publishes the UI string to EVCache from where the application can read it with low latency and high availability.</p></li></ol><p>Reference: <a href="https://www.youtube.com/watch?v=Rzdxgx3RC0Q">"Caching at Netflix: The Hidden Microservice" by Scott Mansfield</a></p><div><hr /></div><h2>Log Parsing Cheat Sheet</h2><p>The diagram below lists the top 6 log parsing commands. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1683" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc28f0cbc-a40f-4bdb-88be-f2a0ab900bbc_1280x1683.jpeg" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><ol><li><p>GREP <br />GREP searches any given input files, selecting lines that match one or more patterns. <br /></p></li><li><p>CUT <br />CUT cuts out selected portions of each line from each file and writes them to the standard output. <br /></p></li><li><p>SED <br />SED reads the specified files, modifying the input as specified by a list of commands. <br /></p></li><li><p>AWK <br />AWK scans each input file for lines that match any of a set of patterns. <br /></p></li><li><p>SORT <br />SORT sorts text and binary files by lines. <br /></p></li><li><p>UNIQ <br />UNIQ reads the specified input file comparing adjacent lines and writes a copy of each unique input line to the output file. </p></li></ol><p>These commands are often used in combination to quickly find useful information from the log files. For example, the below commands list the timestamps (column 2) when there is an exception happening for xxService. <br /> <br />grep &#8220;xxService&#8221; service.log | grep &#8220;Exception&#8221; | cut -d&#8221; &#8220; -f 2 <br /> <br />Over to you: What other commands do you use when you parse logs?</p><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 22 Jun 2024 15:31:01 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Microservice Communication Patterns</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-microservice-communication</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-microservice-communication</guid>
<content:encoded><![CDATA[
<div> 网络挑战,进程通信,通信模式,性能可扩展性,优劣点

总结:<br /><br />微服务架构带来了独立服务开发的优势，但服务间仍需相互通信以构建一个协调的系统。然而，微服务间通信存在网络挑战和进程通信等困难。开发者在选择通信模式时需要考虑问题的特殊需求，否则会导致性能和可扩展性不佳。本文探讨了微服务的各种通信模式，包括其优势、劣势和最佳使用情况。Microservices architecture promotes the development of independent services. However, these services still need to communicate with each other to function as a cohesive system. Getting the communication right between microservices is often a challenge. There are two primary reasons for this: When microservices communicate over a network, they face inherent challenges associated with inter-process communication. Developers often choose a communication pattern without carefully considering the specific needs of the problem. This can lead to suboptimal performance and scalability. In this post, we explore various communication patterns for microservices and discuss their strengths, weaknesses, and ideal use cases. But first, let’s look at the key challenges associated with microservice communication. <div>
<p>Microservices architecture promotes the development of independent services. However, these services still need to communicate with each other to function as a cohesive system.&nbsp;</p><p>Getting the communication right between microservices is often a challenge. There are two primary reasons for this:</p><ul><li><p>When microservices communicate over a network, they face inherent challenges associated with inter-process communication.</p></li></ul><ul><li><p>Developers often choose a communication pattern without carefully considering the specific needs of the problem. This can lead to suboptimal performance and scalability.</p></li></ul><p>In this post, we explore various communication patterns for microservices and discuss their strengths, weaknesses, and ideal use cases.</p><p>But first, let&#8217;s look at the key challenges associated with microservice communication.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1652" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5db07039-ccc9-4fb2-afc3-d9a3b1093d6a_3438x3900.jpeg" width="1456" /><div></div></div></a></figure></div><h2>Why is Microservice Communication Challenging?</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-microservice-communication">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 20 Jun 2024 15:30:51 GMT</pubDate>
</item>
<item>
<title>Scaling to 1.2 Billion Daily API Requests with Caching at RevenueCat</title>
<link>https://blog.bytebytego.com/p/scaling-to-12-billion-daily-api-requests</link>
<guid>https://blog.bytebytego.com/p/scaling-to-12-billion-daily-api-requests</guid>
<content:encoded><![CDATA[
<div> BoldSign, Syncfusion, 缓存, RevenueCat, 数据一致性
总结: BoldSign是Syncfusion提供的电子签名服务，易于开发人员集成。RevenueCat是一个管理应用内订阅和购买的平台，处理大量API请求。RevenueCat通过多种策略保持缓存服务器热。缓存一致性是一个重要挑战，RevenueCat使用写入失败跟踪和一致的CRUD操作来维护数据一致性。缓存服务器迁移时，他们通过逐步切换读取流量来保持数据一致性。 <div>
<h2><a href="https://bit.ly/BoldSign_061824Image">Effortlessly Integrate E-Signatures into Your App with BoldSign (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/BoldSign_061824Image" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="762" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef0928fd-63b8-4e6f-b510-0eb50b2958fe_2400x1256.png" width="1456" /><div></div></div></a></figure></div><p>BoldSign by Syncfusion makes it easy for developers to integrate e-signatures into applications.</p><p>Our powerful <a href="https://bit.ly/BoldSign_061824Image">e-signature API</a> allows you to embed signature requests, create templates, add custom branding, and more.&#8239;&nbsp;</p><p>It&#8217;s so easy to get started that 60% of our customers integrated BoldSign into their apps within one day.&#8239;&nbsp;</p><p>Why BoldSign stands out:&nbsp;</p><ul><li><p>99.999% uptime.&nbsp;</p></li><li><p>Trusted by Ryanair, Cost Plus Drugs, and more.&nbsp;</p></li><li><p>Complies with eIDAS, ESIGN, GDPR, SOC 2, and HIPAA standards.&nbsp;</p></li><li><p>No hidden charges.&nbsp;</p></li><li><p>Free migration support.&#8239;&nbsp;</p></li><li><p>Rated 4.7/5 on G2.&nbsp;</p></li><li><p>Get 20% off the first year with code BYTEBYTEGO20. Valid until Sept. 31, 2024.</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/BoldSign_061824Image"><span>Get started</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the article originally published on the RevenueCat Engineering Blog. All credit for the details about RevenueCat&#8217;s architecture goes to their engineering team. The link to the original article is present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>RevenueCat is a platform that makes it easy for mobile app developers to implement and manage in-app subscriptions and purchases.&nbsp;</p><p>The staggering part is that they handle over 1.2 billion API requests per day from the apps.&nbsp;</p><p>At this massive scale, a fast and reliable performance becomes critical. Some of it is achieved by distributing the workload uniformly across multiple servers.</p><p>However, an efficient caching solution also becomes the need of the hour.</p><p>Caching allows frequently accessed data to be quickly retrieved from fast memory rather than slower backend databases and systems. This can dramatically speed up response times.&nbsp;</p><p>But caching also adds complexity since the cached data must be kept consistent with the source of truth in the databases. Stale or incorrect data in the cache can lead to serious issues.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36d24246-df04-441c-992b-8fad8774a5a5_1600x989.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36d24246-df04-441c-992b-8fad8774a5a5_1600x989.png" /><div></div></div></a></figure></div><p>For an application operating at the scale of RevenueCat, even small inefficiencies or inconsistencies in the caching layer can have a huge impact.&nbsp;</p><p>In this post, we will look at how RevenueCat overcame multiple challenges to build a truly reliable and scalable caching solution using Memcached.</p><div><hr /></div><h2>The Three Key Goals of Caching</h2><p>RevenueCat has three key goals for its caching infrastructure:</p><ul><li><p><strong>Low latency</strong>: The cache needs to be fast because even small delays in the caching layer can have significant consequences at this request volume. Retrying requests and opening new connections are detrimental to the overall performance.&nbsp;</p></li><li><p><strong>Keeping cache servers up and warm</strong>: Cache servers need to stay available and full of frequently accessed data to offload the backend systems.&nbsp;</p></li><li><p><strong>Maintaining data consistency</strong>: Data in the cache needs to be consistent. Inconsistency can lead to serious application issues.&nbsp;</p></li></ul><div class="captioned-image-container"><figure><a class="image-link image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66e961c-d13b-4b35-95e9-92354e2aea7e_1600x1102.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66e961c-d13b-4b35-95e9-92354e2aea7e_1600x1102.png" /><div></div></div></a></figure></div><p>While these main goals are highly relevant to applications operating at scale, a robust caching solution also needs supporting features such as monitoring and observability, optimization, and some sort of automated scaling.</p><p>Let&#8217;s look at each of these goals in more detail and how RevenueCat&#8217;s engineering team achieved them.</p><h2>Low Latency</h2><p>There&#8217;s no doubt that latency has a huge impact on user experience.&nbsp;</p><p>As per a statistic by Amazon, every 100ms of latency costs them 1% in sales. While it&#8217;s hard to confirm whether this is 100% true, there&#8217;s no denying the fact that latency impacts user experience.</p><p>Even small delays of a few hundred milliseconds can make an application feel sluggish and unresponsive. As latency increases, user engagement and satisfaction plummet.</p><p>RevenueCat achieves low latency in its caching layer through two key techniques.</p><h3>1 - Pre-established connections</h3><p>Their cache client maintains a pool of open connections to the cache servers.&nbsp;</p><p>When the application needs to make a cache request, it borrows a connection from the pool instead of establishing a new TCP one. This is because a TCP handshake could nearly double the cache response times. Borrowing the connection avoids the overhead of the TCP handshake on each request.&nbsp;</p><p>But no decision comes without some tradeoff.</p><p>Keeping connections open consumes memory and other resources on both the client and server. Therefore, it&#8217;s important to carefully tune the number of connections to balance resource usage with the ability to handle traffic spikes.</p><h3>2 - Fail-fast approach</h3><p>If a cache server becomes unresponsive, the client immediately marks it as down for a few seconds and fails the request, treating it as a cache miss.&nbsp;</p><p>In other words, the client will not retry the request or attempt to establish new connections to the problematic server during this period.</p><p>The key insight here is that even brief retry delays of 100ms can cause cascading failures under heavy load. Requests pile up, servers get overloaded, and the "retry storm" can bring the whole system down. Though it might sound counterintuitive, failing fast is crucial for a stable system.</p><p>But what&#8217;s the tradeoff here?</p><p>There may be a slight increase in cache misses when servers have temporary issues. But this is far better than risking a system-wide outage. A 99.99% cache hit rate is meaningless if 0.01% of requests trigger cascading failures. Prioritizing stability over perfect efficiency is the right call.</p><p>One potential enhancement over here could be circuit breaking where requests to misbehaving servers can be disabled based on error rates and latency measurements. This is something that Uber uses in their integrated cache solution called CacheFront.</p><p>However, the aggressive timeouts and managing connection pools likely achieve similar results with far less complexity.</p><h2>Keeping Cache Servers Warm</h2><p>The next goal RevenueCat had was keeping the cache servers warm.</p><p>They employed several strategies to achieve this.</p><h3>1 - Planning for Failure with Mirrored and Gutter pool</h3><p>RevenueCat uses fallback cache pools to handle failures.&nbsp;</p><p>Their strategy is designed to handle cache server failures and maintain high availability. The two approaches they use are as follows:</p><ul><li><p><strong>Mirrored pool:</strong> A fully synchronized secondary cache pool that receives all writes and can immediately take over reads if the primary pool fails.</p></li><li><p><strong>Gutter pool:</strong> A small, empty cache pool that temporarily caches values with a short TTL when the primary pool fails, reducing the load on the backend until the primary recovers. For reference, the gutter pool technique was also used by Facebook when they built their caching architecture with Memcached.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50aeb09f-579f-4df0-bb95-31c37134dafb_1600x925.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50aeb09f-579f-4df0-bb95-31c37134dafb_1600x925.png" /><div></div></div></a></figure></div><p>Here also, there are trade-offs to consider concerning server size:</p><p>For example, having smaller servers provides benefits such as:</p><ul><li><p><strong>Granular failure impact:</strong> With many small cache servers, the failure of a single server affects a smaller portion of the cached data. This can make the fallback pool more effective, as it needs to handle a smaller subset of the total traffic.</p></li><li><p><strong>Faster warmup:</strong> When a small server fails and the gutter pool takes over, it can warm up the cache for that server&#8217;s key space more quickly due to the smaller data volume.</p></li></ul><p>However, small servers also have drawbacks:</p><ul><li><p>Increased operational complexity of managing a larger number of servers adds operational complexity.&nbsp;</p></li><li><p>A higher connection overhead where each application server has to maintain connections to all cache servers.&nbsp;</p></li></ul><p>The diagram below from RevenueCat&#8217;s article shows this comparison:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20cacb99-819b-4a42-925c-7eed2bbc2626_1534x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1519" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20cacb99-819b-4a42-925c-7eed2bbc2626_1534x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><ul><li><p><strong>Simplified management:</strong> Fewer large servers are easier to manage and maintain compared to many small instances. There are fewer moving parts and less complexity in the overall system.</p></li><li><p><strong>Improved resource utilization:</strong> Larger servers can more effectively utilize the available CPU, memory, and network resources, leading to better cost efficiency.</p></li><li><p><strong>Fewer connections:</strong> With fewer cache servers, the total number of connections from the application servers is reduced, minimizing connection overhead.</p></li></ul><p>Bigger servers also have some trade-offs:</p><ul><li><p>When a large server fails, a larger portion of the cached data becomes unavailable. The fallback pool needs to handle a larger volume of traffic, potentially increasing the load on the backend.</p></li><li><p>In the case of a failure, warming up the cache for a larger key space may take longer due to the increased data volume.</p></li></ul><p>This is where the strategy of using a mirrored pool for fast failover and a gutter pool for temporary caching strikes a balance between availability and cost.&nbsp;</p><p>The mirrored pool ensures immediate availability. The gutter pool, on the other hand, provides a cost-effective way to handle failures temporarily.</p><p>Generally speaking, it&#8217;s better to design the cache tier based on a solid understanding of the backend capacity. Also, when using sharding, the cache, and the backend sharding should be orthogonal so that a cache server going down translates into a moderate increase on backend servers.</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="883" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84724fef-c993-411d-9ad9-07aabc1d7542_1600x970.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture">A Crash Course on Cell-based Architecture</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-content-delivery">A Crash Course on Content-Delivery Networks (CDN)</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-rest-apis">A Crash Course on REST APIs</a></p></li><li><p><a href="https://blog.bytebytego.com/p/api-security-best-practices">API Security Best Practices</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h3>2 - Dedicated Pools</h3><p>Another technique they employ to keep cache servers warm is to use dedicated cache pools for certain use cases.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48b6bfd0-9267-4a51-bf23-dc1f0cae76c7_1600x1191.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1084" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48b6bfd0-9267-4a51-bf23-dc1f0cae76c7_1600x1191.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Here&#8217;s how the strategy works:</p><ul><li><p><strong>Identifying high-value data</strong>: The first step is to analyze the application's data access patterns and identify datasets that are crucial for performance, accuracy, or user experience. This could include frequently accessed configuration settings, important user-specific data, or computationally expensive results.</p></li><li><p><strong>Creating dedicated pools: </strong>Instead of relying on a single shared cache pool, create separate pools for each identified high-value dataset. These dedicated pools have their own allocated memory and operate independently from the main cache pool.</p></li><li><p><strong>Reserving memory:</strong> By allocating dedicated memory to each pool, they ensure that the high-value data has a guaranteed space in the cache. This prevents other less critical data from evicting the important information, even under high memory pressure.</p></li><li><p><strong>Tailored eviction policies:</strong> Each dedicated pool can have its eviction policy tailored to the specific characteristics of the dataset. For example, a pool holding expensive-to-recompute data might have a longer TTL or a different eviction algorithm compared to a pool with frequently updated data.</p></li></ul><p>The dedicated pools strategy has several advantages:</p><ul><li><p>Improved cache hit ratio for critical data</p></li><li><p>Increased data accuracy</p></li><li><p>Flexibility in cache management</p></li></ul><h3>3 - Handling Hot Keys</h3><p>Hot keys are a common challenge in caching systems.&nbsp;</p><p>They refer to keys that are accessed more frequently than others, leading to a high concentration of requests on a single cache server. This can cause performance issues and overload the server, potentially impacting the overall system.</p><p>There are two main strategies for handling hot keys:</p><h4>Key Splitting</h4><p>The below points explain how key splitting works:</p><ul><li><p>Key splitting involves distributing the load of a hot key across multiple servers.</p></li><li><p>Instead of having a single key, the key is split into multiple versions, such as keyX/1, keyX/2, keyX/3, etc.</p></li><li><p>Each version of the key is placed on a different server, effectively spreading the load.</p></li><li><p>Clients read from one version of the key (usually determined by their client ID) but write to all versions to maintain consistency.</p></li><li><p>The challenge with key splitting is detecting hot keys in real time and coordinating the splitting process across all clients.</p></li><li><p>It requires a pipeline to identify hot keys, determine the splitting factor, and ensure that all clients perform the splitting simultaneously to avoid inconsistencies.</p></li><li><p>The list of hot keys is dynamic and can change based on real-life events or trends, so the detection and splitting process needs to be responsive.</p></li></ul><h4>Local Caching</h4><p>Local caching is simpler when compared to key splitting.</p><p>Here are some points to explain how it works:</p><ul><li><p>Local caching involves caching hot keys directly on the client-side, rather than relying solely on the distributed cache.</p></li><li><p>A key is cached locally on the client with a short TTL (Time-To-Live) when a key is identified as hot.</p></li><li><p>Subsequent requests for that key are served from the local cache, reducing the load on the distributed cache servers.</p></li><li><p>Local caching doesn't require coordination among clients.</p></li><li><p>However, local caching provides weaker consistency guarantees since the locally cached data may become stale if updates occur frequently.</p></li><li><p>To mitigate this, it&#8217;s important to use short TTLs for locally cached keys and only apply local caching to data that changes rarely.</p></li></ul><h3>Avoiding Thundering Herds</h3><p>When a popular key expires, all clients may request it from the backend simultaneously, causing a spike. This is known as the &#8220;thundering herd situation&#8221;.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6d6a925-6fea-4c79-b9c1-7256764f94c0_1532x992.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6d6a925-6fea-4c79-b9c1-7256764f94c0_1532x992.png" /><div></div></div></a></figure></div><p>RevenueCat avoids this situation since it tries to maintain cache consistency by updating it during the writes. However, when using low TTLs and invalidations from DB changes, the thundering herd can cause a lot of problems.</p><p>Some other potential solutions to avoid thundering herds are as follows:</p><ul><li><p><strong>Recache policy: </strong>The GET requests can include a recache policy. When the remaining TTL is less than the given value, one of the clients will get a miss and re-populate the value in the cache while other clients continue to use the existing value.</p></li><li><p><strong>Stale policy: </strong>In the delete command, the key is marked as stale. A single client gets a miss while others keep using the old value.</p></li><li><p><strong>Lease policy:</strong> In this policy, only one client wins the right to repopulate the value while the losers just have to wait for the winner to re-populate. For reference, Facebook uses leasing in its Memcache setup.</p></li></ul><h3>Cache Server Migrations</h3><p>Sometimes cache servers have to be replaced while minimizing impact on hit rates and user experience.</p><p>RevenueCat has built a coordinated cache server migration system that consists of the following steps:</p><ol><li><p>Warming up the new cluster:</p><ul><li><p>Before switching traffic, the team starts warming up the new cache cluster.</p></li><li><p>They populate the new cluster by mirroring all the writes from the existing cluster.</p></li><li><p>This ensures that the new cluster has the most up-to-date data before serving any requests.</p></li></ul></li><li><p>Switching a percentage of reads:</p><ul><li><p>After the new cluster is sufficiently warm, the team gradually switches a percentage of read traffic to it.</p></li><li><p>This allows them to test the new cluster&#8217;s performance and stability under real-world load.</p></li></ul></li><li><p>Flipping all traffic:</p><ul><li><p>Once the new cluster has proven its stability and performance, the traffic is flipped over to it.</p></li><li><p>At this point, the new cluster becomes the primary cache cluster, serving all read and write requests.</p></li><li><p>The old cluster is kept running for a while, with writes still being mirrored to it. This allows quick fallback in case of any issues.</p></li></ul></li><li><p>Decommissioning the old cluster:</p><ul><li><p>After a period of stable operation with the new cluster as the primary, the old cluster is decommissioned.</p></li><li><p>This frees up resources and completes the migration process.</p></li></ul></li></ol><p>The diagram below shows the entire migration process.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8294c-1c80-4b9a-aa1c-14e3f8e819d9_1575x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1479" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8294c-1c80-4b9a-aa1c-14e3f8e819d9_1575x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Maintaining data consistency is one of the biggest challenges when using caching in distributed systems.&nbsp;</p><p>The fundamental issue is that data is stored in multiple places - the primary data store (like a database) and the cache. Keeping the data in sync across these locations in the face of concurrent reads and writes is a non-trivial problem.</p><p>See the example below that shows how a simple race condition can result in a consistency problem between the database and the cache.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede53f17-11f8-400e-9067-a70b78ca3238_1600x948.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede53f17-11f8-400e-9067-a70b78ca3238_1600x948.png" /><div></div></div></a><figcaption class="image-caption">Source: <a href="https://www.revenuecat.com/blog/engineering/data-caching-revenuecat/">RevenueCat Engineering Blog</a></figcaption></figure></div><p>What&#8217;s going on over here?</p><ul><li><p>Web Server 1 gets a cache miss and fetches data from the database.</p></li><li><p>A second request results in Web Server 2 performing a DB Write for the same data. It also updates the cache with the new data</p></li><li><p>Web Server 2 refills the cache with the stale data that it had fetched in step 1.</p></li></ul><p>RevenueCat uses two main strategies to maintain cache consistency.</p><h3>1 - Write Failure Tracking</h3><p>In RevenueCat's system, a cache write failure is a strong signal that there may be an inconsistency between the cache and the primary store.&nbsp;</p><p>However, there are better options than simply retrying the write because that can lead to cascading failures and overload as discussed earlier.</p><p>Instead, RevenueCat's caching client records all write failures. After recording, it deduplicates them and ensures that the affected keys are invalidated in the cache at least once (retrying as needed until successful). This guarantees that the next read for those keys will fetch fresh data from the primary store, resynchronizing the cache.</p><p>This write failure tracking allows them to treat cache writes as if they should always succeed, significantly simplifying their consistency model. They can assume the write succeeded, and if it didn't, the tracker will ensure eventual consistency.</p><h3>2 - Consistent CRUD Operations</h3><p>For each type of data operation (Create, Read, Update, Delete), they have developed a strategy to keep the cache and primary store in sync.</p><p>For reads, they use the standard cache-aside pattern: read from the cache, and on a miss, read from the primary store and populate the cache. They always use an "add" operation to populate, which only succeeds if the key doesn't already exist, to avoid overwriting newer values.</p><p>For updates, they use a clever strategy as follows:</p><ul><li><p>Before the update, they reduce the cache entry's TTL to a low value like 30 seconds</p></li><li><p>They update the primary data store</p></li><li><p>After the update, they update the cache with the new value and reset the TTL</p></li></ul><p>If a failure occurs between steps 1 and 2, the cache remains consistent as the update never reaches the primary store. If a failure occurs between 2 and 3, the cache will be stale, but only for a short time until the reduced TTL expires. Also, any complete failures are caught by the write failure tracker that we talked about earlier.</p><p>For deletes, they use a similar TTL reduction strategy before the primary store delete.&nbsp;</p><p>However, for creation, they rely on the primary store to provide unique IDs to avoid conflicts.</p><h2>Conclusion</h2><p>RevenueCat&#8217;s approach illustrates the complexities of running caches at a massive scale. While some details may be specific to their Memcached setup, the high-level lessons are widely relevant.</p><p>Here are some key takeaways to consider from this case study:</p><ul><li><p>Use low timeouts and fail fast on cache misses. Retries can cause cascading failures under load.</p></li><li><p>Plan cache capacity for failure scenarios. Ensure the system can handle multiple cache servers going down without overloading backends.</p></li><li><p>Use fallback and dedicated cache pools. Mirrored fallback pools and dedicated pools for critical data help keep caches warm and handle failures.</p></li><li><p>Handle hot keys through splitting or local caching. Distribute load from extremely popular keys across servers or cache them locally with low TTLs.</p></li><li><p>Avoid "thundering herds" with techniques like stale-while-revalidate and leasing.</p></li><li><p>Track and handle cache write failures. Assume writes always succeed but invalidate on failure to maintain consistency.</p></li><li><p>Implement well-tested strategies for cache updates during CRUD operations. Techniques like TTL reduction before writes help maintain consistency across cache and database.</p></li></ul><p>References:</p><ul><li><p><a href="https://www.revenuecat.com/blog/engineering/data-caching-revenuecat/">Scaling Smoothly: RevenueCat&#8217;s data-caching techniques for 1.2 billion daily API requests</a></p></li><li><p><a href="https://www.infoq.com/news/2024/01/revenuecat-cache-management/">How RevenueCat Manages Caching for Handling over 1.2 Billion API Requests</a></p></li><li><p><a href="https://research.facebook.com/publications/scaling-memcache-at-facebook/">Scaling Memcache at Facebook</a></p></li><li><p><a href="https://www.uber.com/en-IN/blog/how-uber-serves-over-40-million-reads-per-second-using-an-integrated-cache/">How Uber Serves Over 40 Million Reads Per Second from Online Storage Using Integrated Cache</a></p></li><li><p><a href="https://www.gigaspaces.com/blog/amazon-found-every-100ms-of-latency-cost-them-1-in-sales">Amazon Found Every 100ms of Latency Cost Them 1% in Sales</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></p><p></p>
]]></content:encoded>
<pubDate>Tue, 18 Jun 2024 15:30:55 GMT</pubDate>
</item>
<item>
<title>EP116: 11 steps to go from Junior to Senior Developer</title>
<link>https://blog.bytebytego.com/p/ep116-11-steps-to-go-from-junior</link>
<guid>https://blog.bytebytego.com/p/ep116-11-steps-to-go-from-junior</guid>
<content:encoded><![CDATA[
<div> Data Pipeline, Senior Developer, Docker concepts, Microservice architecture, Open-Source Databases
<br />
Data Pipeline是什么？为什么这么受欢迎？数据管道是数据处理流程的自动化，十分流行。成为高级开发人员的11个步骤包括使用合作工具、精通编程语言、API开发、Web服务器与托管、身份验证与测试、数据库、CI/CD、数据结构与算法、系统设计、设计模式、AI工具等。Docker有8个重要概念，如Dockerfile、Docker镜像、容器、注册表、卷、Compose、网络、CLI等。典型微服务架构包括负载均衡器、CDN、API网关、身份验证提供者、服务注册与发现、管理及微服务等组件。最受欢迎的开源数据库包括MySQL、PostgreSQL、MariaDB、Apache Cassandra、Neo4j、SQLite、CockroachDB、Redis、MongoDB、Couchbase等。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>What is Data Pipeline? | Why Is It So Popular? (Youtube video)</p></li><li><p>11 steps to go from Junior to Senior Developer</p></li><li><p>Top 8 must-know Docker concepts </p></li><li><p>What does a typical microservice architecture look like? </p></li><li><p>Top 10 Most Popular Open-Source Databases </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/NewRelic_061524">New Relic Digital Monitoring Experience (DEM) (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/NewRelic_061524" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="680" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2750532-75bd-4c68-ba57-78c3a39bfb9e_1296x680.png" width="1296" /><div></div></div></a></figure></div><p>New Relic DEM solutions are designed to provide comprehensive insights into digital operations, allowing teams to optimize user experiences in real time.</p><p>Download the datasheet&nbsp;for an overview of New Relic DEM capabilities:</p><ul><li><p><strong>Real user monitoring (RUM):</strong> Browser monitoring and mobile monitoring&nbsp;&nbsp;</p></li><li><p><strong>Pixel-perfect replays:</strong> Session replay</p></li><li><p><strong>Proactive issue detection:&nbsp;</strong>Synthetic monitoring, mobile user journeys (crash analysis), and error tracking (errors inbox)</p></li><li><p><strong>Integration and collaboration:</strong> In-app collaboration capabilities&nbsp;</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/NewRelic_061524"><span>Download the data sheet</span></a></p><div><hr /></div><h2>What is Data Pipeline? | Why Is It So Popular?</h2><div class="youtube-wrap" id="youtube2-kGT4PcTEPP8"><div class="youtube-inner"></div></div><div><hr /></div><h2>11 steps to go from Junior to Senior Developer</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1562" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507dbf27-df25-4d96-b09a-0a89e4b27ea4_1280x1562.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><ol><li><p>Collaboration Tools <br />Software development is a social activity. Learn to use collaboration tools like Jira, Confluence, Slack, MS Teams, Zoom, etc. <br /></p></li><li><p>Programming Languages <br />Pick and master one or two programming languages. Choose from options like Java, Python, JavaScript, C#, Go, etc. <br /></p></li><li><p>API Development <br />Learn the ins and outs of API Development approaches such as REST, GraphQL, and gRPC. <br /></p></li><li><p>Web Servers and Hosting <br />Know about web servers as well as cloud platforms like AWS, Azure, GCP, and Kubernetes <br /></p></li><li><p>Authentication and Testing <br />Learn how to secure your applications with authentication techniques such as JWTs, OAuth2, etc. Also, master testing techniques like TDD, E2E Testing, and Performance Testing <br /></p></li><li><p>Databases <br />Learn to work with relational (Postgres, MySQL, and SQLite) and non-relational databases (MongoDB, Cassandra, and Redis). <br /></p></li><li><p>CI/CD <br />Pick tools like GitHub Actions, Jenkins, or CircleCI to learn about continuous integration and continuous delivery. <br /></p></li><li><p>Data Structures and Algorithms <br />Master the basics of DSA with topics like Big O Notation, Sorting, Trees, and Graphs. <br /></p></li><li><p>System Design <br />Learn System Design concepts such as Networking, Caching, CDNs, Microservices, Messaging, Load Balancing, Replication, Distributed Systems, etc. <br /></p></li><li><p>Design patterns <br />Master the application of design patterns such as dependency injection, factory, proxy, observers, and facade. <br /></p></li><li><p>AI Tools <br />To future-proof your career, learn to leverage AI tools like GitHub Copilot, ChatGPT, Langchain, and Prompt Engineering. </p></li></ol><p>Over to you: What else would you add to the roadmap?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="938" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2490fa0-671a-4248-81d9-7106dd8411eb_1600x1031.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture">A Crash Course on Cell-based Architecture</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-content-delivery">A Crash Course on Content-Delivery Networks (CDN)</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-rest-apis">A Crash Course on REST APIs</a></p></li><li><p><a href="https://blog.bytebytego.com/p/api-security-best-practices">API Security Best Practices</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>Top 8 must-know Docker concepts </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F033ac539-080f-4ad6-87e5-643b18ec3956_1498x1536.gif" title="graphical user interface, application" /><div></div></div></a></figure></div><ol><li><p>Dockerfile: It contains the instructions to build a Docker image by specifying the base image, dependencies, and run command. <br /> </p></li><li><p>Docker Image: A lightweight, standalone package that includes everything (code, libraries, and dependencies) needed to run your application. Images are built from a Dockerfile and can be versioned. <br /></p></li><li><p>Docker Container: A running instance of a Docker image. Containers are isolated from each other and the host system, providing a secure and reproducible environment for running your apps. <br /></p></li><li><p>Docker Registry: A centralized repository for storing and distributing Docker images. For example, Docker Hub is the default public registry but you can also set up private registries. <br /></p></li><li><p>Docker Volumes: A way to persist data generated by containers. Volumes are outside the container&#8217;s file system and can be shared between multiple containers. <br /></p></li><li><p>Docker Compose: A tool for defining and running multi-container Docker applications, making it easy to manage the entire stack. <br /></p></li><li><p>Docker Networks: Used to enable communication between containers and the host system. Custom networks can isolate containers or enable selective communication. <br /></p></li><li><p>Docker CLI: The primary way to interact with Docker, providing commands for building images, running containers, managing volumes, and performing other operations. </p></li></ol><p>Over to you: What other concept should one know about Docker?</p><div><hr /></div><h2>What does a typical microservice architecture look like? &#128071;</h2><p>The diagram below shows a typical microservice architecture.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1779" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c8d4eab-24cb-49c6-92fa-6e914603b21b_1280x1779.gif" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><ul><li><p>Load Balancer: This distributes incoming traffic across multiple backend services.</p></li><li><p>CDN (Content Delivery Network): CDN is a group of geographically distributed servers that hold static content for faster delivery. The clients look for content in CDN first, then progress to backend services.</p></li><li><p>API Gateway: This handles incoming requests and routes them to the relevant services. It talks to the identity provider and service discovery.</p></li><li><p>Identity Provider: This handles authentication and authorization for users.</p></li><li><p>Service Registry &amp; Discovery: Microservice registration and discovery happen in this component, and the API gateway looks for relevant services in this component to talk to.</p></li><li><p>Management: This component is responsible for monitoring the services.</p></li><li><p>Microservices: Microservices are designed and deployed in different domains. Each domain has its database.</p></li></ul><p>Over to you: </p><ol><li><p>What are the drawbacks of the microservice architecture?</p></li><li><p>Have you seen a monolithic system be transformed into microservice architecture? How long does it take?</p></li></ol><div><hr /></div><h2>Top 10 Most Popular Open-Source Databases </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="diagram" class="sizing-normal" height="1622" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea57069-5a32-4f02-b015-2bdb883dd135_1280x1622.gif" title="diagram" width="1280" /><div></div></div></a></figure></div><p>This list is based on factors like adoption, industry impact, and the general awareness of the database among the developer community. </p><ol><li><p>MySQL </p></li><li><p>PostgreSQL </p></li><li><p>MariaDB </p></li><li><p>Apache Cassandra </p></li><li><p>Neo4j </p></li><li><p>SQLite </p></li><li><p>CockroachDB </p></li><li><p>Redis </p></li><li><p>MongoDB </p></li><li><p>Couchbase </p></li></ol><p>Over to you: Which other database would you add to this list?</p><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 15 Jun 2024 15:31:04 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Cell-based Architecture</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture</guid>
<content:encoded><![CDATA[
<div> bulkheads, cell-based architecture, independent, failure isolation, workload
<br />
cell-based architecture是一种软件开发架构，类似于船只内部的防水隔舱，将工作负载分割成多个独立的cell，每个cell独立运行，不共享状态，处理部分流量请求。这种架构减小了故障影响范围，一旦一个cell发生故障，仅影响部分工作负载，不会引起整体系统崩溃。通过实现故障隔离，cell-based architecture提高了系统的稳定性和可靠性。 <div>
<p>No one wants to sail in a ship that can sink because of a single hull breach.</p><p>This led to the development of bulkheads, which are vertical partition walls that divide a ship&#8217;s interior into watertight compartments.</p><p>Cell-based architecture attempts to follow the same concept in software development.</p><p>In cell-based architecture, there are multiple isolated instances of a workload, where each instance is known as a cell. There are three properties of a cell:</p><ul><li><p>Each cell is independent.</p></li><li><p>A cell does not share the state with other cells.</p></li><li><p>Each cell handles a subset of the overall traffic.</p></li></ul><p>For example, imagine a web application that handles user requests. In a cell-based architecture, multiple cells of the same web application would be deployed, each serving a subset of the user requests. These cells are copies of the same application working together to distribute the workload.</p><p>This approach reduces the blast radius of impact. If a workload uses 5 cells to service 50 requests, a failure in only one cell means that 80% of the requests are unaffected by the failure.</p><p>In other words, failure isolation is the biggest benefit of a cell-based architecture.</p><p>In this post, we will learn about the various aspects of cell-based architecture and its various components in more detail.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2490fa0-671a-4248-81d9-7106dd8411eb_1600x1031.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="938" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2490fa0-671a-4248-81d9-7106dd8411eb_1600x1031.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><div><hr /></div><h2>What is a Workload?</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-cell-based-architecture">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 13 Jun 2024 15:30:19 GMT</pubDate>
</item>
<item>
<title>How PayPal Scaled Kafka to 1.3 Trillion Daily Messages</title>
<link>https://blog.bytebytego.com/p/how-paypal-scaled-kafka-to-13-trillion</link>
<guid>https://blog.bytebytego.com/p/how-paypal-scaled-kafka-to-13-trillion</guid>
<content:encoded><![CDATA[
<div> Kafka, PayPal, Cluster Management, Monitoring, ACLs
<br />
Kafka是PayPal的关键组件，他们在Cluster Management和Monitoring方面进行了重要的改进。他们引入了ACLs来控制Kafka集群的访问权限。此外，他们建立了专门的QA环境，用于测试和验证变更。总结: PayPal在Kafka平台上的操作经验为大规模运营提供了重要的启示。他们强调工具化操作和持续监控，通过引入ACLs提高安全性，建立QA环境提高开发效率。 <div>
<h2><a href="https://bit.ly/ScyllaDB_061124">Database Performance at Scale: A Practical Guide [FREE BOOK] (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/ScyllaDB_061124" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Face0df4f-42a2-4e55-aab7-1a4841ad3a4e_1600x840.png" width="1456" /><div></div></div></a></figure></div><p>Discover new ways to optimize database performance &#8211; and avoid common pitfalls &#8211; in this free 270-page book.</p><p>This book shares best practices for achieving predictable low latency at high throughput. It&#8217;s based on learnings from thousands of real-world database use cases &#8211; including Discord, Disney, Strava, Expedia, Epic Games &amp; more.</p><ul><li><p>Explore often-overlooked factors that impact database performance at scale</p></li><li><p>Recognize the performance challenges teams face with different types of workloads</p></li><li><p>Select database infrastructure and topology that&#8217;s suited to your needs</p></li><li><p>Optimize how you benchmark and monitor performance</p></li><li><p>Avoid common mistakes that impact latency and throughput</p></li><li><p>Get practical advice for navigating performance tradeoffs</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/ScyllaDB_061124"><span>Download for free</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from the article originally published on the PayPal Tech Blog. All credit for the details about PayPal&#8217;s architecture goes to their engineering team. The link to the original article is present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>In the 2022 Retail Friday, PayPal&#8217;s Kafka setup witnessed a traffic volume of 21 million messages per second.</p><p>This was about 1.3 trillion messages in a single day.&nbsp;</p><p>Similarly, Cyber Monday resulted in 19 million messages per second coming to around 1.23 trillion messages in a single day.</p><p>How did PayPal scale Kafka to achieve these incredible numbers?</p><p>In this post, we will go through the complete details of PayPal&#8217;s high-performance Kafka setup that made it possible.</p><div><hr /></div><h2>Kafka at PayPal</h2><p>Apache Kafka is an open-source distributed event streaming platform.</p><p>PayPal adopted Kafka in 2015 and they use it for building data streaming pipelines, integration, and ingestion.</p><p>At present, PayPal&#8217;s Kafka fleet consists of over 1500 brokers hosting 20,000 topics. The 85+ clusters are expected to maintain a 99.99% availability.</p><p>Over the years, PayPal has seen tremendous growth in streaming data and they wanted to ensure high availability, fault tolerance, and optimal performance.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f4ab7cc-f445-44f3-8f13-a1b70a41b13b_1600x1512.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1376" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f4ab7cc-f445-44f3-8f13-a1b70a41b13b_1600x1512.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Some of the specific use cases where PayPal uses Kafka are as follows:&nbsp;</p><ul><li><p><strong>First-party Tracking: </strong>Tracking user interactions and clickstream data on PayPal&#8217;s website and mobile app for real-time analytics and personalization.</p></li><li><p><strong>Application Health Metrics: </strong>Collecting and aggregating performance metrics from various PayPal services to monitor system health and detect anomalies.</p></li><li><p><strong>Database Synchronization: </strong>Synchronizing data between PayPal&#8217;s primary database and secondary databases for disaster recovery and high availability.</p></li><li><p><strong>Application Log Aggregation: </strong>Collecting and centralizing log data from different PayPal applications and services for troubleshooting, monitoring, and analysis.</p></li><li><p><strong>Batch Processing: </strong>Processing large batches of payment transaction data using Kafka as a buffer for decoupling the data ingestion and processing stages.</p></li><li><p><strong>Risk Detection and Management: </strong>Streaming real-time payment data through Kafka for feeding the fraud detection and risk assessment models.</p></li><li><p><strong>Analytics and Compliance: </strong>Capturing and analyzing financial transaction data in real-time for regulatory reporting and audit purposes.</p></li></ul><h2>How PayPal Operates Kafka?</h2><p>PayPal&#8217;s infrastructure is spread across multiple geographically distributed data centers and security zones. Kafka clusters are deployed across these zones.</p><p>There are some key differences between data centers and security zones:</p><ul><li><p>A data center is a physical facility that houses computing infrastructure. A security zone is a logical partition with a data center or across data centers, created through network segmentation.</p></li><li><p>While data centers help with isolation and availability, security zones provide an additional level of security isolation beyond the physical boundaries.</p></li><li><p>Security zones are often defined based on data classification levels, such as highly sensitive, confidential, or public data.</p></li></ul><p>In the context of PayPal, Kafka clusters handling sensitive payment data may be placed in a high-security zone with restricted access. Clusters processing less sensitive data may reside in a different security zone.</p><p>One thing, however, is common.</p><p>Whether it is data centers or security zones, MirrorMaker is used to mirror the data across the data centers, which helps with disaster recovery and communication across security zones.&nbsp;</p><p>For reference, Kafka MirrorMaker is a tool for mirroring data between Apache Kafka clusters. It leverages the Kafka Connect framework to replicate data, which improves resiliency.&nbsp;</p><p>See the diagram below to get an idea about PayPal&#8217;s Kafka setup across data centers and security zones:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf13b1a8-70b3-42b5-8a07-dd61adc91818_1388x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf13b1a8-70b3-42b5-8a07-dd61adc91818_1388x1600.png" width="1388" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Operating Kafka at the scale of PayPal is a challenging task. To manage the ever-growing fleet of Kafka clusters, PayPal has focused on some key areas such as:</p><ul><li><p>Cluster Management</p></li><li><p>Monitoring and Alerting</p></li><li><p>Configuration Management</p></li><li><p>Enhancements and Automation</p></li></ul><p>The diagram below shows a high-level view of PayPal&#8217;s Kafka Landscape:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc267306b-64d1-45e2-bc0a-e61e340dbc04_1600x1015.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="924" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc267306b-64d1-45e2-bc0a-e61e340dbc04_1600x1015.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>In the subsequent sections, we will look at each area in greater detail.</p><h2>Cluster Management</h2><p>Cluster management deals with controlling Kafka clusters and reducing operational overhead. Some of the key improvements were done in areas like:</p><ul><li><p>Kafka Config Service</p></li><li><p>ACLs</p></li><li><p>Kafka Libraries for PayPal</p></li><li><p>QA environment</p></li></ul><p>Let&#8217;s look at each improvement in more detail.</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1005" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2785489-6c63-40cf-bb4a-1b8656a81d01_1600x1104.png" title="" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-content-delivery">A Crash Course on Content-Delivery Networks (CDN)</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-rest-apis">A Crash Course on REST APIs</a></p></li><li><p><a href="https://blog.bytebytego.com/p/api-security-best-practices">API Security Best Practices</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h3>Kafka Config Service</h3><p>PayPal built a special Kafka config service to make it easy for clients to connect to the Kafka clusters.</p><p>Before the config service, clients had to hardcode the broker IPs in the connection configuration.&nbsp;</p><p>This created a maintenance nightmare due to a couple of reasons:</p><ul><li><p>Firstly, replacing a broker for upgrades, patching, or disk failures required updates to the client. Missing to update one broker IP somewhere would often lead to multiple incidents.</p></li><li><p>Second, Kafka is configuration-heavy, making it tough for developers to figure out a suitable set of configurations. Often, the Kafka clients would override certain properties without knowing the implications, resulting in support issues due to unexpected behavior.</p></li></ul><p>The Kafka config service solved these issues. The service makes it easy for the Kafka clients to follow a standard configuration. Ultimately, it reduces operational and support overhead across teams.</p><p>The diagram below shows the Kafka client retrieving bootstrap servers and configuration from the config service using the topic information.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcaa0d294-4b44-4c17-b7a8-26b7f8dc2513_1600x1021.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="929" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcaa0d294-4b44-4c17-b7a8-26b7f8dc2513_1600x1021.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Kafka ACLs</h3><p>Initially, any PayPal application could connect to any of the existing Kafka topics. This was an operational risk for the platform considering that it streams business-critical data.</p><p>To ensure controlled access to Kafka clusters, ACLs were introduced.</p><p>For reference, ACLs are used to define which users, groups, or processes have access to specific objects such as files, directories, applications, or network resources. It&#8217;s like a table or list specifying a particular object's permissions.</p><p>With the introduction of ACLs, applications had to authenticate and authorize access to Kafka clusters and topics.&nbsp;</p><p>Apart from making the platform secure, ACLs also provided a record of every application accessing a particular topic or cluster.</p><h3>PayPal Kafka Libraries&nbsp;</h3><p>It was important to ensure that Kafka clusters and the clients connecting to them operate securely. Also, there was a need to ensure easy integration to multiple frameworks and programming languages. They didn&#8217;t want each engineering team to reinvent the wheel.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac03b947-fed2-455d-a3df-9a538691c336_946x624.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="624" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac03b947-fed2-455d-a3df-9a538691c336_946x624.png" width="946" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Supported tech stack for Kafka Libraries. Source: <a href="https://medium.com/paypal-tech/scaling-kafka-to-support-paypals-data-growth-a0b4da420fab">PayPal Tech Blog</a></figcaption></figure></div><p>To facilitate these needs, PayPal built a few important libraries:</p><ul><li><p><strong>Resilient Client Library: </strong>When a client tries to establish a connection to the cluster, this library gets the Kafka broker details and the required configuration for the producer or consumer application.</p></li><li><p><strong>Monitoring Library: </strong>This library publishes critical metrics for client applications, allowing applications to set alerts and get notifications in case of any issues.</p></li><li><p><strong>Kafka Security Library: </strong>The Kafka platform supports more than 800 applications. This library takes care of the required certificates and tokens to enable SSL authentication for applications connecting to the Kafka clusters. It avoids a lot of overhead around key management, certificate updates, and key rotations.</p></li></ul><h3>QA Platform</h3><p>One of the great things PayPal did was to set up a production-like QA platform for Kafka for developers to test changes confidently.</p><p>This is a common problem in many organizations where the testing performed by developers is hardly indicative of the production environment, resulting in issues after launch.&nbsp;</p><p>A dedicated QA platform solves this by providing a direct mapping between production and QA clusters.&nbsp;</p><p>The same security standards are followed. The same topics are hosted on the clusters with the brokers spread across multiple zones within the Google Cloud Platform.</p><h3>Monitoring and Alerting</h3><p>Monitoring and alerting are extremely important aspects for systems operating at a high scale. Teams want to know about issues and incidents quickly so that cascading failures can be avoided.</p><p>At PayPal, the Kafka platform is integrated with the monitoring and alerting systems.</p><p>Apache Kafka provides multiple metrics. However, they have taken out a subset of metrics that help them identify issues faster.&nbsp;</p><p>The Kafka Metrics library filters out the metrics and sends them to the SignalFX backend via SignalFX agents running on all brokers, Zookeepers, MirrorMakers, and Kafka clients. Individual alerts associated with these metrics are triggered whenever abnormal thresholds are breached.&nbsp;</p><h2>Configuration Management</h2><p>Operating a critical system requires one to guard against data loss. This is not only applicable to the application data but also to the infrastructure information.</p><p>What if the infrastructure gets wiped out and we&#8217;ve to rebuild it from scratch?</p><p>At PayPal, configuration management helps them store the complete infrastructure details. This is the single source of truth that allows PayPal to rebuild the clusters in a couple of hours if needed.</p><p>They store the Kafka metadata such as topic details, clusters, and applications in an internal configuration management system. The metadata is also backed up to ensure that they have the most recent data in case it&#8217;s required to re-create clusters and topics in case of a recovery.</p><h2>Enhancements and Automation</h2><p>Large-scale systems require special tools to carry out operational tasks as quickly as possible.</p><p>PayPal built multiple such tools for operating their Kafka cluster. Let&#8217;s look at a few important ones:</p><h3>Patching Security Vulnerabilities</h3><p>PayPal uses BareMetal for deploying the Kafka brokers and virtual machines for Zookeeper and MirrorMakers.</p><p>As we can expect, all of these hosts need to be patched at frequent intervals to fix any security vulnerabilities.&nbsp;</p><p>Patching requires BM restart which can cause partitions to lag. This can also lead to data loss in the case of Kafka topics that are configured with a replica set of three.&nbsp;</p><p>They built a plugin to query whether a partition was lagging before patching the host, thereby ensuring only a single broker is patched at a time with no chances of data loss.</p><h3>Topic Onboarding</h3><p>Application teams require topics for their application functionality. To make this process standardized, PayPal built an Onboarding Dashboard to submit a new topic request.</p><p>The diagram below shows the onboarding workflow for a topic.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5171a6df-fad2-4dad-9709-de223718bb5f_1600x1002.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="912" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5171a6df-fad2-4dad-9709-de223718bb5f_1600x1002.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>A special review team checks the capacity requirements for the new topic and onboards it to one of the available clusters. They use a capacity analysis tool integrated into the onboarding workflow to make the decision.</p><p>For each new application being onboarded to the Kafka system, a unique token is generated. This token is used to authenticate the client&#8217;s access to the Kafka topic. As discussed earlier, an ACL is created for the specific application and topic based on their role.</p><h3>MirrorMaker Onboarding</h3><p>As mentioned earlier, PayPal uses MirrorMaker for mirroring the data from one cluster to another.</p><p>For this setup, developers also use the Kafka Onboarding UI to register their requirements. After due checks by the Kafka team, the MirrorMaker instances are provisioned.</p><p>The diagram below shows the process flow for the same:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdd0e9ef-6fbb-4c47-b748-9d04bdd0617a_1600x851.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="774" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdd0e9ef-6fbb-4c47-b748-9d04bdd0617a_1600x851.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>Conclusion</h2><p>The Kafka platform at PayPal is a key ingredient for enabling seamless integration between multiple applications and supporting the scale of their operation.</p><p>Some important learnings to take away from this study are as follows:</p><ul><li><p>Tooling is a must when operating Kafka on a large scale. This involves operations such as cluster installation, topic management, patching VMs, etc.</p></li><li><p>Availability is as good as the ability of alerting and monitoring systems to provide timely inputs to the infrastructure team.</p></li><li><p>ACLs are a great way to have a better understanding of how the various applications are connected with Kafka.</p></li><li><p>A dedicated QA environment is critical for developers to ship changes with confidence and speed.</p></li></ul><p><strong>References</strong></p><ul><li><p><a href="https://medium.com/paypal-tech/scaling-kafka-to-support-paypals-data-growth-a0b4da420fab">Scaling Kafka to Support PayPal&#8217;s Data Growth</a></p></li><li><p><a href="https://www.confluent.io/resources/kafka-summit-2020/marching-toward-a-trillion-kafka-messages-per-day-running-kafka-at-scale-at-paypal/">Marching towards a Trillion Kafka Messages Per Day</a></p></li><li><p><a href="https://developers.redhat.com/articles/2023/11/13/demystifying-kafka-mirrormaker-2-use-cases-and-architecture">Demystifying Kafka MirrorMaker2</a></p></li><li><p><a href="https://kafka.apache.org/uses">Kafka Use Cases</a></p></li></ul><div><hr /></div><h2>SPONSOR US</h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p><p><br /></p>
]]></content:encoded>
<pubDate>Tue, 11 Jun 2024 15:30:53 GMT</pubDate>
</item>
<item>
<title>EP115: Life is Short, Use Dev Tools</title>
<link>https://blog.bytebytego.com/p/ep115-life-is-short-use-dev-tools</link>
<guid>https://blog.bytebytego.com/p/ep115-life-is-short-use-dev-tools</guid>
<content:encoded><![CDATA[
<div> KISS, SOLID, CAP, BASE, Dev Tools, Production Web Application, Frontend Performance, Developer Standards, Sponsorship<br /><br />总结: 本文介绍了系统设计中的重要术语和开发工具，以及如何构建高性能的生产级Web应用程序。通过讨论8个前端性能提升技巧和开发人员需要了解的8个标准，全面展示了技术行业的最新趋势。此外，还提供了赞助机会，让产品和服务能够直接接触到50万以上的技术专业人士。通过这些内容，读者可以了解如何提升自己在开发领域的水平，以及如何将产品推广给相关目标群体。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>KISS, SOLID, CAP, BASE: Important Terms You Might Not Know! (Youtube video)</p></li><li><p>Life is Short, Use Dev Tools </p></li><li><p>10 Essential Components of a Production Web Application</p></li><li><p>How to load your websites at lightning speed</p></li><li><p>Top 8 Standards Every Developer Should Know</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/LinearB_060824">What You Need to Know About Software Engineering Intelligence [Workshop]&nbsp;(Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/LinearB_060824" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="359.010989010989" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22df6e02-75ef-476b-9cca-3926e28b8dd9_1600x798.png" width="720" /><div></div></div></a></figure></div><p>How can you get holistic visibility across your SDLC, insights about your engineering performance, and implement automations that unblock your bottlenecks? </p><p>Software Engineering Intelligence (SEI) Platforms are solving visibility and operations challenges for engineering teams. Join LinearB&#8217;s workshop on June 20th or 27th to learn about how SEI tools are rapidly accelerating developer productivity in enterprises.</p><p>We&#8217;ll demonstrate how you can use SEI to take on:</p><ul><li><p>Developer Productivity - accelerating output</p></li><li><p>Profitable Engineering - the data behind resourcing and costs</p></li><li><p>Developer Experience - protecting team health and reducing toil</p></li><li><p>GenAI (Copilot) Impact - the effects it&#8217;s having on your software pipelines</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/LinearB_060824"><span>Register now</span></a></p><div><hr /></div><h2>KISS, SOLID, CAP, BASE: Important Terms You Might Not Know!</h2><div class="youtube-wrap" id="youtube2-cTyZ_hbmbDw"><div class="youtube-inner"></div></div><div><hr /></div><h2>Life is Short, Use Dev Tools </h2><p>The right dev tool can save you precious time, energy, and perhaps the weekend as well. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1565" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5e5b9d5-3426-42ab-b690-2413ab249270_1280x1565.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><p>Here are our favorite dev tools: </p><ol><li><p>Development Environment <br />A good local dev environment is a force multiplier. Powerful IDEs like VSCode, IntelliJ IDEA, Notepad++, Vim, PyCharm &amp; Jupyter Notebook can make your life easy. <br /></p></li><li><p>Diagramming <br />Showcase your ideas visually with diagramming tools like DrawIO, Excalidraw, mindmap, Mermaid, PlantUML, Microsoft Visio, and Miro <br /></p></li><li><p>AI Tools <br />AI can boost your productivity. Don&#8217;t ignore tools like ChatGPT, GitHub Copilot, Tabnine, Claude, Ollama, Midjourney, and Stable Diffusion. <br /></p></li><li><p>Hosting and Deployment <br />For hosting your applications, explore solutions like AWS, Cloudflare, GitHub, Fly, Heroku, and Digital Ocean. <br /></p></li><li><p>Code Quality <br />Quality code is a great differentiator. Leverage tools like Jest, ESLint, Selenium, SonarQube, FindBugs, and Checkstyle to ensure top-notch quality. <br /></p></li><li><p>Security <br />Don&#8217;t ignore the security aspects and use solutions like 1Password, LastPass, OWASP, Snyk, and Nmap. <br /></p></li><li><p>Note-taking <br />Your notes are a reflection of your knowledge. Streamline your note-taking with Notion, Markdown, Obsidian, Roam, Logseq, and Tiddly Wiki. <br /></p></li><li><p>Design <br />Elevate your visual game with design tools like Figma, Sketch, Adobe Illustrator, Canva, and Adobe Photoshop. <br /></p></li></ol><p>Over to you: Which dev tools do you use? </p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1198" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F203c1745-28f1-405c-80c1-b456e391c708_1600x1317.png" title="" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-content-delivery">A Crash Course on Content-Delivery Networks (CDN)</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-rest-apis">A Crash Course on REST APIs</a></p></li><li><p><a href="https://blog.bytebytego.com/p/api-security-best-practices">API Security Best Practices</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>10 Essential Components of a Production Web Application</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1568" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cae662e-b5f0-4015-b4b7-7d9eac154903_1280x1568.gif" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><ol><li><p>It all starts with CI/CD pipelines that deploy code to the server instances. Tools like Jenkins and GitHub help over here.</p></li><li><p>The user requests originate from the web browser. After DNS resolution, the requests reach the app servers.</p></li><li><p>Load balancers and reverse proxies (such as Nginx &amp; HAProxy) distribute user requests evenly across the web application servers.</p></li><li><p>The requests can also be served by a Content Delivery Network (CDN).</p></li><li><p>The web app communicates with backend services via APIs.</p></li><li><p>The backend services interact with database servers or distributed caches to provide the data.</p></li><li><p>Resource-intensive and long-running tasks are sent to job workers using a job queue.</p></li><li><p>The full-text search service supports the search functionality. Tools like Elasticsearch and Apache Solr can help here.</p></li><li><p>Monitoring tools (such as Sentry, Grafana, and Prometheus) store logs and help analyze data to ensure everything works fine. </p></li><li><p>In case of issues, alerting services notify developers through platforms like Slack for quick resolution. </p></li></ol><p>Over to you: What other components would you add to the architecture of a production web app?</p><div><hr /></div><h2>How to load your websites at lightning speed?</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1585" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0167296-81d2-46ad-bea6-183cac627c4d_1280x1585.gif" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><p>Check out these 8 tips to boost frontend performance:</p><ol><li><p>Compression<br />Compress files and minimize data size before transmission to reduce network load.</p><p></p></li><li><p>Selective Rendering/Windowing<br />Display only visible elements to optimize rendering performance. For example, in a dynamic list, only render visible items.<br /></p></li><li><p>Modular Architecture with Code Splitting<br />Split a bigger application bundle into multiple smaller bundles for efficient loading.<br /></p></li><li><p>Priority-Based Loading<br />Prioritize essential resources and visible (or above-the-fold) content for a better user experience.<br /></p></li><li><p>Pre-loading<br />Fetch resources in advance before they are requested to improve loading speed.<br /></p></li><li><p>Tree Shaking or Dead Code Removal<br />Optimize the final JS bundle by removing dead code that will never be used.<br /></p></li><li><p>Pre-fetching<br />Proactively fetch or cache resources that are likely to be needed soon.<br /></p></li><li><p>Dynamic Imports<br />Load code modules dynamically based on user actions to optimize the initial loading times.</p></li></ol><p>Over to you: What other frontend performance tips would you add to this cheat sheet?</p><div><hr /></div><h2>Top 8 Standards Every Developer Should Know</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef0ca4ec-4ab6-4bd3-a0f2-bf49da47748b_1536x1536.gif" title="No alternative text description for this image" /><div></div></div></a></figure></div><ul><li><p>TCP/IP <br />Developed by the IETF organization, the TCP/IP protocol is the foundation of the Internet and one of the best-known networking standards. <br /> </p></li><li><p>HTTP <br />The IETF has also developed the HTTP protocol, which is essential for all web developers. <br /> </p></li><li><p>SQL <br />Structured Query Language (SQL) is a domain-specific language used to manage data. <br /> </p></li><li><p>OAuth <br />OAuth (Open Authorization) is an open standard for access delegation commonly used to grant websites or applications limited access to user information without exposing their passwords. <br /> </p></li><li><p>HTML/CSS <br />With HTML, web pages are rendered uniformly across browsers, which reduces development effort spent on compatibility issues.HTML tags. <br /> <br />CSS standards are often used in conjunction with HTML. <br /> </p></li><li><p>ECMAScript <br />ECMAScript is a standardized scripting language specification that serves as the foundation for several programming languages, the most well-known being JavaScript. <br /> </p></li><li><p>ISO Date <br />It is common for developers to have problems with inconsistent time formats on a daily basis. ISO 8601 is a date and time format standard developed by the ISO (International Organization for Standardization) to provide a common format for exchanging date and time data across borders, cultures, and industries. <br /> </p></li><li><p>OpenAPI <br />OpenAPI, also known as the OpenAPI Specification (OAS), is a standardized format for describing and documenting RESTful APIs.</p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 08 Jun 2024 15:30:59 GMT</pubDate>
</item>
<item>
<title>A Crash Course on Content-Delivery Networks (CDN)</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-content-delivery</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-content-delivery</guid>
<content:encoded><![CDATA[
<div> Latency, Conversion Rate, E-commerce, Content-Delivery Network, Market Size
<br />
凭借内容交付网络（CDN）可以减少延迟，提高网站性能。根据Portent的研究，页面加载速度快，转化率会更高，电子商务转化率随着加载时间增加而下降。优化的加载时间是少于2秒。CDN是一个地理分布的服务器网络，通过将用户请求重定向到最近的CDN服务器，提供快速可靠的内容传递。CDN市场预计到2028年将达到近380亿美元，Akamai、Cloudflare和亚马逊CloudFront等公司正在大力投资改进CDN解决方案。总结: CDN的使用可以降低延迟，提高网站性能，有效解决了现代网络时代组织面临的性能问题。 <div>
<p>In the era of the modern web, latency can directly impact an organization&#8217;s bottom line.</p><p>Here are some stats from a study by Portent:</p><ul><li><p>A page that loads in 1 second has a 3x higher conversion rate than a page that takes 5 seconds to load.</p></li><li><p>E-commerce conversion rates decrease by 0.3% for every second of load time.</p></li><li><p>The optimal loading time to maximize sales is less than 2 seconds.</p></li></ul><p>One of the most effective strategies to reduce latency is using a Content-Delivery Network or a CDN.</p><p>A CDN is a geographically distributed network of servers that work together to deliver fast and reliable content delivery across the globe.</p><p>When a user requests content from a website or application that uses a CDN, the request is redirected to the nearest CDN server, which serves the content to the user. This reduces the distance data travels and improves overall performance.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2b44b-f305-4e3c-90d1-24e6c181bd93_1600x896.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="815" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2b44b-f305-4e3c-90d1-24e6c181bd93_1600x896.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Think of a CDN like an ATM. If money were available only from one bank branch in town, everyone would have to make a time-consuming trip to that branch. With ATMs in every locality, everyone has fast and easy access to money.&nbsp;</p><p>The market size for CDN-related solutions is expected to reach nearly $38 billion by 2028, with companies like Akamai, Cloudflare, and Amazon CloudFront investing heavily in improving their CDN offerings.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95404fea-d1ef-45ec-96e2-17796bf260d5_1999x1755.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1278" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95404fea-d1ef-45ec-96e2-17796bf260d5_1999x1755.jpeg" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>Why the Need for a CDN?</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-content-delivery">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 06 Jun 2024 15:30:28 GMT</pubDate>
</item>
<item>
<title>AWS Lambda Under the Hood</title>
<link>https://blog.bytebytego.com/p/aws-lambda-under-the-hood</link>
<guid>https://blog.bytebytego.com/p/aws-lambda-under-the-hood</guid>
<content:encoded><![CDATA[
<div> AWS Lambda, Assignment Service, Firecracker, Chunking, SnapStart  
总结:<br /><br />本文介绍了AWS Lambda的架构和内部机制。AWS Lambda是一个无服务器计算服务，支持同步和异步调用模式。介绍了同步调用和异步调用的工作流程，以及Assignment Service的引入解决了Worker Manager的挑战。Firecracker是用于运行AWS Lambda和AWS Fargate等无服务器工作负载的轻量级虚拟机管理器。讨论了Firecracker的重要技术细节，以及SnapStart功能如何减少冷启动延迟。最后，介绍了AWS Lambda作为存储服务的功能，包括代码和数据的有效存储、共享常用数据和使用SnapStart加速初始化等。AWS Lambda通过这些先进技术实现了高效的无服务器计算服务。 <div>
<h2><a href="https://bit.ly/WorkOS_060424">WorkOS: enterprise-grade auth for modern SaaS apps (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/WorkOS_060424" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="763" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79e9b43-75d7-4a65-a598-c9fe68df9033_1600x838.png" width="1456" /><div></div></div></a></figure></div><p>WorkOS helps companies become Enterprise Ready ridiculously fast.</p><p>&#8594; It supports a complete User Management solution along with SSO, SCIM, RBAC, &amp; FGA.</p><p>&#8594; Unlike other auth providers that rely on user-centric models, WorkOS is designed for B2B SaaS with an org modeling approach.</p><p>&#8594; The APIs are flexible, easy-to-use, and modular. Pick and choose what you need and integrate in minutes.</p><p>&#8594; Best of all, User Management is free up to 1 million MAUs&nbsp;and includes bot protection, impersonation, MFA, &amp; more.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/WorkOS_060424"><span>Future-proof your auth stack with WorkOS</span></a></p><div><hr /></div><p><em>Disclaimer: The details in this post have been derived from multiple AWS reInvent talks and articles by the Amazon engineering team. All credit for information about AWS Lambda&#8217;s internals goes to the presenters and authors. The link to these videos and articles is present in the references section at the end of the post. We&#8217;ve attempted to analyze the details and provide our input about them. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><p>AWS Lambda is a serverless compute service that allows developers to run code without provisioning or managing servers.</p><p>Lambda functions can be triggered by various AWS services, HTTP endpoints, or custom events generated by applications. This makes it highly versatile for building event-driven architectures.</p><p>There are some interesting stats about AWS Lambda:</p><ul><li><p>Processes over 10 trillion invocations per month.</p></li><li><p>More than a million customers have built applications using AWS Lambda.</p></li><li><p>The customer base includes individuals, startups, enterprises, and large organizations.</p></li><li><p>Supports various applications, such as IT automation, data processing pipelines, microservices-based applications, web applications, and machine learning workloads.</p></li></ul><p>The timeline below shows the evolution of AWS Lambda over the years.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd69d56b0-4731-4bde-9b42-59925476826c_1600x852.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="775" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd69d56b0-4731-4bde-9b42-59925476826c_1600x852.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>In this post, we will look at the architecture and internals of AWS Lambda.</p><h2>How is a Lambda Function Invoked?</h2><p>There are two types of invocation models supported by Lambda.</p><h3>1 - Synchronous Invocation</h3><p>In synchronous invocation, the caller directly calls the Lambda function. This can be done using the AWS CLI, SDK, or other services like the API Gateway.</p><p>The diagram below shows the entire process:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d363a4c-7e04-4244-81c1-247d2a4a038e_1600x986.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="897" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d363a4c-7e04-4244-81c1-247d2a4a038e_1600x986.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The process involves the following components:</p><ul><li><p><strong>Front-End Service: </strong>The front-end service performs authentication and authorization of the request. It also loads and caches metadata to optimize performance.</p></li><li><p><strong>Counting Service: </strong>This service checks quota limits based on account, burst, or reserved concurrency. It ensures high throughput and low latency while operating across multiple availability zones (AZs).</p></li><li><p><strong>Assignment Service: </strong>The assignment service routes invocation requests to available worker hosts. It coordinates the creation of secure execution environments, setting up limits such as memory and CPU allocation. We will talk more about it in a later section.</p></li><li><p><strong>Placement Service: </strong>The assignment service communicates with the placement service to determine the best worker host to handle the new execution environment. The placement service uses machine learning models to make informed decisions about where to place new execution environments. These models take into account variables such as packing density and fleet utilization to make the decisions.</p></li><li><p><strong>Worker Hosts: </strong>They are responsible for downloading, mounting, and running the function code within a microVM. They manage multiple runtimes (such as Node.js, Java, and Python) and Lambda agents for monitoring and operation.</p></li></ul><p>When an invocation occurs, it can have a warm or cold start.</p><p>If a warm (already running) execution environment is available, the payload is sent directly to it, and the function runs immediately.</p><p>If no warm environment is available, a new execution environment is created. The process involves initializing the runtime, downloading function code, and preparing the environment for execution.</p><p>The assignment service keeps track of execution environments. If there are errors during initialization, the environment is marked unavailable. When environments are near the end of their lease, they are gracefully shut down to ensure stability and resource efficiency.</p><h3>2 - Asynchronous Invocation</h3><p>In asynchronous invocation, the caller doesn&#8217;t wait for the function&#8217;s response. Instead, the event is placed in an internal queue and processed separately.</p><p>The process involves:</p><ul><li><p><strong>Event Invoke Frontend Service: </strong>This service handles asynchronous requests. It authenticates and authorizes the request, then places the event in an internal SQS queue.</p></li><li><p><strong>Polling Instances: </strong>A fleet of pollers reads messages from the SQS queues and sends them to the synchronous front-end service for processing.</p></li><li><p><strong>Synchronous Processing:</strong> Despite being an async request, the actual function execution uses the same synchronous processing pathway as described earlier.</p></li><li><p><strong>Event Destinations: </strong>These can be configured to handle callbacks after processing, providing notifications of success or failure.</p></li></ul><p>The diagram below shows the asynchronous invocation process:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87e33617-1c2e-43cd-8e40-a7e24105b189_1600x833.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="758" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87e33617-1c2e-43cd-8e40-a7e24105b189_1600x833.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Lambda manages multiple internal queues, scaling them dynamically based on load. The queue manager oversees the creation, deletion, and monitoring of these queues to ensure efficient processing.</p><p>As far as polling is concerned, Lambda can poll from various event sources like Kinesis, DynamoDB, SQS, Kafka, and Amazon MQ. The pollers perform the following functions:</p><ul><li><p><strong>Read and Filter: </strong>Poll messages from the sources, optionally filtering them to reduce traffic and simplify function code.</p></li><li><p><strong>Batch Processing: </strong>Batch records together before sending them to the function, optimizing performance and reducing costs.</p></li><li><p><strong>Synchronous Invocation: </strong>Use the same synchronous pathway to execute the function, ensuring consistency.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d97f56a-30a5-4e3d-b069-4bc8f3262ea1_1600x913.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="831" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d97f56a-30a5-4e3d-b069-4bc8f3262ea1_1600x913.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Control plane services (such as State Manager and Stream Tracker) manage the pollers, ensuring they are assigned correctly, scaling them up or down, and handling failures gracefully.</p><p>In case a function fails, the message is returned to the queue with a visibility timeout, allowing for retries.</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="735" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90d1d63b-f864-41e2-8bec-80a41cc800ac_1600x808.png" title="" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-rest-apis">A Crash Course on REST APIs</a></p></li><li><p><a href="https://blog.bytebytego.com/p/api-security-best-practices">API Security Best Practices</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>State Management in Lambda</h2><p>In the earlier architecture of AWS Lambda, the Worker Manager service was responsible for coordinating the execution environments between the frontend invoke service and the worker hosts.</p><p>The worker manager performed a couple of important tasks:</p><ul><li><p><strong>State Management: </strong>The Worker Manager kept track of which execution environments were available and on which hosts they were running.</p></li><li><p><strong>Execution Lifecycle: </strong>It managed the lifecycle of these environments, ensuring that they were ready to process invocations and handling termination when no longer needed.</p></li></ul><p>There were some challenges with the worker manager:</p><ul><li><p><strong>Single Point of Failure: </strong>Each Worker Manager instance stored the state of execution environments in memory. If a Worker manager failed, the execution environments it managed could become orphaned.</p></li><li><p><strong>Orphaned Environments:</strong> Orphaned environments could continue processing current invocations but could not be used for new invocations, leading to cold starts and inefficient resource utilization.</p></li><li><p><strong>Zonal Failures:</strong> In case of a failure in an Availability Zone (AZ), Worker Managers in that AZ would become unavailable. This will also make the environments managed by them unavailable, resulting in capacity loss and increased cold start latencies.</p></li></ul><h3>Introduction of the Assignment Service</h3><p>To address the challenges with Worker Manager, AWS Lambda introduced the Assignment Service which offers a more robust and resilient way to manage execution environments.</p><p>The Assignment Service is written in Rust, which is chosen for its performance, memory safety, and low latency characteristics.&nbsp;</p><p>The diagram below shows the high-level design of the Assignment Service</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2fd3b32-d588-4aa9-a248-992db2be9894_1600x974.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="886" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2fd3b32-d588-4aa9-a248-992db2be9894_1600x974.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The architecture of the Assignment Service has some key properties:</p><h4>1 - Partitioned Design</h4><p>The Assignment Service is divided into multiple partitions. Each partition consists of one leader and two followers, spread across different Availability Zones (AZs). This ensures high availability and fault tolerance.</p><p>Multiple partitions run simultaneously, with each Assignment Service host managing several partitions. This helps with load distribution and provides redundancy.</p><h4>2 - External Journal Log Service</h4><p>The leader in each partition writes the state of execution environments to an external journal log service. The followers read from this log to keep their state in sync with the leader.</p><p>In case the leader fails, one of the followers can quickly take over as the new leader using the state information from the journal log.</p><h4>3 - Frontend Interaction</h4><p>The frontend invoke service interacts with the leader of the relevant partition to manage invocations.&nbsp;</p><p>The leader coordinates with the placement service to create new execution environments and writes this information to the log for followers to update their state.</p><h4>4 - Failure Handling</h4><p>If an AZ experiences an outage, the leader and followers in other AZs continue to operate. This means that execution environments in the unaffected AZs remain available and aren&#8217;t orphaned.</p><p>If a leader fails, a follower can take over quickly using the replicated state, ensuring that there are no interruptions in service and reducing the chances of cold starts.</p><h2>The Role of Firecracker MicroVM</h2><p>Firecracker is a lightweight virtual machine manager (VMM) designed specifically for running serverless workloads such as AWS Lambda and AWS Fargate. It uses Linux&#8217;s Kernel-based Virtual Machine to create and manage microVMs.</p><p>The primary goal of Firecracker is to provide secure and fast-booting microVMs.</p><p>Initially, AWS Lambda used EC2 instances to run functions, dedicating a T2 instance to each tenant. This was secure but inefficient, leading to high overhead due to the need for provisioning entire EC2 instances for each function.</p><p>In 2018, Lambda transitioned to using Firecracker for managing execution environments. This allowed for much smaller, lightweight microVMs, significantly reducing resource overhead and improving the fleet&#8217;s efficiency.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffac8de07-6586-4bf7-9d48-f6db8322b148_1600x1295.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1178" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffac8de07-6586-4bf7-9d48-f6db8322b148_1600x1295.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Here are some key technical details of the Firecracker implementation.</p><h3>1 - MicroVMs and Isolation</h3><p>Firecracker enables the creation of microVMs, which are smaller and more efficient than traditional VMs. Each microVM runs its isolated execution environment, including the runtime, function code, and any extensions.</p><p>Firecracker ensures a strong solution between microVMs, which is crucial for multi-tenant environments where different customer functions run on the same physical host.</p><h3>2 - Startup Times</h3><p>Firecracker microVMs are designed to boot in milliseconds, significantly reducing the cold start latency for Lambda functions.&nbsp;</p><p>For further optimization, Lambda uses a feature called SnapStart (initially available for the Corretto Java 11 runtime). SnapStart pre-initializes execution environments and takes snapshots, which are restored on demand when needed. We will talk more about it in the next section.</p><h3>3 - Resource Utilization</h3><p>By running multiple microVMs on a single physical host, Firecracker improves CPU utilization, memory, and storage resources. This allows Lambda to handle more functions with fewer resources.</p><p>Also, Firecracker supports the dynamic scaling of resources for quick adjustment to changing workloads.</p><h3>4 - Storage and Networking</h3><p>Lambda uses a virtual file system driver to present an EXT4 file system to the microVMs. This driver handles file system calls and maps them to the underlying storage.</p><p>Firecracker optimizes network performance by providing high-speed virtual network interfaces, ensuring fast and reliable communication.</p><h2>Lambda as a Storage Service</h2><p>AWS Lambda is primarily known as a serverless compute service. However, it also incorporates significant storage service functionalities.</p><p>Think of Lambda not just as a compute service but also as a storage service because it deals with storing and managing a lot of data behind the scenes.</p><p>But why is storage important for Lambda?</p><p>It&#8217;s because when you run a function on Lambda, it needs three main things:</p><ul><li><p><strong>Input Data:</strong> The information or data your function will process</p></li><li><p><strong>Function Code:</strong> The code you wrote that tells Lambda what to do&nbsp;</p></li><li><p><strong>Execution Environment:</strong> The virtual machine (VM) where your code runs.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4de897-b4c0-4f07-84bb-58b1eccf1732_1600x1242.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1130" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4de897-b4c0-4f07-84bb-58b1eccf1732_1600x1242.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>To make all of this work smoothly, Lambda needs to manage and store these components efficiently.</p><h3>1 - Storing Code and Data Efficiently</h3><p>Instead of storing your entire code or data as one big piece, Lambda breaks it down into smaller pieces called chunks.</p><p>This process is known as chunking and it allows Lambda to handle large and complex functions more effectively.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f76a65b-82ed-47c3-80a4-33b2d2e6e477_1600x880.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="801" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f76a65b-82ed-47c3-80a4-33b2d2e6e477_1600x880.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Here&#8217;s how it works:</p><ul><li><p><strong>Breaking down container images: </strong>When you upload a container image (which can be up to 10 GB) to AWS Lambda, it doesn&#8217;t store the entire image as one big file. Instead, Lambda breaks this image down into smaller pieces or chunks. For example, the base OS layer is broken down into smaller pieces, the runtime layer into another set of pieces, and so on.</p></li><li><p><strong>Storage and Access: These chunks are then stored separately, making it easier to manage and retrieve them as needed. When your function runs, Lambda only loads the chunks it needs at that moment, rather than the entire image. This is much faster and more efficient.</strong></p></li><li><p><strong>Retrieving Chunks: When your function is invoked, Lambda checks which parts of the container image it needs to execute the function. It then retrieves only those specific chunks from storage.</strong></p></li><li><p><strong>Optimizing Performance: Since only the required chunks are loaded, this reduces the amount of data that needs to be transferred and processed, leading to faster startup times and better performance. Frequently accessed chunks can be cached to avoid fetching them from the main storage repeatedly.</strong></p></li></ul><h3>2 - Sharing Common Data</h3><p>Lambda often runs many functions that use similar code or data such as base operating system layer or common runtime libraries.&nbsp;</p><p>Instead of storing multiple copies of the same thing, it stores one copy and reuses it.</p><p>To make sure the shared data is secure, Lambda uses a technique called convergent encryption.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88eb699a-bd99-4d18-aea8-c9a3b96b554e_1600x881.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="802" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88eb699a-bd99-4d18-aea8-c9a3b96b554e_1600x881.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>But how does convergent encryption work?</p><ul><li><p><strong>Plaintext Chunk Creation: </strong>Lambda starts with a plaintext chunk, which is a segment of the flattened file system of a container image. Each chunk represents a specific range of the block device that makes up the file system.</p></li><li><p><strong>Appending Extra Data:</strong> To ensure security and integrity, additional data is appended to the chunk. This extra data is generated by the Lambda service and can include metadata or unique identifiers. The purpose of this extra data is to add a layer of uniqueness to the chunk.</p></li><li><p><strong>Hash Computation: </strong>A cryptographic hash function is used to compute a hash of the combined plaintext chunk and extra data. This hash serves as the encryption key for that specific chunk, ensuring that identical chunks produce the same hash and thus the same encryption key.</p></li><li><p><strong>Encryption: </strong>The plaintext chunk, along with the extra data, is then encrypted using the computed hash as the encryption key.</p></li><li><p><strong>Manifest Creation: </strong>Lambda creates a manifest file that contains the keys (hashes) and pointers to the encrypted chunks in the storage subsystem. The manifest itself is encrypted with a customer-specific AWS Key Management Service, ensuring that only authorized entities can access it.</p></li></ul><h3>3 - Making Initialization Faster with SnapStart</h3><p>When a Lambda function is invoked and there isn&#8217;t an already running execution environment, Lambda has to create a new one. This involves several steps:</p><ul><li><p><strong>Provisioning</strong>: Setting up a new virtual machine</p></li><li><p><strong>Downloading: </strong>Retrieving the function code from storage</p></li><li><p><strong>Initializing: </strong>Starting the runtime and initializing the function code.</p></li></ul><p>These steps can take time, especially for complex functions or runtimes like Java, which have longer startup times. This delay is known as cold start latency.</p><p>SnapStart is a feature designed to significantly reduce cold start latency by pre-initializing the execution environment and then using snapshots.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbda5bb9f-e59e-4248-b7f6-aaf55e6e1431_1600x875.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="796" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbda5bb9f-e59e-4248-b7f6-aaf55e6e1431_1600x875.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Here&#8217;s how it works:</p><ul><li><p>When you update your function code or deploy a new version, Lambda performs an initial setup and creates a new execution environment.</p></li><li><p>Once the execution environment is fully set up and ready to handle invocations, Lambda takes a snapshot of this environment. The snapshot captures the entire state of the VM, including the memory state, runtime state, and initialized data. The snapshot is then broken down into smaller pieces and encrypted to ensure security.</p></li><li><p>These encrypted chunks are stored securely.</p></li><li><p>When a new invocation request arrives, Lambda first checks if there&#8217;s an already running execution environment (a warm start). If not, it proceeds to use the snapshot. The necessary chunks are loaded and decrypted and the environment is brought back to the state it was in when the snapshot was taken.</p></li></ul><h2>Conclusion</h2><p>AWS Lambda revolutionized the way developers build and deploy applications by providing a serverless environment that abstracts away the complexities of infrastructure management.</p><p>Here are the main takeaways from the internals of AWS Lambda:</p><ul><li><p>To achieve the impressive statistics associated with Lambda, AWS leverages multiple advanced techniques such as chunking and convergent encryption to manage and share common data.&nbsp;</p></li><li><p>The introduction of SnapStart addressed the critical challenge of cold start latency, particularly for runtimes like Java.</p></li><li><p>The transition from the Worker Manager to the Assignment Service marked a significant improvement in state management.</p></li><li><p>With Firecracker microVMs, Lambda achieves lightweight and secure virtualization.</p></li></ul><p><strong>References:</strong></p><ul><li><p><a href="https://youtu.be/0_jfH6qijVY?si=z_mHoo_lDT3OLrYG">A closer look at AWS Lambda</a></p></li><li><p><a href="https://www.infoq.com/presentations/aws-lambda-arch/">AWS Lambda Under the Hood</a></p></li><li><p><a href="https://youtu.be/AECR8WMHjv0?si=yB6O9FcnmCPyhnUq">AWS Lambda Under the Hood - YouTube</a></p></li><li><p><a href="https://youtu.be/QdzV04T_kec?si=X79wEB5zzmJQLogj">A Serverless Journey: AWS Lambda</a></p></li><li><p><a href="https://www.amazon.science/blog/how-awss-firecracker-virtual-machines-work">How do AWS Firecracker virtual machines work?</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></p>
]]></content:encoded>
<pubDate>Tue, 04 Jun 2024 15:31:00 GMT</pubDate>
</item>
<item>
<title>EP114: 7 Must-know Strategies to Scale Your Database</title>
<link>https://blog.bytebytego.com/p/ep114-7-must-know-strategies-to-scale</link>
<guid>https://blog.bytebytego.com/p/ep114-7-must-know-strategies-to-scale</guid>
<content:encoded><![CDATA[
<div> Scaling databases, Retry strategies, Reddit's architecture, Cross-Site Scripting, Sponsorship
<br />
数据库扩展，重试策略，Reddit架构，跨站脚本攻击，赞助
<br /><br />总结:
<br />
本文讨论了数据库扩展的7个策略，重试失败的方法，Reddit的核心架构，跨站脚本攻击以及赞助广告。数据库扩展策略包括索引、物化视图、去标准化、垂直扩展、缓存、复制和分片。重试策略涵盖了线性、指数和随机的不同方式。Reddit架构采用CDN、微服务以及多种存储和数据处理技术。跨站脚本攻击是常见漏洞，要加强用户意识和防范措施。赞助广告提供推广机会，影响大批技术专业人士。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>7 must-know strategies to scale your database</p></li><li><p>How do we retry on failures?</p></li><li><p>Reddit&#8217;s Core Architecture</p></li><li><p>Everything You Need to Know About Cross-Site Scripting (XSS)</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/Nylas_060124CTA">Your ultimate guide to integrating email, calendars &amp; contacts (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Nylas_060124CTA" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1200" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37925e71-6a06-4d9f-811a-90b0336d8733_1200x1200.png" width="1200" /><div></div></div></a></figure></div><p>Launch native email, calendar, and contacts capabilities with the greatest possible ROI. This latest guide from Nylas walks you through the most common options to launch these integrations for all major email and calendar service providers (Gmail, Outlook, IMAP, etc.) including APIs vs. building yourself. <a href="https://bit.ly/Nylas_060124CTA">Read on to discover best practices and</a>:<br /></p><ul><li><p>How complex it is to build the email, calendar and contacts integration from scratch</p></li><li><p>The true cost of building your own email, calendar, contacts integration</p></li><li><p>6 Questions for CTOs and product managers to future-proof their business</p></li></ul><p>If you're interested in trying out an API that integrates these email and calendar service providers for you, <strong><a href="https://bit.ly/Nylas_060124dashboard">check out Nylas</a></strong>.&nbsp;</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Nylas_060124CTA"><span>Read More</span></a></p><div><hr /></div><h2>7 must-know strategies to scale your database</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f560185-6b26-4359-ae8a-123b44d8884a_1345x1536.gif" title="No alternative text description for this image" width="1345" /><div></div></div></a></figure></div><ol><li><p>Indexing: <br />Check the query patterns of your application and create the right indexes. <br /></p></li><li><p>Materialized Views: <br />Pre-compute complex query results and store them for faster access. <br /></p></li><li><p>Denormalization: <br />Reduce complex joins to improve query performance. <br /></p></li><li><p>Vertical Scaling <br />Boost your database server by adding more CPU, RAM, or storage. <br /></p></li><li><p>Caching <br />Store frequently accessed data in a faster storage layer to reduce database load. <br /></p></li><li><p>Replication <br />Create replicas of your primary database on different servers for scaling the reads. <br /></p></li><li><p>Sharding <br />Split your database tables into smaller pieces and spread them across servers. Used for scaling the writes as well as the reads. </p></li></ol><p>Over to you: What other strategies do you use for scaling your databases?</p><div><hr /></div><h2>How do we retry on failures?</h2><p>In distributed systems and networked applications, retry strategies are crucial for handling transient errors and network instability effectively. The diagram shows 4 common retry strategies.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24495c00-fa48-45c0-90f4-6dbfb6cb1c95_1291x1536.gif" title="No alt text provided for this image" width="1291" /><div></div></div></a></figure></div><ul><li><p>Linear Backoff<br />Linear backoff involves waiting for a progressively increasing fixed interval between retry attempts. <br /><br />Advantages: Simple to implement and understand. <br /><br />Disadvantages: May not be ideal under high load or in high-concurrency environments as it could lead to resource contention or "retry storms".<br /></p></li><li><p>Linear Jitter Backoff<br />Linear jitter backoff modifies the linear backoff strategy by introducing randomness to the retry intervals. This strategy still increases the delay linearly but adds a random "jitter" to each interval. <br /><br />Advantages: The randomness helps spread out the retry attempts over time, reducing the chance of synchronized retries across instances.<br /><br />Disadvantages: Although better than simple linear backoff, this strategy might still lead to potential issues with synchronized retries as the base interval increases only linearly.<br /></p></li><li><p>Exponential Backoff<br />Exponential backoff involves increasing the delay between retries exponentially. The interval might start at 1 second, then increase to 2 seconds, 4 seconds, 8 seconds, and so on, typically up to a maximum delay. This approach is more aggressive in spacing out retries than linear backoff.<br /><br />Advantages: Significantly reduces the load on the system and the likelihood of collision or overlap in retry attempts, making it suitable for high-load environments.<br /><br />Disadvantages: In situations where a quick retry might resolve the issue, this approach can unnecessarily delay the resolution.<br /></p></li><li><p>Exponential Jitter Backoff<br />Exponential jitter backoff combines exponential backoff with randomness. After each retry, the backoff interval is exponentially increased, and then a random jitter is applied. The jitter can be either additive (adding a random amount to the exponential delay) or multiplicative (multiplying the exponential delay by a random factor). <br /><br />Advantages: Offers all the benefits of exponential backoff, with the added advantage of reducing retry collisions even further due to the introduction of jitter.<br /><br />Disadvantages: The randomness can sometimes result in longer than necessary delays, especially if the jitter is significant.</p></li></ul><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1158" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd70ae0a-7b12-4bf7-a15d-ed5a3f4a9117_1600x1272.png" title="" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-on-rest-apis">A Crash Course on REST APIs</a></p></li><li><p><a href="https://blog.bytebytego.com/p/api-security-best-practices">API Security Best Practices</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>Reddit&#8217;s Core Architecture</h2><p>A quick look at Reddit&#8217;s Core Architecture that helps it serve over 1 billion users every month. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application, Teams" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c184477-46df-4f37-b224-262c64a6ec58_1280x1611.gif" title="graphical user interface, application, Teams" /><div></div></div></a></figure></div><p>This information is based on research from many Reddit engineering blogs. But since architecture is ever-evolving, things might have changed in some aspects. <br /> <br />The main points of Reddit&#8217;s architecture are as follows: </p><ol><li><p>Reddit uses a Content Delivery Network (CDN) from Fastly as a front for the application <br /></p></li><li><p>Reddit started using jQuery in early 2009. Later on, they started using Typescript and have now moved to modern Node.js frameworks. Over the years, Reddit has also built mobile apps for Android and iOS. <br /></p></li><li><p>Within the application stack, the load balancer sits in front and routes incoming requests to the appropriate services. <br /></p></li><li><p>Reddit started as a Python-based monolithic application but has since started moving to microservices built using Go. <br /></p></li><li><p>Reddit heavily uses GraphQL for its API layer. In early 2021, they started moving to GraphQL Federation, which is a way to combine multiple smaller GraphQL APIs known as Domain Graph Services (DGS). In 2022, the GraphQL team at Reddit added several new Go subgraphs for core Reddit entities thereby splitting the GraphQL monolith. <br /></p></li><li><p>From a data storage point of view, Reddit relies on Postgres for its core data model. To reduce the load on the database, they use memcached in front of Postgres. Also, they use Cassandra quite heavily for new features mainly because of its resiliency and availability properties. <br /></p></li><li><p>To support data replication and maintain cache consistency, Reddit uses Debezium to run a Change Data Capture process. <br /></p></li><li><p>Expensive operations such as a user voting or submitting a link are deferred to an async job queue via RabbitMQ and processed by job workers. For content safety checks and moderation, they use Kafka to transfer data in real-time to run rules over them. <br /></p></li><li><p>Reddit uses AWS and Kubernetes as the hosting platform for its various apps and internal services. <br /></p></li><li><p>For deployment and infrastructure, they use Spinnaker, Drone CI, and Terraform. </p></li></ol><p>Over to you: what other aspects do you know about Reddit&#8217;s architecture?</p><div><hr /></div><h2>Everything You Need to Know About Cross-Site Scripting (XSS)</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1668" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2debd75-6209-41a3-a13f-f2364c281444_1280x1668.jpeg" title="No alt text provided for this image" width="1280" /><div></div></div></a></figure></div><p>XSS, a prevalent vulnerability, occurs when malicious scripts are injected into web pages, often through input fields. Check out the diagram below for a deeper dive into how this vulnerability emerges when user input is improperly handled and subsequently returned to the client, leaving systems vulnerable to exploitation.<br /><br />Understanding the distinction between Reflective and Stored XSS is crucial. Reflective XSS involves immediate execution of the injected script, while Stored XSS persists over time, posing long-term threats. Dive into the diagrams for a comprehensive comparison of these attack vectors.<br /><br />Imagine this scenario: A cunning hacker exploits XSS to clandestinely harvest user credentials, such as cookies, from their browser, potentially leading to unauthorized access and data breaches. It's a chilling reality.<br /><br />But fret not! Our flyer also delves into effective mitigation strategies, empowering you to fortify your systems against XSS attacks. From input validation and output encoding to implementing strict Content Security Policies (CSP), we've got you covered.<br /><br />Over to you: How can we amplify user awareness to proactively prevent falling victim to XSS attacks? Share your insights and strategies below! Let's collaboratively bolster our web defenses and foster a safer digital environment.</p><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong><br /></p>
]]></content:encoded>
<pubDate>Sat, 01 Jun 2024 15:30:57 GMT</pubDate>
</item>
<item>
<title>A Crash Course on REST APIs</title>
<link>https://blog.bytebytego.com/p/a-crash-course-on-rest-apis</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-on-rest-apis</guid>
<content:encoded><![CDATA[
<div> APIs、软件、接口、通信、REST API<br />
REST API是软件之间通信的重要方式，通过定义一组规则、协议和方法，在不同的应用程序之间建立联系。从60年代的子程序和库到80年代的远程过程调用（RPC），再到2000年代的Web服务和最近的RESTful APIs，API的形式不断演变。如今，REST API已成为开发者的首选，其简单性和可扩展性备受推崇。API-first的软件开发方法也逐渐流行，强调构建解耦服务。通过探索REST API的基本和高级概念，我们能更深入了解这一领域。 <br /><br />总结: <br />APIs是软件之间的重要通信方式，REST API通过定义规则和方法促进了不同应用程序之间的联系，使得软件开发更加灵活和可扩展。 <div>
<p>Application Programming Interfaces (APIs) are the backbone of software communication.</p><p>In the acronym API, the word &#8220;Application&#8221; refers to software that performs a distinct function. An &#8220;Interface&#8221; is a contract between two applications that defines a set of rules, protocols, and methods for communication. &#8220;Programming&#8221; makes all of this possible.</p><p>APIs have been around for a long time in one form or the other:</p><ul><li><p>In the 60s and 70s, we had subroutines and libraries to share code and functionality between programs.&nbsp;</p></li><li><p>In the 1980s, Remote Procedure Calls (RPC) emerged, allowing programs running on different computers to execute procedures on each other.</p></li><li><p>With the widespread adoption of the Internet in the 2000s, web services such as SOAP became widely adopted.</p></li><li><p>The late 2000s and early 2010s marked the rise of RESTful APIs, which have since become the dominant approach due to their simplicity and scalability.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd70ae0a-7b12-4bf7-a15d-ed5a3f4a9117_1600x1272.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1158" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd70ae0a-7b12-4bf7-a15d-ed5a3f4a9117_1600x1272.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>In recent years, the API-first approach to software development has gained significant traction, driven by the emphasis on building loosely coupled services. REST APIs, in particular, have emerged as the go-to choice for developers worldwide.</p><p>In this post, we will explore the world of REST APIs and cover basic to advanced concepts.</p><div><hr /></div><h2>Introduction to REST APIs</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-on-rest-apis">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 30 May 2024 15:30:25 GMT</pubDate>
</item>
<item>
<title>The Scaling Journey of LinkedIn</title>
<link>https://blog.bytebytego.com/p/the-scaling-journey-of-linkedin</link>
<guid>https://blog.bytebytego.com/p/the-scaling-journey-of-linkedin</guid>
<content:encoded><![CDATA[
<div> containerized environments, scaling, service-oriented architecture, caching, real-time analytics

总结：<br /><br />这篇文章主要介绍了LinkedIn在发展过程中的扩展之路。从最初的单一的单体系统到服务面向架构的转变，LinkedIn采取了多种技术手段来应对不断增长的需求。文章中详细介绍了LinkedIn如何通过构建新的服务来解决不同需求，包括成员连接管理、搜索功能、数据库复制等。LinkedIn还采用缓存技术和实时分析技术来支持其日益增长的数据需求。最后，LinkedIn还提出了解决授权管理问题的方案，以保护用户数据安全。LinkedIn的经验对于其他开发者团队来说是宝贵的参考，尤其是在面对快速扩展需求时。 <div>
<h2><a href="https://bit.ly/Datadog_052824Headline">10 Insights On Real-World Container Usage (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Datadog_052824Headline" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1080" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35436006-94db-4232-878b-cbbeae4ea59e_1080x1080.png" width="1080" /><div></div></div></a></figure></div><p>As organizations have scaled their containerized environments, many are now exploring the next technology frontier of containers by building next-gen applications, enhancing developer productivity, and optimizing costs.</p><p>Datadog analyzed telemetry data from over 2.4 billion containers to understand the present container landscape, with key insights into:</p><ul><li><p>Trends in adoption for technologies such as serverless containers and GPU-based compute on containers</p></li><li><p>How organizations are managing container costs and resources through ARM-based compute and horizontal pod autoscaling</p></li><li><p>Popular workload categories and languages for containers</p></li></ul><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Datadog_052824Headline"><span>Read the report</span></a></p><div><hr /></div><p>LinkedIn is one of the biggest social networks in the world with almost a billion members.</p><p>But the platform had humble beginnings.</p><p>The idea of LinkedIn was conceived in Reid Hoffman&#8217;s living room in 2002 and it was officially launched in May 2003. There were 11 other co-founders from Paypal and Socialnet who collaborated closely with Reid Hoffman on this project.</p><p>The initial start was slow and after the first month of operation, LinkedIn had just around 4300 members. Most of them came through personal invitations by the founding members.&nbsp;</p><p>However, LinkedIn&#8217;s user base grew exponentially over time and so did the content hosted on the platform. In a few years, LinkedIn was serving tens of thousands of web pages every second of every day to users all over the world.</p><p>This unprecedented growth had one major implication.</p><p>LinkedIn had to take on some extraordinary challenges to scale its application to meet the growing demand. While it would&#8217;ve been tough for the developers involved in the multiple projects, it&#8217;s a gold mine of lessons for any developer.</p><p>In this article, we will look at the various tools and techniques LinkedIn adopted to scale the platform.</p><div><hr /></div><h2>Humble Beginning with Leo</h2><p>Like many startups, LinkedIn also began life with a <strong>monolithic architecture</strong>.</p><p>There was one big application that took care of all the functionality needed for the website. It hosted web servlets for the various pages, handled business logic, and also connected to the database layer.&nbsp;</p><p>This monolithic application was internally known as Leo. Yes, it was as magnificent as MGM&#8217;s Leo the Lion.</p><p>The below diagram represents the concept of Leo on a high level.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbcf884de-6fbe-4259-a6a5-fd174c2bbd34_1600x1344.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1223" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbcf884de-6fbe-4259-a6a5-fd174c2bbd34_1600x1344.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>However, as the platform grew in terms of functionality and complexity, the monolith wasn&#8217;t enough.</p><h2>The First Need of Scaling</h2><p>The first pinch point came in the form of two important requirements:</p><ul><li><p>Managing member-to-member connections</p></li><li><p>Search capabilities</p></li></ul><p>Let&#8217;s look at both.</p><h3>Member Graph Service</h3><p>A social network depends on connections between people.&nbsp;</p><p>Therefore, it was critical for LinkedIn to effectively manage member to member connections. For example, LinkedIn shows the graph distance and common connections whenever we view a user profile on the site.</p><p>To display this small piece of data, they needed to perform low-latency graph computations, creating the need for a system that can query connection data using <strong>in-memory graph traversals</strong>. The in-memory requirement is key to realizing the performance goals of the system.&nbsp;</p><p>Such a system had a completely different usage profile and scaling need as compared to Leo.</p><p>Therefore, the engineers at LinkedIn built a <strong>distributed and partitioned graph system</strong> that can store millions of members and their connections. It could also handle hundreds of thousands of queries per second (QPS).</p><p>The system was called Cloud and it happened to be the first service at LinkedIn. It consisted of three major subcomponents:</p><ol><li><p><strong>GraphDB</strong> - a partitioned graph database that was also replicated for high availability and durability</p></li><li><p><strong>Network Cache Service</strong> - a distributed cache that stores a member&#8217;s network and serves queries requiring second-degree knowledge</p></li><li><p><strong>API Layer</strong> - the access point for the front end to query the data.</p></li></ol><p>The below diagram shows the high-level architecture of the member graph service.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F141d467c-35c1-4946-8f0e-a9b295e06fcd_1600x1041.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="947" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F141d467c-35c1-4946-8f0e-a9b295e06fcd_1600x1041.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>To keep it separate from Leo, LinkedIn utilized Java RPC for communication between the monolith and the graph service.</p><h3>Search Service</h3><p>Around the same time, LinkedIn needed to support another critical functionality - <strong>the capability to search people and topics</strong>.&nbsp;</p><p>It is a core feature for LinkedIn where members can use the platform to search for people, jobs, companies, and other professional resources. Also, this search feature should aim to provide deeply personalized search results based on a member&#8217;s identity and relationships.</p><p>To support this requirement, a new search service was built using <strong>Lucene</strong>.&nbsp;</p><p>Lucene is an open-source library that provides three functionalities:</p><ul><li><p>Building a search index</p></li><li><p>Searching the index for matching entities</p></li><li><p>Determining the importance of these entities through relevant scores</p></li></ul><p>Once the search service was built, both the monolith and the new member graph service started feeding data into this service.</p><p>While the building of these services solved key requirements, the continued growth in traffic on the main website meant that Leo also had to scale.</p><p>Let&#8217;s look at how that was achieved.</p><h2>Scaling Leo</h2><p>As LinkedIn grew in popularity, the website grew and Leo&#8217;s roles and responsibilities also increased. Naturally, the once-simple web application became more complex.</p><p><strong>So - how was Leo scaled?</strong></p><p>One straightforward method was to spin up multiple instances of Leo and run them behind a Load Balancer that routes traffic to these instances.</p><p>It was a nice solution but it only involved the application layer of Leo and not the database. However, the increased workload was negatively impacting the performance of LinkedIn&#8217;s most critical system - its <strong>member profile database</strong> that stored the personal information of every registered user. Needless to say, this was the heart of LinkedIn.</p><p>A quick and easy fix for this was going for classic vertical scaling by throwing additional compute capacity and memory for running the database. It&#8217;s a good approach to buy some time and get some breathing space for the team to think about a long-term solution to scaling the database.</p><p>The member profile database had one major issue. It handled both read and write traffic, resulting in a heavy load.&nbsp;</p><p>To scale it out, the team turned to <strong>database replication</strong>.</p><p>New replica databases were created. These replicas were a copy of the primary database and stayed in sync with the primary using Databus. While writes were still handled by the primary database, the trick was to send the majority of read requests to the replica databases.</p><p>However, data replication always results in some amount of replication lag. If a request reads from the primary database and the replica database at the same time, it can get different results because the replication may not have completed. A classic example is a user updating her profile information and not able to see the updated data on accessing the profile just after the update.</p><p>To deal with issues like this, special logic was built to decide when it was safe or consistent to read from the replica database versus the primary database.</p><p>The below diagram tries to represent the architecture of LinkedIn along with database replication</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ae6342-e436-4fb9-87e8-319992fd6e71_1600x1035.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="942" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ae6342-e436-4fb9-87e8-319992fd6e71_1600x1035.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>While replication solved a major scaling challenge for LinkedIn, the website began to see more and more traffic. Also, from a product point of view, LinkedIn was evolving rapidly.</p><p>It created two major challenges:</p><ul><li><p>Leo was often going down in production and it was becoming more difficult to recover from failures</p></li><li><p>Releasing new features became tough due to the complexity of the monolithic application</p></li></ul><p>High availability is a critical requirement for LinkedIn. A social network being down can create serious ripple effects for user adoption. It soon became obvious that they had to kill Leo and break apart the monolithic application into more manageable pieces.</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="840" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7bb39625-0f08-43c8-89fb-b9a0b90406f6_2496x1440.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/api-security-best-practices">API Security Best Practices</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li><li><p><a href="https://blog.bytebytego.com/p/what-happens-when-a-sql-is-executed">What Happens When a SQL is Executed?</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>Killing Leo with Service-Oriented Architecture</h2><p>While it sounds easy to break apart the monolithic application, it&#8217;s not easy to achieve in practice.</p><p>You want to perform the migration in a seamless manner without impacting the existing functionality. Think of it as changing a car&#8217;s tires while it is moving on the highway at 60 miles per hour.</p><p>The engineers at LinkedIn started to extract functionalities from the monolith in their own separate services. Each service contained APIs and business logic specific to a particular functionality.</p><p>Next, services to handle the <strong>presentation layer </strong>were built<strong> </strong>such as public profiles or recruiter products. For any new product, brand-new services were created completely outside of Leo.&nbsp;</p><p>Over time, the effort towards SOA led to the emergence of vertical slices where each slice handled a specific functional area.</p><ul><li><p>The frontend servers fetched data from different domains and handled the presentation logic to build the HTML via JSPs.</p></li><li><p>The mid-tier services provided API access to data models.</p></li><li><p>The backend data services provided consistent access to the database.</p></li></ul><p>By 2010, LinkedIn had already built over 150 separate services and by 2015, they had over 750 services.</p><p>The below diagram represents a glimpse of the SOA-based design at LinkedIn:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5da33485-b160-4f78-ac18-37c6e613d63b_1377x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5da33485-b160-4f78-ac18-37c6e613d63b_1377x1600.png" width="1377" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>At this point, you may wonder what was the benefit of this massive change.</p><ul><li><p>First, these services were built in a <strong>stateless manner</strong>. Scaling can be achieved by spinning up new instances of a service and putting them behind a load balancer. Such an approach is known as horizontal scaling and it was more cost-effective compared to scaling the monolithic application.</p></li><li><p>Second, each service was expected to define how much load it could take and the engineering team was able to build out early provisioning and performance monitoring capabilities to support any deviations.</p></li></ul><h2>Managing Hypergrowth with Caching</h2><p>It&#8217;s always a good thing for a business owner to achieve an exponential amount of growth.</p><p>Of course, it does create a bunch of problems. Happy problems but still problems that must be solved.</p><p>Despite moving to service-oriented architecture and going for replicated databases, LinkedIn had to scale even further.</p><p>This led to the adoption of caching.</p><p>Many applications started to introduce mid-tier caching layers like memcached or couchbase. These caches were storing derived data from multiple domains. Also, they added caches to the data layers by using Voldemort to store precomputed results when appropriate.</p><p>However, if you&#8217;ve worked with caching, you would know that caching brings along with it a bunch of new challenges in terms of invalidations, managing consistency, and performance.</p><p>Over time, the LinkedIn team got rid of many of the mid-tier caches.&nbsp;</p><p>Caches were kept close to the data store in order to reduce the latency and support horizontal scalability without the cognitive load of maintaining multiple caching layers.</p><h2>Data Collection with Kafka</h2><p>As LinkedIn&#8217;s footprint grew, it also found itself managing a huge amount of data.</p><p>Naturally, when any company acquires a lot of data, it wants to put that data to good use for growing the business and offering more valuable services to the users. However, to make meaningful conclusions from the data, they have to collect the data and bring it in one place such as a data warehouse.</p><p>LinkedIn started developing many custom data pipelines for streaming and queuing data from one system to another.</p><p>Some of the applications were as follows:</p><ul><li><p>Aggregating logs from every service</p></li><li><p>Collecting data regarding tracking events such as pageviews</p></li><li><p>Queuing of emails for LinkedIn&#8217;s inMail messaging system</p></li><li><p>Keeping the search system up to date whenever someone updates their profile</p></li></ul><p>As LinkedIn grew, it needed more of these custom pipelines and each individual pipeline also had to scale to keep up with the load.</p><p>Something had to be done to support this requirement.</p><p>This led to the development of Kafka, a distributed pub-sub messaging platform. It was built around the concept of a commit log and its main goal was to enable speed and scalability.</p><p>Kafka became a universal data pipeline at LinkedIn and enabled near real-time access to any data source. It empowered the various Hadoop jobs and allowed LinkedIn to build real-time analytics, and improve site monitoring and alerting.</p><p>See the below diagram that shows the role of Kafka at LinkedIn.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac326132-2101-4bdc-895b-0138e1ed9e28_1600x1438.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1309" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac326132-2101-4bdc-895b-0138e1ed9e28_1600x1438.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Over time, Kafka became an integral part of LinkedIn&#8217;s architecture. Some latest facts about Kafka adoption at LinkedIn are as follows:</p><ul><li><p>Over 100 Kafka clusters with more than 4000 brokers</p></li><li><p>100K topics and 7 million partitions</p></li><li><p>7 trillion messages handled per day</p></li></ul><h2>Scaling the Organization with Inversion</h2><p>While scaling is often thought of as a software concern, LinkedIn realized very soon that this is not true.</p><p>At some time, you also need to scale up at an organizational level.&nbsp;</p><p>At LinkedIn, the organizational scaling was carried out via an internal initiative called <strong>Inversion</strong>.</p><p>Inversion put a pause on feature development and allowed the entire engineering organization to focus on improving the tooling and deployment, infrastructure and developer productivity. In other words, they decided to focus on improving the developer experience.</p><p>The goal of Inversion was to increase the engineering capability of the development teams so that new scalable products for the future could be built efficiently and in a cost-effective way.&nbsp;</p><p>Let&#8217;s look at a few significant tools that were built as part of this initiative:</p><h3>Rest.li</h3><p>During the transformation from Leo to a service-oriented architecture, the extracted APIs were based on Java-based RPC.&nbsp;</p><p>Java-based RPC made sense in the early days but it was no longer sufficient as LinkedIn&#8217;s systems evolved into a polyglot ecosystem with services being written in Java, Node.js, Python, Ruby and so on. For example, it was becoming hard for mobile services written in Node.js to communicate with Java object-based RPC services.</p><p>Also, the earlier APIs were tightly coupled with the presentation layer, making it difficult to make changes.</p><p>To deal with this, the LinkedIn engineers created a new API model called <strong>Rest.li</strong>.</p><p><strong>What made Rest.li so special?</strong></p><p>Rest.li was a framework for developing RESTful APIs at scale. It used simple JSON over HTTP, making it easy for non-Java-based clients to communicate with Java-based APIs.</p><p>Also, Rest.li was a step towards a data-model-based architecture that brought a consistent API model across the organization.&nbsp;</p><p>To make things even more easy for developers, they started using Dynamic Discovery (D2) with Rest.li services. With D2, there was no need to configure URLs for each service that you need to talk to. It provides multiple features such as client-based load balancing, service discovery and scalability.</p><p>The below diagram shows the use of Rest.li along with Dynamic Discovery.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba00d83-f7b0-48c2-a0c3-6bcbaf05df3d_1508x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1545" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faba00d83-f7b0-48c2-a0c3-6bcbaf05df3d_1508x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Super Blocks</h3><p>A service-oriented architecture is great for decoupling domains and scale out services independently.</p><p>However, there are also downsides.</p><p>Many of the applications at LinkedIn depend on data from multiple sources. For example, any request for a user&#8217;s profile page not only fetches the profile data but includes other details such as photos, connections, groups, subscription information, following info, long-form blog posts and so on.</p><p>In a service-oriented architecture, it means making hundreds of calls to fetch all the needed data.&nbsp;</p><p>This is typically known as the &#8220;call graph&#8221; and you can see that this call graph can become difficult to manage as more and more services are created.</p><p>To mitigate this issue, LinkedIn introduced the concept of a <strong>super block</strong>.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F051be3c2-80fe-4b60-ba6f-e47027cfcd84_1600x1177.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1071" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F051be3c2-80fe-4b60-ba6f-e47027cfcd84_1600x1177.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>A super block is a grouping of related backend services with a single access API.&nbsp;</p><p>This allows teams to create optimized interfaces for a bunch of services and keep the call graph in check. You can think of the super block as the implementation of the facade pattern.</p><h3>Multi-Data Center</h3><p>In a few years after launch, LinkedIn became a global company with users joining from all over the world.</p><p>They had to scale beyond serving traffic from just one data center. Multiple data centers are incredibly important to maintain high availability and avoid any single point of failure. Moreover, this wasn&#8217;t needed just for a single service but the entire website.</p><p>The first move was to start serving public profiles out of two data centers (Los Angeles and Chicago).&nbsp;</p><p>Once it was proven that things work, they enhanced all other services to support the below features:</p><ul><li><p>Data replication</p></li><li><p>Callbacks from different origins</p></li><li><p>One-way data replication events</p></li><li><p>Pinning users to geographically-close data centers</p></li></ul><p>As LinkedIn has continued to grow, they have migrated the edge infrastructure to Azure Front Door (AFD). For those who don&#8217;t know, AFD is Microsoft&#8217;s global application and content delivery network and migrating to it provided some great benefits in terms of latency and resilience.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82dd657c-0cf0-495c-8e32-62c2445402ce_700x365.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="365" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82dd657c-0cf0-495c-8e32-62c2445402ce_700x365.png" width="700" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Image Source: <a href="https://www.linkedin.com/blog/engineering/scalability/linkedin-scales-edge-with-azure-front-door">Scaling LinkedIn&#8217;s Edge with Azure Front Door</a> </figcaption></figure></div><p>This move scaled them up to 165+ Points of Presence (PoPs) and helped improve median page load times by up to 25 percent.</p><p>The edge infrastructure is basically how our devices connect to LinkedIn today. Data from our device traverses the internet to the closest PoP that houses HTTP proxies that forward those requests to an application server in one of the LinkedIn data centers.</p><h2>Advanced Developments Around Scalability</h2><p>Running an application as complex and evolving as LinkedIn requires the engineering team to keep investing into building scalable solutions.</p><p>In this section, we will look at some of the more recent developments LinkedIn has undergone.</p><h3>Real Time Analytics with Pinot</h3><p>A few years ago, the LinkedIn engineering team hit a wall with regards to analytics</p><p>The scale of data at LinkedIn was growing far beyond what they could analyze. The analytics functionality was built using generic storage systems like Oracle and Voldemort. However, these systems were not specialized for OLAP needs and the data volume at LinkedIn was growing in both breadth and depth.</p><p>At this point, you might be wondering about the need for real-time analytics at LinkedIn.&nbsp;</p><p>Here are three very important use-cases:</p><ol><li><p>The <strong>Who&#8217;s Viewed Your Profile</strong> is LinkedIn&#8217;s flagship analytics product that allows members to see who has viewed their profile in real-time. To provide this data, the product needs to run complex queries on large volumes of profile view data to identify interesting insights.</p></li><li><p><strong>Company Page Analytics</strong> is another premium product offered by LinkedIn. The data provided by this product enables company admins to understand the demographic of the people following their page.</p></li><li><p>LinkedIn also heavily uses analytics internally to support critical requirements such as A/B testing.</p></li></ol><p>To support these key analytics products and many others at scale, the engineering team created Pinot.</p><p><strong>Pinot is a web-scale real-time analytics engine designed and built at LinkedIn</strong>.&nbsp;</p><p>It allows them to slice, dice and scan through massive quantities of data coming from a wide variety of products in real-time.</p><p><strong>But how does Pinot solve the problem?</strong></p><p>The below diagram shows a comparison between the pre-Pinot and post-Pinot setup.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23532699-4d61-491f-9476-05d57b3844b4_1274x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23532699-4d61-491f-9476-05d57b3844b4_1274x1600.png" width="1274" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>As you can see, Pinot supports real-time data indexing from Kafka and Hadoop, thereby simplifying the entire process.</p><p>Some of the other benefits of Pinot are as follows:</p><ul><li><p>Pinot supports low latency and high QPS OLAP queries. For example, it&#8217;s capable of serving thousands of <strong>Who&#8217;s Viewed My Profile</strong> requests while maintaining SLA in the order of 10s of milliseconds</p></li><li><p>Pinot also simplifies operational aspects like cluster rebalancing, adding or removing nodes, and re-bootstrapping</p></li><li><p>Lastly, Pinot has been future-proofed to handle new data dimensions without worrying about scale</p></li></ul><h3>Authorization at LinkedIn Scale</h3><p>Users entrust LinkedIn with their personal data and it was extremely important for them to maintain that trust.</p><p>After the SOA transformation, LinkedIn runs a microservice architecture where each microservice retrieves data from other sources and serves it to the clients. Their philosophy is that a microservice can only access data with a valid business use case. It prevents the unnecessary spreading of data and minimizes the damage if an internal service gets compromised.</p><p>A common industry solution to manage the authorization is to define Access Control Lists (ACLs). An ACL contains a list of entities that are either allowed or denied access to a particular resource.</p><p>For example, let&#8217;s say there is a Rest.li resource to manage <strong>greetings</strong>. The ACL for this resource can look something like this.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F950bb2f6-8976-49e6-b7df-94003dc4782a_1600x1045.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="951" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F950bb2f6-8976-49e6-b7df-94003dc4782a_1600x1045.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>In this case, the <strong>client-service</strong> can read but not write whereas the <strong>admin-service</strong> can both read and write to greetings.</p><p>While the concept of an ACL-based authorization is quite simple, it&#8217;s a challenge to maintain at scale. LinkedIn has over 700 services that communicate at an average rate of tens of millions of calls per second. Moreover, this figure is only growing.</p><p>Therefore, the team had to devise a solution to handle ACLs at scale. Mainly, there were three critical requirements:</p><ul><li><p>Check authorization quickly</p></li><li><p>Deliver ACL changes quickly</p></li><li><p>Track and manage a large number of ACLs</p></li></ul><p>The below diagram shows a high-level view of how LinkedIn manages authorization between services.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98ce60b8-b354-48cc-8f0f-6834403bedbb_1999x1207.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="879" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98ce60b8-b354-48cc-8f0f-6834403bedbb_1999x1207.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Some key points to consider over here are as follows:</p><ul><li><p>To make authorization checks quick, they built an authorization client module that runs on every service at LinkedIn. This module decides whether an action should be allowed or denied. New services pick up this client by default as part of the basic service architecture.</p></li><li><p>Latency is a critical factor during an authorization check and making a network call every time is not acceptable. Therefore, all relevant ACL data is kept in memory by the service.</p></li><li><p>To keep the ACL data fresh, every client reaches out to the server at fixed intervals and updates its in-memory copy. This is done at a fast enough cadence for any ACL changes to be realized quickly.</p></li><li><p>All ACLs are stored in LinkedIn&#8217;s Espresso database. It&#8217;s a fault-tolerant distributed NoSQL database that provides a simple interface.</p></li><li><p>To manage latency and scalability, they also keep a Couchbase cache in front of Espresso. This means even on the server side, the data is served from memory. To deal with stale data in the Couchbase, they use a Change Data Capture system based on LinkedIn&#8217;s Brooklin to notify the service when an ACL has changed so that the cache can be cleared.</p></li><li><p>Every authorization check is logged in the background. This is necessary for debugging and traffic analysis. LinkedIn uses Kafka for asynchronous, high-scale logging. Engineers can check the data in a separate monitoring system known as <strong>inGraphs</strong>.</p></li></ul><h2>Conclusion</h2><p>In this post, we&#8217;ve taken a brief look at the scaling journey of LinkedIn.</p><p>From its simple beginnings as a standalone monolithic system serving a few thousand users, LinkedIn has come a long way. It is one of the largest social networks in the world for professionals and companies, allowing seamless connection between individuals across the globe.</p><p>To support the growing demands, LinkedIn had to undertake bold transformations at multiple steps.&nbsp;</p><p>In the process, they&#8217;ve provided a lot of learnings for the wider developer community that can help you in your own projects.</p><p><strong>References</strong>:</p><ul><li><p><a href="https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin">A Brief History of Scaling LinkedIn</a></p></li><li><p><a href="https://www.linkedin.com/blog/engineering/scalability/linkedin-scales-edge-with-azure-front-door">Scaling LinkedIn&#8217;s Edge with Azure Front Door</a></p></li><li><p><a href="https://www.linkedin.com/blog/engineering/open-source/apache-kafka-trillion-messages">How LinkedIn customizes Apache Kafka for 7 trillion messages per day</a></p></li><li><p><a href="https://engineering.linkedin.com/real-time-distributed-graph/using-set-cover-algorithm-optimize-query-latency-large-scale-distributed">Using Set Cover Algorithm to optimize query latency for a large-scale distributed graph</a></p></li><li><p><a href="https://www.linkedin.com/blog/engineering/scalability/authorization-at-linkedins-scale">Authorization at LinkedIn&#8217;s Scale</a></p></li><li><p><a href="https://www.linkedin.com/blog/engineering/architecture/restli-restful-service-architecture-scale">Rest.li: RESTful Service Architecture at Scale</a></p></li><li><p><a href="https://engineering.linkedin.com/analytics/real-time-analytics-massive-scale-pinot">Real-time analytics at a massive scale with Pinot</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p><p><br /><br /><br /><br /><br /><br /><br /><br /></p>
]]></content:encoded>
<pubDate>Tue, 28 May 2024 15:31:05 GMT</pubDate>
</item>
<item>
<title>EP113: AWS Services Cheat Sheet</title>
<link>https://blog.bytebytego.com/p/ep113-aws-services-cheat-sheet</link>
<guid>https://blog.bytebytego.com/p/ep113-aws-services-cheat-sheet</guid>
<content:encoded><![CDATA[
<div> AWS Services, API designs, Postman, Azure Services, computer programs run

<br />
API设计和发布对于应用程序的成功至关重要。Postman为团队合作提供了有效的工具，使API设计、测试和文档编写更加简单和高效。AWS和Azure作为主要云服务平台，提供了丰富的服务和功能，满足了不同用户的需求。运行计算机程序涉及多个步骤，包括用户交互、程序预加载、依赖解析和加载、内存分配以及程序终止。这些技术和工具的发展不断推动着软件开发和云计算技术的进步。总结:API设计和发布对于应用程序至关重要，Postman为团队合作提供了有效工具，AWS和Azure作为云服务平台提供丰富服务，计算机程序的运行包括多个步骤，推动软件开发和云计算技术的发展。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Do You Know How Mobile Apps Are Released? (Youtube video)</p></li><li><p>AWS Services Cheat Sheet </p></li><li><p>A cheat sheet for API designs</p></li><li><p>Azure Services Cheat Sheet </p></li><li><p>How do computer programs run? </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/Postman_052425Headline">Collaborating on APIs is Easier With Postman (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/Postman_052425Headline" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="819" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e4e769d-06a0-4464-b727-b9b39c6f2133_1660x934.png" width="1456" /><div></div></div></a></figure></div><p>Whether you&#8217;re a team of four or 40,000, development teams need to collaborate on APIs. <a href="https://bit.ly/Postman_052425Headline">API Collaboration</a>&nbsp;improves developer productivity by empowering both API producers and consumers to share, discover, and reuse high-quality API assets.</p><p>Postman revolutionizes the experience of collaborative API development with <a href="https://bit.ly/Postman_052524Collections">Collections</a> and <a href="https://bit.ly/Postman_052425Workspaces">Workspaces</a>. Used together, they enable API design, testing, and documentation, providing a shared canvas for collaborating on API assets.</p><p>Learn how Postman is giving teams of all sizes and functions the ability to rapidly iterate on API development, elevate the quality of their APIs, and extend their API workflows for large-scale initiatives.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/Postman_052425Headline"><span>Learn More</span></a></p><div><hr /></div><h2>Do You Know How Mobile Apps Are Released?</h2><div class="youtube-wrap" id="youtube2-RIX4ufelA58"><div class="youtube-inner"></div></div><div><hr /></div><h2>AWS Services Cheat Sheet </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F915d1659-0779-460e-b71f-a7dd9d812556_1415x1536.gif" title="No alternative text description for this image" width="1415" /><div></div></div></a></figure></div><p>AWS grew from an in-house project to the market leader in cloud services, offering so many different services that even experts can find it a lot to take in. <br /> <br />The platform not only caters to foundational cloud needs but also stays at the forefront of emerging technologies such as machine learning and IoT, establishing itself as a bedrock for cutting-edge innovation. AWS continuously refines its array of services, ensuring advanced capabilities for security, scalability, and operational efficiency are available. <br /> <br />For those navigating the complex array of options, this AWS Services Guide is a helpful visual aid. <br /> <br />It simplifies the exploration of AWS's expansive landscape, making it accessible for users to identify and leverage the right tools for their cloud-based endeavors. <br /> <br />Over to you: What improvements would you like to see in AWS services based on your usage?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1893" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e9dc1a-3ca4-40a6-9578-ee59e9ae0ef8_3000x3900.png" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/api-security-best-practices">API Security Best Practices</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li><li><p><a href="https://blog.bytebytego.com/p/what-happens-when-a-sql-is-executed">What Happens When a SQL is Executed?</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>A cheat sheet for API designs</h2><p>APIs expose business logic and data to external systems, so designing them securely and efficiently is important.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae83c804-21ac-4baa-ab4a-56ca0ba0f4d2_1452x1536.gif" title="No alt text provided for this image" width="1452" /><div></div></div></a></figure></div><ul><li><p>API key generation<br />We normally generate one unique app ID for each client and generate different pairs of public key (access key) and private key (secret key) to cater to different authorizations. For example, we can generate one pair of keys for read-only access and another pair for read-write access. <br /></p></li><li><p>Signature generation<br />Signatures are used to verify the authenticity and integrity of API requests. They are generated using the secret key and typically involve the following steps:<br /><br />- Collect parameters<br />- Create a string to sign<br />- Hash the string: Use a cryptographic hash function, like HMAC (Hash-based Message Authentication Code) in combination with SHA-256, to hash the string using the secret key. <br /></p></li><li><p>Send the requests<br />When designing an API, deciding what should be included in HTTP request parameters is crucial. Include the following in the request parameters:<br /><br />- Authentication Credentials<br />- Timestamp: To prevent replay attacks.<br />- Request-specific Data: Necessary to process the request, such as user IDs, transaction details, or search queries.<br />- Nonces: Randomly generated strings included in each request to ensure that each request is unique and to prevent replay attacks.<br /></p></li><li><p>Security guidelines<br />To safeguard APIs against common vulnerabilities and threats, adhere to these security guidelines.</p></li></ul><div><hr /></div><h2>Azure Services Cheat Sheet </h2><p>Launching in 2010, Microsoft Azure has quickly grown to hold the No. 2 position in market share by evolving from basic offerings to a comprehensive, flexible cloud ecosystem.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alt text provided for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc905b80e-685f-4145-a12c-02d704f70efd_1414x1536.gif" title="No alt text provided for this image" width="1414" /><div></div></div></a></figure></div><p>Today, Azure not only supports traditional cloud applications but also caters to emerging technologies such as AI, IoT, and blockchain, making it a crucial platform for innovation and development.<br /><br />As it evolves, Azure continues to enhance its capabilities to provide advanced solutions for security, scalability, and efficiency, meeting the demands of modern enterprises and startups alike. This expansion allows organizations to adapt and thrive in a rapidly changing digital landscape.<br /><br />The attached illustration can serve as both an introduction and a quick reference for anyone aiming to understand Azure.<br /><br />Over to you: How does your experience with Azure compare to that with AWS?<br /><br />Over to you: Does the card network charge the same interchange fee for big merchants as for small merchants?</p><div><hr /></div><h2>How do computer programs run? </h2><p>The diagram shows the steps. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1536" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F416ff1bc-c6c4-475a-b45f-7842fbfcd726_1435x1536.gif" title="No alternative text description for this image" width="1435" /><div></div></div></a></figure></div><ul><li><p>User interaction and command initiation <br />By double-clicking a program, a user is instructing the operating system to launch an application via the graphical user interface. <br /></p></li><li><p>Program Preloading <br />Once the execution request has been initiated, the operating system first retrieves the program's executable file. <br /> <br />The operating system locates this file through the file system and loads it into memory in preparation for execution. <br /></p></li><li><p>Dependency resolution and loading <br />Most modern applications rely on a number of shared libraries, such as dynamic link libraries (DLLs). </p><p></p></li><li><p>Allocating memory space <br />The operating system is responsible for allocating space in memory. <br /> </p></li><li><p>Initializing the Runtime Environment <br />After allocating memory, the operating system and execution environment (e.g., Java's JVM or the .NET Framework) will initialize various resources needed to run the program. <br /></p></li><li><p>System Calls and Resource Management <br />The entry point of a program (usually a function named `main`) is called to begin execution of the code written by the programmer. <br /></p></li><li><p>Von Neumann Architecture <br />In the Von Neumann architecture, the CPU executes instructions stored in memory. <br /></p></li><li><p>Program termination <br />Eventually, when the program has completed its task, or the user actively terminates the application, the program will begin a cleanup phase. This includes closing open file descriptors, freeing up network resources, and returning memory to the system. </p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong><br /></p>
]]></content:encoded>
<pubDate>Sat, 25 May 2024 15:30:49 GMT</pubDate>
</item>
<item>
<title>API Security Best Practices</title>
<link>https://blog.bytebytego.com/p/api-security-best-practices</link>
<guid>https://blog.bytebytego.com/p/api-security-best-practices</guid>
<content:encoded><![CDATA[
<div> APIs, security vulnerabilities, threats, authentication, best practices

<br />
API安全性是现代应用程序的支柱。API暴露了很大的攻击面，增加了安全漏洞的风险。常见的威胁包括SQL注入、跨站脚本攻击和分布式拒绝服务（DDoS）攻击。因此，实施强大的安全措施来保护API和其处理的敏感数据至关重要。认证确保只有经授权的用户或应用程序可以访问受保护的资源或API端点。在实现认证之前，选择适当的认证机制非常重要，根据我们的用例、安全要求和与客户端应用程序的兼容性。以下是保护API的一些流行认证机制。

<br /><br />总结:API安全性至关重要，需要应对潜在的安全漏洞和威胁。认证是确保只有授权用户访问受保护资源或API端点的关键。选择适当的认证机制，并实施安全措施如身份验证、授权、安全通信和速率限制，是设计安全API的核心策略。 <div>
<p>APIs are the backbone of modern applications. They expose a very large surface area for attacks, increasing the risk of security vulnerabilities. Common threats include SQL injection, cross-site scripting, and distributed denial of service (DDoS) attacks.&nbsp;</p><p>That's why it's crucial to implement robust security measures to protect APIs and the sensitive data they handle. However, many companies struggle to achieve comprehensive API security coverage. They often rely solely on dynamic application security scanning or external pen testing. While these methods are valuable, they may not fully cover the API layer and its increasing attack surface.&nbsp;</p><p>In this week&#8217;s issue, we'll explore API security best practices. From authentication and authorization to secure communication and rate limiting, we&#8217;ll cover essential strategies for designing secure APIs.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e9dc1a-3ca4-40a6-9578-ee59e9ae0ef8_3000x3900.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1893" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66e9dc1a-3ca4-40a6-9578-ee59e9ae0ef8_3000x3900.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><div><hr /></div><h2>Authentication</h2><p>Authentication ensures that only authorized users or applications can access protected resources or API endpoints. Before implementing authentication, choosing the appropriate authentication mechanism is crucial based on our use case, security requirements, and compatibility with client applications.&nbsp;</p><p>Below are some popular authentication mechanisms for securing APIs:</p>
      <p>
          <a href="https://blog.bytebytego.com/p/api-security-best-practices">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 23 May 2024 15:30:58 GMT</pubDate>
</item>
<item>
<title>How Slack Built a Distributed Cron Execution System for Scale</title>
<link>https://blog.bytebytego.com/p/how-slack-built-a-distributed-cron</link>
<guid>https://blog.bytebytego.com/p/how-slack-built-a-distributed-cron</guid>
<content:encoded><![CDATA[
<div> Slack, cron execution system, scalability, reliability, Job Queue

总结:<br /><br />
Slack在处理大规模系统中的cron执行工作时遇到了挑战，决定重新构建分布式cron执行系统。他们采用了Scheduled Job Conductor、Job Queue和Vitess Database Table等组件来解决问题。Scheduled Job Conductor使用Go和Kubernetes实现，采用了Leader-Follower架构和灵活的横向扩展。Job Queue则处理资源密集的任务，通过Kafka和Redis实现高效的作业处理。最后，Vitess表用于数据重复处理和作业跟踪，提高了系统的可靠性和监控。Slack的经验表明，通过简单的解决方案构建大规模系统是可行的。 <div>
<h2><a href="https://bit.ly/QAWolf_052124">&#128075;Goodbye low test coverage and slow QA cycles (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/QAWolf_052124" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="750" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2e17972-3779-4bb2-beb3-1fadf11f4c85_1000x750.png" width="1000" /><div></div></div></a></figure></div><p>Bugs sneak out when less than 80% of user flows are tested before shipping. But getting that kind of coverage &#8212; and staying there &#8212; is hard and pricey for any sized team.</p><p><a href="https://bit.ly/QAWolf_052124">QA Wolf</a> takes testing off your plate:</p><p>&#8594; Get to 80% test coverage in just 4 months.</p><p>&#8594; Stay bug-free with <a href="https://bit.ly/QAWolf_052124">24-hour maintenance</a> and on-demand test creation.</p><p>&#8594; Get <a href="https://bit.ly/QAWolf_052124">unlimited parallel test runs</a></p><p>&#8594; Zero Flakes guaranteed</p><p>QA Wolf has generated <a href="https://bit.ly/QAWolf_052124">amazing results</a> for companies like Salesloft, AutoTrader, Mailchimp, and Bubble.</p><p>&#127775; Rated 4.5/5 on G2</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/QAWolf_052124"><span>Learn more about their 90-day pilot</span></a></p><div><hr /></div><p>Have you ever stretched a simple tool to its absolute limits before you upgraded?</p><p>We do it all the time. And so do the big companies that operate on a massive scale. This is because simple things can sometimes take you much further than you initially think. Of course, you may have to pay with some toil and tears.</p><p>This is exactly what Slack, a $28 billion company with 35+ million users, did for its cron execution workflow that handles critical functionalities. Instead of moving to some other new-age solutions, they rebuilt their cron execution system from the ground up to run jobs reliably at their scale.&nbsp;</p><p>In today&#8217;s post, we&#8217;ll look at how Slack architected a distributed cron execution system and the choices made in the overall design.</p><div><hr /></div><h2>The Role of Cron Jobs at Slack</h2><p>As you already know, Slack is one of the most popular platforms for team collaboration.&nbsp;</p><p>Due to its primary utility as a communication tool, Slack is super dependent on the right notification reaching the right person at the right time.&nbsp;</p><p>However, as the platform witnessed user and feature growth, Slack faced challenges in maintaining the reliability of its notification system, which largely depended on cron jobs.&nbsp;</p><p>For reference, cron jobs are used to automate repetitive tasks. You can configure a cron job to ensure that specific scripts or commands run at predefined intervals without manual intervention.</p><p>Cron jobs play a crucial role in Slack's notification system by making sure that messages and reminders reach users on time. A lot of the critical functionality at Slack relies on these cron scripts. For example,&nbsp;</p><ul><li><p>Sending timely reminders to users.</p></li><li><p>Delivering email notifications to keep the users informed about important updates in the team.</p></li><li><p>Pushing message notifications to users so that they don&#8217;t miss critical conversations.</p></li><li><p>Performing regular database clean-ups to maintain system performance.</p></li></ul><p>As Slack grew, there has been a massive growth in the number of cron scripts and the amount of data processed by these scripts. Ultimately, this caused a dip in the overall reliability of the execution environment.</p><h2>The Issues with Cron Jobs</h2><p>Some of the challenges and issues Slack faced with their original cron execution approach were as follows:</p><ul><li><p><strong>Maintainability Issues:</strong> Managing many cron scripts on a single node became cumbersome and error-prone as Slack&#8217;s functionalities expanded. Tasks like updating, troubleshooting, and monitoring the scripts required a lot of effort from the engineering team.</p></li><li><p><strong>Cost-ineffective Vertical Scaling: </strong>As the number of cron scripts increased, Slack tried to vertically scale the node by adding more CPU and RAM. However, this approach quickly became cost-ineffective, as the hardware requirements grew disproportionately to the number of scripts.</p></li><li><p><strong>Single Point of Failure: </strong>Relying on a single node to execute all cron jobs introduced a significant risk to Slack&#8217;s critical functionality. Any misconfigurations, hardware failures, or software issues on the node could bring down the entire notification system.</p></li></ul><p>To solve these issues, Slack decided to build a brand new cron execution service that was more reliable and scalable than the original approach.</p><h2>The High-Level Cron Execution Architecture</h2><p>The below diagram shows the high-level cron execution architecture.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F494650c6-0993-4b60-b69f-f1cd05c464c6_1600x995.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="905" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F494650c6-0993-4b60-b69f-f1cd05c464c6_1600x995.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>There are 3 main components in this design. Let&#8217;s look at each of them in more detail.</p><h3>Scheduled Job Conductor</h3><p>Slack developed a new execution service. It was written in Go and deployed on Bedrock.</p><p>Bedrock is Slack&#8217;s in-house platform that wraps around Kubernetes, providing an additional abstraction layer and functionality for Slack&#8217;s specific needs. It builds upon Kubernetes and adds some key features such as:</p><ul><li><p>Simplified deployment</p></li><li><p>Custom resource definitions specific to Slack&#8217;s infrastructure</p></li><li><p>Enhanced monitoring and logging</p></li><li><p>Integration with Slack&#8217;s infrastructure</p></li></ul><p>The new service mimics the behavior of cron by utilizing a Go-based cron library while benefiting from the scalability and reliability provided by Kubernetes.&nbsp;</p><p>The below diagram shows how the Scheduled Job Conductor works in practice.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3c56544-a9d4-40e6-88b9-68fee6d3cefc_1600x998.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="908" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3c56544-a9d4-40e6-88b9-68fee6d3cefc_1600x998.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>It had some key properties that we should consider.</p><h4>1 - Scalability through Kubernetes Deployment</h4><p>By deploying the cron execution service on Bedrock, Slack gains the ability to easily scale up multiple pods as needed.&nbsp;</p><p>As you might be aware, Kubernetes provides a flexible infrastructure for containerized applications. You can dynamically adjust the number of pods based on the workload.</p><h4>2 - Leader Follower Architecture</h4><p>Interestingly, Slack's cron execution service does not process requests on multiple pods simultaneously.&nbsp;</p><p>Instead, they adopt a leader-follower architecture, where only one pod (the leader) is responsible for scheduling jobs, while the other pods remain in standby mode.&nbsp;</p><p>This design decision may seem counterintuitive, as it appears to introduce a single point of failure. However, the Slack team determined that synchronizing the nodes would be a more significant challenge than the potential risk of having a single leader.</p><p>A couple of advantages of the leader-follower architecture are as follows:</p><ul><li><p><strong>Rapid Failover: </strong>If the leader pod goes down, Kubernetes can quickly promote one of the standby pods to take over the work. This minimizes downtime and makes the cron execution service highly available.</p></li><li><p><strong>Simplified Synchronization: </strong>By having a single leader, Slacks avoids the complexity of dealing with conflicts.</p></li></ul><h4>3 - Offloading Resource-Intensive Tasks</h4><p>The job conductor service is only responsible for job scheduling. The actual execution is handled by worker nodes.</p><p>This separation of concerns allows the cron execution service to focus on job scheduling while the job queue handles resource-intensive tasks.</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4e965c2-b545-4f79-a489-e0e72d473c06_1109x1600.png" title="" width="1109" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li><li><p><a href="https://blog.bytebytego.com/p/what-happens-when-a-sql-is-executed">What Happens When a SQL is Executed?</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-api-versioning">A Crash Course in API Versioning Strategies</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h3>The Job Queue</h3><p>Slack's cron execution service relies on a powerful asynchronous compute platform called the Job Queue to handle the resource-intensive task of running scripts.&nbsp;</p><p>The Job Queue is a critical component of Slack's infrastructure, processing a whopping 9 billion jobs per day.</p><p>The Job Queue consists of a series of so-called theoretical &#8220;queues&#8221; through which various types of jobs flow. Each script triggered by a cron job is treated as a single job within the Job Queue.</p><p>See the below diagram for reference:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9144169-fc4e-474b-b6e5-d900ae76ff66_1600x996.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="906" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9144169-fc4e-474b-b6e5-d900ae76ff66_1600x996.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The key components of the job queue architecture are as follows:</p><ul><li><p><strong>Kafka: </strong>Jobs are initially stored in Kafka since it provides durable storage. Kafka ensures that jobs are persisted and can be recovered in case of system failures or backups.</p></li><li><p><strong>Redis: </strong>From Kafka, jobs are moved into Redis (an in-memory data store) for short-term storage. Redis allows additional metadata, such as the identity of the worker executing the job, to be stored alongside the job itself. This metadata is important for tracking and managing job execution.</p></li><li><p><strong>Job Workers:</strong> Jobs are dispatched from Redis to job workers, which are nodes that execute the script. Each job worker is capable of running the scripts associated with the jobs it receives.</p></li></ul><p>Slack achieves several important benefits by using the Job Queue:</p><ul><li><p><strong>Offloading compute and memory concerns: </strong>The Job Queue is designed to handle a massive amount of work, with a capacity far exceeding the requirements of the cron execution service. Offloading the compute and memory demands of the running scripts helps keep the cron execution service lightweight.</p></li><li><p><strong>Isolation and performance: </strong>The Job Queue lets them keep the jobs isolated on their specific &#8220;queues&#8221; so that the jobs triggered by the cron execution service are processed smoothly. There is no impact on them due to other jobs in the system.</p></li><li><p><strong>Reliability and fault tolerance: </strong>The Job Queue&#8217;s architecture ensures that jobs are stored durably and recoverable in case of failures.</p></li><li><p><strong>Reduce development and maintenance efforts: </strong>This was probably the most important factor since leveraging an existing, battle-tested system like the Job Queue helped Slack reduce the development time required to build the cron execution service.</p></li></ul><h3>Vitess Database Table for Job Tracking</h3><p>To boost the reliability of their cron execution service, Slack also employed a Vitess table for deduplication and job tracking.</p><p>Vitess is a database clustering system for horizontal scaling of MySQL. It provides a scalable and highly available solution for managing large-scale data.</p><p>A couple of important requirements handled by Vitess are as described as follows:</p><h4>1 - Deduplication</h4><p>Within Slack&#8217;s original cron system, they used <strong>flocks</strong>, a Linux utility for managing locking in scripts so that only one copy of a script runs at a time.&nbsp;</p><p>While this approach worked fine, there were cases where a script&#8217;s execution time exceeded its recurrence intervals, leading to the possibility of two copies running concurrently.</p><p>To handle this issue, Slack introduced a Vitess table to handle deduplication.</p><p>Here&#8217;s how it works:</p><ul><li><p>When a new job is triggered, Slack records its execution as a new row in the Vitess table.</p></li><li><p>The job&#8217;s status is updated as it progresses through the system, with possible states including &#8220;enqueued&#8221;, &#8220;in progress&#8221;, and &#8220;done&#8221;.</p></li><li><p>Before kicking off a new run of a job, Slack queries the table to check if there are any active instances of the same job already running. This query is made efficient by using an index on the script names.</p></li></ul><p>The below diagram shows some sample data stored in Vitess.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbe4dc22-6be8-4a6d-b0b3-14029d47718b_1600x941.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="856" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbe4dc22-6be8-4a6d-b0b3-14029d47718b_1600x941.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h4>2 - Job Tracking and Monitoring</h4><p>The Vitess table also helps with job tracking and monitoring since it contains information about the state of each job.</p><p>The job tracking functionality is exposed through a simple web page that displays the execution details. Developers can easily look up the state of their script runs and any errors encountered during execution.</p><h2>Conclusion</h2><p>One can debate whether using cron was the right decision for Slack when it came to job scheduling.</p><p>But it can also be said that organizations that choose the simplest solution for critical functionalities are more likely to grow into organizations of Slack&#8217;s size. On the other hand, companies that deploy solutions that try to solve all the problems when they have 0 customers never become that big. The difference is an unrelenting focus on solving the business problem rather than building fancy solutions from the beginning.</p><p>Slack&#8217;s journey from a single cron box to a sophisticated, distributed cron execution service shows how simple solutions can be used to build large-scale systems.</p><p><strong>References:</strong></p><ul><li><p><a href="https://slack.engineering/executing-cron-scripts-reliably-at-scale/">Executing Cron Scripts Reliably at Scale</a></p></li><li><p><a href="https://slack.engineering/scaling-slacks-job-queue/">Scaling Slack&#8217;s Job Queue</a></p></li><li><p><a href="https://slack.engineering/applying-product-thinking-to-slacks-internal-compute-platform/">Applying Product Thinking to Slack&#8217;s Internal Compute Platform</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p><p></p>
]]></content:encoded>
<pubDate>Tue, 21 May 2024 15:31:08 GMT</pubDate>
</item>
<item>
<title>Cloudflare’s Trillion-Message Kafka Infrastructure: A Deep Dive</title>
<link>https://blog.bytebytego.com/p/cloudflares-trillion-message-kafka</link>
<guid>https://blog.bytebytego.com/p/cloudflares-trillion-message-kafka</guid>
<content:encoded><![CDATA[
<div> 云服务、Kafka、挑战、解决方案、扩展规模
<br />
总结:云服务公司Cloudflare通过Kafka处理了1万亿条消息。早期使用PHP的单块架构导致挑战，因此采用Kafka解决服务耦合和重试问题。后来使用Protocol Buffers标准化消息，提高传输效率。引入连接器框架简化数据同步流程。面临的扩展挑战包括可见性不足、过于频繁的告警以及无法跟上生产速率。通过改进健康检查，批量消费等技术，成功解决这些挑战。 <div>
<p><em>Disclaimer: The content (text and diagrams) in this post has been derived and compiled from the fantastic articles originally published on the Cloudflare Tech Blog by Matt Boyle, Andrea Medda, and Chris Shepherd. All credit for the details covered here goes to them. The links to the exact articles are in the references section at the end of the post. If you find any inaccuracies or omissions, please leave a comment, and we will do our best to fix them.</em></p><div><hr /></div><p>How much is 1 trillion?</p><p>If you were to start counting one number per second, it would take you over 31000 years to reach 1 trillion.</p><p>Now, imagine processing 1 trillion messages. This is the incredible milestone Cloudflare&#8217;s Kafka infrastructure achieved recently.</p><p>Cloudflare&#8217;s vision is to build a better Internet by providing a global network. They enable customers to secure and accelerate their websites, APIs, and applications.</p><p>However, as Cloudflare&#8217;s customer base grew rapidly, they needed to handle massive volumes of data and traffic while maintaining high availability and performance. Both scale and resilience were significant for their Kafka infrastructure.</p><p>Cloudflare has been using Kafka in production since 2014. They currently run 14 distinct Kafka clusters with roughly 330 nodes spread over multiple data centers.&nbsp;</p><p>In this article, we will explore the challenges solved and lessons learned by Cloudflare&#8217;s engineering team in their journey of scaling Kafka.</p><div><hr /></div><h2>The Early Days</h2><p>In the early days of Cloudflare, their architecture was built around a monolithic PHP application.&nbsp;</p><p>While this approach worked well initially, it created challenges as their product offerings grew. With numerous teams contributing to the same codebase, the monolithic architecture started to impact Cloudflare's ability to deliver features and updates safely and efficiently.</p><p>A couple of major issues were as follows:</p><ul><li><p>There was a tight coupling between services. Any change had a significant impact radius and required a lot of coordination between teams.&nbsp;</p></li><li><p>It was difficult to implement retry mechanisms to handle scenarios when something went wrong.</p></li></ul><p>To address these challenges, Cloudflare turned to Apache Kafka as a solution for decoupling services and enabling retry mechanisms.&nbsp;</p><p>See the diagram below that demonstrates this scenario.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9289fec1-bb42-4a98-9079-b5111f4df6cd_1600x927.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="844" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9289fec1-bb42-4a98-9079-b5111f4df6cd_1600x927.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://youtu.be/_booBjCB7rU?si=TONQvvYstgGmmke0">Tales of Kafka Presentation</a></figcaption></figure></div><p>As you can notice, if multiple teams needed to emit messages that the audit log system was interested in, they could simply produce messages for the appropriate topics.&nbsp;</p><p>The audit log system could then consume from those topics without any direct coupling to the producing services. Adding a new service that needed to emit audit logs was as simple as producing for the right topics, without requiring any changes to the consuming side.</p><p>Kafka, a distributed streaming platform, had already proven its ability to handle massive volumes of data within Cloudflare's infrastructure.&nbsp;</p><p>As a first step, they created a message bus cluster on top of Kafka. It became the most general-purpose cluster available to all application teams at Cloudflare.&nbsp;</p><p>Onboarding to the cluster was made simple through a pull request process, which automatically set up topics with the desired replication strategy, retention period, and access control lists (ACLs).</p><p>The impact of the message bus cluster on loose coupling was evident.&nbsp;</p><ul><li><p>Teams could now emit messages to topics without being aware of the specific services consuming those messages.</p></li><li><p>Consuming services could listen to relevant topics without needing to know the details of the producing services.</p></li></ul><p>There was greater flexibility and independence among teams.&nbsp;</p><h2>Standardizing Communication</h2><p>As Cloudflare&#8217;s architecture evolved towards a decoupled and event-driven system, a new challenge emerged: unstructured communication between services.&nbsp;</p><p>There was no well-defined contract for message formats, leading to issues.</p><p>A common pitfall was the lack of agreement on message structure. For example, a producer might send a JSON blob with certain expected keys, but if the consumer wasn&#8217;t aware of this expectation, it could lead to unprocessable messages and tight coupling between services.</p><p>To address this challenge, Cloudflare turned to Protocol Buffers (Protobuf) as a solution for enforcing message contracts.</p><p>Protobuf, developed by Google, is a language-agnostic data serialization format that allows for defining strict message types and schemas.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff812fa60-0040-4dc0-b07e-2438cb6fe131_1600x946.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="861" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff812fa60-0040-4dc0-b07e-2438cb6fe131_1600x946.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption"><strong>Source: <a href="https://blog.cloudflare.com/using-apache-kafka-to-process-1-trillion-messages">Cloudflare Tech Blog</a></strong></figcaption></figure></div><p>It provided several benefits such as:</p><ul><li><p>Producers and consumers could have a shared understanding of message structures using a schema registry.</p></li><li><p>Since Protobuf supports versioning and schema evolution, it was possible to make changes to message formats without breaking existing consumers. This enabled both forward and backward compatibility.</p></li><li><p>Protobuf&#8217;s compact binary format results in smaller message sizes compared to JSON, improving efficiency.</p></li></ul><p>To streamline Kafka usage and enforce best practices, Cloudflare developed an internal message bus client library in Go. This library handled the complexities of configuring Kafka producers and consumers. All the best practices and opinionated defaults were baked into this library.</p><p>There was one controversial decision at this point.</p><p>The message bus client library enforced a one-to-one mapping between Protobuf message types and Kafka topics. This meant that each topic could only contain messages of a single Protobuf type. The idea was to avoid the confusion and complexity of handling multiple message formats within a single topic.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b9c584c-a7de-43af-9f7f-f9f9176786c0_1600x949.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="864" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b9c584c-a7de-43af-9f7f-f9f9176786c0_1600x949.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://youtu.be/_booBjCB7rU?si=TONQvvYstgGmmke0">Tales of Kafka Presentation</a></figcaption></figure></div><p>There was a major trade-off to consider.&nbsp;</p><p>The strict one-to-one mapping between message types and topics resulted in a larger number of topics, partitions, and replicas while impacting resource utilization. On the other side, it also improved the developer experience, reduced coupling, and increased reliability.</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="775" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F118d82e9-3c58-4214-b418-c6b270c6f968_1600x852.png" title="" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li><li><p><a href="https://blog.bytebytego.com/p/what-happens-when-a-sql-is-executed">What Happens When a SQL is Executed?</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-api-versioning">A Crash Course in API Versioning Strategies</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>Abstracting Common Patterns</h2><p>As Cloudflare&#8217;s adoption of Kafka grew and more teams started leveraging the message bus client library, a new pattern emerged.</p><p>Teams were using Kafka for similar styles of jobs. For example, many teams were building services that read data from one system of record and pushed it to another system, such as Kafka or Cloudflare edge database called Quicksilver. These services often involved similar configurations and boilerplate code.</p><p>There were a couple of problems such as:</p><ul><li><p>Duplicated efforts across teams.</p></li><li><p>Following best practices was harder.</p></li></ul><p>To address this, the application services team developed the connector framework.</p><p>Inspired by Kafka connectors, the framework allows engineers to quickly spin up services that can read from one system and push data to another with minimal configuration. The framework abstracts away the common patterns and makes it easy to build data synchronization pipelines.</p><p>The diagram below shows a high-level view of the connector approach.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4111e4-a323-41da-851a-3aa30be08adb_1600x945.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="860" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4111e4-a323-41da-851a-3aa30be08adb_1600x945.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://blog.cloudflare.com/using-apache-kafka-to-process-1-trillion-messages">Cloudflare Tech Blog</a></figcaption></figure></div><p>Using the connector framework is straightforward.</p><ul><li><p>The developers use a CLI tool to generate a ready-to-go service by providing just a few parameters.</p></li><li><p>The generated service includes all the necessary components, such as a reader, a writer, and optional transformations.</p></li><li><p>To configure the connector, you just need to tweak the environment variables, eliminating the need for code changes in most cases. For example, developers can specify the reader (let&#8217;s say, Kafka), the writer (let&#8217;s say Quicksilver), and any transformations using environment variables. The framework takes care of the rest.</p></li></ul><p>Here&#8217;s a more concrete example of how the connector framework is used in Cloudflare&#8217;s communication preferences service.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdaa21e79-97c5-46d3-9e11-57c6cc0880d1_1595x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1461" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdaa21e79-97c5-46d3-9e11-57c6cc0880d1_1595x1600.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://blog.cloudflare.com/using-apache-kafka-to-process-1-trillion-messages">Cloudflare Tech Blog</a></figcaption></figure></div><p>The communication preferences service allows customers to opt out of marketing information through the Cloudflare dashboard. When a customer updates their communication preferences, the service stores the change in its database and emits a message to Kafka.</p><p>To keep other systems in sync with the communication preferences, the team leverages the connector framework. They have set up three different connectors that read the communication preference changes from Kafka and synchronize them to three separate systems such as:</p><ul><li><p>Transactional email service</p></li><li><p>Customer management system</p></li><li><p>Marketing email system</p></li></ul><h2>Scaling Challenges and Solutions</h2><p>Cloudflare faced multiple scaling challenges with its Kafka adoption. Let&#8217;s look at a few of the major ones and how the team solved those challenges.</p><h3>1 - Visibility</h3><p>When Cloudflare experienced a surge in internet usage and customer growth during the pandemic, the audit logs system faced a lot of challenges in keeping up with the increased traffic.</p><p>Audit logs are a crucial feature for customers to track changes to their resources, such as the deletion of a website or modifications to security settings. As more customers relied on Cloudflare&#8217;s services, the audit logs systems struggled to process the growing volume of events on time.</p><p>As a first fix, the team invested in a pipeline that pushes audit log events directly into customer data buckets, such as R2 or S3.&nbsp;</p><p>See the diagram below:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32e096a0-91c8-42db-a652-520c04b2903c_1600x934.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="850" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32e096a0-91c8-42db-a652-520c04b2903c_1600x934.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://youtu.be/_booBjCB7rU?si=TONQvvYstgGmmke0">Tales of Kafka Presentation</a></figcaption></figure></div><p>However, when the pipeline was deployed to production, they encountered multiple issues.</p><ul><li><p>The system was accruing logs and failing to clear them fast enough.</p></li><li><p>Breaches in service level objectives (SLOs).</p></li><li><p>Unacceptable delays in delivering audit log data to customers.</p></li></ul><p>Initially, the team lacked the necessary tooling in their SDK to understand the root cause of the performance issues. They couldn&#8217;t determine whether the bottleneck was in reading from Kafka, performing transformations, or saving data to the database.</p><p>To gain visibility, they enhanced their SDK with Prometheus metrics, specifically using histograms to measure the duration of each step in the message processing flow. They also explored OpenTelemetry, a collection of SDKs and APIs that made it easy to collect metrics across services written in different programming languages.</p><p>With better visibility provided by Prometheus and OpenTelemetry, the team could identify the longest-running parts of the pipeline. Both reading from Kafka and pushing data to the bucket were slow.</p><p>By making targeted improvements, they were able to reduce the lag and ensure that audit logs were delivered to customers on time, even at high throughput rates of 250 to 500 messages per second.</p><h3>2 - Noisy On-call</h3><p>One thing leads to another.</p><p>Adding metrics to the Kafka SDK provided valuable insights into the health of the cluster and the services using it. However, it also led to a new challenge: a noisy on-call experience.</p><p>The teams started receiving frequent alerts related to unhealthy applications unable to reach the Kafka brokers. There were also alerts about lag issues and general Kafka cluster health problems.</p><p>The existing alerting pipeline was fairly basic. Prometheus collected the metrics and AlertManager continuously monitored them to page the team via PagerDuty.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe01a0ae3-a74c-40de-a1d4-5985cba79f5c_1600x934.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="850" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe01a0ae3-a74c-40de-a1d4-5985cba79f5c_1600x934.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://youtu.be/_booBjCB7rU?si=TONQvvYstgGmmke0">Tales of Kafka Presentation</a></figcaption></figure></div><p>Most problems faced by services concerning Kafka were due to deteriorating network conditions. The common solution was to restart the service manually. However, this often required on-call engineers to wake up during the night to perform manual restarts or scale services up and down, which was far from ideal.</p><p>To address this challenge, the team decided to leverage Kubernetes and their existing knowledge to improve the situation. They introduced health checks.</p><p>Health checks allow applications to report their readiness to operate, enabling the orchestrator to take appropriate actions when issues arise.</p><p>In Kubernetes, there are three types of health checks:</p><ol><li><p><strong>Liveness Probe: </strong>They indicate whether a service is <em>ready to run</em>.</p></li><li><p><strong>Readiness Probe: </strong>They determine if a service is <em>prepared</em> to receive HTTP traffic.</p></li><li><p><strong>Startup Probes: </strong>They act as an extended liveness check for slow-starting services.</p></li></ol><p>Kubernetes periodically pings the service at a specific endpoint (example: /health), and the service must respond with a successful status code to be considered healthy.</p><p>For Kafka consumers, implementing a readiness probe doesn&#8217;t make much sense, as they typically don&#8217;t expose an HTTP server. Therefore, the team focused on implementing simple liveness checks that worked as follows:</p><ul><li><p>Perform a basic operation with the Kafka broker, such as listing topics.&nbsp;</p></li><li><p>If the operation fails, the health check fails, indicating an issue with the service&#8217;s ability to communicate with the broker.</p></li></ul><p>The diagram below shows the approach:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd1b41db-69ec-4047-8539-e51eb855a57b_1600x1194.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1087" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd1b41db-69ec-4047-8539-e51eb855a57b_1600x1194.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://blog.cloudflare.com/intelligent-automatic-restarts-for-unhealthy-kafka-consumers">Cloudflare Tech Blog</a></figcaption></figure></div><p>There were still cases where the application appeared healthy but was stuck and unable to produce or consume messages. To handle this, the team implemented smarter health checks for Kafka consumers.</p><p>The smart health checks rely on two key concepts:</p><ul><li><p>The current offset represents the last available offset on a partition.</p></li><li><p>The committed offset is the last offset committed by a specific consumer for that partition.</p></li></ul><p>Here&#8217;s what happens during the health check:</p><ul><li><p>The consumer retrieves both offsets.&nbsp;</p></li><li><p>If it fails to retrieve them, the consumer is considered unhealthy.&nbsp;</p></li><li><p>If the offsets are successfully retrieved, the consumer compares the last committed offset with the current offset.&nbsp;</p></li><li><p>If they are the same, no new messages have been appended to the partition, and the consumer is considered healthy.</p></li><li><p>If the last committed offset hasn&#8217;t changed since the previous check, the consumer is likely stuck and needs to be restarted.</p></li></ul><p>Implementing these smart health checks led to improvements in the on-call experience as well as overall customer satisfaction.</p><h3>3 - Inability to Keep Up</h3><p>Another challenge that sprang up as Cloudflare&#8217;s customer base grew was with the email system.&nbsp;</p><p>The email system is responsible for sending transactional emails to customers, such as account verification emails, password reset emails, and billing notifications. These emails were critical for customer engagement and satisfaction, and any delays or failures in delivering them can hurt the user experience.</p><p>During traffic spikes, the email system struggled to process the high volume of messages being produced to Kafka.&nbsp;</p><p>The system was designed to consume messages one at a time, process them, and send the corresponding emails. However, as the production rate increased, the email system fell behind, creating a growing backlog of messages and increased lag.</p><p>One thing was clear to the engineering team at Cloudflare. The existing architecture wasn&#8217;t scalable enough to handle the increasing production rates.</p><p>Therefore, they introduced batch consumption to optimize the email system&#8217;s throughput.</p><p>Batch consumption is a technique where instead of processing messages one at a time, the consumer retrieves a batch of messages from Kafka and processes them together. This approach has several advantages, particularly in scenarios with high production rates.</p><p>The diagram below shows the batching approach.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e3e82df-4d72-438c-8be7-1526950b2413_1600x830.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="755" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e3e82df-4d72-438c-8be7-1526950b2413_1600x830.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Source: <a href="https://youtu.be/_booBjCB7rU?si=TONQvvYstgGmmke0">Tales of Kafka Presentation</a></figcaption></figure></div><p>The changes made were as follows:</p><ul><li><p>The Kafka consumer was modified to retrieve a configurable number of messages in each poll.</p></li><li><p>They updated the email-sending logic to process the batch of messages. Techniques like bulk database inserts and parallel email dispatching were used.</p></li></ul><p>Batch consuming with emails was soon put to the test during a major product launch that generated a surge in sign-ups and account activations.&nbsp;</p><p>This resulted in a massive increase in the number of account verification emails that had to be sent. However, the email system was able to handle the increased load efficiently.</p><h2>Conclusion</h2><p>Cloudflare&#8217;s journey of scaling Kafka to handle 1 trillion messages is remarkable.&nbsp;</p><p>From the early days of their monolithic architecture to the development of sophisticated abstractions and tools, we&#8217;ve seen how Cloudflare tackled multiple challenges across coupling, unstructured communication, and common usage patterns.</p><p>Along the way, we&#8217;ve also learned valuable lessons that can be applied to any organization. Here are a few of them:</p><ul><li><p>It&#8217;s important to strike a balance between configuration and simplicity. While flexibility is necessary for diverse use cases, it&#8217;s equally important to standardize and enforce best practices.</p></li><li><p>Visibility is the key in distributed systems. If you can&#8217;t see what&#8217;s happening in a certain part of your application, it becomes hard to remove bottlenecks.</p></li><li><p>Clear and well-defined contracts between producers and consumers are essential for building loosely coupled systems that can evolve independently.</p></li><li><p>Sharing knowledge and best practices across the organization helps streamline the adoption process and creates opportunities for improvement.</p></li></ul><p><strong>References:</strong></p><ul><li><p><a href="https://blog.cloudflare.com/using-apache-kafka-to-process-1-trillion-messages">Using Apache Kafka to process 1 trillion messages</a></p></li><li><p><a href="https://www.infoq.com/articles/kafka-clusters-cloudflare/">Tales of Kafka at Cloudflare</a></p></li><li><p><a href="https://blog.cloudflare.com/intelligent-automatic-restarts-for-unhealthy-kafka-consumers">Intelligent automatic restarts for unhealthy Kafka consumers</a></p></li><li><p><a href="https://youtu.be/_booBjCB7rU?si=M4UYkZ7ltxSC2GJW">Lessons learned on the way to 1 trillion messages</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Configure Liveness, Readiness, and Startup Probes</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Mon, 20 May 2024 15:31:15 GMT</pubDate>
</item>
<item>
<title>EP112: What is a deadlock?</title>
<link>https://blog.bytebytego.com/p/ep112-what-is-a-deadlock</link>
<guid>https://blog.bytebytego.com/p/ep112-what-is-a-deadlock</guid>
<content:encoded><![CDATA[
<div> API Protocols, Deadlock, Authentication, ElasticSearch, CPU Usage
<br /><br />总结: 本周文章介绍了最流行的API协议、死锁、基于会话和JWT的身份验证区别、ElasticSearch的应用以及CPU使用率100%的常见情况。其中详细解释了死锁的概念、预防和恢复方法，以及基于会话和JWT的身份验证方式的比较。此外，还介绍了ElasticSearch在文本搜索、实时分析、机器学习、地理数据应用、日志和事件数据分析、安全信息与事件管理等方面的典型用例，以及导致CPU使用率达到100%的常见原因。整体来看，这篇文章涵盖了系统设计中的重要概念和技术，对读者了解和运用这些知识具有一定的参考价值。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Top 9 Most Popular API Protocols (Youtube video)</p></li><li><p>What is a deadlock? </p></li><li><p>What&#8217;s the difference between Session-based authentication and JWTs? </p></li><li><p>Top 6 ElasticSearch Use Cases</p></li><li><p>Top 9 Cases Behind 100% CPU Usage</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/LinearB_051824">[Complimentary Download] Gartner Market Guide: Software Engineering Intelligence (SEI) Platforms (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/LinearB_051824" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="371.0192307692308" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47201120-d762-43fb-b2de-5ac248a959c1_1600x838.png" width="708" /><div></div></div></a></figure></div><p>Engineering teams are rapidly adopting Software Engineering Intelligence (SEI) Platforms to improve productivity and value delivery. According to Gartner&#8217;s recent Market Guide, use of SEI platforms by engineering organizations will rise to 50% by 2027, compared to 5% in 2024. LinearB was recognized by Gartner as a representative vendor, so we&#8217;re offering ByteByteGo readers a complimentary copy.&nbsp;</p><p>Learn how you can unlock the transformative potential of SEI platforms by leveraging key features like: </p><ul><li><p>Extensive data from DevOps tools for critical metrics and insights.</p></li><li><p>Customizable dashboards that highlight pivotal trends and inform strategic decisions.</p></li><li><p>Insights into KPIs that showcase your team's achievements.</p><p></p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/LinearB_051824"><span>Download your guide now</span></a></p></li></ul><div><hr /></div><h2>Top 9 Most Popular API Protocols</h2><div class="youtube-wrap" id="youtube2-zY2DMpCUfCg"><div class="youtube-inner"></div></div><div><hr /></div><h2>What is a deadlock? </h2><p>A deadlock occurs when two or more transactions are waiting for each other to release locks on resources they need to continue processing. This results in a situation where neither transaction can proceed, and they end up waiting indefinitely. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3702d55e-77eb-4dc6-9a57-2d3272fa5c4f_1280x1664.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><ul><li><p>Coffman Conditions <br />The Coffman conditions, named after Edward G. Coffman, Jr., who first outlined them in 1971, describe four necessary conditions that must be present simultaneously for a deadlock to occur: <br /> <br />- Mutual Exclusion <br />- Hold and Wait <br />- No Preemption <br />- Circular Wait <br /></p></li><li><p>Deadlock Prevention <br />- Resource ordering: impose a total ordering of all resource types, and require that each process requests resources in a strictly increasing order. <br /> <br />- Timeouts: A process that holds resources for too long can be rolled back. <br /> <br />- Banker&#8217;s Algorithm: A deadlock avoidance algorithm that simulates the allocation of resources to processes and helps in deciding whether it is safe to grant a resource request based on the future availability of resources, thus avoiding unsafe states. <br /></p></li><li><p>Deadlock Recovery <br />- Selecting a victim: Most modern Database Management Systems (DBMS) and Operating Systems implement sophisticated algorithms for detecting deadlocks and selecting victims, often allowing customization of the victim selection criteria via configuration settings. The selection can be based on resource utilization, transaction priority, cost of rollback etc. <br /> <br />- Rollback: The database may roll back the entire transaction or just enough of it to break the deadlock. Rolled-back transactions can be restarted automatically by the database management system. </p></li></ul><p>Over to you: have you solved any tricky deadlock issues?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4e965c2-b545-4f79-a489-e0e72d473c06_1109x1600.png" title="" width="1109" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">A Crash Course in GraphQL</a></p></li><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li><li><p><a href="https://blog.bytebytego.com/p/what-happens-when-a-sql-is-executed">What Happens When a SQL is Executed?</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-api-versioning">A Crash Course in API Versioning Strategies</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>What&#8217;s the difference between Session-based authentication and JWTs? </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1560" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02c27d00-914e-4626-b507-51627646af11_1280x1560.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><p>Here&#8217;s a simple breakdown for both approaches: <br /> <br /><strong>Session-Based Authentication</strong> <br /> <br />In this approach, you store the session information in a database or session store and hand over a session ID to the user. <br /> <br />Think of it like a passenger getting just the Ticket ID of their flight while all other details are stored in the airline&#8217;s database. <br /> <br />Here&#8217;s how it works: </p><ol><li><p>The user makes a login request and the frontend app sends the request to the backend server. </p></li><li><p>The backend creates a session using a secret key and stores the data in session storage.</p></li><li><p>The server sends a cookie back to the client with the unique session ID. </p></li><li><p>The user makes a new request and the browser sends the session ID along with the request. </p></li><li><p>The server authenticates the user using the session ID. </p></li></ol><p><strong>JWT-Based Authentication</strong> <br /> <br />In the JWT-based approach, you don&#8217;t store the session information in the session store. <br /> <br />The entire information is available within the token. <br /> <br />Think of it like getting the flight ticket along with all the details available on the ticket but encoded. <br /> <br />Here&#8217;s how it works: </p><ol><li><p>The user makes a login request and it goes to the backend server. </p></li><li><p>The server verifies the credentials and issues a JWT. The JWT is signed using a private key and no session storage is involved. </p></li><li><p>The JWT is passed to the client, either as a cookie or in the response body. Both approaches have their pros and cons but we&#8217;ve gone with the cookie approach. </p></li><li><p>For every subsequent request, the browser sends the cookie with the JWT. </p></li><li><p>The server verifies the JWT using the secret private key and extracts the user info.</p></li></ol><div><hr /></div><h2>Top 6 ElasticSearch Use Cases</h2><p>Elasticsearch is widely used for its powerful and versatile search capabilities. The diagram below shows the top 6 use cases: </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c1dd39f-3e32-4fd7-9360-7e460357e008_1280x1664.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><ul><li><p>Full-Text Search <br />Elasticsearch excels in full-text search scenarios due to its robust, scalable, and fast search capabilities. It allows users to perform complex queries with near real-time responses. <br /></p></li><li><p>Real-Time Analytics <br />Elasticsearch's ability to perform analytics in real-time makes it suitable for dashboards that track live data, such as user activity, transactions, or sensor outputs. <br /></p></li><li><p>Machine Learning <br />With the addition of the machine learning feature in X-Pack, Elasticsearch can automatically detect anomalies, patterns, and trends in the data. <br /></p></li><li><p>Geo-Data Applications <br />Elasticsearch supports geo-data through geospatial indexing and searching capabilities. This is useful for applications that need to manage and visualize geographical information, such as mapping and location-based services. <br /></p></li><li><p>Log and Event Data Analysis <br />Organizations use Elasticsearch to aggregate, monitor, and analyze logs and event data from various sources. It's a key component of the ELK stack (Elasticsearch, Logstash, Kibana), which is popular for managing system and application logs to identify issues and monitor system health. </p><p></p></li><li><p>Security Information and Event Management (SIEM) <br />Elasticsearch can be used as a tool for SIEM, helping organizations to analyze security events in real time. <br /></p></li></ul><p>Over to you: What did we miss?</p><div><hr /></div><h2>Top 9 Cases Behind 100% CPU Usage</h2><p>The diagram below shows common culprits that can lead to 100% CPU usage. Understanding these can help in diagnosing problems and improving system efficiency. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c1af2cb-4c71-424e-a0ef-f1c618a78666_1280x1664.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><ol><li><p>Infinite Loops </p></li><li><p>Background Processes </p></li><li><p>High Traffic Volume </p></li><li><p>Resource-Intensive Applications </p></li><li><p>Insufficient Memory </p></li><li><p>Concurrent Processes </p></li><li><p>Busy Waiting </p></li><li><p>Regular Expression Matching </p></li><li><p>Malware and Viruses </p></li></ol><p>Over to you: Did we miss anything important?</p><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Sat, 18 May 2024 15:30:55 GMT</pubDate>
</item>
<item>
<title>A Crash Course in GraphQL</title>
<link>https://blog.bytebytego.com/p/a-crash-course-in-graphql</link>
<guid>https://blog.bytebytego.com/p/a-crash-course-in-graphql</guid>
<content:encoded><![CDATA[
<div> GraphQL, API, maintenance costs, interface, connectivity 
<br />
GraphQL 是一项可改变客户端和服务器交互方式的工具。它可以作为在完全重构应用和什么也不做之间找到的一个折中点。虽然不是解决所有问题的灵丹妙药，但可以通过减少维护成本、简化开发流程来帮助解决应用程序接口日益复杂的问题。相比于REST API和BFFs，GraphQL具有各自的优缺点。在2023年，API协议的格局正在逐渐演变，适应这个变化的一个解决方案就是GraphQL。 <br /><br />总结:GraphQL作为一种新的API工具，可以帮助解决日益增长的维护成本和接口连接问题，是在API协议演变中的一种解决方案，以其灵活性和优势为用户提供了更好的开发体验。 <div>
<p></p><p>The complexity of software applications has grown by leaps and bounds over the years.</p><p>This has led to a rise in the number of interfaces between various systems, resulting in an ever-growing API footprint.</p><p>While APIs have revolutionized the connectivity between systems, the explosion of integrations between clients and servers often leads to maintenance problems. Even minor backend changes take more implementation time since developers must analyze and test more. Despite all the effort, there are still high chances that issues creep into the application.</p><p>Refactoring the application interfaces is one way to address growing maintenance costs. However, this is costly, and there&#8217;s no guarantee that we won&#8217;t encounter similar issues as the system evolves.</p><p>What&#8217;s the solution to this problem?</p><p>GraphQL is a tool that brings a major change in how clients and servers interact. While it&#8217;s not a silver bullet, it can be a sweet spot between a complete application overhaul and doing absolutely nothing</p><p>In this issue, we&#8217;ll explore GraphQL's features and concepts, compare it to REST API and BFFs, and discuss its advantages and disadvantages.</p><div><hr /></div><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4e965c2-b545-4f79-a489-e0e72d473c06_1109x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1018.5752930568079" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4e965c2-b545-4f79-a489-e0e72d473c06_1109x1600.png" title="" width="706" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">The Evolving Landscape of API Protocols in 2023</figcaption></figure></div><h2>What is GraphQL?</h2>
      <p>
          <a href="https://blog.bytebytego.com/p/a-crash-course-in-graphql">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 16 May 2024 15:30:42 GMT</pubDate>
</item>
<item>
<title>How Facebook served billions of requests per second Using Memcached</title>
<link>https://blog.bytebytego.com/p/how-facebook-served-billions-of-requests</link>
<guid>https://blog.bytebytego.com/p/how-facebook-served-billions-of-requests</guid>
<content:encoded><![CDATA[
<div> Facebook, Memcached, scalability, challenges, optimizations
<br /><br />总结:
Facebook通过优化和实现Memcached，解决了处理海量数据和为数十亿用户提供服务的挑战。他们在架构决策和服务器优化方面取得了重要进展。重要的学习点是：接受最终一致性，设计系统以应对故障，以及可以在多个级别进行优化。 <div>
<h2><a href="https://bit.ly/WorkOS_051424">Creating stronger passwords with AuthKit (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/WorkOS_051424" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="764" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38010da7-970d-40ed-96fa-e17bc7a96a77_1600x840.png" width="1456" /><div></div></div></a></figure></div><p>A common cause of data breaches and account hijacking is customers using weak or common passwords.<br /><br />One way to mitigate this risk is to inform new users about the strength of their passwords as they create them.<br /><br />To solve this problem, Dropbox created zxcvbn, an open-source library that calculates password strength based on factors like entropy, dictionary checks, and pattern recognition.<br /><br />If you want an easy way to implement user password security in your app, check out AuthKit, an open-source login box that incorporates zxcvbn and other best practices to provide a much more secure onboarding experience for new users.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/WorkOS_051424"><span>Read the guide</span></a></p><div><hr /></div><p>There are two absolute truths about running a social network at the scale of Facebook:</p><ul><li><p>First, it cannot go down.</p></li><li><p>Second, it cannot run slow.</p></li></ul><p>These two factors determine whether people are going to stay on your social network or not.&nbsp;</p><p>Even a few people leaving impacts the entire user base because the users are interconnected. Most people are online because their friends or relatives are online and there&#8217;s a domino effect at play. If one user drops off due to issues, there are chances that other users will also leave.</p><p>Facebook had to deal with these issues early on because of its popularity. At any point in time, millions of people were accessing Facebook from all over the world.</p><p>In terms of software design, this meant a few important requirements:</p><ul><li><p>Facebook had to support real-time communication.</p></li><li><p>They had to build capabilities for on-the-fly content aggregation.</p></li><li><p>Scale to handle billions of user requests.</p></li><li><p>Store trillions of items across multiple geographic locations.</p></li></ul><p>To achieve these goals, Facebook took up the open-source version of Memcached and enhanced it to build a distributed key-value store.</p><p>This enhanced version was known as Memcache.</p><p>In this post, we will look at how Facebook solved the multiple challenges in scaling memcached to serve billions of requests per second.</p><div><hr /></div><h2>Introduction to Memcached</h2><p>Memcached is an in-memory key-value store that supports a simple set of operations such as set, get, and delete.</p><p>The open-source version provided a single-machine in-memory hash table. The engineers at Facebook took up this version as a basic building block to create a distributed key-value store known as Memcache.</p><p>In other words, &#8220;Memcached&#8221; is the source code or the running binary whereas &#8220;Memcache&#8221; stands for the distributed system behind it.</p><p>Technically, Facebook used Memcache in two main ways:</p><h3>Query Cache</h3><p>The job of the query cache was to reduce the load on the primary source-of-truth databases.</p><p>In this mode, Facebook used Memcache as a demand-filled look-aside cache. You may have also heard about it as the cache-aside pattern.</p><p>The below diagram shows how the look-aside cache pattern works for the read and write path.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10c6a41b-307f-45f2-b2e0-ff6da2454400_1600x959.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="873" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10c6a41b-307f-45f2-b2e0-ff6da2454400_1600x959.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>The read path utilizes a cache that is filled on-demand. This means that data is only loaded into the cache when a client specifically requests it.&nbsp;</p><p>Before serving the request, the client first checks the cache. If the desired data is not found in the cache (a cache miss), the client retrieves the data from the database and also updates the cache.</p><p>The write path takes a more interesting approach to updating data.&nbsp;</p><p>After a particular key is updated in the database, the system doesn&#8217;t directly update the corresponding value in the cache. Instead, it removes the data for that key from the cache entirely. This process is known as cache invalidation.</p><p>By invalidating the cache entry, the system ensures that the next time a client requests data for that key, it will experience a cache miss and be forced to retrieve the most up-to-date value directly from the database. This approach helps maintain data consistency between the cache and the database.</p><h3>Generic Cache</h3><p>Facebook also leverages Memcache as a general-purpose key-value store. This allows different teams within the organization to utilize Memcache for storing pre-computed results generated from computationally expensive machine learning algorithms.</p><p>By storing these pre-computed ML results in Memcache, other applications can quickly and easily access them whenever needed.</p><p>This approach offers several benefits such as improved performance and resource optimization.</p><h2>High-Level Architecture of Facebook</h2><p>Facebook&#8217;s architecture is built to handle the massive scale and global reach of its platform.&nbsp;</p><p>At the time of their Memcached adoption, Facebook&#8217;s high-level architecture consisted of three key components:</p><h3>1 - Regions</h3><p>Facebook strategically places its servers in various locations worldwide, known as regions. These regions are classified into two types:</p><ul><li><p><strong>Primary Region: </strong>The primary region is responsible for handling the majority of user traffic and data management.</p></li><li><p><strong>Secondary Region: </strong>Multiple secondary regions are distributed globally to provide redundancy, load balancing, and improved performance for users in different geographical areas.</p></li></ul><p>Each region, whether primary or secondary, contains multiple frontend clusters and a single storage cluster.</p><h3>2 - Frontend Clusters</h3><p>Within each region, Facebook employs frontend clusters to handle user requests and serve content. A frontend cluster consists of two main components:</p><ul><li><p><strong>Web Servers: </strong>These servers are responsible for processing user requests, rendering pages, and delivering content to the users.</p></li><li><p><strong>Memcache Servers: </strong>Memcache servers act as a distributed caching layer, storing frequently accessed data in memory for quick retrieval.</p></li></ul><p>The frontend clusters are designed to scale horizontally based on demand. As user traffic increases, additional web and Memcache servers can be added to the cluster to handle the increased load.</p><h3>3 - Storage Cluster</h3><p>At the core of each region lies the storage cluster. This cluster contains the source-of-truth database, which stores the authoritative copy of every data item within Facebook&#8217;s system.</p><p>The storage cluster takes care of data consistency, durability, and reliability.</p><p>By replicating data across multiple regions and employing a primary-secondary architecture, Facebook achieves high availability and fault tolerance.</p><p>The below diagram shows the high-level view of this architecture:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46f8ebeb-fc61-4427-bb0b-dbc7aa8ef619_1600x923.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="840" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46f8ebeb-fc61-4427-bb0b-dbc7aa8ef619_1600x923.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>One major philosophy that Facebook adopted was a willingness to expose slightly stale data instead of allowing excessive load on the backend.&nbsp;</p><p>Rather than striving for perfect data consistency at all times, Facebook accepted that users may sometimes see outdated information in their feeds. This approach allowed them to handle high traffic loads without crumbling under excessive strain on the backend infrastructure.</p><p>To make this architecture work at an unprecedented scale of billions of requests every day, Facebook had to solve multiple challenges such as:</p><ul><li><p>Managing latency and failures within a cluster.</p></li><li><p>Managing data replication within a region.</p></li><li><p>Managing data consistency across regions.</p></li></ul><p>In the next few sections, we will look at how Facebook handled each of these challenges.</p><h2>Within Cluster Challenges</h2><p>There were three important goals for the within-cluster operations:</p><ul><li><p>Reducing latency</p></li><li><p>Reducing the load on the database</p></li><li><p>Handling failures</p></li></ul><h3>1 - Reducing Latency</h3><p>As mentioned earlier, every frontend cluster contains hundreds of Memcached servers, and items are distributed across these servers using techniques like Consistent Hashing.</p><p>For reference, Consistent Hashing is a technique that allows the distribution of a set of keys across multiple nodes in a way that minimizes the impact of node failures or additions. When a node goes down or a new node is introduced, Consistent Hashing ensures that only a small subset of keys needs to be redistributed, rather than requiring a complete reshuffling of data.</p><p>The diagram illustrates the concept of Consistent Hashing where keys are mapped to a circular hash space, and nodes are assigned positions on the circle. Each key is assigned to the node that falls closest to it in a clockwise direction.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac03e7f-77db-4b52-9c30-bd2718bd8bfe_1600x1461.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1330" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcac03e7f-77db-4b52-9c30-bd2718bd8bfe_1600x1461.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>At Facebook's scale, a single web request can trigger hundreds of fetch requests to retrieve data from Memcached servers. Consider a scenario where a user loads a popular page containing numerous posts and comments.&nbsp;</p><p>Even a single request can require the web servers to communicate with multiple Memcached servers in a short timeframe to populate the necessary data.</p><p>This high-volume data fetching occurs not only in cache hit situations but also when there&#8217;s a cache miss. The implication is that a single Memcached server can turn into a bottleneck for many web servers, leading to increased latency and degraded performance for the end user.</p><p>To reduce the possibility of such a scenario, Facebook uses a couple of important tricks visualized in the diagram.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa67ef54c-971d-43fa-bba6-6ed418fdefcc_1600x1123.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1022" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa67ef54c-971d-43fa-bba6-6ed418fdefcc_1600x1123.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h4>Parallel Requests and Batching</h4><p>To understand the concept of parallel requests and batching, consider a simple analogy.&nbsp;</p><p>Imagine going to the supermarket every time you need an item. It would be incredibly time-consuming and inefficient to make multiple trips for individual items. Instead, it&#8217;s much more effective to plan your shopping trip and purchase a bunch of items together in a single visit.</p><p>The same optimization principle applies to data retrieval in Facebook's frontend clusters.&nbsp;</p><p>To maximize the efficiency of data retrieval, Facebook constructs a Directed Acyclic Graph (DAG) that represents the dependencies between different data items.</p><p>The DAG provides a clear understanding of which data items can be fetched concurrently and which items depend on others.</p><p>By analyzing the DAG, the web server can determine the optimal order and grouping of data fetches. It identifies data items that can be retrieved in parallel, without any dependencies, and groups them in a single batch request.</p><h4>Using UDP&nbsp;</h4><p>Facebook employed a clever strategy to optimize network communication between the web servers and the Memcache server.</p><p>For fetch requests, Facebook configured the clients to use UDP instead of TCP.&nbsp;</p><p>As you may know, UDP is a connectionless protocol and much faster than TCP. By using UDP, the clients can send fetch requests to the Memcache servers with less network overhead, resulting in faster request processing and reduced latency.</p><p>However, UDP has a drawback: it doesn&#8217;t guarantee the delivery of packets. If a packet is lost during transmission, UDP doesn&#8217;t have a built-in mechanism to retransmit it.&nbsp;</p><p>To handle such cases, they treated UDP packet loss as a cache miss on the client side. If a response isn&#8217;t received within a specific timeframe, the client assumes that the data is not available in the cache and proceeds to fetch it from the primary data source.</p><p>For update and delete operations, Facebook still used TCP since it provided a reliable communication channel that ensured the delivery of packets in the correct order. It removed the need for adding a specific retry mechanism, which is important when dealing with update and delete operations.</p><p>All these requests go through a special proxy known as <strong>mcrouter</strong> that runs on the same machine as the webserver. Think of the <strong>mcrouter</strong> as a middleman that performs multiple duties such as data serialization, compression, routing, batching, and error handling. We will look at <strong>mcrouter</strong> in a later section.</p><h3>2 - Reducing Load</h3><p>The most important goal for Memcache is to reduce the load on the database by reducing the frequency of data fetching from the database.&nbsp;</p><p>Using Memcache as a look-aside cache solves this problem significantly. But at Facebook&#8217;s scale, two caching-related issues can easily appear.</p><ul><li><p><strong>Stale Set: </strong>This happens when the cache is set with outdated data and there&#8217;s no easy way of invalidating it.</p></li><li><p><strong>Thundering Herd: </strong>This problem occurs in a highly concurrent environment when a cache miss triggers a thundering herd of requests to the database.</p></li></ul><p>The below diagram visualizes both of these issues.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3bd7af6-8955-4c31-a20f-40216163cab3_1396x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3bd7af6-8955-4c31-a20f-40216163cab3_1396x1600.png" title="" width="1396" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>To minimize the probability of these two critical issues, Facebook used a technique known as leasing.</p><p>Leasing helped solve both stale sets and thundering herds, helping Facebook reduce peak DB query rates from 17K/second to 1.3K/second.</p><h4>Stale Sets</h4><p>Consider that a client requests memcache for a particular key and it results in a cache miss.</p><p>Now, it&#8217;s the client&#8217;s responsibility to fetch the data from the database and also update memcache so that future requests for the same key don&#8217;t result in a cache miss.</p><p>This works fine most of the time but in a highly concurrent environment, the data being set by the client may get outdated by the time it gets updated in the cache.</p><p>Leasing prevents this from happening.&nbsp;</p><p>With leasing, Memcache hands over a lease (a 64-bit token bound to a specific key) to a particular client to set data into the cache whenever there&#8217;s a cache miss.&nbsp;</p><p>The client has to provide this token when setting the value in the cache and memcache can verify whether the data should be stored by verifying the token. If the item was already invalidated by the time the client tried to update, Memcache will invalidate the lease token and reject the request.</p><p>The below diagram shows the concept of leasing in a much better manner.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fea2405-ad1a-4c6f-b378-268362482977_1573x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1481" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fea2405-ad1a-4c6f-b378-268362482977_1573x1600.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h4>Thundering Herds</h4><p>A slight modification to the leasing technique also helps solve the thundering herd issue.</p><p>In this modification, Memcache regulates the rate of issuing the lease tokens. For example, it may return a token once every 5 seconds per key.</p><p>For any requests for the key within 5 seconds of the lease token being issued, Memcache sends a special response requesting the client to wait and retry so that these requests don&#8217;t hit the database needlessly. This is because there&#8217;s a high probability that the client holding the lease token will soon update the cache and the waiting clients will get a cache hit when they retry.</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="927" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3e5a5e3-b6f2-4f89-98f5-8c5f59c5d795_1600x1019.png" title="" width="1456" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li><li><p><a href="https://blog.bytebytego.com/p/what-happens-when-a-sql-is-executed">What Happens When a SQL is Executed?</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-api-versioning">A Crash Course in API Versioning Strategies</a></p></li><li><p><a href="https://blog.bytebytego.com/p/embracing-chaos-to-improve-system">Embracing Chaos to Improve System Resilience: Chaos Engineering</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h3>3 - Handling Failures</h3><p>In a massive-scale system like Facebook, failures are an inevitable reality.</p><p>With millions of users using the platform, any disruption in data retrieval from Memcache can have severe consequences. If clients are unable to fetch data from Memcache, it places an excessive load on the backend servers, potentially leading to cascading failures in downstream services.</p><h3>Two Levels of Failure</h3><p>Facebook faced two primary levels of failures when it comes to Memcache:</p><ul><li><p><strong>Small-Scale Outages:</strong> A small number of hosts may become inaccessible due to network issues or other localized problems. While these outages are limited in scope, they can still impact the overall system performance.</p></li><li><p><strong>Widespread Outages: In more severe cases, an entire cluster may go down, affecting a significant percentage of the Memcache hosts. Such widespread outages create a greater threat to the stability and availability of the system.</strong></p></li></ul><h4>Handling Widespread Outages</h4><p>To mitigate the impact of a cluster going down, Facebook diverts web requests to other functional clusters.</p><p>By redistributing the load, Facebook ensures that the problematic cluster is relieved of its responsibilities until it can be restored to health.</p><h4>Automated Remediation for Small Outages</h4><p>For small-scale outages, Facebook relies on an automated remediation system that automatically detects and responds to host-level issues by bringing up new instances to replace the affected ones.</p><p>However, the remediation process is not instantaneous and can take some time to complete. During this time window, the backend services may experience a surge in requests as clients attempt to fetch data from the unavailable Memcache hosts.&nbsp;</p><p>The common way of handling this is to rehash keys and distribute them among the remaining servers.&nbsp;</p><p>However, Facebook&#8217;s engineering team realized that this approach was still prone to cascading failures. In their system, many keys can account for a significant portion of the requests (almost 20%) to a single server. Moving these high-traffic keys to another server during a failure scenario could result in overload and further instability.&nbsp;</p><p>To mitigate this risk, Facebook went with the approach of using Gutter machines. Within each cluster, they allocate a pool of machines (typically 1% of the Memcache servers) specifically designated as Gutter machines. These machines are designed to take over the responsibilities of the affected Memcache servers during an outage.</p><p>Here&#8217;s how they work:</p><ul><li><p>If a Memcache client receives no response (not even a cache miss), the client assumes that the server has failed and issues a request to the Gutter pool.</p></li><li><p>If the request to the Gutter pool returns a cache miss, the client queries the database and inserts the data into the Gutter pool so that subsequent requests can be served from Memcache.</p></li><li><p>Gutter entries expire quickly to remove the need for invalidations.</p></li></ul><p>The below diagram shows how the Gutter pool works:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1671dbf-d312-46f4-8de9-42c1062cdc5d_1600x916.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="834" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1671dbf-d312-46f4-8de9-42c1062cdc5d_1600x916.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Though there are chances of serving stale data, the backend is protected. Remember that this was an acceptable trade-off for them when compared to availability.</p><h2>Region Level Challenges</h2><p>At the region level, there were multiple frontend clusters to deal with and the main challenge was handling Memcache invalidations across all of them.</p><p>Depending on the load balancer, users can connect to different front-end clusters when requesting data. This results in caching a particular piece of data in multiple clusters.</p><p>In other words, you can have a scenario where a particular key is cached in the Memcached servers of multiple clusters within the region. The below diagram shows this scenario:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc775036-767c-4d47-b834-8ee6b8010f65_1600x926.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="843" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffc775036-767c-4d47-b834-8ee6b8010f65_1600x926.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>As an example, the keys &#8220;abc&#8221; and &#8220;xyz&#8221; are present in multiple frontend clusters within a region and need to be invalidated in case of an update to their values.</p><h3>Cluster Level Invalidation</h3><p>Invalidating this data at the cluster level is reasonably simpler. Any web server that modifies the data is responsible for invalidating the data in that cluster. This provides read-after-write consistency for the user who made the request. It also reduces the lifetime of the stale data within the cluster.</p><p>For reference, read-after-write consistency is a guarantee that if a user makes some updates, he/she should always see those updates when they reload the page.</p><h3>Region Level Invalidation</h3><p>For region-level invalidation, the invalidation process is a little more complex and the webserver doesn&#8217;t handle it.</p><p>Instead, Facebook created an invalidation pipeline that works like this:</p><ul><li><p>An invalidation daemon known as <strong>mcsqueal</strong> runs on every database server within the storage cluster.</p></li><li><p>This daemon inspects the commit log, extracts any deletes, and broadcasts them to the Memcache deployments in every frontend cluster within the region.</p></li><li><p>For better performance, <strong>mcsqueal</strong> batches these deletes into fewer packets and sends them to dedicated servers running <strong>mcrouter</strong> instances in each cluster.</p></li><li><p>The <strong>mcrouter </strong>instance iterates over the individual deletes within the batch and routes them to the correct Memcache server.</p></li></ul><p>The below diagram explains this process.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3cb6fbf-eb4d-496d-b11e-0235fbc12d95_1600x1070.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="974" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3cb6fbf-eb4d-496d-b11e-0235fbc12d95_1600x1070.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>Challenges with Global Regions</h2><p>Operating at the scale of Facebook requires them to run and maintain data centers globally.</p><p>However, expanding to multiple regions also creates multiple challenges. The biggest one is maintaining consistency between the data in Memcache and the persistent storage across the regions.</p><p>In Facebook&#8217;s region setup, one region holds the primary databases while other geographic regions contain read-only replicas. The replicas are kept in sync with the primary using MySQL&#8217;s replication mechanism.</p><p>However, when replication is involved, there is bound to be some replication lag. In other words, the replica databases can fall behind the primary database.</p><p>There are two main cases to consider when it comes to consistency here:</p><h3>Writes from the Primary Region</h3><p>Let&#8217;s say a web server in the primary region (US) receives a request from the user to update their profile picture.</p><p>To maintain consistency, this change needs to be propagated to other regions as well.</p><ul><li><p>The replica databases have to be updated.</p></li><li><p>Also, the Memcache instances in the secondary regions need to be invalidated.</p></li></ul><p>The tricky part is managing the invalidation along with the replication.</p><p>If the invalidation arrives in the secondary region (Europe) before the actual change is replicated to the database in the region, there are chances of a race condition as follows:</p><ul><li><p>Someone in the Europe region tries to view the profile picture.</p></li><li><p>The system fetches the information from the cache but it has been invalidated.</p></li><li><p>Data is fetched from the read-only database in the region, which is still lagging. This means that the fetch request gets the old picture and sets it within the cache.</p></li><li><p>Eventually, the replication is successful but the cache is already set with stale data and future requests will continue fetching this stale data from the cache.</p></li></ul><p>The below diagram shows this scenario:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51b29010-3717-46a9-8fb7-52403f6026db_1600x980.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="892" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51b29010-3717-46a9-8fb7-52403f6026db_1600x980.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>To avoid such race conditions, Facebook implemented a solution where the storage cluster having the most up-to-date information is responsible for sending invalidations within a region. It uses the same <strong>mcsqueal</strong> setup we talked about in the previous section.</p><p>This approach ensures that invalidations don&#8217;t get sent prematurely to the replica regions before the change has been fully replicated in the databases.</p><h3>Writes from the Non-Primary Region</h3><p>When dealing with writes originating from non-primary regions, the sequence of events is as follows:</p><ul><li><p>User updates their profile picture from a secondary region. While reads are served from the replica or secondary regions, the writes go to the primary region.</p></li><li><p>After the writes are successful, the changes also need to be replicated in the secondary region as well.</p></li><li><p>However, there&#8217;s a risk that before the replication catches up, a read request on the replica region may fetch and cache stale data in Memcache.</p></li></ul><p>To solve this problem, Facebook used the concept of a remote marker.</p><p>The remote marker is used to indicate whether the data in the local replica is potentially stale and it should be queried from the primary region.</p><p>It works as follows:</p><ul><li><p>When a client web server requests to update the data for key K, it sets a remote marker R for that key in the replica region.</p></li><li><p>Next, it performs the write to the primary region.</p></li><li><p>Also, the key K is deleted from the replica region&#8217;s Memcache servers.</p></li><li><p>A read request comes along for K in the replica region but the webserver would get a cache miss.</p></li><li><p>It checks whether the remote marker R exists and if found, the query is directed to the primary region.</p></li></ul><p>The below diagram shows all the steps in more detail.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7903d4-f4e3-4d7e-8d10-641e507c8f7e_1600x981.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="893" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7903d4-f4e3-4d7e-8d10-641e507c8f7e_1600x981.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>At this point, you may think that this approach is inefficient because they are first checking the cache, then the remote marker, and then making the query to the primary region.</p><p>In this scenario, Facebook chose to trade off latency for a cache miss in exchange for a reduced probability of reading stale data.</p><h2>Single Server Optimizations</h2><p>As you can see, Facebook implemented some big architectural decisions to scale Memcached for their requirements. However, they also spent a significant time optimizing the performance of individual Memcache servers.</p><p>While the scope of these improvements may seem small in isolation, their cumulative impact at Facebook&#8217;s scale was significant.</p><p>Here are a few important optimizations that they made:</p><h3>Automatic Hash Table Expansion</h3><p>As the number of stored items grows, the time complexity of lookups in a hash table can degrade to O(n) if the table size remains fixed. This reduces the performance.</p><p>Facebook implemented an automatic expansion mechanism for the hash table. When the number of items reaches a certain threshold, the hash table automatically doubles in size, ensuring that the time complexity of the lookups remains constant even as the dataset grows.</p><h3>Multi-Threaded Server Architecture</h3><p>Serving a high volume of requests on a single thread can result in increased latency and reduced throughput.</p><p>To deal with this, they enhanced the Memcache server to support multiple threads and handle requests concurrently.</p><h3>Dedicated UDP Port for Each Thread</h3><p>When multiple threads share the same UDP port, contentions can occur and lead to performance problems.</p><p>They implemented support for each thread to have its own dedicated UDP port so that the threads can operate more efficiently.</p><h3>Adaptive Slab Allocator</h3><p>Inefficient memory allocation and management can lead to fragmentation and suboptimal utilization of system resources.</p><p>Facebook implemented an Adaptive Slab Allocator to optimize memory organization within each Memcache server. The slab allocator divides the available memory into fixed-size chunks called slabs. Each slab is further divided into smaller units of a specific size.&nbsp;</p><p>The allocator dynamically adapts the slab sizes based on the observed request patterns to maximize memory utilization.</p><h2>Conclusion</h2><p>Facebook&#8217;s journey in scaling Memcached serves as a fantastic case study for developers and engineers. It highlights the challenges that come up when building a globally distributed social network that needs to handle massive amounts of data and serve billions of users.</p><p>With their implementation and optimization of Memcache, Facebook demonstrates the importance of tackling scalability challenges at multiple levels. From high-level architectural decisions to low-level server optimizations, every aspect plays an important role in ensuring the performance, reliability, and efficiency of the system.</p><p>Three key learning points to take away from this study are as follows:</p><ul><li><p>Embracing eventual consistency is the key to performance and availability. However, every decision has to be taken based on a good understanding of the trade-offs.</p></li><li><p>Failures are inevitable and it&#8217;s critical to design your system for failures.</p></li><li><p>Optimization can be done at multiple levels.</p></li></ul><p><strong>References:</strong></p><ul><li><p><a href="https://research.facebook.com/publications/scaling-memcache-at-facebook/">Scaling Memcache at Facebook</a></p></li><li><p><a href="https://www.youtube.com/watch?v=UH7wkvcf0ys">Facebook and Memcached</a></p></li><li><p><a href="https://pdos.csail.mit.edu/6.824/papers/memcache-faq.txt">Memcache FAQ by MIT</a></p></li><li><p><a href="https://www.ehcache.org/documentation/2.8/recipes/thunderingherd.html">Thundering Herd</a></p></li><li><p><a href="https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside">Cache-Aside Pattern</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p>
]]></content:encoded>
<pubDate>Tue, 14 May 2024 15:30:27 GMT</pubDate>
</item>
<item>
<title>EP111: My Favorite 10 Books for Software Developers</title>
<link>https://blog.bytebytego.com/p/ep111-my-favorite-10-books-for-software</link>
<guid>https://blog.bytebytego.com/p/ep111-my-favorite-10-books-for-software</guid>
<content:encoded><![CDATA[
<div> 编程原则、软件开发书籍、重要论文、实时数据、IPv4 vs. IPv6
编程原则的重要性、软件开发人员必读的经典书籍如《The Pragmatic Programmer》、重要的改变计算机世界的25篇论文、实时数据利用的关键Change Data Capture、IPv4与IPv6的区别。
<br /><br />总结: 本文介绍了编程原则、软件开发书籍、重要论文、实时数据和IPv4与IPv6之间的区别。重点强调编程原则的重要性，推荐了经典的软件开发书籍，列举了改变计算机世界的25篇重要论文，介绍了实时数据利用的关键Change Data Capture以及IPv4和IPv6之间的差异。文章内容涵盖了编程知识、软件开发经验、计算机技术的重要变革和网络协议的演变。 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>10 Coding Principles Explained in 5 Minutes (Youtube video)</p></li><li><p>My Favorite 10 Books for Software Developers </p></li><li><p>25 Papers That Completely Transformed the Computer World</p></li><li><p>Change Data Capture: Key to Leverage Real-time Data</p></li><li><p>IPv4 vs. IPv6, what are the differences? </p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65b29d8d-75dd-4d36-b1af-08b6227f6138_1355x1600.png" title="" width="1355" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</a></p></li><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li><li><p><a href="https://blog.bytebytego.com/p/what-happens-when-a-sql-is-executed">What Happens When a SQL is Executed?</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-api-versioning">A Crash Course in API Versioning Strategies</a></p></li><li><p><a href="https://blog.bytebytego.com/p/embracing-chaos-to-improve-system">Embracing Chaos to Improve System Resilience: Chaos Engineering</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>10 Coding Principles Explained in 5 Minutes</h2><div class="youtube-wrap" id="youtube2-GmXPwRNIrAU"><div class="youtube-inner"></div></div><div><hr /></div><h2>My Favorite 10 Books for Software Developers </h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1040" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12d664a0-da33-43da-9059-48075c1cad7d_800x1040.gif" title="No alternative text description for this image" width="800" /><div></div></div></a></figure></div><p>General Advice </p><ol><li><p>The Pragmatic Programmer by Andrew Hunt and David Thomas </p></li><li><p>Code Complete by Steve McConnell: Often considered a bible for software developers, this comprehensive book covers all aspects of software development, from design and coding to testing and maintenance. </p></li></ol><p>Coding </p><ol><li><p>Clean Code by Robert C. Martin </p></li><li><p>Refactoring by Martin Fowler </p></li></ol><p>Software Architecture </p><ol><li><p>Designing Data-Intensive Applications by Martin Kleppmann </p></li><li><p>System Design Interview (our own book :)) </p></li></ol><p>Design Patterns </p><ol><li><p>Design Patterns by Eric Gamma and Others </p></li><li><p>Domain-Driven Design by Eric Evans </p></li></ol><p>Data Structures and Algorithms </p><ol><li><p>Introduction to Algorithms by Cormen, Leiserson, Rivest, and Stein </p></li><li><p>Cracking the Coding Interview by Gayle Laakmann McDowell </p></li></ol><p>Over to you: What is your favorite book?</p><div><hr /></div><h2>25 Papers That Completely Transformed the Computer World</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="text" class="sizing-normal" height="1644" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F951c4ad6-986d-490c-a2b7-bfe0d0385a1f_1280x1644.jpeg" title="text" width="1280" /><div></div></div></a></figure></div><ol><li><p><a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo - Amazon&#8217;s Highly Available Key Value Store</a></p></li><li><p><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf">Google File System</a>: Insights into a highly scalable file system</p></li><li><p><a href="https://research.facebook.com/file/839620310074473/scaling-memcache-at-facebook.pdf">Scaling Memcached at Facebook</a>: A look at the complexities of Caching</p></li><li><p><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">BigTable</a>: The design principles behind a distributed storage system</p></li><li><p><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43438.pdf">Borg - Large Scale Cluster Management at Google</a></p></li><li><p><a href="https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf">Cassandra</a>: A look at the design and architecture of a distributed NoSQL database</p></li><li><p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>: Into a new deep learning architecture known as the transformer</p></li><li><p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf">Kafka</a>: Internals of the distributed messaging platform</p></li><li><p><a href="https://www.foundationdb.org/files/fdb-paper.pdf">FoundationDB</a>: A look at how a distributed database</p></li><li><p><a href="https://web.stanford.edu/class/cs245/readings/aurora.pdf">Amazon Aurora</a>: To learn how Amazon provides high-availability and performance</p></li><li><p><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf">Spanner</a>: Design and architecture of Google&#8217;s globally distributed databas</p></li><li><p><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/16cb30b4b92fd4989b8619a61752a2387c6dd474.pdf">MapReduce</a>: A detailed look at how MapReduce enables parallel processing of massive volumes of data</p></li><li><p><a href="https://dl.acm.org/doi/pdf/10.1145/3477132.3483546">Shard Manager</a>: Understanding the generic shard management framework</p></li><li><p><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/papers/dapper-2010-1.pdf">Dapper</a>: Insights into Google&#8217;s distributed systems tracing infrastructure</p></li><li><p><a href="https://www.researchgate.net/publication/308993790_Apache_Flink_Stream_and_Batch_Processing_in_a_Single_Engine">Flink</a>: A detailed look at the uni&#64257;ed architecture of stream and batch processing</p></li><li><p><a href="https://arxiv.org/pdf/2310.11703.pdf">A Comprehensive Survey on Vector Databases</a></p></li><li><p><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/10683a8987dbf0c6d4edcafb9b4f05cc9de5974a.pdf">Zanzibar</a>: A look at the design, implementation and deployment of a global system for managing access control lists at Google&nbsp;</p></li><li><p><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/d84ab6c93881af998de877d0070a706de7bec6d8.pdf">Monarch</a>: Architecture of Google&#8217;s in-memory time series database&nbsp;</p></li><li><p><a href="https://thrift.apache.org/static/files/thrift-20070401.pdf">Thrift</a>: Explore the design choices behind Facebook&#8217;s code-generation tool</p></li><li><p><a href="https://bitcoin.org/bitcoin.pdf">Bitcoin</a>: The ground-breaking introduction to the peer-to-peer electronic cash system</p></li><li><p><a href="https://web.stanford.edu/~rezab/papers/wtf_overview.pdf">WTF - Who to Follow Service at Twitter</a>: Twitter&#8217;s (now X) user recommendation system</p></li><li><p><a href="https://www.vldb.org/pvldb/vol13/p3217-matsunobu.pdf">MyRocks: LSM-Tree Database Storage Engine</a></p></li><li><p><a href="https://homepages.cwi.nl/~storm/teaching/reader/Dijkstra68.pdf">GoTo Considered Harmful</a></p></li><li><p><a href="https://raft.github.io/raft.pdf">Raft Consensus Algorithm</a>: To learn about the more understandable consensus algorithm</p></li><li><p><a href="https://lamport.azurewebsites.net/pubs/time-clocks.pdf">Time Clocks and Ordering of Events</a>: The extremely important paper that explains the concept of time and event ordering in a distributed system&nbsp;</p></li></ol><p>Over to you: I&#8217;m sure we missed many important papers. Which ones do you think should be included?</p><div><hr /></div><h2>Change Data Capture: Key to Leverage Real-time Data</h2><p>90% of the world&#8217;s data was created in the last two years and this growth will only get faster. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface" class="sizing-normal" height="1557" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc324fb65-98cb-44b4-878f-f56d1bd432a1_1280x1557.gif" title="graphical user interface" width="1280" /><div></div></div></a></figure></div><p><br /> <br />However, the biggest challenge is to leverage this data in real-time. Constant data changes make databases, data lakes, and data warehouses out of sync. <br /> <br />CDC or Change Data Capture can help you overcome this challenge. <br /> <br />CDC identifies and captures changes made to the data in a database, allowing you to replicate and sync data across multiple systems. <br /> <br />So, how does Change Data Capture work? Here's a step-by-step breakdown: <br /> <br />1 - Data Modification: A change is made to the data in the source database. It could be an insert, update, or delete operation on a table. <br /> <br />2 - Change Capture: A CDC tool monitors the database transaction logs to capture the modifications. It uses the source connector to connect to the database and read the logs. <br /> <br />3 - Change Processing: The captured changes are processed and transformed into a format suitable for the downstream systems. <br /> <br />4 - Change Propagation: The processed changes are published to a message queue and propagated to the target systems, such as data warehouses, analytics platforms, distributed caches like Redis, and so on. <br /> <br />5 - Real-Time Integration: The CDC tool uses its sink connector to consume the log and update the target systems. The changes are received in real time, allowing for conflict-free data analysis and decision-making. <br /> <br />Users only need to take care of step 1 while all other steps are transparent. <br /> <br />A popular CDC solution uses Debezium with Kafka Connect to stream data changes from the source to target systems using Kafka as the broker. Debezium has connectors for most databases such as MySQL, PostgreSQL, Oracle, etc. <br /> <br />Over to you: have you leveraged CDC in your application before?</p><div><hr /></div><h2>IPv4 vs. IPv6, what are the differences? </h2><p>The transition from Internet Protocol version 4 (IPv4) to Internet Protocol version 6 (IPv6) is primarily driven by the need for more internet addresses, alongside the desire to streamline certain aspects of network management.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7680062-0800-404d-86bf-ef4237788b2a_1536x1536.gif" title="graphical user interface, application" /><div></div></div></a></figure></div><ul><li><p>Format and Length <br />IPv4 uses a 32-bit address format, which is typically displayed as four decimal numbers separated by dots (e.g., 192.168.0. 12). The 32-bit format allows for approximately 4.3 billion unique addresses, a number that is rapidly proving insufficient due to the explosion of internet-connected devices. <br /> <br />In contrast, IPv6 utilizes a 128-bit address format, represented by eight groups of four hexadecimal digits separated by colons (e.g., 50B3:F200:0211:AB00:0123:4321:6571:B000). This expansion allows for approximately much more addresses, ensuring the internet's growth can continue unabated. </p><p></p></li><li><p>Header <br />The IPv4 header is more complex and includes fields such as the header length, service type, total length, identification, flags, fragment offset, time to live (TTL), protocol, header checksum, source and destination IP addresses, and options. <br /> <br />IPv6 headers are designed to be simpler and more efficient. The fixed header size is 40 bytes and includes less frequently used fields in optional extension headers. The main fields include version, traffic class, flow label, payload length, next header, hop limit, and source and destination addresses. This simplification helps improve packet processing speeds. </p><p></p></li><li><p>Translation between IPv4 and IPv6 <br />As the internet transitions from IPv4 to IPv6, mechanisms to allow these protocols to coexist have become essential: <br /> <br />- Dual Stack: This technique involves running IPv4 and IPv6 simultaneously on the same network devices. It allows seamless communication in both protocols, depending on the destination address availability and compatibility. The dual stack is considered one of the best approaches for the smooth transition from IPv4 to IPv6.</p><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p></p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p></p><p>Space Fills Up Fast - Reserve Today</p><p></p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a></strong></p><div><hr /></div></li></ul>
]]></content:encoded>
<pubDate>Sat, 11 May 2024 15:30:45 GMT</pubDate>
</item>
<item>
<title>HTTP1 vs HTTP2 vs HTTP3 - A Deep Dive</title>
<link>https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive</link>
<guid>https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive</guid>
<content:encoded><![CDATA[
<div> HTTP1, HTTP/1.1, Persistent Connections, Pipelining, Caching and Conditional Requests

总结:<br /><br />HTTP/1是互联网世界增长的推动力之一，HTTP/1.1作为其主要版本，在满足需求和解决问题的基础上，引入了持久连接、管线化、分块传输编码以及缓存和条件请求等重要特性。虽然HTTP/1.1在过去的20多年间发挥了巨大作用，但随着网站规模和资源增加，HTTP/1.1的性能问题也逐渐暴露出来。随着网站越来越大，HTTP1.1的性能问题变得越来越突出。Persistent Connections解决了连接频繁开关的问题，Pipelining提高了性能，Chunked Transfer Encoding改善了页面加载速度，而Caching和Conditional Requests优化了缓存和数据传输，但仍然存在一些问题。(HTTP/1.1 introduced several key features that improved performance, but as websites grew in size, limitations of HTTP/1.1's capabilities started to become apparent.) <div>
<p>What has powered the incredible growth of the World Wide Web?</p><p>There are several factors, but HTTP or Hypertext Transfer Protocol has played a fundamental role.</p><p>Once upon a time, the name may have sounded like a perfect choice. After all, the initial goal of HTTP was to transfer hypertext documents. These are documents that contain links to other documents.</p><p>However, developers soon realized that HTTP can also help transfer other content types, such as images and videos. Over the years, HTTP has become critical to the existence and growth of the web.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7880fc22-160a-4d20-9e74-00f9ded06681_1600x938.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="854" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7880fc22-160a-4d20-9e74-00f9ded06681_1600x938.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>In today&#8217;s deep dive, we&#8217;ll unravel the evolution of HTTP, from its humble beginnings with HTTP1 to the latest advancements of HTTP2 and HTTP3. We&#8217;ll look at how each version addressed the limitations of its predecessor, improving performance, security, and user experience.</p><p>By the end of this article, you&#8217;ll have a solid understanding of the key differences between HTTP1, HTTP2, and HTTP3, helping you make informed decisions when designing web applications.</p><h2>HTTP1 - The Foundation</h2><p>HTTP/1 was introduced in 1996. Before that, there was HTTP/0.9, a simple protocol that only supported the GET method and had no headers. Only HTML files were included in HTTP responses. There were no HTTP headers and no HTTP status codes.</p><p>HTTP/1.0 added headers, status codes, and additional methods such as POST and HEAD. However, HTTP/1 still had limitations. For example, each request-response pair needed a new TCP connection&nbsp;</p><p>In 1997, HTTP/1.1 was released to address the limitations of HTTP/1. Generally speaking, HTTP/1.1 is the definitive version of HTTP1. This version powered the growth of the World Wide Web and is still used heavily despite being over 25 years old.</p><p>What contributed to its incredible longevity?</p><p>There were a few important features that made it so successful.</p><h3>1 - Persistent Connections&nbsp;</h3><p>As mentioned, HTTP started as a single request-response protocol.&nbsp;</p><p>A client opens a connection to the server, makes a request, and gets the response. The connection is then closed. If there&#8217;s a second request, the cycle repeats. The same cycle repeats for subsequent requests.</p><p>It&#8217;s like a busy restaurant where a single waiter handles all orders. For each customer, the waiter takes the order, goes to the kitchen, prepares the food, and then delivers it to the customer&#8217;s table. Only then does the waiter move on to the next customer.</p><p>As the web became more media-oriented, closing the connection constantly after every response proved wasteful. If a web page contains multiple resources that have to be fetched, you would have to open and close the connection multiple times.</p><p>Since HTTP/1 was built on top of TCP (Transmission Control Protocol), every new connection meant going through the 3-way handshake process.</p><p>HTTP/1.1 got rid of this extra overhead by supporting persistent connections. It assumed that a TCP connection must be kept open unless directly told to close. This meant:</p><ul><li><p>No closing of the connection after every request</p></li><li><p>No multiple TCP handshakes.</p></li></ul><p>The diagram below shows the difference between multiple connections and persistent connections.</p><h3></h3><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b23944-a2ab-432e-aad0-0ecfb9ae5144_1600x1018.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="926" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b23944-a2ab-432e-aad0-0ecfb9ae5144_1600x1018.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>2 - Pipelining</h3><p>HTTP/1.1 also introduced the concept of pipelining.&nbsp;</p><p>The idea was to allow clients to send multiple requests over a single TCP connection without waiting for corresponding responses. For example, when the browser sees that it needs two images to render a web page, it can request them one after the other.</p><p>The below diagram explains the concept of pipelining in more detail.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97c703ce-13e1-4afc-8242-de06e90491cb_1600x1018.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="926" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97c703ce-13e1-4afc-8242-de06e90491cb_1600x1018.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>Pipelining further improved performance by reducing the latency for each response before sending the next request. However, pipelining had some limitations around head-of-line blocking that we will discuss shortly.</p><h3>3 - Chunked Transfer Encoding</h3><p>HTTP/1.1 introduced chunked transfer encoding that allowed servers to send responses in smaller chunks rather than waiting for the entire response to be generated.&nbsp;</p><p>This enabled faster initial page rendering and improved the user experience, particularly for large or dynamically generated content.</p><h3>4 - Caching and Conditional Requests</h3><p>HTTP/1.1 introduced sophisticated caching mechanisms and conditional requests.</p><p>It added headers like Cache-Control and ETag, which allowed clients and servers to better manage cached content and reduce unnecessary data transfers.&nbsp;</p><p>Conditional requests, using headers like If-Modified-Since and If-None-Match, enabled clients to request resources only if they had been modified since a previous request, saving bandwidth and improving performance.</p><h2>The Problem with HTTP/1.1</h2><p>There&#8217;s no doubt that HTTP/1.1 was game-changing and enabled the amazing growth trajectory of the web over the last 20+ years.</p><p>However, the web has also evolved considerably since the time HTTP/1.1 was launched.</p><p>Websites have grown in size, with more resources to download and more data to be transferred over the network. According to the HTTP Archive, the average website these days requests around 80 to 90 resources and downloads nearly 2 MB of data.</p><p>The next graph shows the steady growth of website size over the last 10+ years.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bda88ee-86d9-4798-aa45-40d10b1fa4b2_1600x1032.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="939" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bda88ee-86d9-4798-aa45-40d10b1fa4b2_1600x1032.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption"><strong>Source: <a href="https://httparchive.org/reports/state-of-the-web">HTTP Archive State of the Web</a></strong></figcaption></figure></div><p>This growth exposed a fundamental performance problem with HTTP/1.1. </p><p>The diagram below explains the problem visually.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46611727-f8b9-4b47-b1d1-92b6458fd746_1600x1195.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1087" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46611727-f8b9-4b47-b1d1-92b6458fd746_1600x1195.png" title="" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/http1-vs-http2-vs-http3-a-deep-dive">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 09 May 2024 15:30:46 GMT</pubDate>
</item>
<item>
<title>100X Scaling: How Figma Scaled its Databases</title>
<link>https://blog.bytebytego.com/p/100x-scaling-how-figma-scaled-its</link>
<guid>https://blog.bytebytego.com/p/100x-scaling-how-figma-scaled-its</guid>
<content:encoded><![CDATA[
<div> 数据库扩展，垂直分区，水平分片，Figma，DBProxy
<br />
<br />
总结: Figma是一个快速增长的设计平台，面临着数据库规模的迅速增长，通过垂直分区和水平分片的方式来解决数据库扩展的挑战。他们首先进行垂直分区，将高流量的相关表移动到单独的数据库中，然后实施水平分片来将大表跨多个物理数据库分割，通过引入新服务DBProxy来解决路由和查询执行问题。通过这些创新的解决方案，Figma达到了逐步实现数据库横向分片的目标，确保了服务的稳定性和可扩展性。 <div>
<h2><a href="https://bit.ly/QAWolf_050724">&#128536; Kiss bugs goodbye with fully automated end-to-end test coverage (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/QAWolf_050724" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="752" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94f074d9-31a4-450a-9b8b-84ea8556f86d_1200x752.png" width="1200" /><div></div></div></a></figure></div><p>Bugs sneak out when less than 80% of user flows are tested before shipping. But getting that kind of coverage &#8212; and staying there &#8212; is hard and pricey for any sized team.</p><p><a href="https://bit.ly/QAWolf_050724">QA Wolf</a> takes testing off your plate:</p><p>&#8594; Get to 80% test coverage in just 4 months.</p><p>&#8594; Stay bug-free with <a href="https://bit.ly/QAWolf_050724">24-hour maintenance</a> and on-demand test creation.</p><p>&#8594; Get <a href="https://bit.ly/QAWolf_050724">unlimited parallel test runs</a></p><p>&#8594; Zero Flakes guaranteed</p><p>QA Wolf has generated <a href="https://bit.ly/QAWolf_050724">amazing results</a> for companies like Salesloft, AutoTrader, Mailchimp, and Bubble.</p><p>&#127775; Rated 4.5/5 on G2</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/QAWolf_050724"><span>Learn more about their 90-day pilot</span></a></p><div><hr /></div><p>Figma, a collaborative design platform, has been on a wild growth ride for the last few years. Its user base has grown by almost 200% since 2018, attracting around 3 million monthly users.</p><p>As more and more users have hopped on board, the infrastructure team found themselves in a spot. They needed a quick way to scale their databases to keep up with the increasing demand.</p><p>The database stack is like the backbone of Figma. It stores and manages all the important metadata, like permissions, file info, and comments. And it ended up growing a whopping 100x since 2020!&nbsp;</p><p>That's a good problem to have, but it also meant the team had to get creative.</p><p>In this article, we'll dive into Figma's database scaling journey. We'll explore the challenges they faced, the decisions they made, and the innovative solutions they came up with. By the end, you'll better understand what it takes to scale databases for a rapidly growing company like Figma.</p><h2>The Initial State of Figma&#8217;s Database Stack</h2><p>In 2020, Figma still used a single, large Amazon RDS database to persist most of the metadata. While it handled things quite well, one machine had its limits.&nbsp;</p><p>During peak traffic, the CPU utilization was above 65% resulting in unpredictable database latencies.</p><p>While complete saturation was far away, the infrastructure team at Figma wanted to proactively identify and fix any scalability issues. They started with a few tactical fixes such as:</p><ul><li><p>Upgrade the database to the largest instance available (from r5.12xlarge to r5.24xlarge).</p></li><li><p>Create multiple read replicas to scale read traffic.</p></li><li><p>Establish new databases for new use cases to limit the growth of the original database.</p></li><li><p>Add PgBouncer as a connection pooler to limit the impact of a growing number of connections.</p></li></ul><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa398da3d-1c1e-46b9-9cff-942a7b6ff403_1600x990.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="901" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa398da3d-1c1e-46b9-9cff-942a7b6ff403_1600x990.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>These fixes gave them an additional year of runway but there were still limitations:</p><ul><li><p>Based on the database traffic, they learned that write operations contributed a major portion of the overall utilization.&nbsp;</p></li><li><p>All read operations could not be moved to replicas because certain use cases were sensitive to the impact of replication lag.</p></li></ul><p>It was clear that they needed a longer-term solution.</p><h2>The First Step: Vertical Partitioning</h2><p>When Figma's infrastructure team realized they needed to scale their databases, they couldn't just shut everything down and start from scratch. They needed a solution to keep Figma running smoothly while they worked on the problem.&nbsp;</p><p>That's where vertical partitioning came in.</p><p>Think of vertical partitioning as reorganizing your wardrobe. Instead of having one big pile of mess, you split things into separate sections. In database terms, it means moving certain tables to separate databases.</p><p>For Figma, vertical partitioning was a lifesaver. It allowed them to move high-traffic, related tables like those for &#8220;Figma Files&#8221; and &#8220;Organizations&#8221; into their separate databases. This provided some much-needed breathing room.</p><p>To identify the tables for partitioning, Figma considered two factors:</p><ol><li><p><strong>Impact: Moving the tables should move a significant portion of the workload.</strong></p></li><li><p><strong>Isolation: The tables should not be strongly connected to other tables.</strong></p></li></ol><p>For measuring impact, they looked at average active sessions (AAS) for queries. This stat describes the average number of active threads dedicated to a given query at a certain point in time.</p><p>Measuring isolation was a little more tricky. They used runtime validators that hooked into ActiveRecord, their Ruby ORM. The validators sent production query and transaction information to Snowflake for analysis, helping them identify tables that were ideal for partitioning based on query patterns and table relationships.</p><p>Once the tables were identified, Figma needed to migrate them between databases without downtime. They set the following goals for their migration solution:</p><ul><li><p>Limit potential availability impact to less than 1 minute.</p></li><li><p>Automate the procedure so it is easily repeatable.</p></li><li><p>Have the ability to undo a recent partition.</p></li></ul><p>Since they couldn&#8217;t find a pre-built solution that could meet these requirements, Figma built an in-house solution. At a high level, it worked as follows:</p><ul><li><p>Prepared client applications to query from multiple database partitions.</p></li><li><p>Replicated tables from the original database to a new database until the replication lag was near 0.</p></li><li><p>Paused activity on the original database.</p></li><li><p>Waited for databases to synchronize.</p></li><li><p>Rerouted query traffic to the new database.</p></li><li><p>Resumed activity.</p></li></ul><p>To make the migration to partitioned databases smoother, they created separate PgBouncer services to split the traffic virtually. Security groups were implemented to ensure that only PgBouncers could directly access the database.</p><p>Partitioning the PgBouncer layer first gave some cushion to the clients to route the queries incorrectly since all PgBouncer instances initially had the same target database. During this time, the team could also detect the routing mismatches and make the necessary corrections.</p><p>The below diagram shows this process of migration.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F227d8304-f560-4987-95de-5afcc7b02a86_1244x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F227d8304-f560-4987-95de-5afcc7b02a86_1244x1600.png" width="1244" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="741" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b106e37-e264-4af2-98ce-26cd979aa36f_801x741.png" title="" width="801" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li><li><p><a href="https://blog.bytebytego.com/p/what-happens-when-a-sql-is-executed">What Happens When a SQL is Executed?</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-api-versioning">A Crash Course in API Versioning Strategies</a></p></li><li><p><a href="https://blog.bytebytego.com/p/embracing-chaos-to-improve-system">Embracing Chaos to Improve System Resilience: Chaos Engineering</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-cicd">A Crash Course in CI/CD</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>Implementing Replication</h2><p>Data replication is a great way to scale the read operations for your database. When it came to replicating data for vertical partitioning, Figma had two options in Postgres: streaming replication or logical replication.</p><p>They chose logical replication for 3 main reasons:</p><ul><li><p>Logical replication allowed them to port over a subset of tables so that they could start with a much smaller storage footprint in the destination database.</p></li><li><p>It enabled them to replicate data into a database running a different Postgres major version.</p></li><li><p>Lastly, it allowed them to set up reverse replication to roll back the operation if needed.</p></li></ul><p>However, logical replication was slow. The initial data copy could take days or even weeks to complete.&nbsp;</p><p>Figma desperately wanted to avoid this lengthy process, not only to minimize the window for replication failure but also to reduce the cost of restarting if something went wrong.</p><p>But what made the process so slow?</p><p>The culprit was how Postgres maintains indexes in the destination database. While the replication process copies rows in bulk, it also updates the indexes one row at a time. By removing indexes in the destination database and rebuilding them after the data copy, Figma reduced the copy time to a matter of hours.</p><h2>Need for Horizontal Scaling</h2><p>As Figma's user base and feature set grew, so did the demands on their databases.&nbsp;</p><p>Despite their best efforts, vertical partitioning had limitations, especially for Figma&#8217;s largest tables. Some tables contained several terabytes of data and billions of rows, making them too large for a single database.</p><p>A couple of problems were especially prominent:</p><ul><li><p><strong>Postgres Vacuum Issue: Vacuuming is an essential background process in Postgres that reclaims storage occupied by deleted or obsolete rows. Without regular vacuuming, the database would eventually run out of transaction IDs and grind to a halt. However, vacuuming large tables can be resource-intensive and cause performance issues and downtime.</strong></p></li><li><p><strong>Max IO Operations Per Second: Figma&#8217;s highest write tables were growing so quickly that they would soon exceed the max IOPS limit of Amazon&#8217;s RDS.</strong></p></li></ul><p>For a better perspective, imagine a library with a rapidly growing collection of books. Initially, the library might cope by adding more shelves (vertical partitioning). But eventually, the building itself will run out of space. No matter how efficiently you arrange the shelves, you can&#8217;t fit an infinite number of books in a single building. That&#8217;s when you need to start thinking about opening branch libraries.</p><p>This is the approach of horizontal sharding.</p><p>For Figma, horizontal sharding was a way to split large tables across multiple physical databases, allowing them to scale beyond the limits of a single machine.&nbsp;</p><p>The below diagram shows this approach:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0570f4-0d65-49bd-9cb6-e581e08f4269_1244x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0570f4-0d65-49bd-9cb6-e581e08f4269_1244x1600.png" width="1244" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>However, horizontal sharding is a complex process that comes with its own set of challenges:</p><ul><li><p>Some SQL queries become inefficient to support.</p></li><li><p>Application code must be updated to route queries efficiently to the correct shard.</p></li><li><p>Schema changes must be coordinated across all shards.&nbsp;</p></li><li><p>Postgres can no longer enforce foreign keys and globally unique indexes.</p></li><li><p>Transactions span multiple shards, which means Postgres cannot be used to enforce transactionality.</p></li></ul><h2>Exploring Alternative Solutions</h2><p>The engineering team at Figma evaluated alternative SQL options such as CockroachDB, TiDB, Spanner, and Vitess as well as NoSQL databases.</p><p>Eventually, however, they decided to build a horizontally sharded solution on top of their existing vertically partitioned RDS Postgres infrastructure.&nbsp;</p><p>There were multiple reasons for taking this decision:</p><ul><li><p>They could leverage their existing expertise with RDS Postgres, which they had been running reliably for years.</p></li><li><p>They could tailor the solution to Figma&#8217;s specific needs, rather than adapting their application to fit a generic sharding solution.</p></li><li><p>In case of any issues, they could easily roll back to their unsharded Postgres databases.</p></li><li><p>They did not need to change their complex relational data model built on top of Postgres architecture to a new approach like NoSQL. This allowed the teams to continue building new features.</p></li></ul><h2>Figma&#8217;s Unique Sharding Implementation</h2><p>Figma&#8217;s approach to horizontal sharding was tailored to their specific needs as well as the existing architecture. They made some unusual design choices that set their implementation apart from other common solutions.</p><p>Let&#8217;s look at the key components of Figma&#8217;s sharding approach:</p><h3>Colos (Colocations) for Grouping Related Tables</h3><p>Figma introduced the concept of &#8220;colos&#8221; or colocations, which are a group of related tables that share the same sharding key and physical sharding layout.&nbsp;</p><p>To create the colos, they selected a handful of sharding keys like UserId, FileId, or OrgID. Almost every table at Figma could be sharded using one of these keys.</p><p>This provides a friendly abstraction for developers to interact with horizontally sharded tables.&nbsp;</p><p>Tables within a colo support cross-table joins and full transactions when restricted to a single sharding key. Most application code already interacted with the database in a similar way, which minimized the work required by applications to make a table ready for horizontal sharding.</p><p>The below diagram shows the concept of colos:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F991f6f8e-c62a-4071-bf54-7bfdac7e9e95_1600x1087.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="989" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F991f6f8e-c62a-4071-bf54-7bfdac7e9e95_1600x1087.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h3>Logical Sharding vs Physical Sharding</h3><p>Figma separated the concept of &#8220;logical sharding&#8221; at the application layer from &#8220;physical sharding&#8221; at the Postgres layer.</p><p>Logical sharding involves creating multiple views per table, each corresponding to a subset of data in a given shard. All reads and writes to the table are sent through these views, making the table appear horizontally sharded even though the data is physically located on a single database host.</p><p>This separation allowed Figma to decouple the two parts of their migration and implement them independently. They could perform a safer and lower-risk logical sharding rollout before executing the riskier distributed physical sharding.&nbsp;</p><p>Rolling back logical sharding was a simple configuration change, whereas rolling back physical shard operations would require more complex coordination to ensure data consistency.</p><h3>DBProxy Query Engine for Routing and Query Execution</h3><p>To support horizontal sharding, the Figma engineering team built a new service named DBProxy that sits between the application and connection pooling layers such as the PGBouncer.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b03c9da-f712-44b7-aaa2-386f433ee45f_1600x1188.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1081" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b03c9da-f712-44b7-aaa2-386f433ee45f_1600x1188.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>DBProxy includes a lightweight query engine capable of parsing and executing horizontally sharded queries. It consists of three main components:</p><ol><li><p>A query parser that reads the SQL sent by the application and transforms it into an Abstract Syntax Tree (AST).</p></li><li><p>A logical planner that parses the AST, extracts the query type (insert, update, etc.), and logical shard IDs from the query plan.</p></li><li><p>A physical planner that maps the query from logical shard IDs to physical databases and rewrites queries to execute on the appropriate physical shard.</p></li></ol><p>The below diagram shows the practical use of these three components within the query processing workflow.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76c90f1-4149-48bd-9615-faa6fd8d0a16_1600x982.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="894" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76c90f1-4149-48bd-9615-faa6fd8d0a16_1600x982.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>There are always trade-offs when it comes to queries in a horizontally sharded world. Queries for a single shard key are relatively easy to implement. The query engine just needs to extract the shard key and route the query to the appropriate physical database.</p><p>However, if the query does not contain a sharding key, the query engine has to perform a more complex &#8220;scatter-gather&#8221; operation. This operation is similar to a hide-and-seek game where you send the query to every shard (scatter), and then piece together answers from each shard (gather).&nbsp;</p><p>The below diagram shows how single-shard queries work when compared to scatter-gather queries.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b900cff-db00-4b9b-84c4-1bc3abb2d8fd_1373x1600.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="1600" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b900cff-db00-4b9b-84c4-1bc3abb2d8fd_1373x1600.png" width="1373" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>As you can see, this increases the load on the database, and having too many scatter-gather queries can hurt horizontal scalability.</p><p>To manage things better, DBProxy handles load-shedding, transaction support, database topology management, and improved observability.</p><h3>Shadow Application Readiness Framework</h3><p>Figma added a &#8220;shadow application readiness&#8221; framework capable of predicting how live production traffic would behave under different potential sharding keys.&nbsp;</p><p>This framework helped them keep the DBProxy simple while reducing the work required for the application developers in rewriting unsupported queries.&nbsp;</p><p>All the queries and associated plans are logged to a Snowflake database, where they can run offline analysis. Based on the data collected, they were able to pick a query language that supported the most common 90% of queries while avoiding the worst-case complexity in the query engine.</p><h2>Conclusion</h2><p>Figma&#8217;s infrastructure team shipped their first horizontally sharded table in September 2023, marking a significant milestone in their database scaling journey.</p><p>It was a successful implementation with minimal impact on availability. Also, the team observed no regressions in latency or availability after the sharding operation.</p><p>Figma&#8217;s ultimate goal is to horizontally shard every table in their database and achieve near-infinite scalability. They have identified several challenges that need to be solved such as:</p><ul><li><p>Supporting horizontally sharded schema updates</p></li><li><p>Generating globally unique IDs for horizontally sharded primary keys</p></li><li><p>Implementing atomic cross-shard transactions for business-critical use cases.</p></li><li><p>Enabling distributed globally unique indexes.</p></li><li><p>Developing an ORM model to improve developer velocity</p></li><li><p>Automatic reshard operations to enable shard splits at the click of a button.</p></li></ul><p>Lastly, after achieving a sufficient runway, they also plan to reassess their current approach of using in-house RDS horizontal sharding versus switching to an open-source or managed alternative in the future.</p><p><strong>References:</strong></p><ul><li><p><a href="https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/">How Figma&#8217;s databases team lived to tell the scale</a></p></li><li><p><a href="https://www.figma.com/blog/how-figma-scaled-to-multiple-databases/">The growing pains of database architecture</a></p></li><li><p><a href="https://www.pgbouncer.org/features.html">Features of PgBouncer</a></p></li><li><p><a href="https://zipdo.co/statistics/figma-user/">Figma User Statistics</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a>.</strong></p>
]]></content:encoded>
<pubDate>Tue, 07 May 2024 15:30:45 GMT</pubDate>
</item>
<item>
<title>EP110: Top 5 Strategies to Reduce Latency</title>
<link>https://blog.bytebytego.com/p/ep110-top-5-strategies-to-reduce</link>
<guid>https://blog.bytebytego.com/p/ep110-top-5-strategies-to-reduce</guid>
<content:encoded><![CDATA[
<div> Latency, Load Balancer, Data Sharding Algorithms, C++ Use Cases, Apache Kafka

<br />
Latency对于高规模用户系统来说是一项重要影响收入的因素，而减少延迟的策略包括数据库索引、缓存、负载均衡、内容交付网络、异步处理、数据压缩等。负载均衡的实际用例包括故障处理、实例健康检查、SSL终止、跨区负载均衡和用户粘性，能提升系统可靠性。数据分片算法有基于范围、基于哈希、一致性哈希和虚拟桶分片方法。C++的用途广泛，包括嵌入式系统、游戏开发、操作系统、数据库、网络、科学计算等。Apache Kafka是分布式事件流平台，包含broker、topic、partition、producer、consumer和consumer group等重要组件，并可用作消息传递和日志聚合。 

<br /><br />总结: 
- 减少延迟的策略包括数据库索引、缓存、负载均衡、内容交付网络、异步处理、数据压缩
- 负载均衡的实际用例有故障处理、实例健康检查、SSL终止、跨区负载均衡、用户粘性
- 常用数据分片算法有基于范围、基于哈希、一致性哈希、虚拟桶分片方法
- C++适用于嵌入式系统、游戏开发、操作系统、数据库、网络、科学计算等用例
- Apache Kafka作为分布式事件流平台，包含多个关键组件和用途 <div>
<p>This week&#8217;s system design refresher:</p><ul><li><p>Top 5 Strategies to Reduce Latency</p></li><li><p>Load Balancer Realistic Use Cases You May Not Know</p></li><li><p>Top 4 data sharding algorithms explained</p></li><li><p>Top 8 C++ Use Cases</p></li><li><p>Apache Kafka in 100 Seconds</p></li><li><p>SPONSOR US</p></li></ul><div><hr /></div><h2><a href="https://bit.ly/NewRelic_050324">New Relic AI monitoring, the industry&#8217;s first APM for AI, now generally available (Sponsore</a>d)</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://bit.ly/NewRelic_050324" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="630" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6192fcd8-53fc-4461-9e2c-8a73050c1d7c_1200x630.png" width="1200" /><div></div></div></a></figure></div><p>New Relic AI monitoring provides unprecedented visibility and insights to engineers and developers who are modernizing their tech stacks. With New Relic AI monitoring, engineering teams can monitor, alert, debug, and root-cause AI-powered applications.</p><p class="button-wrapper"><a class="button primary" href="https://bit.ly/NewRelic_050324"><span>Get started</span></a></p><div><hr /></div><h2>Top 5 Strategies to Reduce Latency</h2><p>10 years ago, Amazon found that every 100ms of latency cost them 1% in sales. <br /> <br />That&#8217;s a staggering $5.7 billion in today&#8217;s terms. <br /> <br />For high-scale user-facing systems, high latency is a big loss of revenue. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F801df2c4-056a-400c-91cc-8dc09fced890_1280x1664.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><p>Here are the top strategies to reduce latency: </p><ol><li><p>Database Indexing </p></li><li><p>Caching </p></li><li><p>Load Balancing </p></li><li><p>Content Delivery Network </p></li><li><p>Async Processing </p></li><li><p>Data Compression </p></li></ol><p>Over to you: What other strategies to reduce latency have you seen? </p><div><hr /></div><h2>Load Balancer Realistic Use Cases You May Not Know</h2><p>Load balancers are inherently dynamic and adaptable, designed to efficiently address multiple purposes and use cases in network traffic and server workload management. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="No alternative text description for this image" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d1ff6df-980d-43e9-aba9-7498a2ce3237_1280x1664.gif" title="No alternative text description for this image" width="1280" /><div></div></div></a></figure></div><p>Let's explore some of the use cases: </p><ol><li><p>Failure Handling: <br />Automatically redirects traffic away from malfunctioning elements to maintain continuous service and reduce service interruptions. </p></li><li><p>Instance Health Checks: <br />Continuously evaluates the functionality of instances, directing incoming requests exclusively to those that are fully operational and efficient. </p></li><li><p>Platform Specific Routing: <br />Routes requests from different device types (like mobiles, desktops) to specialized backend systems, providing customized responses based on platform. </p></li><li><p>SSL Termination: <br />Handles the encryption and decryption of SSL traffic, reducing the processing burden on backend infrastructure. </p></li><li><p>Cross Zone Load Balancing: <br />Distributes incoming traffic across various geographic or network zones, increasing the system's resilience and capacity for handling large volumes of requests. </p></li><li><p>User Stickiness: <br />Maintains user session integrity and tailored user interactions by consistently directing requests from specific users to designated backend servers. </p></li></ol><p>Over to you: <br />Which of these use cases would you consider adding to your network to enhance system reliability and why?</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="741" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b106e37-e264-4af2-98ce-26cd979aa36f_801x741.png" width="801" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">Unlocking the Power of SQL Queries for Improved Performance</a></p></li><li><p><a href="https://blog.bytebytego.com/p/what-happens-when-a-sql-is-executed">What Happens When a SQL is Executed?</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-api-versioning">A Crash Course in API Versioning Strategies</a></p></li><li><p><a href="https://blog.bytebytego.com/p/embracing-chaos-to-improve-system">Embracing Chaos to Improve System Resilience: Chaos Engineering</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-cicd">A Crash Course in CI/CD</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>Top 4 Data Sharding Algorithms Explained</h2><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f404d8a-413b-4874-a9f7-90763e635ed9_1280x1664.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><p>We are dealing with massive amounts of data. Often we need to split data into smaller, more manageable pieces, or &#8220;shards&#8221;. Here are some of the top data sharding algorithms commonly used: </p><ul><li><p>Range-Based Sharding <br />This involves partitioning data based on a range of values. For example, customer data can be sharded based on alphabetical order of last names, or transaction data can be sharded based on date ranges. </p></li><li><p>Hash-Based Sharding <br />In this method, a hash function is applied to a shard key chosen from the data (like a customer ID or transaction ID). <br />This tends to distribute data more evenly across shards compared to range-based sharding. However, we need to choose a proper hash function to avoid hash collision. </p></li><li><p>Consistent Hashing <br />This is an extension of hash-based sharding that reduces the impact of adding or removing shards. It distributes data more evenly and minimizes the amount of data that needs to be relocated when shards are added or removed. </p></li><li><p>Virtual Bucket Sharding <br />Data is mapped into virtual buckets, and these buckets are then mapped to physical shards. This two-level mapping allows for more flexible shard management and rebalancing without significant data movement.</p></li></ul><div><hr /></div><h2>Top 8 C++ Use Cases</h2><p>C++ is a highly versatile programming language that is suitable for a wide range of applications. </p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="graphical user interface, application" class="sizing-normal" height="1664" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9dbf2f6-84ed-4533-a681-1ef110eb4084_1280x1664.gif" title="graphical user interface, application" width="1280" /><div></div></div></a></figure></div><ul><li><p>Embedded Systems <br />The language's efficiency and fine control over hardware resources make it excellent for embedded systems development. </p></li><li><p>Game Development <br />C++ is a staple in the game development industry due to its performance and efficiency. </p></li><li><p>Operating Systems <br />C++ provides extensive control over system resources and memory, making it ideal for developing operating systems and low-level system utilities. </p></li><li><p>Databases <br />Many high-performance database systems are implemented in C++ to manage memory efficiently and ensure fast execution of queries. </p></li><li><p>Financial Applications </p></li><li><p>Web Browsers <br />C++ is used in the development of web browsers and their components, such as rendering engines. </p></li><li><p>Networking <br />C++ is often used for developing network devices and simulation tools. </p></li><li><p>Scientific Computing <br />C++ finds extensive use in scientific computing and engineering applications that require high performance and precise control over computational resources. </p></li></ul><p>Over to you - What did we miss? </p><div><hr /></div><h2>Apache Kafka in 100 Seconds</h2><p>This post is written by guest author <a href="https://www.linkedin.com/in/sanazzakeri/">Sanaz Zakeri</a>, who is a Senior Software Engineer @Uber.</p><p>Apache Kafka is a distributed event streaming platform used for building real-time data processing pipelines and streaming applications. It is highly scalable, fault-tolerant, reliable, and can handle large volumes of data.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="Image" class="sizing-normal" height="1066" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22ade5eb-1223-4cdc-a2ed-dccd1e42dc6e_1202x1066.jpeg" title="Image" width="1202" /><div></div></div></a></figure></div><p>In order to understand Kafka, we need to define two terms:</p><ul><li><p>Events: a log of state of something at a specific point in time</p></li><li><p>Event streams: continuous and unbounded series of events</p></li></ul><p>Kafka can be used as a Messaging in a publish-subscribe model, where producers write event streams, and consumers read the events. This publish-subscribe model enables decoupling of event stream producers and consumers. Also, Kafka can be used as a log aggregation platform, ingesting and storing logs from multiple sources in a durable and fault-tolerant way.</p><ul><li><p><strong>Kafka Components: </strong></p><p>Kafka cluster has multiple key components to provide the distributed infrastructure and reliably capture, store, order and provide event streams to client applications.</p></li><li><p><strong>Brokers: </strong></p><p>At the heart of the Kafka cluster lies the brokers which are physical servers that handle event streams. After events are published by producers, the broker makes the events available to consumers. Brokers bring scalability to Kafka as Kafka clusters can span multiple brokers across a variety of infrastructure setup to handle large volumes of events. They also bring fault tolerance since events can be stored and replicated across multiple brokers.</p></li><li><p><strong>Topics: </strong></p><p>Topic is the subject name of where the events are published by producers. Topics can have zero or more consumers listening to them and processing the events.</p></li><li><p><strong>Partition: </strong></p><p>In a topic, data is organized into partitions which store ordered streams of events. Each event within a partition is assigned a unique sequential identifier called offset that represents its position in the partition. Events are appended&nbsp; continually to the partition. A Topics can have one or more partitions. Having more than one partition in a topic enables parallelism as more consumers can read from the topic.</p><p>Partitions belonging to a topic can be distributed across separate brokers in the cluster, which brings high data availability and scalability. If one broker fails, the partitions on the remaining brokers can continue to serve data, ensuring fault tolerance.</p></li><li><p><strong>Producers: </strong></p><p>Producers are client applications&nbsp; that write events to Kafka topics as a stream of events.</p></li><li><p><strong>Consumers: </strong></p><p>Consumers are the client applications that subscribe to topics and process or store the events coming to the specific topic. Consumers read events in the order they were received within each partition.</p><p>Applications which require real time processing of data will have multiple consumers in a consumer group which can read from partitions on the subscribed topic.</p></li><li><p><strong>Consumer Groups: </strong></p><p>Consumer group is used to organize consumers that are reading a stream of events from one or more topics. Consumer groups enable parallel processing of events and each consumer in the consumer group can read from one partition to enable load balancing on the client application. This functionality not only brings the parallel processing but also brings fault tolerance since if a consumer fails in a consumer group, Partition can be reassigned to another group member.&nbsp;</p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a>.</strong></p>
]]></content:encoded>
<pubDate>Sat, 04 May 2024 15:30:41 GMT</pubDate>
</item>
<item>
<title>Unlocking the Power of SQL Queries for Improved Performance</title>
<link>https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries</link>
<guid>https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries</guid>
<content:encoded><![CDATA[
<div> SQL, 数据管理, 数据库, 查询优化, Explain Plan

总结:<br /><br />SQL是现代数据管理的基础，通过优化查询可以提升数据库性能和降低成本。使用Explain Plan工具可以分析查询执行计划，帮助识别潜在问题。在MySQL中，使用EXPLAIN命令可以获取执行查询的详细信息，包括表、操作、索引等。优化查询时，可以根据EXPLAIN的输出结果找到潜在瓶颈，并进行调整。通过实际示例演示了如何使用EXPLAIN命令来分析查询的执行计划。 <div>
<p>SQL, or Structured Query Language, is the backbone of modern data management. It enables efficient retrieval, manipulation, and management of data in a Database Management System (DBMS). Each SQL command taps into a complex sequence within a database, building on concepts like the connection pool, query cache, command parser, optimizer, and executor, which we covered in our last issue.</p><p>Crafting effective queries is essential. The right SQL can enhance database performance; the wrong one can lead to increased costs and slower responses. In this issue, we focus on strategies such as using the Explain Plan, adding proper indexes, and optimizing commands like COUNT(*) and ORDER BY. We also dive into troubleshooting slow queries.</p><p>While MySQL is our primary example, the techniques and strategies discussed are applicable across various database systems. Join us as we refine SQL queries for better performance and cost efficiency.</p><div><hr /></div><h2>Explain Plan</h2><p>In MySQL, the EXPLAIN command, known as EXPLAIN PLAN in systems like Oracle, is a useful tool for analyzing how queries are executed. By adding EXPLAIN before a SELECT statement, MySQL provides information about how it processes the SQL. This output shows the tables involved, operations performed (such as sort, scan, and join), and the indexes used, among other execution details. This tool is particularly useful for optimizing SQL queries, as it helps developers see the query execution plan and identify potential bottlenecks.</p><p>When an EXPLAIN statement is executed in MySQL, the database engine simulates the query execution. This simulation generates a detailed report without running the actual query. This report includes several important columns:</p><ul><li><p>id: Identifier for each step in the query execution.</p></li><li><p>select_type: The type of SELECT operation, like SIMPLE (a basic SELECT without unions or subqueries), SUBQUERY, or UNION.</p></li><li><p>table: The table involved in a particular part of the query.</p></li><li><p>type: The join type shows how MySQL joins the tables. Common types include ALL (full table scan), index (index scan), range (index range scan), eq_ref (unique index scan), const/system (constant value optimization).</p></li><li><p>possible_keys: Potential indexes that might be used.</p></li><li><p>key: The key (index) chosen by MySQL.</p></li><li><p>key_len: The length of the chosen key.</p></li><li><p>ref: Columns or constants used with the key to select rows.</p></li><li><p>rows: Estimated number of rows MySQL expects to examine when executing the query.</p></li><li><p>Extra: Additional details, such as the use of temporary tables or filesorts.</p></li></ul><p>Let's explore a practical application of the EXPLAIN command using a database table named <em>orders</em>. Suppose we want to select orders with user_id equal to 100.</p><pre><code><strong>SELECT</strong> * <strong>FROM</strong> orders <strong>WHERE</strong> user_id = 100;</code></pre><p>To analyze this query with EXPLAIN, we would use:</p><pre><code><strong>EXPLAIN SELECT</strong> * <strong>FROM</strong> orders <strong>WHERE</strong> user_id = 100;</code></pre><p>The output might look like this:&nbsp;</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72d33f7e-6fca-47da-9e4a-04a9e053c2b8_1470x210.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="208" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72d33f7e-6fca-47da-9e4a-04a9e053c2b8_1470x210.png" width="1456" /><div></div></div></a></figure></div>
      <p>
          <a href="https://blog.bytebytego.com/p/unlocking-the-power-of-sql-queries">
              Read more
          </a>
      </p>
]]></content:encoded>
<pubDate>Thu, 02 May 2024 15:31:02 GMT</pubDate>
</item>
<item>
<title>How to Execute End-to-End Tests at Scale</title>
<link>https://blog.bytebytego.com/p/how-to-execute-end-to-end-tests-at</link>
<guid>https://blog.bytebytego.com/p/how-to-execute-end-to-end-tests-at</guid>
<content:encoded><![CDATA[
<div> GCP, Argo CD, Kubernetes, Playwright, QA Wolf
总结:<br /><br />文章介绍了QA Wolf公司的专门基础设施，能够在短短几分钟内并发运行数千个端到端测试，满足客户的期望。他们的技术栈包括Google Cloud Platform(GCP)、Argo CD、Kubernetes、Playwright等工具。利用GitOps方法构建基础设施，采用Pulumi作为IaC工具，使用Typescript编写测试，通过运行数据文件执行测试。他们的执行流程由Run Director服务调度，运行Shard集群提供高可用性和更快测试，同时报告测试结果。该基础设施建立在全云原生架构上，支持快速迭代，为客户快速交付提供价值。 <div>
<p></p><p>Running E2E tests reliably and efficiently is a critical piece of the puzzle for any software organization.&nbsp;</p><p>There are mainly two expectations software teams have when it comes to testing:</p><ul><li><p>Ship as fast as possible without introducing (or reintroducing) bugs</p></li><li><p>Run tests as cheaply as possible without compromising on quality.</p></li></ul><p>In today&#8217;s issue, we are fortunate to host guest author John Gluck, Principal Testing Advocate at QA Wolf. He&#8217;ll be sharing insights into QA Wolf&#8217;s specialized infrastructure capable of running thousands of concurrent E2E tests in just a few minutes and meeting the expectations of their customers.</p><p><a href="https://www.qawolf.com/">QA Wolf</a> is a full-service solution for mid-to-large product teams who want to speed up their QA cycles and reduce the cost of building, running, and maintaining comprehensive regression test coverage.&nbsp;&nbsp;</p><p>Also, <a href="https://sched.co/1YeP3">Mufav Onus</a> of QA Wolf spoke at Kubecon 2024 in Paris about how they automatically resume pods on spot instances after unexpected shutdowns. <a href="https://www.youtube.com/watch?v=c2MbSM9-7Xs">Take a look</a>.</p><div><hr /></div><h2>The Challenge of Running E2E Tests</h2><p>Running E2E tests efficiently is challenging for any organization. The runners tend to cause resource spikes, which cause tests and applications to behave unpredictably.&nbsp; That&#8217;s why it&#8217;s fairly common for large product teams to strategically schedule their test runs. As the number of tests and the number of runs increases, the challenges become exponentially more difficult to overcome.&nbsp;&nbsp;</p><p>While the largest companies in the world may run 10,000 end-to-end tests each month, and a handful run 100,000, QA Wolf runs more than 2 million. At our scale, to support the number of customers that we do, our infrastructure has to address three major concerns:&nbsp;</p><ul><li><p><strong>Availability -</strong> Customers can execute their tests at any time with no restrictions on the number or frequency of parallel runs. The system must be highly available. We can&#8217;t use scheduling tricks to solve this.&nbsp;</p></li><li><p><strong>Speed - </strong>Tests need to run fast. DORA recommends 30 minutes as the maximum time for test suite execution, and customers want to follow this principle.</p></li><li><p><strong>Reliability - </strong>&nbsp;Node contention issues, instance hijacking, and test system execution outages (the things in-house test architects deal with on a regular basis) are simply not tolerable when people are paying you to execute their tests.</p></li></ul><p>For better or worse, StackOverflow didn&#8217;t have blueprints for the kind of test-running infrastructure we needed to build. Success came from lots of experimentation and constant refinement.&nbsp;</p><p>In this post, we discuss the problems we faced and the decisions we made so that we could solve them through experimentation.</p><h2>The Tech Stack Breakdown</h2><p></p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5150f81a-b38f-46c8-b7d2-cf27a9e5f84a_2410x896.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="541" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5150f81a-b38f-46c8-b7d2-cf27a9e5f84a_2410x896.png" width="1456" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">The tech stack</figcaption></figure></div><p>To set the stage, we are completely cloud-native and built our infrastructure on the Google Cloud Platform (GCP).&nbsp;</p><p>We went with GCP for its GKE (Kubernetes) implementation and cluster autoscaling capabilities, which are critical for handling the demand for test execution nodes. There are similar tools out there, but our engineers also had previous experience with GCP, which helped us get started.&nbsp;</p><p>We adopted a GitOps approach so we could run lots of configuration experiments on our infrastructure quickly and safely without disrupting ongoing operations.&nbsp;</p><p>Argo CD was a good choice because of its support for GitOps and Kubernetes. A combination of Helm and Argo Workflows helps make the deployment process consistent and organized. We used Argo CD Application Sets and App of Apps patterns which are considered best practices.</p><p>For IaC, we chose Pulumi because it&#8217;s open source, and unlike Terraform, it doesn&#8217;t force developers to adopt another DSL (Domain-Specific Language)</p><p>Lastly, we used Typescript to write the tests. Our customers look at the test code written for them, and Typescript makes it easy to understand. We chose Playwright as the test executor and test framework for multiple reasons, such as:</p><ol><li><p>Playwright can handle the complex tests that customers may need to automate.</p></li><li><p>Simpler APIs and an easier install prevent customers from being locked into our solution.&nbsp;</p></li><li><p>It&#8217;s backed by Microsoft, and more active development is expanding the list of native capabilities.</p></li></ol><h2>The Ecosystem</h2><p>For the infrastructure ecosystem, we went with one VPC and three main application clusters.&nbsp;</p><p>Each of the three clusters has a specific role:</p><ul><li><p>The application cluster</p></li><li><p>The test instance or runner cluster</p></li><li><p>Operations cluster</p></li></ul><p>The operations cluster is the primary cluster and manages the other two clusters. Argo CD runs within this cluster.</p><p>See the diagram below that shows this arrangement.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a34a0b7-6d02-405c-93cd-3341f60efb8a_980x597.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="441.0489795918367" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a34a0b7-6d02-405c-93cd-3341f60efb8a_980x597.png" width="724" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>At the time of startup, the operations cluster creates both the application and runner clusters. It provisions warm nodes on the runner cluster, each containing two pods, and each pod is built on a single container image.&nbsp;</p><p>See the diagram below for reference:</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90386158-64cc-4736-a8b0-3569b3ba7e0d_1600x974.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="435.6978021978022" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F90386158-64cc-4736-a8b0-3569b3ba7e0d_1600x974.png" width="716" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>This structure is fully expendable. Our developers can tear down the entire system and rebuild it from scratch with the touch of a button, which increases predictability for developers and is also great for supporting disaster recovery.&nbsp;</p><p>GKE&#8217;s cluster autoscaler scales the warm nodes on the runner cluster up and down based on demand.&nbsp;</p><div><hr /></div><h2>Latest articles</h2><p>If you&#8217;re not a paid subscriber, here&#8217;s what you missed.</p><div class="captioned-image-container"><figure><a class="image-link image2" href="https://blog.bytebytego.com/subscribe" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="662" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbec68d98-d61b-411c-824c-64205354de37_661x662.png" title="" width="661" /><div></div></div></a></figure></div><ol><li><p><a href="https://blog.bytebytego.com/p/what-happens-when-a-sql-is-executed">What Happens When a SQL is Executed?</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-api-versioning">A Crash Course in API Versioning Strategies</a></p></li><li><p><a href="https://blog.bytebytego.com/p/embracing-chaos-to-improve-system">Embracing Chaos to Improve System Resilience: Chaos Engineering</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-cicd">A Crash Course in CI/CD</a></p></li><li><p><a href="https://blog.bytebytego.com/p/a-crash-course-in-ipv4-addressing">A Crash Course in IPv4 Addressing</a></p></li></ol><p>To receive all the full articles and support ByteByteGo, consider subscribing:</p><p class="button-wrapper"><a class="button primary" href="https://blog.bytebytego.com/subscribe"><span>Subscribe now</span></a></p><div><hr /></div><h2>The Customer-Facing Application</h2><p>The customer-facing application is a specialized IDE where our QA engineers can write, run, and maintain Playwright tests. It has views for managing configuration and third-party integrations with visualization dashboards.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1917074-931e-4567-b53d-d4abc6b6dfe9_1600x785.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="351.11538461538464" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1917074-931e-4567-b53d-d4abc6b6dfe9_1600x785.png" width="716" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">QA Wolf&#8217;s Test Editor</figcaption></figure></div><h2>Writing Tests</h2><p>The tests built and maintained by our in-house QA engineers are autonomous, isolated, idempotent, and atomic, so they can run predictably in a fully parallelized context.</p><p>When a QA engineer saves a test, the application persists the code for the test onto GCS with its corresponding helpers and any associated parsed configuration needed to run it in Playwright. This is the Run Data File for the test. In case you don&#8217;t know, GCS is the GCP equivalent of AWS S3.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27e79e48-de79-49d0-924a-48b903a67644_542x701.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="804.4686346863468" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27e79e48-de79-49d0-924a-48b903a67644_542x701.png" width="622" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><p>In the initial implementation, we tried passing the Run Data File as a payload in HTML, but the payload containing the test code for all tests in a run was too large for Kubernetes etcd. To get around this, we took the path of least resistance by writing all the code to a central file and giving the client a reference to the file location to pass back to the application.</p><h2>The Execution Flow</h2><p>As mentioned earlier, we orchestrate runs with Argo Workflows because it can run on a Kubernetes cluster without external dependencies.</p><p>Customers or QA engineers can use a scheduler in the application or an API call to start a test run. When a test run is triggered, the application gathers the locations of all necessary Run Data Files. It also creates a new database record for each test run, including a unique build number that acts as an identifier for the test run request. The application uses the build number later to associate system logs and video locations.</p><p>Lastly, it passes the Run Data file locations list to the Run Director service.</p><p>The below diagram depicts the entire execution flow at a high level.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a79ebdf-390b-4d61-91e5-ad9337c5c23a_1305x1077.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="590.9057471264368" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a79ebdf-390b-4d61-91e5-ad9337c5c23a_1305x1077.png" width="716" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a></figure></div><h2>The Run Director</h2><p>The Run Director is a simple, long-living, horizontally-scalable HTTP service.&nbsp;</p><p>When invoked, the Run Director reports the initial test run status to the application via a webhook and the build number. For each location in the list, the Run Director invokes an Argo Workflows template and hydrates it with the Run Data file at that location. By performing both actions simultaneously, individual test runs can be started faster so that all the tests in the run can finish more quickly.</p><p>The Argo Workflow then provisions a Kubernetes pod for each test run requested from the available warm nodes. It attaches the code for each test to a volume on a corresponding container on the pod. This approach allows us to use the same container build for every test execution. If there aren&#8217;t enough pods for the run on warm nodes, GKE uses cluster autoscaling to meet demand.&nbsp;</p><p>Each test runs in its own pod and container, which isolates the tests and makes it easier for the developers to troubleshoot them. Running tests like this also confines resource consumption issues to the node where the specific tests are having trouble.</p><p>The test code runs from the container entry point. Argo Workflow drives the provisioning process and starts each container with the help of Kubernetes</p><p>The application runs all the tests in headed browsers. This is important because the container is destroyed after the test finishes, and the headed browser makes it possible to capture videos of tests. The videos are an essential debugging tool to know about what happened at the moment, especially in cases where it&#8217;s difficult to recreate a particular failure.&nbsp;</p><p>Due to the high standard for test authorship and the infrastructure reliability, the primary cause of test failure is when the system-under-test (SUT) is not optimized for testing. It makes sense when you think about it. The slower the SUT, the more the test is required to poll, increasing the demand on the processor running the test. Though we can&#8217;t tell the customer how to build their application to improve test performance, we can isolate each test&#8217;s resource consumption to prevent it from impacting other tests.&nbsp;</p><h2>The Flake Detection</h2><p>We maintain a very high standard of test authorship, which allows us to make certain assumptions.</p><p>Since the tests are expected to pass, we can safely assume that a test failure or error is due to an anomaly, such as a temporarily unavailable SUT. The application schedules such failures for automatic retry. It flags any other failure &#8211; such as a suspected infrastructure problem &#8211; for investigation and doesn&#8217;t retry. Argo Workflows will attempt to re-run a failed test three times.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae5939b-9b9a-468f-8f32-86bc55ddbfc0_1600x1124.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="491.8269230769231" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae5939b-9b9a-468f-8f32-86bc55ddbfc0_1600x1124.png" width="700" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">QA Wolf Flake Detection Process</figcaption></figure></div><p>If the tests pass on retry, the application resumes as usual and assumes the failure was anomalous. In case all retries fail, the system creates a defect report, and a QA engineer investigates to confirm whether the failure is due to a bug in the application or some other issue.</p><h2>The Run Shard Clusters</h2><p>One of the most significant advantages of the Run Director service is the concept of Run Shard Clusters.</p><p>The sharding strategy allows us to spread the various test runs across clusters located worldwide. We have a GCP global VPC with a bunch of different subnets in different regions. This makes it possible to provision sharding clusters in different regions that can be accessed privately via the Run Director service.</p><p>Shard clusters provide several advantages, such as:</p><ul><li><p><strong>Replicated high availability </strong>- If one region goes down, not everything grinds to a halt.</p></li><li><p><strong>Closer-to-home testing</strong> - The ability to run customer tests close to their home region results in a more accurate performance of their applications and systems.</p></li><li><p><strong>Experimentation </strong>-<strong> </strong>We can experiment with different versions of our Argo Workflows implementation or different run engines without cutting overall traffic to the same version. This also allows us to experiment with cost-saving measures such as spot instances.</p></li></ul><h2>Reporting</h2><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17ef5006-498e-41f9-9a2c-d7b3f2000dff_1600x769.jpeg" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="338.46153846153845" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17ef5006-498e-41f9-9a2c-d7b3f2000dff_1600x769.jpeg" width="704" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">The QA Wolf Application Dashboard</figcaption></figure></div><p>Of course, our customers also want to see test results, so we needed to create a reliable system that allowed them to do so.</p><p>Once the test finishes running and retrying (if needed), the Argo Workflow template uploads any run artifacts saved by Playwright back to GCS using the build number. Some of this information will be aggregated and appear on our application&#8217;s dashboard.&nbsp; Other pieces of information from these artifacts are displayed at the test level, such as logs and run history.</p><div class="captioned-image-container"><figure><a class="image-link is-viewable-img image2" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20875576-d0ff-42a0-9704-65465133489e_317x494.png" target="_blank"><div class="image2-inset"><source type="image/webp" /><img alt="" class="sizing-normal" height="687.2365930599369" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20875576-d0ff-42a0-9704-65465133489e_317x494.png" width="441" /><div class="image-link-expand"><svg class="lucide lucide-maximize2 " fill="none" height="20" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="20" xmlns="http://www.w3.org/2000/svg"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></a><figcaption class="image-caption">Run History for a QA Wolf test</figcaption></figure></div><p>On the infrastructure side, the Argo Workflow triggers Kubernetes to shut down the container and detach the volume, ensuring that the system doesn&#8217;t leave unnecessary resources running. This helps keep down operational costs.</p><h2>Conclusion</h2><p>Our unique approach was developed to meet customer needs for speed, availability, and reliability. We are one of the few companies running e2e tests at this scale, so we needed to discover how to create a system to support that through trial and error; therefore, we designed our system to also support fast iteration.&nbsp; Our cost-efficient, full parallel test execution is the backbone of our application and we see it delivering value for our customers on a daily basis.</p><div><hr /></div><p>If you&#8217;d like to learn more about QA Wolf&#8217;s test run infrastructure or how it can help you ship faster with fewer escapes, visit their website to <a href="http://www.qawolf.com/?utm_source=bytebytego&amp;utm_medium=newsletter&amp;utm_campaign=bbgnewsletter">schedule a demo</a>.&nbsp;</p><p>Related Links</p><ul><li><p><a href="https://www.qawolf.com/blog/flaky-coverage-is-fake-coverage">Flaky coverage is fake coverage</a></p></li><li><p><a href="https://www.qawolf.com/blog/the-challenges-and-rewards-of-full-test-parallelization">Parallel testing: what it is, why it's hard, and why you should do it</a></p></li><li><p><a href="https://www.qawolf.com/blog/principles-for-writing-parallizable-tests">Three ways to improve your parallelized tests</a></p></li></ul><div><hr /></div><h2><strong>SPONSOR US</strong></h2><p>Get your product in front of more than 500,000 tech professionals.</p><p>Our newsletter puts your products and services directly in front of an audience that matters - hundreds of thousands of engineering leaders and senior engineers - who have influence over significant tech decisions and big purchases.</p><p>Space Fills Up Fast - Reserve Today</p><p>Ad spots typically sell out about 4 weeks in advance. To ensure your ad reaches this influential audience, reserve your space now by emailing <strong><a href="mailto:hi@bytebytego.com">hi@bytebytego.com</a>.</strong></p>
]]></content:encoded>
<pubDate>Tue, 30 Apr 2024 16:24:12 GMT</pubDate>
</item>
</channel>
</rss>