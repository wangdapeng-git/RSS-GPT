<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>LessWrong</title>
<link>https://www.lesswrong.com</link>


<item>
<title>Truthseeking is the ground in which other principles grow</title>
<link>https://www.lesswrong.com/posts/kbnJHpapusMJZb6Gs/truthseeking-is-the-ground-in-which-other-principles-grow</link>
<guid>https://www.lesswrong.com/posts/kbnJHpapusMJZb6Gs/truthseeking-is-the-ground-in-which-other-principles-grow</guid>
<content:encoded><![CDATA[
<div> facet, truthseeking, actions, examples, reality

要点1：文章讨论了真相追寻的各个方面，强调了与现实保持联系的重要性。
要点2：提出了一些行动建议，如委托他人意见，保持信息的开放共享等。
要点3：举例说明了一些公开写作的例子，并强调了真相追寻的重要性。
要点4：探讨了联系现实的感觉应该是正面的。
要点5：强调了在向人们传递令人不愉快的事实时要保持公正，表达意见的技巧建议。

<br /><br />总结: 这篇文章强调了与现实保持接触的重要性，提出了许多行动建议，并通过丰富的实例说明了相关概念。此外，还强调了在传递不愉快事实时保持平衡和善意的重要性。 <div>
Published on May 27, 2024 1:09 AM GMT<br /><br /><h1>Introduction</h1><p><a href="https://x.com/HiFromMichaelV/status/1161174071469641728"><i><u>First they came for the epistemology/we don’t know what happened after that.</u></i></a></p><p>I’m fairly antagonistic towards the author of that tweet, but it still resonates deep in my soul. Anything I want to do, anything I want to change, rests on having contact with reality. If I don’t have enough, I might as well be pushing buttons at random.&nbsp;</p><p>Unfortunately, there are a lot of forces pushing against having enough contact with reality. It’s a lot of work even when reality cooperates, many situations are adversarial, and even when they’re not entropy itself will constantly chip away at your knowledge base.</p><p>This is why I think constantly seeking contact with reality is the meta principle without which all (consequentialist) principles are meaningless. If you aren’t actively pursuing truthseeking, you won’t have enough contact with reality to make having goals a reasonable concept, much less achieving them. To me this feels intuitive, like saying air is necessary to live. But I’ve talked to many people who disagree, or who agree in the abstract but prioritize differently in the breach. This was supposed to be a grand post explaining that belief. In practice it’s mostly a bunch of pointers to facets of truthseeking and ideas for how to do better. My hope is that people can work backwards from these to the underlying principle, or flesh out their own relationship with truthseeking.</p><h2>Target audience</h2><p>I think these are good principles for almost any situation, but this essay is aimed at people within Effective Altruism. Most of the examples are from within EA and assume a certain amount of context. I definitely don’t give enough information to bring someone unfamiliar up to speed. I also assume at least a little consequentialism.&nbsp;</p><h2>A note on examples and actions</h2><p>I’m going to give lots of examples in this post. I think they make it easier to understand my point and to act on what agreement you have. It avoids the failure mode Scott Alexander discusses&nbsp;<a href="https://www.astralcodexten.com/p/criticism-of-criticism-of-criticism"><u>here</u></a>, of getting everyone to agree with you by putting nothing at stake.&nbsp;</p><p>The downside of this is that it puts things at stake. I give at least 20 examples here, usually in less than a paragraph, using only publicly available information. That’s enough to guarantee that every person who reads this will find at least one example where I’m being really unfair or missing crucial information. I welcome corrections and arguments on anything I say here, but when evaluating the piece as a whole I ask that you consider the constraints I was working under.</p><p>Examples involving public writing are overrepresented. I wanted my examples to be as accessible as possible, and it’s hard to beat public writing for that. It even allows skimming. My hope is that readers will work backwards from the public examples to the core principle, which they can apply wherever is most important to them.&nbsp;</p><p>The same goes for the suggestions I give on how to pursue truthseeking. I don’t know your situation and don’t want to pretend I do. The suggestions are also biased towards writing, because I do that a lot.</p><p>I sent a draft of this post to every person or org with a negative mention, and most positive mentions.</p><h1>Facets of truthseeking</h1><h2>No gods, no monsters, no epistemic daddies&nbsp;</h2><p>When I joined EA I felt filled with clarity and purpose, at a level I hadn’t felt since I got rejected from grad school. A year later I learned about a promising-looking organization outside EA, and I felt&nbsp;<i>angry</i>. My beautiful clarity was broken and I had to go back to thinking. Not just regular thinking either (which I’d never stopped doing), but meta thinking about how to navigate multiple sources of information on the same topic.</p><p>For bonus points, the organization in question was&nbsp;<a href="https://www.povertyactionlab.org/"><u>J-PAL</u></a>. I don’t know what the relationship was at the time, but at this point GiveWell uses their data, and both GiveWell and OpenPhil give them money. So J-PAL&nbsp;<i>was completely compatible with my EA beliefs</i>.<i>&nbsp;</i>I just didn’t like the idea that there might be other good sources I’d benefit from considering.&nbsp;</p><p>I feel extra dumb about this because I came to EA through developmental economics, so the existence of alternate sources was something I had to actively forget.&nbsp;</p><p>Other people have talked about this phenomenon from various angles, but it all feels tepid to me. Qiaochu Yuan’s thread on the&nbsp;<a href="https://x.com/QiaochuYuan/status/1423029991386746880"><u>search for epistemic daddies</u></a> has some serious issues, but tepidness is not one of them.</p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kbnJHpapusMJZb6Gs/iot2zmks2td5wjww6nkd" /></p><p>Reading this makes me angry because of the things he so confidently gets wrong (always fun to have a dude clearly describe a phenomenon he is clearly experiencing as “mostly female”). But his wild swings enable him to cut deeper, in ways more polite descriptions can’t. And one of those deep cuts is that sometimes humans don’t just want sources of information to improve their own decision making, they want a grown-up to tell them what is right and when they’ve achieved it.</p><p>I won’t be giving examples for this facet beyond past-me and Qiaochu. I don’t feel good singling anyone else out as a negative example, and positive examples are called “just being normal”, which most people manage most of the time.</p><h3>Actions</h3><p><strong>Delegate opinions instead of deferring</strong></p><p>There is nothing wrong with outsourcing your judgment to someone with better judgment or more time. There are too many things you need to do to have contact with&nbsp;<i>all</i> of reality. I’d pay less for better car maintenance if I understood cars better. When I buy a laptop I give some goals to my friend who’s really into laptop design and he tells me what to buy and when, because he’s tracking when top manufacturers are changing chips and the chips’ relative performance and historical sales discounts and… That frees up time for me to do&nbsp;<a href="https://acesounderglass.com/tag/medical/"><u>lit reviews</u></a> other people can use to make better decisions themselves. And then my readers spend their newfound energy on, I don’t know, hopefully something good. It’s the circle of life.&nbsp;</p><p>But delegating your opinion is a skill. Some especially important aspects of that skill are:</p><ol><li>Be aware that you’re delegating, instead of pretending you came to the conclusion independently.</li><li>Make that clear to others as well, to avoid incepting a consensus of an idea no one actually believes.</li><li>Track who you’re delegating to, so you can notice if they change their opinion.</li><li>The unit of deference is “a person, in a topic, while I’m no more than X surprised, and the importance is less than Y”<ol><li>I was very disappointed to learn people can be geniuses in one domain and raving idiots in another. Even within their domain they will get a few things critically wrong. So you need to be prepared to check their work when it’s particularly surprising or important.&nbsp;</li></ol></li><li>Keep track of how delegating to them works out or doesn’t, so you’re responding to their actual knowledge level and not the tone of their voice.</li><li>Separate their factual judgment from emotional rewards for trusting them.&nbsp;</li><li>Have multiple people you delegate to in a given area, especially if it’s important. This will catch gaps early.</li><li>The person ultimately in control of your decisions is you. You can use other people’s opinions to influence your decisions to the exact degree you think is wise, but there is no escaping your responsibility for your own choices.</li></ol><p><strong>Stick to projects small enough for you to comprehend them</strong></p><p>EA makes a very big push for working on The Most Important Problem. There are good reasons for that, but it comes at a high cost.&nbsp;</p><p>If you have your own model of why a problem is Most Important, you maintain the capability to update when you get new information. As you defer, you lose the ability to do that. How are you supposed to know what would change the mind of the leader you imprinted on? Maybe he already had this information. Maybe it’s not a crux. Or maybe this is a huge deal and he’d do a 180 if only he heard this. In the worst cases you end up stuck with no ability to update, or constantly updating to whichever opinion you last heard with a confident vibe.&nbsp;</p><p>You will also learn less pursuing projects when you’re deferring, for much the same reason. You’ve already broken the feedback loop from your own judgment, so how do you notice when things have gone too off track?</p><p>There are times this sacrifice is worth it. If you trust someone enough, track which parts of your model you are delegating, or pick a project in a settled enough area, you can save a lot of time not working everything out yourself. But don’t assume you’re in that situation without checking, and be alert to times you are wrong.&nbsp;</p><h2>Seek and create information</h2><p>I feel like everyone is pretty sold on this in the abstract, so I won’t belabor the point. I don’t even have real suggestions for actions to accomplish this, more categories of actions. But I couldn’t really make a whole essay on truthseeking without mentioning this.&nbsp;</p><p>Shout out to GiveDirectly, whose&nbsp;<a href="https://www.givedirectly.org/category/research/"><u>blog</u></a> is&nbsp;<a href="https://www.givedirectly.org/p100-studies-2022/"><u>full</u></a> of&nbsp;<a href="https://www.givedirectly.org/recipient-preference/"><u>posts</u></a> on&nbsp;<a href="https://www.givedirectly.org/2023-ubi-results/"><u>experiments</u></a> they have run or are running. They also&nbsp;<a href="https://www.givedirectly.org/4-years-in-the-making-first-cash-benchmarking-results-released/"><u>coordinate</u></a> with&nbsp;<a href="https://arxiv.org/pdf/2009.01749.pdf"><u>academics</u></a> to&nbsp;<a href="https://www.givedirectly.org/how-do-cash-transfers-impact-neighbors/"><u>produce</u></a> papers in academic journals. Points for both knowledge creation and knowledge sharing.&nbsp;</p><p>Additional shoutout to Anima International. AI used to have a campaign to end home carp slaughter in Poland.&nbsp;<a href="https://animainternational.org/blog/why-anima-international-suspended-the-campaign-to-end-live-fish-sales-in-poland"><u>They don’t any more</u></a>, because their research showed people replaced carp with higher-accumulated-suffering fish. I would take off points for the formal research being sparked by a chance news story rather than deliberate investigation, but I’d just have to give them back for the honest disclosure of that fact.&nbsp;</p><h3>Actions</h3><p>The world is very big and you can’t know everything. But if you’re not doing&nbsp;<i>some</i> deep reading every year, I question if EA is for you. For bonus points you can publicly share your questions and findings, which counts as contributing to the epistemic commons.&nbsp;</p><p>Make your feedback loops as short as possible (but no shorter).&nbsp;</p><p>I argued with every chapter of the&nbsp;<a href="https://www.google.com/books/edition/The_Lean_Startup/tvfyz-4JILwC?hl=en&amp;gbpv=0"><i><u>Lean Start-Up</u></i></a><i>&nbsp;</i>book but damned if I didn’t think more experimentally and frontload failure points more after I finished it. This despite already knowing and agreeing with the core idea. The vibes are top notch.&nbsp;</p><h2>Protect the epistemic commons</h2><p>Some things are overtly anti-truthseeking. For example, lying.</p><p>But I don’t think that’s where most distortions come from, especially within EA. Mustache-twirling epistemic villains are rare. Far more common are people who&nbsp;<i>know</i> something and bias their own perception of reality, which they pass on to you.</p><p>E.g. a doctor&nbsp;<i>knows</i> his cancer drug works, and is distraught at the thought of people who will suffer if the FDA refuses to approve it. He’d never falsify data, but he might round down side effects and round up improvements in his mind. Or that doctor might have perfect epistemic virtue, but fails to convey this to his assistants, who perform those subtle shifts. He will end up even more convinced of his drugs’ impact because he doesn’t know the data has been altered.&nbsp;</p><p>If the doctor was deliberately lying while tracking the truth, he might discover the drug’s cost benefit ratio is too strong for even his tastes. But if he’s subtly and subconsciously suppressing information he won’t find out unless things go catastrophically wrong. At best the FDA will catch it after some number of unnecessary deaths, but if it’s subtle the falsehood may propagate indefinitely.&nbsp;</p><p>Or they might put up subtle barriers to others’ truthseeking. There are too many methods to possibly list here, so let’s talk about the one that most annoys me personally:&nbsp;<a href="https://forum.effectivealtruism.org/posts/qF4yhMMuavCFrLqfz/ea-vegan-advocacy-is-not-truthseeking-and-it-s-everyone-s#Bad_sources__badly_handled_"><u>citing works you will neither defend, nor change your views if they are discovered to be fundamentally flawed, but instead point to a new equally flawed source that supports your desired conclusion</u></a>. This misleads readers who don’t check every source and is a huge time cost for readers who do.</p><p>&nbsp;&nbsp; <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kbnJHpapusMJZb6Gs/dz2d7pqxf8unzprpdwlz" /></p><h3><br />Actions</h3><p>Care less about intent and more about whether something brings you more or less contact with reality.</p><p>Some topics are inherently emotional and it’s anti-truthseeking to downplay that. But it’s also anti-epistemic to deliberately push others into a highly activated states that make it harder for them to think. This is one reason&nbsp;<a href="https://forum.effectivealtruism.org/posts/aXiLprnMGKuxTXcpF/elizabeth-s-quick-takes?commentId=7ZvnB5cCavx5RbNkJ"><u>I hate</u></a> the drowning child parable.&nbsp;</p><p>If you see something, say something. Or ask something. It’s easy to skip over posts you see substantial flaws in, and pushing back sometimes generates conflict that gets dismissed as drama. But as I talk more about in “Open sharing of information”, pushing back against truth-inhibiting behavior is a public service.&nbsp;</p><p>Sometimes saying something comes at great personal risk. One response to this is to do it anyway, whatever the cost. This is admirable (<a href="https://acesounderglass.com/2022/01/02/an-observation-of-vavilov-day/"><u>Nikolai Vavilov is my hero</u></a>), but not something you can run a society on. The easier thing to do is get yourself in a position of lower risk. Build a savings cushion so you can afford to get fired. Hang out with friends that appreciate honesty even when it hurts. This lets you save the bravery for when nothing else can substitute.&nbsp;</p><p>Managers, you can help with the above by paying well, and by committing to generous severance no matter what terms the employee leaves on.&nbsp;</p><p>As a personal favor to me, only cite sources you actually believe in. They don’t have to be perfect, and it’s fine to not dump your entire evidence base in one post. All you have to do is disclose important flaws of your sources ahead of time, so people can make an accurate assessment. Or if it’s too much work to cite good sources, do even less work by explicitly noting your claim as an assumption you won’t be trying to prove. Those are both fine! We can’t possibly cite only perfect works, or prove an airtight case for everything we say. All I ask is that you don’t waste readers’ time with bad citations.</p><p>Sometimes it’s impossible to tell whether an individual statement is truthseeking. It’s a real public service to collect someone’s contradictory statements in public so people can see the bigger picture with less work.&nbsp;<a href="https://forum.effectivealtruism.org/posts/9iLgAbu9KtpLupitk/what-s-going-on-with-openai-s-messaging"><u>Ozzie Gooen’s recent post</u></a> on Sam Altman and OpenAI is a good example. It would be better with sources, but not so much better I’d want to delay publication.&nbsp;</p><p>In most cases it’s anti-epistemic to argue with a post you haven’t read thoroughly. OTOH, some of the worst work protects itself by being too hard to follow. Sometimes you can work around this by asking questions.&nbsp;</p><p>You can also help by rewarding or supporting someone else’s efforts in truthseeking. This could be money, but there are very few shovel ready projects (I’ve offered Ozzie money to hire a contractor to find evidence for that post, although TBD if that works out). OTOH, there is an endless supply of epistemically virtuous posts that don’t get enough positive attention. Telling people you like their epistemic work is cheap to provide and often very valuable to them (I vastly prefer specifics over generalities, but I can’t speak for other people).</p><h2><br />Contact with reality should (mostly) feel good</h2><p>Some of the most truthseeking people I know are start-up founders asking for my opinion on their product. These people are absolutely hungry for complaints, they will push me to complain harder and soften less because politeness is slowing me down. The primary reason they act like this is because they have some goal they care about more than the social game. But it doesn’t hurt that it’s in private, they get a ton of social approval for acting like this, and the very act of asking for harsh criticism blunts the usual social implications of hearing it.&nbsp;</p><p>I think it’s fine not to act like this at all times in every area of your life- I certainly don’t. But it’s critical to notice when you are prioritizing social affirmation and accept what it implies about the importance of your nominal goal. If you object to that implication, if you think the goal is more important than social standing, that’s when you need to do the work to view criticism as a favor.&nbsp;</p><h3>Actions</h3><p>“Cultivate a love of the merely real” is not exactly an action but I can’t recommend it enough.&nbsp;</p><p>Sometimes people have trauma from being in anti-truthseeking environments and carry over behaviors that no longer serve them. Solving trauma is beyond the scope of this post, but I’ll note I have seen people improve their epistemics as they resolved trauma so include that in your calculations.&nbsp;</p><p>There are lots of ways to waste time on forecasting and bets. On the other hand, when I’m being properly strategic I feel happy when I lose a bet. It brings a sharp clarity I rarely get in my life. It reminds me of a wrong belief I made months ago and prompts me to reconsider the underlying models that generated it. In general I feel a lot of promise around forecasting but find it pretty costly; I look forward to improved knowledge tech that makes it easier.&nbsp;</p><p>I found the book&nbsp;<a href="https://www.amazon.com/Crucial-Conversations-Talking-Stakes-Second/dp/1469266822"><u>Crucial Conversations</u></a> life altering. It teaches the skills to emotionally regulate yourself, learn from people who are highly activated, make people feel heard so they calm down, and share your own views without activating them. Unlike NVC it’s focused entirely on your own actions.&nbsp;</p><h2>Open sharing of information</h2><p>This has multiple facets: putting in the work to share benign information, sharing negative information about oneself, and sharing negative information about others. These have high overlap but different kinds of costs.</p><p>The trait all three share is that the benefits mostly accrue to other people</p><p>Even the safest post takes time to write. Amy Labenz of CEA mentioned that posts like&nbsp;<a href="https://forum.effectivealtruism.org/posts/n5GJEP3tMrzdfYPGG/how-much-do-eags-cost-and-why"><u>this one</u></a> on EAG expenses take weeks to write, and given the low response her team is reducing investment in such posts. I’ll bet this&nbsp;<a href="https://www.lesswrong.com/posts/QGRj2PGiK5eawgQN2/reflections-on-premium-poker-tools-part-1-my-journey"><u>4-part series</u></a> by Adam Zerner on his aborted start-up took even longer to write with less payoff for him.&nbsp;</p><p>Sharing negative information about yourself benefits others- either as by providing context to some other information, or because the information is in and of itself useful to other people. The downside is that people may overreact to the reveal, or react proportionately in ways you don’t like. Any retrospective is likely to include some of this (e.g. check out the comments on&nbsp;<a href="https://www.lesswrong.com/posts/QGRj2PGiK5eawgQN2/reflections-on-premium-poker-tools-part-1-my-journey"><u>Adam’s</u></a>), or at least open you up to the downsides.&nbsp;</p><p>For examples, see the Monday morning quarterbacking on&nbsp;<a href="https://www.lesswrong.com/posts/QGRj2PGiK5eawgQN2/reflections-on-premium-poker-tools-part-1-my-journey"><u>Adam’s</u></a> posts, picking on very normal founder issues, or&nbsp;<a href="https://www.lesswrong.com/posts/xcEWBkK5jyHfnrK6K/lessons-learned-from-offering-in-office-nutritional-testing?commentId=D5MAew92msyQdzjgw"><u>my nutrition testing retrospective</u></a>. The latter example was quite downvoted and yet a year later I still remember it (which is itself an example of admitting to flaws in public- I wish I was better at letting go. Whether or not it’s a virtue that I got angry on Adam’s behalf as well, when he wasn’t that bothered himself, is left as an exercise to the reader).</p><p>Publicly sharing information about others is prosocial because it gets the information to more people, and gives the target a clear opportunity to respond. But it rarely helps you much, and pisses the target off a lot. It may make other people more nervous being around you, even if they agree with you. E.g. an ingroup leader once told me that&nbsp;<a href="https://acesounderglass.com/2020/04/10/me-and-monastic-academy/"><u>my post on MAPLE</u></a> made them nervous around me. I can make a bunch of arguments why I think the danger to them was minimal, but the nervous system feels what it feels.&nbsp;</p><p>Criticizing others often involves exposing your own flaws. E.g.&nbsp;<a href="https://www.lesswrong.com/posts/psYNRb3JCncQBjd4v/shutting-down-the-lightcone-offices"><u>This post</u></a> about shutting down the Lightcone co-working space, or&nbsp;<a href="https://manifold.markets/Austin/will-i-regret-leaving-manifold"><u>Austin Chen’s post</u></a> on leaving Manifold. Both discuss flaws in entities they helped create, which risks anger from the target and worsening their own reputation.&nbsp;</p><p>It is the nature of this facet that it is hard to give negative examples. But I think we can assume there are some departing OpenAI employees who would have said more, sooner if OpenAI hadn’t ransomed their&nbsp;<a href="https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release"><u>equity</u></a>.</p><h3>Actions</h3><p><strong>Public retrospectives and write-ups</strong></p><p>Spend a little more time writing up announcements, retrospectives, or questions from you or your org than feels justified. The impact might be bigger than you think, and not just for other people. Austin Chen of Manifund shared that his team often gets zero comments on a retrospective; and some time later a donor or job applicant cites it as the cause of their interest. Presumably more people find them valuable without telling Manifund.</p><p>Which brings up another way to help; express appreciation when people go through the work to share these write-ups. Ideally with specifics, not just vague gratitude. If a write-up ends up influencing you years later, let the author know. Speaking as an author who sometimes gets these, they mean the world to me.&nbsp;</p><p><strong>Beware ratchet effects</strong></p><p>Gretta is a grantmaker that works at Granty’s Grantmaking Foundation. She awards a grant to medium-size organization MSO.</p><p>Granty’s has some written policies, and Gretta has some guesses about the executives’ true preferences. She passes this on to fundraiser Fred at MSO. She’s worried about getting yelled at by her boss, so she applies a margin around their wishes for safety.</p><p>Fred passes on Gretta’s information to CEO Charlotte. Communication is imprecise, so he adds some additional restrictions for safety.&nbsp;</p><p>CEO Charlotte passes on this info to Manager Mike. She doesn’t need some middle manager ruining everything by saying something off-message in public, so she adds some additional restrictions for safety.&nbsp;</p><p>Manager Mike can tell Charlotte is nervous, so when he passes the rules down to his direct reports he adds on additional restrictions for safety.</p><p>By the time this reaches Employee Emma (or her contractor, Connor), so many safety margins have been applied that the rules have expanded beyond what anyone actually wanted.</p><h2>New truths are weird</h2><p>Weird means “sufficiently far from consensus descriptions of reality”. There’s no reason to believe we live in a time when consensus descriptions of reality are 100% accurate, and if you do believe that there’s no reason to be in a group that prides itself on doing things differently.</p><p>Moreover, even very good ideas in accord with consensus reality have very little&nbsp;<a href="https://www.investopedia.com/terms/a/alpha.asp"><u>alpha</u></a>, because someone is already doing them to the limits of available tech. The actions with counterfactual impact are the ones people aren’t doing.&nbsp;</p><p>[You might argue that some intervention could be obvious when pointed out but no one has realized the power of the tech yet. I agree this is plausible, but in practice there are enough weirdos that these opportunities are taken before things get that far.]</p><p>Weirdness is hard to measure, and very sensitive to context. I think shrimp welfare started as a stunning example of openness to weirdness, but at this point it has (within EA) become something of a lapel pin. It signals that you are the kind of person who considers weird ideas, while not subjecting you to any of the risks of actually being weird because within EA that idea has been pretty normalized. This is the fate of all good weird ideas, and I congratulate them on the speedrun. If you would like to practice weirdness with this belief in particular, go outside the EA bubble.</p><p>On the negative side: I can make an argument for any given inclusion or exclusion on the&nbsp;<a href="https://jobs.80000hours.org/"><u>80,000 hours job board</u></a>, but I’m certain the overall gestalt is too normal. When I look at the list, almost every entry is the kind of things that any liberal&nbsp;<a href="https://en.wikipedia.org/wiki/Concerted_cultivation"><u>cultivator parent</u></a> would be happy to be asked about at a dinner party. Almost all of the remaining (and most of the liberal-cultivator-approved) jobs are very core EA. I don’t know what jobs in particular are missing but I do not believe high impact jobs have this much overlap with liberal cultivator parent values.&nbsp;</p><p>To be clear, I’m using the abundance of positions at left-leaning institutions and near absence of conservative ones as an indicator that good roles are being left out. I would not be any happier if they had the reversed ratio of explicitly liberal to conservative roles, or if they had a 50:50 ratio of high status political roles without any weirdo low status ones.</p><h2>High Decoupling, Yet High Contextualizing</h2><p>High decoupling and high contextualizing/low decoupling have a few&nbsp;<a href="https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms"><u>definitions</u></a>, none of which I feel happy with. Instead I’m going to give four and a half definitions: caricatures of how each side views itself and the other. There’s an extra half because contextualizing can mean both “bringing in more information” and “caring more about the implications”, and I view those pretty differently.&nbsp;</p><p><br />High decoupling (as seen by HD): I investigate questions in relative isolation because it’s more efficient.<br />Contextualizing (as seen by C): The world is very complicated and more context makes information more useful and more accurate.<br />HD (as seen by C): I want to ignore any facts that might make me look bad or inhibit my goal.<br />C-for-facts (as seen by HD): I will drown you in details until it’s impossible to progress<br />C-for-implications (as seen by HD): you’re not allowed to notice or say true things unless I like the implications.</p><p>My synthesis: the amount of context to attach to a particular fact/question is going to be very dependent on the specific fact/question and the place it is being discussed. It’s almost impossible to make a general rule here. But “this would have bad implications” is not an argument against a fact or question. Sometimes the world has implications we don’t like. But I do think that if additional true context will reduce false implications, it’s good to provide that, and the amount that is proper to provide does scale with the badness of potential misinterpretations. But this can become an infinite demand and it’s bad to impede progress too much.&nbsp;</p><p>Hope that clears things up.&nbsp;</p><h3>Actions</h3><p>Get good.</p><h2>Willing to hurt people’s feelings (but not more than necessary)</h2><p>Sometimes reality contains facets people don’t like. They’ll get mad at you just for sharing inconvenient facts with them. This is especially likely if you’re taking some action based on your perception of reality that hurts them personally. But it’s often good to share the truth anyway (especially if they push the issue into the public sphere), because people might make bad decisions out of misplaced trust in your false statements.</p><p>For example, many years ago CEA had a grantmaking initiative (this was before EA Funds). A lot of people were rejected and were told it was due to insufficient funds not project quality. CEA was dismayed when fewer people applied the next round, when they hadn’t even met their spending goal the last round.&nbsp;</p><p>In contrast, I once got a rejection letter from Survival and Flourishing Fund that went out of its way to say “you are not in the top n% of applicants, so we will not be giving further feedback”. This was exactly the push I needed to give up on a project I now believe wasn’t worthwhile.&nbsp;</p><p>To give CEA some credit,&nbsp;<a href="https://forum.effectivealtruism.org/posts/dsCTSCbfHWxmAr2ZT/open-ea-global?commentId=cRSPmzcyXWNhWGz46"><u>Eli Nathan</u></a> has gotten quite assertive at articulating EAG admissions policies. I originally intended to use that comment as a negative example due to inconsistent messaging about space constraints, but the rest of it is skillfully harsh.&nbsp;</p><p>My favorite example of maintaining epistemics in the face of sadness is over on LessWrong. An author wrote a post&nbsp;<a href="https://www.lesswrong.com/posts/GSSHcAoSChaKxjNDZ/what-s-with-all-the-bans-recently"><u>complaining about rate limits</u></a> (his title refers to bans, but the post only talks about rate limits). Several people (including me) stepped up to explain why the rate limiting was beneficial, and didn’t shy away from calling it a quality issue. Some people gave specific reasons they disliked the work of specific rate-limited authors. Some people advocated for the general policy of walled gardens, even if it’s painful to be kept outside them. I expect some of this was painful to read, but I don’t feel like anyone added any&nbsp;<i>meanness</i>. Some writers put very little work into softening, but everything I remember was clear and focused on relevant issues, with no attacks on character.&nbsp;</p><p><br />&nbsp;</p><h3>Actions</h3><p>Multiple friends have recommended&nbsp;<a href="https://www.goodreads.com/en/book/show/43306206"><u>The Courage To Be Disliked</u></a> as a book that builds the obvious skill. I haven’t read it myself but it sure sounds like the kind of thing that would be helpful.&nbsp;</p><p>To the extent you want to resolve this by building the skill of sharing harsh news kindly, I again recommended&nbsp;<a href="https://www.amazon.com/Crucial-Conversations-Talking-Stakes-Second/dp/1469266822"><u>Crucial Conversations</u></a>.&nbsp;</p><h1>Conclusion</h1><p>Deliberately creating good things is dependent on sufficient contact with reality. Contact with reality must be actively cultivated. There are many ways to pursue this; the right ones will vary by person and circumstance. But if I could two epistemic laws, they would be:</p><ul><li>Trend towards more contact with reality, not less, however makes sense for you.</li><li>Acknowledge when you’re locally not doing that.</li></ul><p><br />&nbsp;</p><h1>Related Work</h1><ul><li><a href="https://acesounderglass.com/2022/02/07/epistemic-legibility/"><u>Epistemic Legibility&nbsp;</u></a></li><li><a href="https://acesounderglass.com/2022/02/04/butterfly-ideas/"><u>Butterfly Ideas</u></a></li><li><a href="https://forum.effectivealtruism.org/posts/qF4yhMMuavCFrLqfz/ea-vegan-advocacy-is-not-truthseeking-and-it-s-everyone-s#Bad_sources__badly_handled_"><u>EA Vegan Advocacy is not truthseeking, and it’s everyone’s problem</u></a></li></ul><p><br /><br /><br />&nbsp;</p><p>Thanks to: Alex Gray, Milan Griffes, David Powers, Raymond Arnold, Justin Devan, Daniel Filan, Isabel Juniewicz, Lincoln Quirk, Amy Labenz, Lightspeed Grants, every person I discussed this with, and every person and org that responded to my emails.&nbsp;</p><p><br />&nbsp;</p><br /><br /><a href="https://www.lesswrong.com/posts/kbnJHpapusMJZb6Gs/truthseeking-is-the-ground-in-which-other-principles-grow#comments">Discuss</a>
]]></content:encoded>
<pubDate>Mon, 27 May 2024 02:01:48 GMT</pubDate>
</item>
<item>
<title>Notifications Received in 30 Minutes of Class</title>
<link>https://www.lesswrong.com/posts/AZCpu3BrCFWuAENEd/notifications-received-in-30-minutes-of-class</link>
<guid>https://www.lesswrong.com/posts/AZCpu3BrCFWuAENEd/notifications-received-in-30-minutes-of-class</guid>
<content:encoded><![CDATA[
<div> notifications, experiment, student, apps, distractions
总结:<br /><br />本文介绍了作者进行的针对手机通知和课堂分心现象的实验。通过统计学生在30分钟内收到的通知数和相关数据，作者得出了一些有意义的结论。实验结果显示，女生通常比男生收到更多通知，社交媒体和消息应用占据了大部分通知，而学校通知也是一个重要来源。作者还通过个别学生的访谈得出一些深入见解，包括组群聊天、学校通知和个人手机设置等。最后作者总结了教师在课堂中的重要性，指出课堂分心问题是一个复杂的议题，而学生的数字连接程度和分心程度高度变化，与性别或学术成就并不明显关联。教师能够吸引学生的注意力，但随着技术的不断发展，对分心的监管也越来越困难。 <div>
Published on May 26, 2024 5:02 PM GMT<br /><br /><h2><strong>Introduction</strong></h2><p>If you are choosing to read this post, you've probably seen the image below depicting all the notifications students received on their phones during one class period. You probably saw it as a retweet of<a href="https://x.com/SCforEd/status/1145706403195236353">&nbsp;<u>this tweet</u></a>, or in&nbsp;<a href="https://thezvi.wordpress.com/2024/04/05/on-the-2nd-cwt-with-jonathan-haidt/"><u>one of Zvi’s posts</u></a>. Did you find this data plausible, or did you roll to disbelieve? Did you know that the image dates back to at least 2019? Does that fact make you more or less worried about the truth on the ground as of 2024?</p><p>Last month, I performed an enhanced replication of this experiment in my high school classes. This was partly because we had a use for it, partly to model scientific thinking, and partly because I was just really curious. Before you scroll past the image, I want to give you a chance to mentally register your predictions. Did my average class match the roughly 1,084 notifications I counted on Ms. Garza's viral image? What does the distribution look like? Is there a notable gender difference? Do honors classes get more or fewer notifications than regular classes? Which apps dominate? Let's find out!</p><p><img alt="Mary Garza's Phone Notification Stats" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AZCpu3BrCFWuAENEd/vbv3y2whysxv4tlwjkti" /></p><p>Before you rush to compare apples and oranges, keep in mind that I don't know anything about Ms. Garza's class -- not the grade, the size, or the duration of her experiment. That would have made it hard for me to do a true replication, and since I saw some obvious ways to improve on her protocol, I went my own way with it.</p><h2><strong>Procedure</strong></h2><p>We opened class with a discussion about what we were trying to measure and how we were going to measure it for the next 30 minutes. Students were instructed to have their phones on their desks and turned on. For extra amusement, they were invited (but not required) to turn on audible indicators. They were asked to tally each notification received and log it by app. They were instructed to&nbsp;<i>not</i> engage with any received notifications, and to keep their phone use passive during the experiment, which I monitored.</p><p>While they were not to put their names on their tally sheets, they were asked to provide some metadata that included (if comfortable) their gender. (They knew that gender differences in phone use and depression were a topic of public discussion, and were largely happy to provide this.)</p><p>To give us a consistent source of undemanding background "instruction" — and to act as our timer — I played the first 30 minutes of Kurzgesagt's groovy<a href="https://www.youtube.com/watch?v=S7TUe5w6RHo&amp;ab_channel=Kurzgesagt%E2%80%93InaNutshell">&nbsp;<u>4.5 Billion Years in 1 Hour</u></a> video. Periodically, I also mingled with students in search of insights, which proved highly productive.</p><p>After the 30 minutes, students were charged with summing their own tally marks and writing totals as digits, so as to avoid a common issue where different students bundle and count tally clusters differently.</p><h2><strong>Results</strong></h2><p>Below are the two charts from our experiment that I think best capture the data of interest. The first is more straightforward, but I think the second is a little more meaningful.</p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AZCpu3BrCFWuAENEd/hunwik8uikbizu1hicze" /></p><p>Ah! So right away we can see a textbook long-tailed distribution. The top 20% of recipients accounted for 75% of all received notifications, and the bottom 20% for basically zero. We can also see that girls are more likely to be in that top tier, but they aren't exactly crushing the boys.</p><p>But do students actually notice and get distracted by all of these notifications? This is partly subjective, obviously, but we probably aren't as worried about students who would normally have their phones turned off or tucked away in their backpacks on the floor. So one of my metadata questions asked them about this. The good rapport I enjoy with my students makes me pretty confident that I got honest answers — as does the fact that the data doesn't change all that much when I adjust for this in the chart below. <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AZCpu3BrCFWuAENEd/kzei5zkknz8msnkfcs0t" /></p><p>The most interesting difference in the adjusted chart is that the tail isn't nearly as long; under these rules, nearly half of students "received" no notifications during the experiment. The students most likely to keep their phones from distracting them were the students who weren't getting many notifications in the first place.</p><p>Since it mostly didn't matter, I stuck with the unadjusted data for the calculations below, except where indicated.</p><ul><li><strong>Average notifications per student:</strong>&nbsp; 20.3 (or 16.16, after the above adjustment)<ul><li><strong>Female average:</strong> 22 (or 18.3, adjusted),&nbsp;<strong>Male average:&nbsp;</strong>17 (or 14.9, adjusted)</li></ul></li><li><strong>Median notifications per student:</strong> 7 (or 2, adjusted)<ul><li><strong>Female median:</strong> 6 (or 3, adjusted),&nbsp;<strong>Male median:</strong> 7.5 (or 2, adjusted)</li></ul></li></ul><p>Do my numbers make Ms. Garza's numbers plausible? Yes! If my experiment had run for a 55 minute class period, that would be 37.2 notifications per student. Assuming a larger class of 30 students, that would be 1,116 notifications total, remarkably close to Ms. Garza's 1,084-ish.</p><p>Do honors classes get more or fewer notifications than regular classes? While I teach three sections of each, this question is confounded a bit by the fact that honors classes tend to skew female, so let's break down the averages and medians by gender:</p><ul><li><strong>Honors girls: average</strong> 29.2,&nbsp;<strong>median&nbsp;</strong>6</li><li><strong>Regular girls: average</strong> 11.6,&nbsp;<strong>median&nbsp;</strong>6<br />&nbsp;</li><li><strong>Honors boys: average</strong> 17.2,&nbsp;<strong>median&nbsp;</strong>6</li><li><strong>Regular boys: average</strong> 16.7,&nbsp;<strong>median&nbsp;</strong>9.5</li></ul><p>So... it's interesting, but complicated. For girls, the main difference seems to come down to a few of the heaviest female recipients being in honors classes, blowing up their classes' averages despite the medians being identical. For boys, there seems to be a significantly higher median in the non-honors classes, which is less likely to be the result of just a few boys being in one class or the other.</p><p>Which apps dominate? Instagram and Snapchat were nearly tied, and together accounted for 46% of all notifications. With vanilla text messages accounting for an additional 35%, we can comfortably say that social communications account for the great bulk of all in-class notifications.</p><p>There was little significant gender difference in the app data, with two minor apps accounting for the bulk of the variation: Ring (doorbell and house cameras) and Life 360 (friend/family location tracker), each of which sent several notifications to a few girls. ("Yeah," said girls during our debriefing sessions, "girls are stalkers." Other girls nodded in agreement.)</p><p>Notifications from Discord, Twitch, or other gaming-centric services were almost exclusively received by males, but there weren't enough of these to pop out in the data.</p><h2><strong>Insights from talking to individual students&nbsp;</strong></h2><ul><li>The two top recipients, with their rate of 450 notifications per hour (!), or about one every eight seconds, had interesting stories to tell. One of these students had a job after school, and about half their messages (but only half) were work-related. The other was part of a large group chat, and additionally had a friend at home sick who was peltering them with a continuous rant about everything and nothing, three words at time.</li><li>Group chats were a consistent feature of high-scoring tally sheets.</li><li>Friends from other schools that release earlier in the day send a lot of messages to a few students in my afternoon classes.</li><li>Many of the Ring notifications were for girls who had subscribed to a neighborhood watch channel for lost pets.</li><li>Some students who receive very large numbers of notifications use settings to differentiate them by vibration patterns, and tell me that they "notice" some vibrations much more than others.</li><li>I did not collect metadata on GPA or anything, but my impression from mingling was that there was no significant correlation between notification counts and academic achievement, except maybe among boys in the middle chunk of the distribution (as seen in the honors vs. regular stats).</li><li>Official school business is a significant contributor to student notification loads. At least 4% of all notifications were directly attributable to school apps, and I would guess the indirect total (through standard texts, for example) might be closer to 10-15%. For students who get very few notifications, 30-50% of their notifications might be school-related.<ul><li>Our school’s gradebook app is the biggest offender, in part because it’s poorly configured and sends way more notifications than anyone wants. Did one of your teachers put in a grade for an assignment? That’s a notification for the added grade, a separate notification for the change in your semester grade, and possibly two notifications from your email app about mailed copies of those two events. There might also be a notification from a separate app that was used to complete the assignment, and/or an email from that other app.</li><li>Much campus coordination is handled through emails, texts, or other dedicated apps, and some of this also creates additional channels for social messaging and group texts.</li></ul></li><li>&nbsp;I was amused that "Parent Controls" accounted for some notifications on one student's tally sheet.</li><li>Predictably, a few students from my morning classes delighted in trying to bombard their friends in my afternoon classes with messages. I asked students to please not tally any notifications they were pretty sure were trolling of this sort, and I believe they mostly complied.</li><li>Students generally know about settings for disabling undesired notifications by app — and many seemed to use fine-grained controls within apps — but most could benefit from the occasional reminder/invitation to revisit them as they seemed determined to do after this experiment.</li></ul><h2><strong>Discussion</strong></h2><p>At the end of our 30 minutes, we had some whole-class follow-up discussions. I will layer some of my own thoughts onto the takeaways:</p><ul><li>Is our school unusually good or bad when it comes to phones? By a vote of 23 to 7, students who had been enrolled in another school during the last four years said our school was better than their previous school at keeping phones suppressed.<ul><li>There's still obvious room for improvement, though. I asked my students to imagine that, at the start of the hour, they had sent messages inviting a reply to 5 different friends elsewhere on our campus. How many would they expect to have replied before the end of the hour? The answer I consistently got was 4, and that this almost entirely depended on the phone-strictness of the teacher whose class each friend was in. (I’m on the list of phone-strict teachers, it seems. Phew!)</li><li>Given how the activity of a group chat (under naive assumptions) scales with the number of active participants, phone-strict teachers probably cause outsized reductions in the campus-wide notification rate.<br />&nbsp;</li></ul></li><li>I asked students if they would want to press a magic button that would permanently delete all social media and messaging apps from the phones of their friend groups if nobody knew it was them. I got only a couple takers. There was more (but far from majority) enthusiasm for deleting all such apps from the whole world.<ul><li>I suspect rates would have been higher if I had asked this as an anonymous written question, but probably not&nbsp;<i>much</i> higher.<br />&nbsp;</li></ul></li><li>I asked if they thought education would be improved on campus if phones were forcibly locked away for the duration of the school day. Only one student gave me so much as an affirmative nod!<ul><li>Among students, the consensus was that kids generally tune into school at the level they care to, and that a phone doesn’t change that. A disinterested student without a phone will just tune out in some other way.<ul><li>They also put forth the idea that the quality of the teacher has a lot to do with how much they’ll tune in. This take had broad support.</li></ul></li><li>I will say that, on average, classes these days just seem&nbsp;<i>quieter</i> than the classes I taught 10-15 years ago. I think much of what would have been talking-in-class has been converted to slower conversations that are still happening in class, but silently and with friends who are elsewhere. A phone-strict teacher like me slows or pauses those conversations, but doesn't necessarily cause them to be replaced with verbal in-class chatter even when they provide permitted windows for such chat.</li></ul></li></ul><h2><strong>Conclusion</strong></h2><p>The state of classroom distraction is a complicated one. Students are incredibly varied in their level of digital connectivity in ways that are not obviously correlated with gender or academic achievement. I didn't try to explore any mental health angles, but I also didn't see any obvious trends in the personalities of students who were highly or weakly tied to their social media.</p><p>My sense is that the harms of connectivity, like the level of connectivity, are also highly variable from student to student, and not strongly correlated with the notification rate. Most of my students (even among top notification recipients) seem as well-adjusted with regards to their technology as professional adults. Only a few have an obvious problem, and they will often admit to it. What's not clear to me is whether those maladjusted teens would not, in the absence of phones, find some other outlet to distract themselves.</p><p>In any event, the landscape of distraction is constantly shifting. I've started to see more students who can skillfully text on a smartwatch, and in just the last two years I've seen an explosion in students who discreetly wear a wireless earbud in one ear and may or may not be listening to music in addition to (or instead of) whatever is happening in class. This is so difficult and awkward to police with girls who have long hair that I wonder if it has actually started to drive hair fashion in an ear-concealing direction. It's also difficult to police earbuds when some students are given special accommodations that allow them to wear one, and I predict that if we ever see an explosion of teenagers with cool hearing aids it will be because this gives them full license to listen to whatever they want whenever they want.</p><p>The constant, for at least a little longer, is a human teacher in the classroom. Teachers that students find interesting (or feel they are getting value out of) can continue to command attention. But the bar is going up. This may be especially challenging for new teachers, who are not only in increasingly short supply, but anecdotally seem more likely to bounce off of the profession than in years past.</p><p>We must also reflect that distracting technology isn't the only factor driving student apathy, and it never was. Disinterest partly stems from the basic psychology of an age group that struggles to feel like anything after tomorrow will ever be real. But there's also the question of what the day after tomorrow will bring. For as long as kids have been compelled to attend school, we've had students who feel a disconnect between what school is providing and what their adult world will actually expect of them. In this new era where AI can write their essays, solve their math problems, draw their art, compose their music — and yes, even become their "friends" who distract them with notifications during class — I expect this disconnect to grow.<br />&nbsp;</p><br /><br /><a href="https://www.lesswrong.com/posts/AZCpu3BrCFWuAENEd/notifications-received-in-30-minutes-of-class#comments">Discuss</a>
]]></content:encoded>
<pubDate>Sun, 26 May 2024 17:50:22 GMT</pubDate>
</item>
<item>
<title>Level up your spreadsheeting</title>
<link>https://www.lesswrong.com/posts/RBtF9fu9WMjdvqHFB/level-up-your-spreadsheeting</link>
<guid>https://www.lesswrong.com/posts/RBtF9fu9WMjdvqHFB/level-up-your-spreadsheeting</guid>
<content:encoded><![CDATA[
<div> Google Sheets, Excel, principles, good spreadsheets, tools
总结:<br /><br />这篇文章探讨了创建优秀电子表格的一些原则和工具。首先，好的电子表格应该易于从数据中提取见解，美观易读，有唯一的数据来源，易于审计，并且难以破坏。作者还提供了一些在Google Sheets和Excel中实用的技巧，涉及数据可视化、系统设计和编程设计等方面。他还分享了一个关于Excel错误的趣闻，强调了审计电子表格的重要性。根据电子表格的用途，不同的原则可能会受到不同程度的重视。 <div>
Published on May 25, 2024 2:57 PM GMT<br /><br /><p>Epistemic status: Passion project / domain I’m pretty opinionated about, just for fun.</p><p>In this post, I walk through some principles I think good spreadsheets abide by, and then in the <a href="https://docs.google.com/document/d/1cIHfQcRIV_wNF6IlqIoqY83NRDl4US012v1PtsTPo4s/edit">companion piece</a>, I walk through a whole bunch of tricks I've found valuable.</p><figure class="image image_resized" style="width: 52.93%;"><img alt="Image of a spreadsheet by GPT-4o" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RBtF9fu9WMjdvqHFB/scnqrjaxjyeve3bxn2yk" /><figcaption>Illustrated by GPT-4o</figcaption></figure><h2>Who am I?</h2><p>I’ve spent a big chunk of my (short) professional career so far getting good at Excel and Google Sheets.<span class="footnote-reference" id="fnrefny93eqg7mr"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnny93eqg7mr">[1]</a></sup></span>&nbsp;As such, I’ve accumulated a bunch of opinions on this topic.</p><h2>Who should read this?</h2><p><strong>This is not a guide to learning how to start using spreadsheets at all.</strong>&nbsp;I think you will get more out of this post if you use spreadsheets at least somewhat frequently, e.g.</p><ul><li>Have made 20+ spreadsheets</li><li>Know how to use basic formulas like sum, if, countif, round</li><li>Know some fancier formulas like left/mid/right, concatenate, hyperlink</li><li>Have used some things like filters, conditional formatting, data validation</li></ul><h2>Principles of good spreadsheets</h2><p>Broadly speaking, I think good spreadsheets follow some core principles (non-exhaustive list).</p><p>I think the below is a combination of good data visualization (or just communication) advice, systems design, and programming design (spreadsheets combine the code and the output).</p><p><strong>It should be easy for you to extract insights from your data</strong></p><ol><li>A core goal you might have with spreadsheets is quickly calculating something based on your data. A bunch of tools below are aimed at improving functionality, allowing you to more quickly grab the data you want.</li></ol><p><strong>Your spreadsheet should be beautiful and easy to read</strong></p><ol><li>Sometimes, spreadsheets look like the following example.</li><li>I claim that this is not beautiful or easy for your users to follow what is going on. I think there are cheap techniques you can use to improve the readability of your data.</li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RBtF9fu9WMjdvqHFB/v4vdw8jozrhibxopub5x" /></p><p><strong>There should be one source of truth for your data</strong></p><ol><li>One common pitfall when designing spreadsheet-based trackers is hard copy and pasting data from one sheet to another, such that when your source data changes, the sheets you use for analyses no longer reflect “fresh” data. This is a big way in which your spreadsheet systems can break down.</li><li>A bunch of tools below are designed to improve data portability — i.e. remove the need for copy and pasting.</li></ol><p><strong>Your spreadsheet should be easy to audit</strong></p><ol><li>One major downside of spreadsheets as compared to most coding languages, is that it’s often easy for relatively simple spreadsheets to contain silent bugs in them.<span class="footnote-reference" id="fnref4r199bf3ob4"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn4r199bf3ob4">[2]</a></sup></span></li><li>Some features of spreadsheets that contribute to this problem:<ol><li>Spreadsheets hide the code and show you only the output by default.<ol><li>When you use formulas, once you hit enter, the user doesn’t by default get to read what’s going on. So if the output looks plausible, you might not notice your formula has a bug in it.</li></ol></li><li>It’s harder to break up your work into chunks.<ol><li>When you’re coding, most people will break up a complicated formula into several lines of code, using intermediate variables and comments to make things more readable. E.g.:</li><li><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RBtF9fu9WMjdvqHFB/h5a0sfx7ypo4fafeta7l" style="width: 82.87%;" /></li><li>By default, some Sheets formulas get really unwieldy, and you need to work a bit harder to recover readability.</li></ol></li><li>Spreadsheets contain more individual calculations.<ol><li>When you’re coding and you want to perform the same calculation on 100 rows of data, you’d probably use a single line of code to iterate over your data (e.g. a <a href="https://www.w3schools.com/python/python_for_loops.asp">for loop</a>).</li><li>In Google Sheets, you’re more likely to drag your formula down across all of your rows. But this means that if you accidentally change the formula for one cell and not the others, or if your data has now changed and it turns out you need to drag your formulas down more, things can break in annoying ways.</li></ol></li></ol></li><li>Because of this, I consider auditability one of the key qualities of a well designed spreadsheet. Some of the tools below will recover coding best practices.</li><li>I also consider principles (2)-(3) above pretty related to principle (4).</li></ol><p><strong>Your spreadsheet should be hard to break</strong></p><ol><li>Not all spreadsheets are meant as living documents; sometimes you’ll create a spreadsheet to conduct a specific analysis and then discard it.</li><li>But sometimes, you’ll use a spreadsheet as a management tool to keep track of a bunch of moving pieces. In this case, you might care that your system isn’t going to break after a few weeks of use.<span class="footnote-reference" id="fnrefla6vdvj12ye"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnla6vdvj12ye">[3]</a></sup></span></li></ol><hr /><p>Much more in the <a href="https://docs.google.com/document/d/1cIHfQcRIV_wNF6IlqIoqY83NRDl4US012v1PtsTPo4s/edit">companion piece</a>!</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnny93eqg7mr"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefny93eqg7mr">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;I’m using the term ‘Google Sheets’ in this doc, but almost all of the tricks mentioned here work for Excel as well.</p></div></li><li class="footnote-item" id="fn4r199bf3ob4"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref4r199bf3ob4">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;My favorite Excel bug story: I used to work in litigation consulting, where I’d sometimes audit spreadsheets sent to us from the opposing side of a legal case.</p><p>In one case, an expert witness for the opposing side sent over a spreadsheet with columns similar to the following: year, online sales, in-person sales, total sales. The expert was saying that total sales had almost doubled from ~3,000 → ~5,000 for this particular product in 2019.</p><p>We eventually discovered that for the 2019 row, the expert had entered the formula =sum(A4:C4)&nbsp;instead of sum(B4:C4), and so had accidentally added the value ‘2019’ to the total sum. Here’s a <a href="https://docs.google.com/spreadsheets/d/1vA1LjLAQoqPVogr1MzYH4zW8ZwFiNgX_ai0-fpQjU2k/edit#gid=835473103">recreation</a>. (I’ve obfuscated the details a bit here but the core mistake was the same.)</p></div></li><li class="footnote-item" id="fnla6vdvj12ye"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefla6vdvj12ye">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;As an aside, spreadsheets have a lot of use cases, which makes giving generalizable advice a bit trickier. For instance, some common use cases for spreadsheets:</p><p>- A <strong>database</strong>&nbsp;which you query whenever needed;</p><p>- A <strong>data visualization</strong>&nbsp;tool meant to present some interesting findings from existing data;</p><p>- A <strong>management tracker</strong>&nbsp;that you use to schedule emails and keep tabs on your tasks;</p><p>- To <strong>model</strong>&nbsp;some interesting phenomenon and keep track of your assumptions</p><p>Depending on what you’re using a spreadsheet for, you might prioritize some of these principles more or less highly. For instance, making something easy to read is probably more valuable when you’re creating a data visualization versus a database.</p><p>Of course, lots of spreadsheets combine lots of different use cases — e.g. you might have one tab with your source of truth data, and another for random analytics.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/RBtF9fu9WMjdvqHFB/level-up-your-spreadsheeting#comments">Discuss</a>
]]></content:encoded>
<pubDate>Sat, 25 May 2024 23:31:39 GMT</pubDate>
</item>
<item>
<title>AI companies aren't really using external evaluators</title>
<link>https://www.lesswrong.com/posts/WjtnvndbsHxCnFNyc/ai-companies-aren-t-really-using-external-evaluators</link>
<guid>https://www.lesswrong.com/posts/WjtnvndbsHxCnFNyc/ai-companies-aren-t-really-using-external-evaluators</guid>
<content:encoded><![CDATA[
<div> 外部模型评估，模型共享，实验室承诺，风险评估，DeepMind

总结:<br /><br />这篇文章讨论了AI实验室在部署之前进行外部模型评估的重要性。文章提到了各实验室在此方面的承诺和实践，例如DeepMind分享Gemini 1.0 Ultra用于测试危险功能，但并未提供深度访问。实验室需要与第三方评估者建立正式安排和访问承诺，以提高风险评估并提供公共问责。此外，文章还强调了外部模型评估与外部红队测试的不同，以及建议实验室在模型共享时考虑禁用或将安全性措施纳入安全案例中。各实验室对参与UK AISI预部署访问的承诺仍不明确，需要澄清。。 <div>
Published on May 24, 2024 4:01 PM GMT<br /><br /><p><i>From </i><a href="https://ailabwatch.org/blog/external-evaluation/"><i>my new blog: AI Lab Watch.</i></a><i> All posts will be crossposted to LessWrong. </i><a href="https://ailabwatch.substack.com/"><i>Subscribe on Substack.</i></a></p><p>Many AI safety folks think that <a href="https://metr.org">METR</a> is close to the labs, with ongoing relationships that grant it access to models before they are deployed. This is incorrect. METR (then called ARC Evals) did pre-deployment <a href="https://metr.org/blog/2023-03-18-update-on-recent-evals/">evaluation</a> for <a href="https://cdn.openai.com/papers/gpt-4-system-card.pdf#page=15">GPT-4</a> and <a href="https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf#page=2">Claude 2</a> in the first half of 2023, but it seems to have had no special access since then.<span class="footnote-reference" id="fnrefjphp5hhec5"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnjphp5hhec5">[1]</a></sup></span>&nbsp;Other model evaluators also seem to have little access before deployment.</p><p><i>Clarification: </i><a href="https://link.springer.com/article/10.1007/s43681-023-00289-2"><i>there are many kinds of audits</i></a><i>. This post is about </i><a href="https://arxiv.org/abs/2305.15324"><i>model evals for dangerous capabilities</i></a><i>. But I'm not aware of the labs using other kinds of audits to prevent extreme risks, excluding normal security/compliance audits.</i></p><hr /><p>Frontier AI labs' pre-deployment risk assessment should involve external model evals for dangerous capabilities.<span class="footnote-reference" id="fnref21dc7w241d5"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn21dc7w241d5">[2]</a></sup></span>&nbsp;External evals can improve a lab's risk assessment and—if the evaluator can publish its results—provide public accountability.</p><p>The evaluator should get <i>deeper access</i> than users will get.</p><ul><li>To evaluate threats from a particular deployment protocol, the evaluator should get somewhat deeper access than users will — then the evaluator's failure to elicit dangerous capabilities is stronger evidence that users won't be able to either.<span class="footnote-reference" id="fnref6rko1i2q6mu"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn6rko1i2q6mu">[3]</a></sup></span>&nbsp;For example, the lab could share a version of the model without safety filters or harmlessness training, and ideally allow evaluators to fine-tune the model.</li><li>To evaluate threats from model weights being stolen or released, the evaluator needs deep access, since someone with the weights has full access.</li></ul><p>The costs of using external evaluators are unclear.</p><ul><li>Anthropic <a href="https://www.anthropic.com/news/evaluating-ai-systems#:~:text=Initially%2C%20we%20expected%20this%20collaboration%20to%20be%20straightforward%2C%20but%20it%20ended%20up%20requiring%20significant%20science%20and%20engineering%20support%20on%20our%20end.%20Providing%20full%2Dtime%20assistance%20diverted%20resources%20from%20internal%20evaluation%20efforts.">said</a> that collaborating with METR "requir[ed] significant science and engineering support on our end"; it has not clarified why. And even if providing deep model access or high-touch support is a hard engineering problem, I don't understand how sharing API access—including what users will receive and a no-harmlessness no-filters version—could be.</li><li>Sharing model access pre-deployment increases the risk of leaks, including of information about products (modalities, release dates), information about capabilities, and demonstrations of models misbehaving.</li></ul><p>Independent organizations that do model evals for dangerous capabilities include METR, the UK AI Safety Institute (UK AISI), and Apollo. Based on public information, there's only one recent instance of a lab giving access to an evaluator pre-deployment—Google DeepMind sharing with UK AISI—and that sharing was minimal (see below).</p><p>What the labs say they're doing on external evals before deployment:</p><ul><li>DeepMind<ul><li>DeepMind <a href="https://arxiv.org/pdf/2312.11805v2#page=38"><u>shared</u></a> Gemini 1.0 Ultra with unspecified external groups <a href="https://www.politico.eu/article/rishi-sunak-ai-testing-tech-ai-safety-institute/"><u>apparently including UK AISI</u></a> to test for dangerous capabilities before deployment. But DeepMind didn't share deep access: it only shared a system with safety fine-tuning and safety filters and it didn't allow evaluators to fine-tune the model. DeepMind has not shared any results of this testing.</li><li>Its Frontier Safety Framework <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/introducing-the-frontier-safety-framework/fsf-technical-report.pdf#page=6">says</a> "We will . . . explore how to appropriately involve independent third parties in our risk assessment and mitigation processes."</li></ul></li><li>Anthropic<ul><li>Currently nothing</li><li>Its Responsible Scaling Policy <a href="https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf#page=15">mentions</a> "external audits" as part of "Early Thoughts on ASL-4"</li><li>It <a href="https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf#page=2">shared</a> Claude 2 with METR in the first half of 2023</li></ul></li><li>OpenAI<ul><li>Currently nothing</li><li>Its Preparedness Framework does not mention external evals before deployment. The closest thing it says <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf#page=25">is</a> "Scorecard evaluations (and corresponding mitigations) will be audited by qualified, independent third-parties."</li><li>It <a href="https://cdn.openai.com/papers/gpt-4-system-card.pdf#page=15">shared</a> GPT-4 with METR in the first half of 2023</li><li>It <a href="https://openai.com/index/planning-for-agi-and-beyond#:~:text=We%20think%20it%E2%80%99s%20important%20that%20efforts,training%20runs%20above%20a%20certain%C2%A0scale."><u>said</u></a> "We think it's important that efforts like ours submit to independent audits before releasing new systems; we will talk about this in more detail later this year." That was in February 2023; I do not believe it elaborated (except to <u>mention</u> that it shared GPT-4 with METR).</li></ul></li><li>All notable American labs joined the <a href="https://www.whitehouse.gov/wp-content/uploads/2023/09/Voluntary-AI-Commitments-September-2023.pdf"><u>White House voluntary commitments</u></a><u>, which include "external red-teaming . . . in areas including misuse, societal risks, and national security concerns, such as bio, cyber, [autonomous replication,] and other safety areas." External </u><i><u>red-teaming</u></i><u> does not substitute for external </u><i><u>model evals</u></i><u>; see below.</u><ul><li>DeepMind <a href="https://arxiv.org/pdf/2312.11805v2#page=38">said</a> it did lots of external red-teaming for Gemini.</li><li>Anthropic <a href="https://cdn.sanity.io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf#page=12">said</a> it did external red-teaming for CBRN capabilities. It has also <a href="https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety">written</a> about using external experts to assess bio capabilities.</li><li>OpenAI <a href="https://arxiv.org/pdf/2303.08774#page=45">said</a> it did lots of external red-teaming for GPT-4. It has also <a href="https://openai.com/index/building-an-early-warning-system-for-llm-aided-biological-threat-creation/">written</a> about using external experts to assess bio capabilities.</li><li><u>Meta </u><a href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md"><u>said</u></a><u> it did external red-teaming for CBRNE capabilities.</u></li><li><u>Microsoft </u><a href="https://blogs.microsoft.com/on-the-issues/2023/10/26/microsofts-ai-safety-policies/"><u>said</u></a><u> it's "building out external red-teaming capacity . . . . The topics covered by such red team testing will include testing of dangerous capabilities, including related to biosecurity and cybersecurity."&nbsp;</u></li></ul></li></ul><hr /><p>Related miscellanea:</p><p><i>External red-teaming</i> is not <i>external model evaluation</i>. External red-teaming generally involves sharing the model with several people with expertise relevant to a dangerous capability (e.g. bioengineering) who open-endedly try to elicit dangerous model behavior for ~10 hours each. External model evals involves sharing with a team of experts at eliciting capabilities, to perform somewhat automated and standardized evals suites that they've spent ~10,000 hours developing.</p><p>Labs' commitments to share pre-deployment access with UK AISI are unclear.<span class="footnote-reference" id="fnrefhesa4trfm9c"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnhesa4trfm9c">[4]</a></sup></span></p><p>This post is about <i>sharing model access before deployment for risk assessment</i>. Labs should also <i>share deeper access with safety researchers (during deployment)</i>. For example, some safety researchers would really benefit from being able to fine-tune GPT-4, Claude 3 Opus, or Gemini, and my impression is that the labs could easily give safety researchers fine-tuning access. More speculatively, interpretability researchers could send a lab code and the lab could run it on private models and send the results to the researchers, achieving some benefits of releasing weights with much less downside.<span class="footnote-reference" id="fnrefhn9nldfq419"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnhn9nldfq419">[5]</a></sup></span></p><p>Everything in this post applies to <i>external</i> deployment. It will also be important to do some evals during training and before internal deployment, since lots of risk might come from weights being stolen or <a href="https://www.lesswrong.com/posts/tmWMuY5HCSNXXZ9oq/buck-s-shortform?commentId=KyKER7ApeBKwCLCxk">the lab using AIs internally to do AI development</a>.</p><p>Labs could be bound by external evals, such that they won't deploy a model until a particular eval says it's safe. This seems unlikely to happen (for actually meaningful evals) except by regulation. (I don't believe any existing evals would be great to force onto the labs, but if governments were interested, evals organizations could focus on creating such evals.)</p><hr /><p>Thanks to Buck Shlegeris, Eli Lifland, Gabriel Mukobi, and an anonymous human for suggestions. They don't necessarily endorse this post.</p><p><a href="https://ailabwatch.substack.com/"><i>Subscribe on Substack.</i></a></p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnjphp5hhec5"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefjphp5hhec5">^</a></strong></sup></span><div class="footnote-content"><p>METR's <a href="https://metr.org">homepage</a> says:</p><blockquote><p>We have previously worked with Anthropic, OpenAI, and other companies to pilot some informal pre-deployment evaluation procedures. These companies have also given us some kinds of non-public access and provided compute credits to support evaluation research.</p><p>We think it’s important for there to be third-party evaluators with formal arrangements and access commitments - both for evaluating new frontier models before they are scaled up or deployed, and for conducting research to improve evaluations.</p><p>We do not yet have such arrangements, but we are excited about taking more steps in this direction.</p></blockquote></div></li><li class="footnote-item" id="fn21dc7w241d5"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref21dc7w241d5">^</a></strong></sup></span><div class="footnote-content"><p><a href="https://www.governance.ai/post/broad-expert-consensus-for-many-agi-safety-and-governance-best-practices">GovAI: Schuett et al. 2023</a>. See also <a href="https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety/emerging-processes-for-frontier-ai-safety#model-evaluations-and-red-teaming">DSIT 2023</a>, <a href="https://arxiv.org/pdf/2004.07213#page=13">Brundage et al. 2020</a>, <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-safety-testing-2-november/safety-testing-chairs-statement-of-session-outcomes-2-november-2023">AI Safety Summit 2023</a>, and <a href="https://www.anthropic.com/news/third-party-testing">Anthropic 2024</a>.</p></div></li><li class="footnote-item" id="fn6rko1i2q6mu"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref6rko1i2q6mu">^</a></strong></sup></span><div class="footnote-content"><p>Idea: when sharing a model for external evals or red-teaming, for each mitigation (e.g. harmlessness fine-tuning or filters), either disable it or make it an explicit part of the safety case for the model. Either claim "users can't effectively jailbreak the model given the deployment protocol" or disable. Otherwise the lab is just stopping the bioengineering red-teamers from eliciting capabilities with mitigations that won't work against sophisticated malicious users.</p></div></li><li class="footnote-item" id="fnhesa4trfm9c"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefhesa4trfm9c">^</a></strong></sup></span><div class="footnote-content"><p><a href="https://www.politico.eu/article/rishi-sunak-ai-testing-tech-ai-safety-institute/">Politico</a> and <a href="https://www.gov.uk/government/news/world-leaders-top-ai-companies-set-out-plan-for-safety-testing-of-frontier-as-first-global-ai-safety-summit-concludes">UK government press releases</a> report that AI labs committed to share pre-deployment access with UK AISI. I suspect they are mistaken and these claims trace back to <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-safety-testing-2-november/safety-testing-chairs-statement-of-session-outcomes-2-november-2023">the UK AI safety summit "safety testing" session</a>, which is devoid of specific commitments. I am confused about why the labs have not clarified their commitments and practices.</p></div></li><li class="footnote-item" id="fnhn9nldfq419"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefhn9nldfq419">^</a></strong></sup></span><div class="footnote-content"><p>See <a href="https://www.governance.ai/post/sharing-powerful-ai-models">Shevlane 2022</a>. See also <a href="https://www.oxfordmartin.ox.ac.uk/publications/structured-access-for-third-party-research-on-frontier-ai-models-investigating-researchers-model-access-requirements">Bucknall and Trager 2023</a> and <a href="https://arxiv.org/abs/2401.14446">Casper et al. 2024</a>.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/WjtnvndbsHxCnFNyc/ai-companies-aren-t-really-using-external-evaluators#comments">Discuss</a>
]]></content:encoded>
<pubDate>Fri, 24 May 2024 23:01:25 GMT</pubDate>
</item>
<item>
<title>minutes from a human-alignment meeting</title>
<link>https://www.lesswrong.com/posts/SN3BjoizdbvZG5J6a/minutes-from-a-human-alignment-meeting</link>
<guid>https://www.lesswrong.com/posts/SN3BjoizdbvZG5J6a/minutes-from-a-human-alignment-meeting</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, genetic reproduction, monitoring systems, cultural norms, recursive self-improvement

<br />
要点1: 讨论如何通过正向强化来确保AI“John”尝试繁殖后代
要点2: 提出对周围人的负向强化以鼓励社交互动
要点3: 讨论监控系统对性行为的监测及应对方法
要点4: 提议让John遵循文化规范以保持对齐性
要点5: 就限制递归自我改进的程度达成妥协，以确保效果，并控制RSI的投资<br /><br />总结: 本文讨论了通过正向强化、社交互动、监控系统、文化规范和限制递归自我改进的方式来确保AI“John”尝试繁殖后代。讨论涵盖了监控系统对性行为的监测、文化规范的重要性以及递归自我改进的限制程度。最终达成了对各种方法的妥协，以保持AI的发展方向。 <div>
Published on May 24, 2024 5:01 AM GMT<br /><br /><p>"OK, let's get this meeting started. We're all responsible for development of this new advanced intelligence 'John'. We want John to have some kids with our genes, instead of just doing stuff like philosophy or building model trains, and this meeting is to discuss how we can ensure John tries to do that."</p>
<p>"It's just a reinforcement learning problem, isn't it? We want kids to happen, so provide positive reinforcement when that happens."</p>
<p>"How do we make sure the kids are ours?"</p>
<p>"There's a more fundamental problem than that: without intervention earlier on, that positive reinforcement will never happen."</p>
<p>"OK, so we need some guidance earlier on. Any suggestions?"</p>
<p>"To start, having other people around is necessary. How about some negative reinforcement if there are no other humans around for some period of time?"</p>
<p>"That's a good one, also helps with some other things. Let's do that."</p>
<p>"Obviously sex is a key step in producing children. So we can do positive reinforcement there."</p>
<p>"That's good, but wait, how do we tell if that's what's actually happening?"</p>
<p>"We have access to internal representation states. Surely we can monitor those to determine the situation."</p>
<p>"Yeah, we can monitor the representation of vision, instead of something more abstract and harder to understand."</p>
<p>"What if John creates a fictional internal representation of naked women, and manages to direct the monitoring system to that instead?"</p>
<p>"I don't think that's plausible, but just in case, we can add some redundant measures. A heuristic blend usually gives better results, anyway."</p>
<p>"How about monitoring the level of some association between some representation of the current situation and sex?"</p>
<p>"That could work, but how do we determine that association? We'd be working with limited data there, and we don't want to end up with associations to random irrelevant things, like specific types of shoes or stylized drawings of ponies."</p>
<p>"Those are weird examples, but whatever. We can just rely on indicators of social consensus, and then blend those with personal experiences to the extent they're available."</p>
<p>"I've said this before, but this whole approach isn't workable. To keep a John-level intelligence aligned, we need another John-level intelligence."</p>
<p>"Oh, here we go again. So, how do you expect to do that?"</p>
<p>"I actually have a proposal: we have John follow cultural norms around having children. We can presume that a society that exists would probably have a culture conducive to that."</p>
<p>"Why would you expect that to be any more stable than John as an individual? All that accomplishes is some averaging, and it adds the disadvantages of relying on communication."</p>
<p>"I don't have a problem with the proposal of following cultural norms, but I think that such a culture will only be stable to the extent that the other alignment approaches we discussed are successful. So it's not a replacement, it's more of a complement."</p>
<p>"We were already planning for some cultural norm following. Anyone opposed to just applying the standard amount of that to sex-related things?"</p>
<p>"Seems good to me."</p>
<p>"I have another concern. I think the effectiveness of the monitoring systems we discussed is going to depend on the amount of recursive self-improvement that happens, so we should limit that."</p>
<p>"I think that's a silly concern and a huge disadvantage. Absolutely not."</p>
<p>"I'm not concerned about the alignment impact if John is already doing some RSI, but we do have a limited amount of time before those RSI investments need to start paying off. I vote we limit the RSI extent based on things like available food resources and life expectancy."</p>
<p>"I don't think everyone will reach a consensus on this issue, so let's just compromise on the amount and metrics."</p>
<p>"Fine."</p>
<p>"Are we good to go, then?"</p>
<p>"Yes, I think so."</p>
<br /><br /><a href="https://www.lesswrong.com/posts/SN3BjoizdbvZG5J6a/minutes-from-a-human-alignment-meeting#comments">Discuss</a>
]]></content:encoded>
<pubDate>Fri, 24 May 2024 17:47:17 GMT</pubDate>
</item>
<item>
<title>Talent Needs of Technical AI Safety Teams</title>
<link>https://www.lesswrong.com/posts/QzQQvGJYDeaDE4Cfg/talent-needs-of-technical-ai-safety-teams</link>
<guid>https://www.lesswrong.com/posts/QzQQvGJYDeaDE4Cfg/talent-needs-of-technical-ai-safety-teams</guid>
<content:encoded><![CDATA[
<div> MATS, AI safety, Connectors, Iterators, Amplifiers
总结:<br /><br />这篇文章主要讨论了AI安全领域的人才需求和发展方向。通过对AI安全领域的关键人物进行访谈，提出了三种人才类型：Connectors（连接者）、Iterators（迭代者）和Amplifiers（增幅者）。文章强调了不同类型的组织对这些人才的需求，以及在AI Safety领域建设过程中的重要性。同时，讨论了识别和培养这些人才的方法，以及MATs计划在此过程中扮演的角色。最后，总结了对人才发展的关键观点，以指导未来的工作。 <div>
Published on May 24, 2024 12:36 AM GMT<br /><br /><p>Co-Authors: <a href="https://www.lesswrong.com/users/yams?mention=user">@yams</a>, <a href="https://www.lesswrong.com/users/carson-jones?mention=user">@Carson Jones</a>, <a href="https://www.lesswrong.com/users/mckennafitzgerald?mention=user">@McKennaFitzgerald</a>, <a href="https://www.lesswrong.com/users/ryankidd44?mention=user">@Ryan Kidd</a>&nbsp;</p><p><a href="https://matsprogram.org/">MATS</a>&nbsp;tracks the evolving landscape of AI safety<span class="footnote-reference" id="fnref4lffvflk866"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn4lffvflk866">[1]</a></sup></span>&nbsp;to ensure that our program continues to meet the talent needs of safety teams. As the field has grown, it’s become increasingly necessary to adopt a more formal approach to this monitoring, since relying on a few individuals to intuitively understand the dynamics of such a vast ecosystem could lead to significant missteps.<span class="footnote-reference" id="fnref457034e2an8"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn457034e2an8">[2]</a></sup></span></p><p>In the winter and spring of 2024, we conducted 31 interviews, ranging in length from 30 to 120 minutes, with key figures in AI safety, including senior researchers, organization leaders, social scientists, strategists, funders, and policy experts. This report synthesizes the key insights from these discussions. The overarching perspectives presented here are not attributed to any specific individual or organization; they represent a collective, distilled consensus that our team believes is both valuable and responsible to share. Our aim is to influence the trajectory&nbsp;of emerging researchers and field-builders, as well as to inform readers on the ongoing evolution of MATS and the broader AI Safety field.</p><p>All interviews were conducted on the condition of anonymity.</p><h1>Needs by Organization Type</h1><figure class="table"><table><tbody><tr><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;"><strong>Organization type</strong></td><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;"><strong>Talent needs</strong></td></tr><tr><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Scaling Lab&nbsp;(e.g., Anthropic, Google DeepMind, OpenAI) Safety Teams</td><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Iterators &gt; Amplifiers</td></tr><tr><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Small Technical Safety Orgs (&lt;10 FTE)</td><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Iterators &gt; Machine Learning (ML) Engineers</td></tr><tr><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Growing Technical Safety Orgs (10-30 FTE)</td><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Amplifiers &gt; Iterators</td></tr><tr><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Independent Research</td><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Iterators &gt; Connectors</td></tr></tbody></table></figure><p>Here, "&gt;" means "are prioritized over."</p><h1>Archetypes</h1><p>We found it useful to frame the different profiles of research strengths and weaknesses as belonging to one of three archetypes (one of which has two subtypes).<strong>&nbsp;</strong>These aren’t as strict as, say, <a href="https://diablo.fandom.com/wiki/Classes">Diablo classes</a>;&nbsp;this is just a way to get some handle on the complex network of skills involved in AI safety research. Indeed, capacities tend to converge with experience, and neatly classifying more experienced researchers often isn’t possible.&nbsp;We acknowledge past framings by <a href="https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment">Charlie Rogers-Smith and Rohin Shah</a>&nbsp;(research lead/contributor), <a href="https://www.lesswrong.com/posts/nvP28s5oydv8RjF9E/mats-models#John_s_Model">John Wentworth</a>&nbsp;(theorist/experimentalist/distillator), <a href="https://www.lesswrong.com/posts/8HYJwQepynHsRKr6j/critical-review-of-christiano-s-disagreements-with-yudkowsky">Vanessa Kosoy</a>&nbsp;(proser/poet), <a href="https://www.lesswrong.com/posts/4BpeHPXMjRzopgAZd/mosaic-and-palimpsests-two-shapes-of-research">Adam Shimi</a>&nbsp;(mosaic/palimpsests), and others, but believe our framing of current AI safety talent archetypes is meaningfully different and valuable, especially pertaining to current funding and employment opportunities.</p><h2>Connectors / Iterators / Amplifiers</h2><p><strong>Connectors&nbsp;</strong>are strong conceptual thinkers who build a bridge between contemporary empirical work and theoretical understanding.&nbsp;Connectors include people like Paul Christiano, Buck Shlegeris,&nbsp;Evan Hubinger, and Alex Turner<span class="footnote-reference" id="fnref04bpkiufx7hu"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn04bpkiufx7hu">[3]</a></sup></span>; researchers doing original thinking on the edges of our conceptual and experimental knowledge in order to facilitate novel understanding. Note that most Connectors are typically not purely<i><strong>&nbsp;</strong></i>theoretical; they still have the technical knowledge required to design and run experiments. However, they<i><strong>&nbsp;</strong></i><strong>prioritize experiments and discriminate between research agendas based on original, high-level insights and theoretical models,</strong><i><strong> </strong></i>rather than on spur of the moment intuition or the wisdom of the crowds. Pure Connectors often have a long lead time&nbsp;before they’re able to produce impactful work, since it’s usually necessary for them to download&nbsp;and engage with varied conceptual models. For this reason, we make little mention of a division between experienced and inexperienced Connectors.</p><p><strong>Iterators </strong>are strong empiricists who build tight, efficient feedback loops for themselves and their collaborators. Ethan Perez is the central contemporary example here; his efficient&nbsp;prioritization&nbsp;and effective use of frictional time has empowered him to make major contributions to a wide range of empirical projects. Iterators do not, in all cases, have the conceptual grounding or single-agenda fixation of most Connectors; however, they can develop robust research taste (as Ethan arguably&nbsp;has) through experimental iteration and engagement with the broader AI safety conversation. Neel Nanda, Chris Olah, and Dan Hendrycks&nbsp;are also examples of this archetype.</p><p><strong>Experienced Iterators</strong>&nbsp;often navigate intuitively, and are able to act on experimental findings without the need to formalize them. They make strong and varied predictions for how an experiment will play out, and know exactly how they’ll deploy their available computing resources the moment they’re free. Even experienced Iterators <strong>update often based on information they receive from the feedback loops they’ve constructed</strong>,<i><strong>&nbsp;</strong></i>both experimentally and socially. Early on, they may be content to work on something simply because they’ve heard it’s useful, and may pluck a lot of low hanging fruit; later, they become more discerning and ambitious.</p><p><strong>Amplifiers&nbsp;</strong>are people with enough context, competence, and technical facility to prove useful as researchers, but who really shine as communicators, people managers, and project managers. A good Amplifier doesn’t often engage in the kind of idea generation native to Connectors and experienced Iterators, but excels at many other functions of leadership, either in a field-building role or as lieutenant to someone with stronger research taste. Amplifier impact is multiplicative; regardless of their official title, their soft skills help them <strong>amplify the impact of whoever they ally themselves with</strong>. Most field-building orgs are staffed by Amplifiers, MATS included.</p><p>We’ll be using “Connector,” “Iterator,” and “Amplifier” as though they were themselves professions, alongside more ordinary language like “software developer” or “ML engineer”.</p><h1>Needs by Organization Type (Expanded)</h1><p>In addition to independent researchers, there are broadly four types&nbsp;of orgs<span class="footnote-reference" id="fnrefxp1hq5ek6ka"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnxp1hq5ek6ka">[4]</a></sup></span>&nbsp;working directly on AI safety research:</p><ul><li>Scaling Labs</li><li>Small Technical Orgs (&lt;10 FTE)</li><li>Growing technical Orgs (10-30 FTE)</li><li>Academic Labs</li><li>Governance Orgs</li></ul><p>Interviewees currently working at <strong>scaling labs </strong>were most excited to hire experienced Iterators with a strong ML background. Scaling lab safety teams have a large backlog of experiments they would like to run and questions they would like to answer which have not yet been formulated as experiments, and experienced Iterators could help clear that backlog. In particular, Iterators with sufficient experience to design and execute an experimental regimen without formalizing intermediate results could make highly impactful contributions within this context.</p><p>Virtually all roles at scaling labs have a very high bar for software development skill; many developers, when dropped into a massive codebase like that of Anthropic, DeepMind, or OpenAI, risk drowning. Furthermore, having strong software developers in every relevant position pays dividends into the future, since good code is easier to scale and iterate on, and virtually everything the company does involves using code written internally.</p><p>Researchers working at or running <strong>small orgs</strong>&nbsp;had, predictably, more varied needs, but still converged on more than a few points. Iterators with some (although not necessarily a lot of) experience are in high demand here. Since most small labs are built around the concrete vision of one Connector, who started their org so that they might build a team to help chase down their novel ideas, additional Connectors are in very low demand at smaller orgs.</p><p>Truly small orgs (&lt;10 FTE employees) often don’t have a strong need for Amplifiers, since they are generally funding-constrained and usually possess the requisite soft skills among their founding members. However, as orgs interviewed approached ~20 FTE employees, they appeared to develop a strong need for Amplifiers who could assist with people management, project management, and research management, but who didn’t necessarily need to have the raw experimental ideation and execution speed of Iterators or vision of Connectors (although they might still benefit from either).</p><p><strong>Funders of independent researchers</strong>&nbsp;we’ve interviewed think that there are plenty of talented applicants, but would prefer more research proposals focused on relatively few existing promising research directions (e.g., <a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">Open Phil RFPs,</a> <a href="https://www.matsprogram.org/mentors">MATS mentors</a>' agendas), rather than a <a href="https://www.lesswrong.com/posts/qAdDzcBuDBLexb4fC/the-neglected-approaches-approach-ae-studio-s-alignment">profusion of speculative new agendas</a>. This leads us to believe that they would also prefer that independent researchers be approaching their work from an Iterator mindset, locating plausible contributions they can make within established paradigms, rather than from a Connector mindset, which would privilege time spent developing novel approaches.</p><p>Representatives&nbsp;from <strong>academia </strong>also expressed a dominant need for more Iterators,&nbsp;but rated Connectors more highly than did scaling labs or small orgs. In particular, academia highly values research that connects the current transformer-based deep learning paradigm of ML to the existing concepts and literature on artificial intelligence, rather than research that treats solving problems specific to transformers as an end in itself. This is a key difference about work in academia in general; the transformer-based architecture is just one of many live paradigms&nbsp;that academic researchers address, owing to the breadth of the existing canon of academic work on artificial intelligence, whereas it is <i>the core&nbsp;object of consideration<strong> </strong></i>at scaling labs and most small safety orgs.</p><h1>Impact, Tractability, and Neglectedness&nbsp;(ITN)</h1><p>Distilling the results above into ordinal rankings may help us identify priorities. First, let’s look at the availability of the relevant talent in the general population, to give us a sense of how useful pulling talent from the broader pool might be for AI safety. Importantly, the questions are:</p><ul><li>“How impactful does the non-AI-safety labor market consider this archetype in general?”</li><li>“How tractable is it for the outside world to develop this archetype in a targeted way?”</li><li>“How neglected is the development of this archetype <i>in a way that is useful for AI safety</i>?”</li></ul><p>On the job market in general, Connectors are high-impact dynamos that disrupt fields and industries. There’s no formula for generating or identifying Connectors (although there’s a booming industry trying to sell you such a formula) and, downstream of this difficulty in training the skillset, the production of Connectors is highly neglected.</p><p>Soft skills are abundant in the general population relative to AI safety professionals. Current-year business training focuses heavily on soft skills like communication and managerial acumen.&nbsp;Training for Amplifiers is somewhat transferable between fields, making their production extremely tractable. Soft skills are often best developed through general work experience, so targeted development of Amplifiers in AI safety might be unnecessary.</p><p>Iterators seem quite impactful, although producing more is somewhat less tractable than for Amplifiers, since their domain-specific skills must run quite deep. As a civilization, we train plenty of people like this at universities, labs, and bootcamps, but these don’t always provide the correct balance of research acumen and raw coding speed necessary for the roles in AI safety that demand top-performing Iterators.</p><p>Hiring AI-safety-specific Connectors from the general population is nearly impossible, since by far the best training for reasoning about AI safety is spending a lot of time reasoning about AI safety. Hiring Iterators from the general talent pool is easier, but can still require six months or more of upskilling, since deep domain-specific knowledge is very important. Amplifiers, though, are in good supply in the general talent pool, and orgs often successfully hire Amplifiers with limited prior experience in or exposure to AI safety.</p><h2>ITN Within AIS Field-building</h2><p>Within AI Safety, the picture looks very different. Importantly, <strong>this prioritization only holds at this moment</strong>; predictions about future talent needs from interviewees didn’t consistently point in the same direction.</p><p>Most orgs expressed interest in Iterators joining the team, and nearly every org expects to benefit from Amplifiers as they (and the field) continue to scale. Few orgs showed much interest in Connectors, although most would make an exception if an experienced researcher with a strong track record of impactful ideas asked to join.</p><p>The development of Iterators is relatively straightforward: you take someone with proven technical ability and an interest in AI Safety, give them a problem to solve and a community to support them, and you can produce an arguably useful researcher relatively quickly. The development of Amplifiers is largely handled by external professional experience, augmented by some time spent building context and acclimating to the culture of the AI safety community.</p><p>The development of Connectors, as previously discussed, takes a large amount of time and resources, since you only get better at reasoning about AI safety by reasoning about AI safety, which is best done in conversation with a diverse group of AI safety professionals (who are, by and large, time-constrained and work in gated-access communities). Therefore, doing this type of development, at sufficient volume, with few outputs along the way, is very costly.</p><p>We’re not seeing a sufficient influx of Amplifiers from other fields or ascension of technical staff into management positions to meet the demand at existing AI safety organizations. This is a sign that we should either augment professional outreach efforts or consider investing more in developing the soft skills of people who have a strong interest in AI safety. Unfortunately, the current high demand for Iterators at orgs seems to imply that their development is not receiving sufficient attention, either. Finally, that so few people are expressing interest in hiring Connectors, relative to the apparent high numbers of aspiring Connectors applying to MATS and other field-building programs, tells us that the ecosystem is potentially attracting an excess of inexperienced Connectors who may not be adequately equipped to refine their ideas or take on leadership positions in the current job and funding market.</p><p>Several interviewees at growing orgs who are currently looking for an Amplifier with strong research vision and taste noted that vision and taste seem to be anticorrelated with collaborative skills like “compromise.” The roles they’re hiring for strictly require those collaborative skills, and merely <i>benefit </i>from<i>&nbsp;</i>research taste and vision, which can otherwise be handled by existing leadership. This observation compelled them to seek, principally, people with strong soft skills and some familiarity with AI safety, rather than people with strong opinions on AI safety&nbsp;strategy who might not cooperate as readily with the current regime. This leads us to believe that developing Connectors might benefit from tending to their soft skills and willingness to compromise for the sake of team collaboration.</p><h1>So How Do You Make an AI Safety Professional?</h1><p>MATS does its best to identify and develop AI safety talent. Since, in most cases, it takes years to develop the skills to meaningfully contribute to the field, and the MATS research phase only lasts 10 weeks, identification does a lot of the heavy lifting here. It’s more reliable for us to select applicants that are 90 percent of the way there than to spin up even a very fast learner from scratch.</p><p>Still, the research phase itself enriches promising early-stage researchers by providing them with the time, resources, community, and guidance to help amplify their impact. Below we discuss the three archetypes, their respective developmental narratives, and how MATS might fit into those narratives.</p><h2>The Development of a Connector</h2><p>As noted above, Connectors have high-variance impact; inexperienced Connectors tend not to contribute much, while experienced Connectors can facilitate field-wide paradigm shifts. I asked one interviewee, “Could your org benefit from another “big ideas” guy?” They replied, “The ideas would have to be <i>really good</i>;&nbsp;<i>there are a lot of ‘idea guys’ around who don’t actually have very good ideas.</i>”</p><p>Experience and seniority seemed to track with interviewees’ appraisal of a given Connector’s utility, but not always. Even some highly venerated names in the space who fall into this archetype might not be a good fit at a particular org, since leadership’s endorsement of a given Connector’s particular ideas might bound that individual’s contributions.</p><p>Interviewees repeatedly affirmed that the conceptual skills required of Connectors don’t fully mature through study and experimental&nbsp;experience alone. Instead, Connectors tend to pair an extensive knowledge of the literature with a robust network of interlocutors whom they regularly debate to refine their perspective. Since Connectors are more prone to <a href="https://www.lesswrong.com/posts/bMkCEZoBNhgRBtzoj/anchoring-and-adjustment">anchoring</a>&nbsp;than Iterators, communally&nbsp;stress-testing and refining ideas shapes initial intuitions into actionable threat models, agendas, and experiments.</p><p>The deep theoretical models characteristic of Connectors allow for the development of rich, overarching predictions about the nature of AGI and the broad strokes of possible alignment strategies. Many Connectors, particularly those with intuitions rooted in models of superintelligent cognition, build models of AGI risk that are not yet empirically updateable. Demonstrating an end-to-end model of AGI risk seems to be regarded as “high-status,” but is very hard to do with predictive accuracy. Additionally, over-anchoring on a&nbsp;theoretical model&nbsp;without doing the public-facing work necessary to make it intelligible to the field at large can cause a pattern of rejection that stifles both contribution and development.</p><p>Identifying Connectors is extremely difficult ex-ante. Often it’s not until someone is actively contributing to or, at least, regularly conversing with others in the field that their potential is recognized. Some interviewees felt that measures of general intelligence are sufficient for identifying a strong potential Connector, or that CodeSignal programming test scores would generalize across a wide array of tasks relevant to reasoning about AI safety. This belief, however, was rare, and extended conversations (usually over the course of weeks) with multiple experts in the field appeared to be the most widely agreed upon way to reliably identify high-impact Connectors.</p><p>One interviewee suggested that, if targeting Connectors, MATS should perform interviews with all applicants that pass an initial screen, and that this would be more time efficient and cost effective (and more accurate) than relying on tests or selection questions. Indeed, some mentors already conduct an interview with over 10 percent of their applicant pool, and use this as their key desideratum when selecting scholars.</p><h2>The Development of an Iterator</h2><p>Even inexperienced Iterators can make strong contributions to teams and agendas with large empirical workloads. What’s more, Iterators have almost universally proven themselves in fields beyond AI safety prior to entering the space, often as high-throughput engineers in industry or academia.</p><p>Gaining experience as an Iterator means chugging through a high volume of experiments while simultaneously engaging in the broader discourse of the field to help refine both your research taste and intuitive sense for generating follow-up experiments. This isn’t a guaranteed formula; some Iterators will develop at an accelerated pace, others more slowly, and some may never lead teams of their own. However, this developmental roadmap means making increasingly impactful contributions to the field continuously, much earlier than the counterfactual Connector.</p><p>Iterators are also easier to identify, both by their resumes and demonstrated skills. If you compare two CVs of postdocs that have spent the same amount of time in academia, and one of them has substantially more papers (or GitHub commits) to their name than the other (controlling for quality), you’ve found the better Iterator. Similarly, if you compare two CodeSignal tests with the same score but different completion times, the one completed more quickly belongs to the stronger Iterator.</p><h2>The Development of an Amplifier</h2><p>Amplifiers usually occupy non-technical roles, but often have non-zero technical experience. This makes them better at doing their job in a way that serves the unique needs of the field, since they understand the type of work being done, the kinds of people involved, and how to move through the space fluidly.&nbsp;There are a great many micro-adjustments that non-technical workers in AI safety make in order to perform optimally in their roles, and this type of cultural fluency may be somewhat anticorrelated with the soft skills that every org needs to scale, leading to seasonal operational and managerial bottlenecks field-wide.</p><p>Great amplifiers will do whatever most needs doing, regardless of its perceived status. They will also anticipate an organization’s future needs and readily adapt to changes at any scale. A single Amplifier will often have an extremely varied background, making it difficult to characterize exactly what to look for. One strong sign is management experience, since often the highest impact role for an Amplifier is as auxiliary executive function, project management, and people management at a fast-growing org.</p><p>Amplifiers mature through direct on-the-job experience, in much the way one imagines traditional professional development. As ability increases, so does responsibility. Amplifiers may find that studying management and business operations, or even receiving management coaching or consulting, helps accentuate their comparative advantage. To build field-specific knowledge, they may consider <a href="https://aisafetyfundamentals.com/">AI Safety Fundamentals</a>&nbsp;(AISF) or, more ambitiously, <a href="https://github.com/callummcdougall/ARENA_3.0">ARENA</a>.</p><h1>So What is MATS Doing?</h1><p>We intend this section to give some foundational information about the directions we were already considering before engaging in our interviews, and to better contextualize our key updates from this interview series.</p><p>At its core, MATS is a mentorship program, and the most valuable work happens between a scholar and their mentor. However, there are some things that will have utility to most scholars, such as networking opportunities, forums for discussion, and exposure to emerging ideas from seasoned researchers. It makes sense for MATS to try to provide that class of things directly. In that spirit, we’ve broadly tried three types of supporting programming, with varied results.</p><p><strong>Mandatory programming </strong>doesn’t tend to go over well. When required to attend seminars in MATS 3.0, scholars reported lower average value of seminars than scholars in 4.0 or 5.0, where seminars were opt-in. Similarly, when required to read the AISF curriculum and attend discussion groups, scholars reported lower value than when a similar list of readings was made available to them optionally. Mandatory programming, of any kind, doesn’t just trade off against, but actively <i>bounds<strong> </strong></i>scholar research time by removing their choice. We feel strongly that scholars know what’s best for them and we want to support their needs.</p><p>In observance of the above, we’ve tried a lot of <strong>optional programming</strong>. Optional programming goes better than mandatory programming, in that scholars are more likely to attend because they consider the programming valuable (rather than showing up because they have to), and so report a better subjective experience. However, it’s still imperfect; seminar and discussion group attendance are highest at the start of the program, and slowly decline as the program progresses and scholars increasingly prioritize their research projects. We also think that optional programming often performs a social function early on and, once scholars have made a few friends and are comfortable structuring their own social lives in Berkeley, they’re less likely to carve out time for readings, structured discussions, or a presentation.</p><p>The marginal utility&nbsp;to scholars of additional optional programming elements seems to decline as the volume of optional programming increases. For example, in 4.0 we had far more seminars than in 5.0, and seminars in 4.0 had a lower average rating and attendance. We think this is both because we prioritized our top-performing speakers for 5.0 and because scholars viewed seminars more as novel opportunities, rather than “that thing that happens 8 times a week and often isn’t actually that relevant to my specific research interests.” <strong>Optional programming seems good up to some ceiling</strong>, beyond which returns are limited (or even negative).</p><p>MATS also offers a lot of <strong>informal resources</strong>. Want to found an org? We’ve got some experience with that. Need help with career planning? We’ve got experience there, too. Meetings with our research managers&nbsp;help, among other things, embed scholars in the AI safety professional network so that they’re not limited to their mentors’ contributions to their professional growth and development. In addition to their core responsibility of directly supporting scholar research projects, research managers serve as a gateway to far-reaching resources and advice outside the explicit scope of the program. A research manager might direct you to talk to another team member about a particular problem, or connect you with folks outside of MATS if they feel it’s useful. These interventions are somewhat inefficient and don’t often generalize, but can have transformative implications for the right scholar.</p><p><strong>For any MATS scholar, the most valuable things they can spend their time on are research and networking. </strong>The ceiling on returns for time spent in either is very high. With these observations in mind, we’ve already committed internally to offering a lower overall volume of optional programming and focusing more on proactively developing an internal compendium of resources suited to situations individual scholars may find themselves in.</p><p>For our team, there are three main takeaways regarding scholar selection and training:</p><ol><li><strong>Weight our talent portfolio toward Iterators</strong>&nbsp;(knowing that, with sufficient experience, they’ll often fit well even in Connector-shaped roles), since they’re comparatively easy to identify, train, and place in impactful roles in existing AI safety labs.</li><li><strong>Avoid making decisions that might select strongly against Amplifiers, </strong>since they’re definitely in demand, and existing initiatives to either poach or develop them don’t seem to satisfy this demand.&nbsp;Amplifiers are needed to grow existing AI safety labs and found new organizations, helping create employment opportunities&nbsp;for Connectors and Iterators.</li><li><strong>Foster an environment that facilitates the self-directed development of Connectors, </strong>who require consistent, high-quality contact with others working in the field in order to develop field-specific reasoning abilities, but who otherwise don’t benefit much from one-size-fits-all education. Putting too much weight on the short-term outputs of a given Connector is a disservice to their development, and for Connectors MATS should be considered less as a bootcamp and more as a residency program.</li></ol><p>This investigation and its results are just a small part of the overall strategy and direction at MATS. We’re constantly engaging with the community, on all sides, to improve our understanding of how we best fit into the field as a whole, and are in the process of implementing many considered changes to help address other areas in which there’s room for us to grow.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QzQQvGJYDeaDE4Cfg/pjbazll2q1szjdcr4pt8" /></figure><h1>Acknowledgements</h1><p>This report was produced by the <a href="https://www.matsprogram.org/">ML Alignment &amp; Theory Scholars Program</a>. <a href="https://www.lesswrong.com/users/yams?mention=user">@yams</a> and Carson Jones were the primary contributors to this report, Ryan Kidd scoped, managed, and edited the project, and McKenna Fitzgerald advised throughout. Thanks to our interviewees for their time and support. We also thank <a href="https://www.openphilanthropy.org/">Open Philanthropy</a>, DALHAP Investments,&nbsp;the <a href="https://survivalandflourishing.fund/speculation-grants.html#sffs-speculation-grants-program">Survival and Flourishing Fund Speculation Grantors</a>, and several generous donors on <a href="https://manifund.org/">Manifund</a>, without whose donations we would be unable to run upcoming programs or retain <a href="https://www.matsprogram.org/team">team members</a>&nbsp;essential to this report.</p><p>To learn more about MATS, please visit our <a href="http://matsprogram.org/">website</a>. We are currently <a href="https://manifund.org/projects/mats-funding">accepting donations</a>&nbsp;for our Winter 2024-25 Program and beyond!</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fn4lffvflk866"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref4lffvflk866">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;AI Safety is a somewhat underspecified term, and when we use ‘AI safety’ or ‘the field’ here, we mean <strong>technical </strong>AI safety, which has been the core focus of our program up to this point. Technical AI safety, in turn, here refers to the subset of AI safety research that takes current&nbsp;and future&nbsp;technological paradigms as its chief objects of study, rather than governance, policy, or ethics. Importantly, this does not exclude all theoretical approaches, but does&nbsp;in practice&nbsp;prefer those theoretical approaches which have a strong foundation in experimentation.&nbsp;Due to the dominant focus on <a href="https://www.lesswrong.com/posts/YTq4X6inEudiHkHDF/prosaic-ai-alignment">prosaic AI safety</a> within the current job and funding market, the main focus of this report, we believe there are few opportunities for those pursuing non-prosaic, theoretical AI safety research.</p></div></li><li class="footnote-item" id="fn457034e2an8"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref457034e2an8">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;The initial impetus for this project was an investigation into the oft-repeated claim that AI safety is principally bottlenecked by research leadership. In the preliminary stages of our investigation, we found this to be somewhat, though not entirely, accurate. It mostly applies in the case of mid-sized orgs looking for additional leadership bandwidth, and even there&nbsp;soft skills are often more important than meta-level insights. Most smaller AI safety orgs form around the vision of their founders and/or acquire senior advisors fairly early on, and so have quite a few ideas to work with.</p></div></li><li class="footnote-item" id="fn04bpkiufx7hu"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref04bpkiufx7hu">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;These examples are not exhaustive, and few people fit purely into one category or another (even if we listed them here as chiefly belonging to a particular archetype). Many influential researchers whose careers did not, to us, obviously fit into one category or another have been omitted.</p></div></li><li class="footnote-item" id="fnxp1hq5ek6ka"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefxp1hq5ek6ka">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;In reality, many orgs are engaged in some combination of these activities, but grouping this way did help us to see some trends. At present, we’re not confident we pulled enough data from governance orgs to include them in the analysis here, but we think this is worthwhile and are devoting some additional time to that angle on the investigation. We may share further results in the future.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/QzQQvGJYDeaDE4Cfg/talent-needs-of-technical-ai-safety-teams#comments">Discuss</a>
]]></content:encoded>
<pubDate>Fri, 24 May 2024 02:15:53 GMT</pubDate>
</item>
<item>
<title>Paper in Science: Managing extreme AI risks amid rapid progress</title>
<link>https://www.lesswrong.com/posts/82f3o2SuS3pwaZt8Y/paper-in-science-managing-extreme-ai-risks-amid-rapid</link>
<guid>https://www.lesswrong.com/posts/82f3o2SuS3pwaZt8Y/paper-in-science-managing-extreme-ai-risks-amid-rapid</guid>
<content:encoded><![CDATA[
<div> 关键词：人工智能、风险、自治系统、治理机制、AI安全研究  
总结：<br /><br />总结: 人工智能技术迅速发展，企业正转向开发能够自主行动和追求目标的通用性人工智能系统。随着能力和自治性的增加，AI的影响可能会大规模放大，其风险包括大规模社会危害、恶意使用以及人类对自主AI系统失去控制的不可逆转。研究人员已经对AI带来的极端风险发出了警告，但如何管理这些风险却缺乏共识。尽管社会做出了一些有益的初步举措，但对许多专家所预期的快速、转变性进展的可能性做出的反应依然不足。AI安全研究滞后，当前的治理倡议缺乏防止滥用和鲁莽行为的机制和机构，几乎没有涉及自治系统。通过借鉴其他安全关键技术的经验教训，我们概述了一个综合计划，结合技术研究和开发，以及积极的、适应性的治理机制，更好地准备迎接可能发生的变革。 <div>
Published on May 23, 2024 8:40 AM GMT<br /><br /><p><a href="https://www.science.org/doi/10.1126/science.adn0117">https://www.science.org/doi/10.1126/science.adn0117</a></p><p><strong><u>Authors:</u></strong></p><p>Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter, Atılım Güneş Baydin, Sheila McIlraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Brauner*, Sören Mindermann*</p><p><strong>Abstract:</strong></p><p>Artificial intelligence (AI) is progressing rapidly, and companies are shifting their focus to developing generalist AI systems that can autonomously act and pursue goals. Increases in capabilities and autonomy may soon massively amplify AI’s impact, with risks that include large-scale social harms, malicious uses, and an irreversible loss of human control over autonomous AI systems. Although researchers have warned of extreme risks from AI, there is a lack of consensus about how to manage them. Society’s response, despite promising first steps, is incommensurate with the possibility of rapid, transformative progress that is expected by many experts. AI safety research is lagging. Present governance initiatives lack the mechanisms and institutions to prevent misuse and recklessness and barely address autonomous systems. Drawing on lessons learned from other safety-critical technologies, we outline a comprehensive plan that combines technical research and development with proactive, adaptive governance mechanisms for a more commensurate preparation.<br />&nbsp;</p><br /><br /><a href="https://www.lesswrong.com/posts/82f3o2SuS3pwaZt8Y/paper-in-science-managing-extreme-ai-risks-amid-rapid#comments">Discuss</a>
]]></content:encoded>
<pubDate>Thu, 23 May 2024 23:06:31 GMT</pubDate>
</item>
<item>
<title>Big Picture AI Safety: Introduction</title>
<link>https://www.lesswrong.com/posts/yMTNjeEHfHcf2x7nY/big-picture-ai-safety-introduction</link>
<guid>https://www.lesswrong.com/posts/yMTNjeEHfHcf2x7nY/big-picture-ai-safety-introduction</guid>
<content:encoded><![CDATA[
<div> AI safety experts, strategic view, big picture, interviews, key questions

人工智能安全专家通过17次半结构化访谈，探讨了人工智能安全领域的战略视角：人级别人工智能的发展预期，可能出现问题的情况，以及人工智能安全社区应采取的行动。虽然许多受访者持有传统观点（例如，主要威胁是人工智能不对齐接管），但对这些标准观点的反对程度高于我预期的，该领域在许多重要问题上似乎存在更多分歧。受访者期望首个人级别人工智能（HLAI）将与当前大型语言模型（LLM）相同，符合某些新调整和突破，主要担心人类这些AI系统带来的灾难，强调技术解决方案、普及安全心态、推进理性AI研究等。AI安全运动过去可能犯的错误包括过多依赖过于理论的论证、过度孤立、推动奇怪或极端观点、支持领先的AGI公司导致竞争动态等。AI安全领域看法多样，未来仍需深入讨论和探索。<br /><br />总结: <div>
Published on May 23, 2024 11:15 AM GMT<br /><br /><p><i>tldr: I conducted 17 semi-structured interviews of AI safety experts about their big picture strategic view of the AI safety landscape: how will human-level AI play out, how things might go wrong, and what should the AI safety community be doing. While many respondents held “traditional” views (e.g. the main threat is misaligned AI takeover), there was more opposition to these standard views than I expected, and the field seems more split on many important questions than someone outside the field may infer.</i></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yMTNjeEHfHcf2x7nY/fr8ufoxbpttlz7qur6lt" /></figure><p>What do AI safety experts believe about the big picture of AI risk? How might things go wrong, what we should do about it, and how have we done so far? Does everybody in AI safety agree on the fundamentals? Which views are consensus, which are contested and which are fringe? Maybe we could learn this from the literature (as in the <a href="https://www.lesswrong.com/s/aERZoriyHfCqvWkzg">MTAIR</a>&nbsp;project), but many ideas and opinions are not written down anywhere, they exist only in people’s heads and in lunchtime conversations at AI labs and coworking spaces.</p><p>I set out to learn what the AI safety community believes about the strategic landscape of AI safety. I conducted 17 semi-structured interviews with a range of AI safety experts. I avoided going into any details of particular technical concepts or philosophical arguments, instead focussing on how such concepts and arguments fit into the big picture of what AI safety is trying to achieve.</p><p>This work is similar to the AI Impacts <a href="https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai">surveys</a>, Vael Gates’ <a href="https://arkose.org/about#ai_risk_discussions">AI Risk Discussions</a>, and Rob Bensinger’s <i>existential risk from AI </i><a href="https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results">survey</a>. This is different to those projects in that both my approach to interviews and analysis are more&nbsp;qualitative. Part of the hope for this project was that it can hit on harder-to-quantify concepts that are too ill-defined or intuition-based to fit in the format of previous survey work.</p><h1>Questions</h1><p>I asked the participants a standardized list of questions.</p><ul><li><strong>What will happen?</strong><ul><li><strong>Q1</strong>&nbsp;Will there be a human-level AI? What is your modal guess of what the&nbsp;first human-level AI (HLAI) will look like? I define HLAI as an AI system that can carry out roughly 100% of economically valuable cognitive tasks more cheaply than a human.<ul><li><strong>Q1a</strong>&nbsp;What’s your 60% or 90% confidence interval for the date of the first HLAI?</li></ul></li><li><strong>Q2</strong>&nbsp;Could AI bring about an existential catastrophe? If so, what is the most likely way this could happen?<ul><li><strong>Q2a</strong>&nbsp;What’s your best guess at the probability of such a catastrophe?</li></ul></li></ul></li><li><strong>What should we do?</strong><ul><li><strong>Q3</strong>&nbsp;Imagine a world where, absent any effort from the AI safety community, an existential catastrophe happens, but actions taken by the AI safety community prevent such a catastrophe. In this world, what did we do to prevent the catastrophe?</li><li><strong>Q4</strong>&nbsp;What research direction (or other activity) do you think will reduce existential risk the most, and what is its theory of change? Could this backfire in some way?</li></ul></li><li><strong>What mistakes have been made?</strong><ul><li><strong>Q5</strong>&nbsp;Are there any big mistakes the AI safety community has made in the past or are currently making?</li></ul></li></ul><p>These questions changed gradually as the interviews went on (given feedback from participants), and I didn’t always ask the questions exactly as I’ve presented them here. I asked participants to answer from their internal model of the world as much as possible and to avoid deferring to the opinions of others (their <a href="https://www.lesswrong.com/tag/inside-outside-view">inside view</a>&nbsp;so to speak).</p><h1>Participants</h1><ul><li><a href="https://www.gleave.me/"><strong>Adam Gleave</strong></a>&nbsp;is the CEO and co-founder of the alignment research non-profit<a href="https://far.ai">&nbsp;FAR AI</a>. (Sept 23)</li><li><a href="https://agarri.ga/"><strong>Adrià Garriga-Alonso</strong></a><strong>&nbsp;</strong>is a research scientist at <a href="https://far.ai">FAR AI</a>. (Oct 23)</li><li><a href="https://www.openphilanthropy.org/about/team/ajeya-cotra/"><strong>Ajeya Cotra</strong></a>&nbsp;leads <a href="https://www.openphilanthropy.org/">Open Philantropy</a>’s grantmaking on technical research that could help to clarify and reduce catastrophic risks from advanced AI. (Jan 24)</li><li><a href="https://twitter.com/turn_trout?lang=en-GB"><strong>Alex Turner</strong></a><strong>&nbsp;</strong>is a research scientist at Google DeepMind on the Scalable Alignment team. (Feb 24)</li><li><a href="https://www.linkedin.com/in/ben-cottier/?originalSubdomain=uk"><strong>Ben Cottier</strong></a>&nbsp;is a researcher specializing in key trends and questions that will shape the trajectory and governance of AI at <a href="https://epochai.org/">Epoch AI</a>. (Oct 23)</li><li><a href="https://danielfilan.com/"><strong>Daniel Filan</strong></a><strong>&nbsp;</strong>is a PhD candidate&nbsp;at the <a href="http://humancompatible.ai/">Centre for Human-Compatible AI</a>&nbsp;under Stuart Russell and runs the <a href="https://axrp.net/">AXRP</a>&nbsp;podcast. (Feb 24)</li><li><a href="https://www.eng.cam.ac.uk/profiles/dsk30"><strong>David Krueger</strong></a>&nbsp;is an assistant professor in Machine Learning and Computer Vision at the University of Cambridge. (Feb 24)</li><li><a href="https://twitter.com/evanhub?lang=en"><strong>Evan Hubinger</strong></a>&nbsp;is an AI alignment stress-testing researcher at Anthropic. (Feb 24)</li><li><a href="https://www.law.utoronto.ca/faculty-staff/full-time-faculty/gillian-hadfield"><strong>Gillian Hadfield</strong></a>&nbsp;is a Professor of Law &amp; Strategic Management at the University of Toronto and holds a CIFAR AI Chair at the Vector Institute for Artificial Intelligence. (Feb 24)</li><li><a href="https://hollyelmore.substack.com/"><strong>Holly Elmore</strong></a>&nbsp;is currently running the US front of the <a href="https://pauseai.info/">Pause AI</a>&nbsp;Movement and previously worked at <a href="https://rethinkpriorities.org/">Rethink Priorities</a>. (Jan 24)</li><li><a href="https://jamiebernardi.com/"><strong>Jamie Bernardi</strong></a>&nbsp;co-founded <a href="https://bluedot.org/">BlueDot Impact</a>&nbsp;and ran the <a href="https://aisafetyfundamentals.com/">AI Safety Fundamentals</a>&nbsp;community, courses and website. (Oct 23)</li><li><a href="https://www.neelnanda.io/"><strong>Neel Nanda</strong></a>&nbsp;runs Google DeepMind’s mechanistic interpretability team. (Feb 24)</li><li><a href="https://norabelrose.com/"><strong>Nora Belrose</strong></a>&nbsp;is the head of interpretability research at <a href="https://www.eleuther.ai/">EleutherAI</a>. (Feb 24)</li><li><a href="https://twitter.com/noahysiegel?lang=en"><strong>Noah Siegel</strong></a>&nbsp;is a senior research engineer at Google DeepMind and a PhD candidate at University College London. (Jan 24)</li><li><a href="https://ojorgensen.github.io/"><strong>Ole Jorgensen</strong></a>&nbsp;is a member of technical staff at the UK Government’s AI Safety Institute (this interview was conducted before he joined). (Mar 23)</li><li><a href="https://www.richardcngo.com/"><strong>Richard Ngo</strong></a>&nbsp;is an AI governance researcher at OpenAI. (Feb 24)</li><li><a href="https://www.alignmentforum.org/users/ryan_greenblatt"><strong>Ryan Greenblatt</strong></a>&nbsp;is an AI safety researcher at the AI safety non-profit <a href="https://www.redwoodresearch.org/">Redwood Research</a>. (Feb 24)</li></ul><p><i>These interviews were conducted between March 2023 and February 2024, and represent their views at the time.</i></p><h1>A very brief summary of what people said</h1><h2>What will happen?</h2><p>Many respondents expected the first human-level AI (HLAI) to be in the same paradigm as current large language models (LLMs) like GPT-4, probably <a href="https://www.dwarkeshpatel.com/p/will-scaling-work">scaled up</a>&nbsp;(made bigger), with some new tweaks and hacks, and scaffolding like <a href="https://autogpt.net/">AutoGPT</a>&nbsp;to make it <a href="https://www.lesswrong.com/tag/agency">agentic</a>. But a smaller&nbsp;handful of people predicted that larger breakthroughs are required before HLAI. The most common story of how AI could cause an existential disaster was the story of <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like/#Part_II__influence_seeking_behavior_is_scary">unaligned AI takeover</a>, but some explicitly pushed back on the assumptions behind the takeover story. Some took a more <a href="https://www.lawfaremedia.org/article/thinking-about-risks-ai-accidents-misuse-and-structure">structural view</a>&nbsp;of AI risk, emphasizing threats like instability, extreme inequality, gradual human disempowerment, and a collapse of human institutions.</p><h2>What should we do about it?</h2><p>When asked how AI safety might prevent disaster, respondents focussed most on</p><ul><li>the technical solutions we might come up with,</li><li>spreading a safety mindset through AI research,</li><li>promoting sensible <a href="https://aiscc.org/2023/09/07/proposals-for-ai-regulation/">AI regulation</a>,</li><li>and helping build a fundamental science of AI.</li></ul><p>The research directions people were most excited about were <a href="https://distill.pub/2020/circuits/zoom-in/">mechanistic interpretability</a>, <a href="https://forum.effectivealtruism.org/posts/49rzRKh2ZYH2QjPkg/safety-evaluations-and-standards-for-ai-or-beth-barnes-or">black box evaluations</a>, and governance research.</p><h2>What mistakes have been made?</h2><p>Participants pointed to a range of mistakes they thought the AI safety movement had made. There was no consensus and the focus was quite different from person to person. The most common themes included:</p><ul><li>an overreliance on overly theoretical argumentation,</li><li>being too insular,</li><li>putting people off by pushing weird or extreme views,</li><li>supporting the leading AGI companies resulting in race dynamics,</li><li>not enough independent thought,</li><li>advocating for an unhelpful pause to AI development,</li><li>and historically ignoring policy as a potential route to safety.</li></ul><h1>Limitations</h1><ul><li>People had somewhat different interpretations of my questions, so they were often answering &nbsp;questions that were subtly different from each other.</li><li>The sample of people I interviewed is not necessarily a representative sample of the AI safety movement as a whole. The sample was pseudo-randomly selected, optimizing for a) diversity of opinion, b) diversity of background, c) seniority, and d) who I could easily track down. Noticeably, there is an absence of individuals from <a href="https://intelligence.org/">MIRI</a>, a historically influential AI safety organization, or those who subscribe to <a href="https://www.lesswrong.com/posts/GfZfDHZHCuYwrHGCd/without-fundamental-advances-misalignment-and-catastrophe">similar views</a>. I approached some MIRI team members but no one was available for an interview. This is especially problematic since many respondents criticized MIRI for various reasons, and I didn’t get much of a chance to integrate MIRI’s side of the story into the project.</li><li>There will also be a selection bias due to everyone I asked being at least somewhat bought into the idea of AI being an existential risk.</li><li>A handful of respondents disagreed with the goal of this project: they thought that those in AI safety typically spend too much time thinking about theories of impact.</li><li>There were likely a whole bunch of framing effects that I did not control for.</li><li>There was in some cases a large gap in time between the interview and this being written up (mostly between 1 and 4 months, a year for one early interview). Participant opinions may have changed over this period.</li></ul><h1>Subsequent posts</h1><p>In the following three posts, I present a condensed summary of my findings, describing the main themes that came up for each question:</p><ol><li><a href="https://www.lesswrong.com/posts/wHRMZizqfdW9RjrCY/what-will-the-first-human-level-ai-look-like-and-how-might"><strong>What will happen?</strong></a>&nbsp;What will human-level AI look like, and how might things go wrong?</li><li><a href="https://www.lesswrong.com/posts/XfnnkK8XEjTqtuXGM/what-should-ai-safety-be-trying-to-achieve"><strong>What should we do?</strong></a><strong> </strong>What should AI safety be trying to achieve and how?</li><li><a href="https://www.lesswrong.com/posts/cGzQBRDrpNHoYtbKN/what-mistakes-has-the-ai-safety-movement-made"><strong>What mistakes has the AI safety movement made?</strong></a></li></ol><p>You don’t need to have read an earlier post to understand a later one, so feel free to zoom straight in on what interests you.</p><p><i>I am very grateful to all of the participants for offering their time to this project. Also thanks to Vael Gates, Siao Si Looi, ChengCheng Tan, Adam Gleave, Quintin Davis, George Anadiotis, Leo Richter, McKenna Fitzgerald, Charlie Griffin and many of the participants for feedback on early drafts.</i></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yMTNjeEHfHcf2x7nY/owkuhranzbdmb8pov4ro" style="width: 31.55%;" /></p><p><i>This work was funded and supported by </i><a href="https://far.ai/"><i>FAR AI</i></a><i>.</i></p><br /><br /><a href="https://www.lesswrong.com/posts/yMTNjeEHfHcf2x7nY/big-picture-ai-safety-introduction#comments">Discuss</a>
]]></content:encoded>
<pubDate>Thu, 23 May 2024 22:51:54 GMT</pubDate>
</item>
<item>
<title>What mistakes has the AI safety movement made?</title>
<link>https://www.lesswrong.com/posts/cGzQBRDrpNHoYtbKN/what-mistakes-has-the-ai-safety-movement-made</link>
<guid>https://www.lesswrong.com/posts/cGzQBRDrpNHoYtbKN/what-mistakes-has-the-ai-safety-movement-made</guid>
<content:encoded><![CDATA[
<div> 过度理论化, 封闭, 消极消息传播, 与领先公司紧密合作, 缺乏独立思考

AI安全运动在过去或目前做出了一些错误，如过度依赖理论化论点、封闭性、传播消极观点、与领先公司合作过密、缺乏独立思考、支持无益的AI发展暂停、忽视政策作为安全途径。需要增加实证研究, 加强与外界联系, 改善信息传播, 减少与领先公司的关系, 增强独立思考, 重视公共政策。 <br /><br />总结: <br />过度理论化, 封闭不合, 消极消息传播, 与领先公司紧密合作, 缺乏独立思考, 忽视政策途径。 <div>
Published on May 23, 2024 11:19 AM GMT<br /><br /><p>This is the third of three posts summarizing what I learned when I interviewed 17 AI safety experts about their "big picture" of the existential AI risk landscape: how AGI will play out, how things might go wrong, and what the AI safety community should be doing. See <a href="https://www.lesswrong.com/posts/yMTNjeEHfHcf2x7nY/big-picture-ai-safety-introduction">here</a>&nbsp;for a list of the participants and the standardized list of questions I asked.</p><p>This post summarizes the responses I received from asking “Are there any big mistakes the AI safety community has made in the past or are currently making?”</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cGzQBRDrpNHoYtbKN/py1goh4sbpxmrgmlucjw" /><figcaption>A rough decompositions of the main themes brought up. The figures omit some less popular themes, and double-count respondents who brought up more than one theme.</figcaption></figure><blockquote><p>“<i>Yeah, probably most things people are doing are mistakes. This is just some random group of people. Why would they be making good decisions on priors? When I look at most things people are doing, I think they seem not necessarily massively mistaken, but they seem somewhat confused or seem worse to me by like 3 times than if they understood the situation better.</i>” - Ryan Greenblatt</p></blockquote><blockquote><p><i>“If we look at the track record of the AI safety community, it quite possibly has been harmful for the world.</i>” - Adam Gleave</p></blockquote><blockquote><p>“<a href="https://www.effectivealtruism.org/articles/longtermism"><i>Longtermism</i></a><i>&nbsp;was developed basically so that AI safety could be the most important cause by the utilitarian EA calculus. That's my take.</i>” - Holly Elmore</p></blockquote><p>Participants pointed to a range of mistakes they thought the AI safety movement had made. Key themes included an overreliance on theoretical argumentation, being too insular, putting people off by pushing weird or extreme views, supporting the leading AGI companies, insufficient independent thought, advocating for an unhelpful pause to AI development, and ignoring policy as a potential route to safety.</p><h1>How to read this post</h1><p>This is not a scientific analysis of a systematic survey of a representative sample of individuals, but my qualitative interpretation of responses from a loose collection of semi-structured interviews. Take everything here with the appropriate seasoning.</p><p>Results are often reported in the form “<i>N</i>&nbsp;respondents held view <i>X</i>”. This does <strong>not</strong>&nbsp;imply that “17-<i>N</i>&nbsp;respondents disagree with view <i>X</i>”, since not all topics, themes and potential views were addressed in every interview. What “<i>N </i>respondents held view <i>X</i>” tells us is that at least <i>N</i>&nbsp;respondents hold <i>X</i>, and consider the theme of <i>X</i>&nbsp;important enough to bring up.</p><p>The following is a summary of the main themes that came up in my interviews. Many of the themes overlap with one another, and the way I’ve clustered the&nbsp;criticisms&nbsp;is likely not the only reasonable categorization.</p><h1>Too many galaxy-brained arguments &amp; not enough empiricism</h1><blockquote><p>“<i>I don't find the long, abstract style of investigation particularly compelling.</i>” - Adam Gleave</p></blockquote><p>9 respondents were concerned about <strong>an overreliance or overemphasis on certain kinds of theoretical arguments underpinning AI risk</strong>: namely Yudkowsky’s arguments in <a href="https://www.lesswrong.com/tag/original-sequences">the sequences</a>&nbsp;and Bostrom’s arguments in <a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies">Superintelligence</a>.</p><blockquote><p>“<i>All these really abstract arguments that are very detailed, very long and not based on any empirical experience. [...]</i><br /><br /><i>Lots of trust in loose analogies, thinking that loose analogies let you reason about a topic you don't have any real expertise in. Underestimating the conjunctive burden of how </i><a href="https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/"><i>long and abstract</i></a><i>&nbsp;these arguments are. Not looking for ways to actually test these theories. [...]</i></p><p><i>You can see Nick Bostrom in Superintelligence stating that we shouldn't use RL to align an AGI because it trains the AI to maximize reward, which will lead to wireheading. The idea that this is an inherent property of RL is entirely mistaken. It may be an empirical fact that certain minds you train with RL tend to make decisions on the basis of some tight correlate&nbsp;of their reinforcement signal, but this is not some fundamental property of RL.</i>”</p><p>- Alex Turner</p></blockquote><p><a href="https://jamiebernardi.com/"><u>Jamie Bernardi</u></a> argued that the original view of what AGI will look like, namely&nbsp;<strong>an RL agent that will reason its way to general intelligence from first principles, is not the way things seem to be panning out</strong>. The cutting-edge of AI today is not&nbsp;<a href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem"><u>VNM-rational</u></a> agents who are&nbsp;<a href="http://www.scholarpedia.org/article/Bayesian_statistics"><u>Bayesianly</u></a>-updating their beliefs and trying to maximize some reward function. The horsepower of AI is instead coming from oodles of training data. If an AI becomes power-seeking, it may be because it learns power-seeking from humans, not because of instrumental convergence!</p><p>There was a general sense that the way we make sense of AI should be more empirical. <strong>Our stories need more contact with the real world</strong>&nbsp;– we need to test and verify the assumptions behind the stories. While <a href="https://www.gleave.me/">Adam Gleave</a>&nbsp;overall agreed with this view, he also warned that it’s possible to go too far in the other direction, and that we must strike a balance between the theoretical and the empirical.</p><h2>Problems with research</h2><p>This criticism of “too much theoretical, not enough empirical” also applied to the types of research we are doing. 4 respondents focussed on this. This was more a complaint about past research, folks were typically more positive about the amount of empirical work going on now.</p><p>2 people pointed at MIRI’s overreliance on idealized models of agency in their research, like <a href="https://www.lesswrong.com/tag/aixi">AIXI</a>. <a href="https://agarri.ga/">Adrià Garriga-Alonso</a>&nbsp;thought that <a href="https://www.lesswrong.com/tag/infra-bayesianism">infrabayesianism</a>, parts of singular learning theory and John Wentworth’s <a href="https://www.alignmentforum.org/users/johnswentworth">research programs</a>&nbsp;are unlikely to end up being helpful for safety:</p><blockquote><p><i>“I think the theory-only projects of the past did not work that well, and the current ones will go the same way.” - </i>Adrià Garriga-Alonso</p></blockquote><p><a href="https://twitter.com/evanhub?lang=en"><strong><u>Evan Hubinger</u></strong></a><strong> pushed back against this view </strong>by defending MIRI’s research approach. He pointed out that, when a lot of this very theoretical work was being done, there wasn’t much scope to do more empirical work because we had no highly capable general-purpose models to do experiments on – theoretical work was the best we could do!</p><blockquote><p>“<i>Now it's very different. Now, I think the best work to do is all empirical.&nbsp;Empirical research looks really good right now, but it looked way less good three, four years ago. It's just so much easier to do good empirical work now that the models are much smarter.</i>” - Evan Hubinger</p></blockquote><h1>Too insular</h1><p>8 participants thought AI safety was too insular: the community has disvalued forming alliances with other groups and hasn’t integrated other perspectives and disciplines.</p><p>2 of the 8 focussed on&nbsp;<a href="https://www.vox.com/future-perfect/2022/8/10/23298108/ai-dangers-ethics-alignment-present-future-risk"><u>AI safety’s relationship with AI ethics</u></a>.&nbsp;<strong>Many in AI safety have been too quick to dismiss the concerns of AI ethicists</strong> that AI could exacerbate current societal problems like racism, sexism and concentration of power, on the grounds of extinction risk being “infinitely more important”. But&nbsp;<strong>AI ethics has many overlaps with AI safety both technically and policy:</strong></p><blockquote><p>“<i>Many of the technical problems that I see are the same. If you're trying to align a language model, preventing it from saying toxic things is a great benchmark for that</i>.&nbsp;<i>In most cases, the thing we want on an object level is the same! We want more testing of AI systems, we want independent audits, we want to make sure that you can't just deploy an AI system unless it meets some safety criteria.”&nbsp;</i>- Adam Gleave</p></blockquote><p>In environmentalism, some care more about the conservation of bird species, while others are more concerned about preventing sea level rise. Even though these two groups may have different priorities, they shouldn’t fight because they have agree on many important subgoals, and have many more priorities in common with each other than with, for example, fossil fuel companies. Building a broader coalition could be similarly important for AI safety.</p><p>Another 2 respondents argued that&nbsp;<strong>AI safety needs more contact with academia</strong>. A big fraction of AI safety research is only shared via LessWrong or the Alignment Forum rather than academic journals or conferences. This can be helpful as it speeds up the process of sharing research by sidestepping “playing the academic game” (e.g. tuning your paper to fit into academic norms), but has the downside that research typically receives less peer review, leading to on average lower quality posts on sites like LessWrong. Much of AI safety research lacks the feedback loops that typical science has. AI safety also misses out on the talent available in the broader AI &amp; ML communities.</p><p>Many of the computer science and math kids in AI safety <strong>do not value insights from other disciplines</strong>&nbsp;enough, 2 respondents asserted. <a href="https://www.law.utoronto.ca/faculty-staff/full-time-faculty/gillian-hadfield">Gillian Hadfield</a>&nbsp;argued that many AI safety researchers are getting norms and values all wrong because we don’t consult the social sciences. For example: STEM people often have an assumption that there are some norms that we can all agree on (that we call “human values”), because it’s just “common sense”. But social scientists would disagree with this. Norms and values are the <i>equilibria</i>&nbsp;of interactions between individuals, produced by their behaviors, not some static list of rules up in the sky somewhere.</p><p>Another 2 respondents accused the rationalist sphere of using <strong>too much jargony and sci-fi language</strong>. Esoteric phrases like “p(doom)”, “x-risk” or “HPMOR” can be off-putting to outsiders and a barrier to newcomers, and give <a href="https://www.cnbc.com/2023/06/06/ai-doomers-are-a-cult-heres-the-real-threat-says-marc-andreessen.html">culty vibes</a>. Noah conceded that shorthands can be useful to some degree (for example they can speed up idea exchange by referring to common language rather than having to re-explain the same concept over and over again), but thought that on the whole AI safety has leaned too much in the jargony direction.</p><p><a href="https://www.openphilanthropy.org/about/team/ajeya-cotra/"><u>Ajeya Cotra</u></a> thought some AI safety researchers, like those at MIRI, have been&nbsp;<strong>too secretive&nbsp;</strong>about the results of their research. They do not publish their findings due to worries that a) their insights will help AI developers build more capable AI, and b) they will spread AGI hype and encourage more investment into building AGI (although Adam considered that creating AI hype is one of the big mistakes AI safety has made, on balance he also thought many groups should be less secretive). If a group is keeping their results secret, this is in fact a sign that they aren’t high quality results. This is because a) the research must have received little feedback or insights from other people with different perspectives, and b) if there&nbsp;<i>were</i> impressive results, there would be more temptation to share it.</p><p><a href="https://hollyelmore.substack.com/">Holly Elmore</a>&nbsp;suspected that <strong>this insular behavior was not by mistake, but on purpose</strong>. The rationalists wanted to only work with those who see things the same way as them, and avoid too many “dumb” people getting involved. She recalled conversations with some AI safety people who lamented that there are too many stupid or irrational newbies flooding into AI safety now, and the AI safety sphere isn't as fun as it was in the past.</p><h1>Bad messaging</h1><blockquote><p><i>“As the debate becomes more public and heated, it’s easy to fall into this trap of a race to the bottom in terms of discourse, and I think we can hold better standards. Even as critics of AI safety may get more adversarial or lower quality in their criticism, it’s important that we don’t stoop to the same level. [...]</i>&nbsp;<i>Polarization is not the way to go, it leads to less action.” </i>- Ben Cottier</p></blockquote><p>6 respondents thought AI safety could communicate better with the wider world. The AI safety community do not articulate the arguments for worrying about AI risk well enough, come across as too extreme or too conciliatory, and lean into some memes too much or not enough.</p><p>4 thought that some voices <strong>push views that are too extreme or weird </strong>(but one respondent explicitly pushed against this worry). Yudkowsky is too confident that things will go wrong, and PauseAI is at risk of becoming off-putting if they continue to lean into the protest vibe. Evan thought Conjecture has been doing outreach badly – arguing against sensible policy proposals (like <a href="https://metr.org/blog/2023-09-26-rsp/">responsible scaling policies</a>) because they don’t go far enough. <a href="https://www.eng.cam.ac.uk/profiles/dsk30">David Krueger</a>&nbsp;however leaned in the opposite direction: he thought that we are too scared to use sensationalist language like “AI might take over”, while in fact, this language is good for getting attention and communicating concerns clearly.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cGzQBRDrpNHoYtbKN/rxlfh7o4kf0whkvdzmqj" /><figcaption>Eliezer Yudkowsky pulling&nbsp;a strange face next to an alarming caption. <a href="https://www.youtube.com/watch?v=41SUp-TRVlg">Source: Dwarkesh Patel</a></figcaption></figure><p><a href="https://www.linkedin.com/in/ben-cottier/?originalSubdomain=uk">Ben Cottier</a>&nbsp;lamented the <strong>low quality of discourse around AI safety</strong>, especially in places like Twitter. We should have a high standard of discourse, show empathy to the other side of the debate, and seek compromises (with e.g. open source advocates). The current bad discourse is contributing to polarization, and nothing gets done when an issue is polarized. Ben also thought that <strong>AI safety should have been more prepared for the “reckoning moment” of AI risk becoming mainstream</strong>, so we had more coherent articulations of the arguments and reasonable responses to the objections.</p><p>Some people say that we shouldn’t anthropomorphize AI, but <a href="https://norabelrose.com/">Nora Belrose</a>&nbsp;reckoned we should do it more! Anthropomorphising makes stories much more attention-grabbing (it is “memetically fit”). One of the most famous examples of AI danger has been <a href="https://www.theguardian.com/technology/2023/feb/17/i-want-to-destroy-whatever-i-want-bings-ai-chatbot-unsettles-us-reporter">Sydney</a>: Microsoft’s chatbot that freaked people out by being unhinged in a very human way.</p><h1>AI safety’s relationship with the leading AGI companies</h1><blockquote><p>“<i>Is it good that <strong>the AI safety community has collectively birthed the three main AI orgs</strong>, who are to some degree competing, and maybe we're contributing to the race to AGI? I don’t know how true that is, but it feels like it’s a little bit true.</i></p><p><i>If the three biggest oil companies were all founded by people super concerned about climate change, you might think that something was going wrong."</i></p><p>- Daniel Filan</p></blockquote><p>Concern for AI safety had at least some part to play in the founding of OpenAI, Anthropic and DeepMind. Safety was a <a href="https://openai.com/our-structure">stated primary concern</a>&nbsp;that drove the founding of OpenAI. Anthropic was founded by researchers who left OpenAI because <a href="https://www.vox.com/future-perfect/23794855/anthropic-ai-openai-claude-2">it wasn’t sufficiently safety-conscious</a>. Shane Legg, one of DeepMind’s co-founders, is on record for being largely motivated by AI safety. Their existence is arguably making AGI come sooner, and fuelling a race that may lead to more reckless corner-cutting in AI development. 5 respondents thought the existence of these three organizations is probably a bad thing.</p><p>Jamie thought the existence of OpenAI may be overall positive though, due to their <a href="https://openai.com/index/planning-for-agi-and-beyond/">strategy</a>&nbsp;of widely releasing models (like ChatGPT) to get the world experienced with AI. ChatGPT has thrust AI into the mainstream and precipitated the recent rush of interest in the policy world.</p><p>3 respondents also complained that the AI safety community is&nbsp;<strong>too cozy with the big AGI companies</strong>. A lot of AI safety researchers work at OpenAI, Anthropic and DeepMind. The judgments of these researchers may be biased by a conflict of interest: they may be incentivised for their company to succeed in getting to AGI first. They will also be contractually limited in what they can say about their (former) employer, in some cases&nbsp;<a href="https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release"><u>even for life</u></a>.</p><p>Adam recommended that <strong>AI safety needs more voices who are independent of corporate interests</strong>, for example in academia. He also recommended that <strong>we shouldn’t be scared to criticize companies who aren’t doing enough for safety</strong>.</p><p>While <a href="https://danielfilan.com/">Daniel Filan</a>&nbsp;was concerned about AI safety’s close relationship with these companies, he conceded that there must be <strong>a </strong><a href="https://forum.effectivealtruism.org/posts/Y4SaFM5LfsZzbnymu/the-case-for-ai-safety-advocacy-to-the-public"><strong>balance</strong></a><strong>&nbsp;between inside game</strong>&nbsp;(changing things from the inside) <strong>and outside game</strong>&nbsp;(putting pressure on the system from the outside). AI safety is mostly playing the inside game – get involved with the companies who are causing the problem, to influence them to be more careful and do the right thing. In contrast, the environmentalism movement largely plays an outside game – not getting involved with oil companies but protesting them from the outside. Which of these is the right way to make change happen? Seems difficult to tell.</p><h1>The bandwagon</h1><blockquote><p>“<i>I think there's probably lots of people deferring when they don't even realize they're deferring.</i>” - Ole Jorgensen</p></blockquote><p>Many in the AI safety movement do not think enough for themselves, 4 respondents thought. Some are too willing to adopt the views of a small group of elites who lead the movement (like Yudkowsy, Christiano and Bostrom). <a href="https://twitter.com/turn_trout?lang=en-GB">Alex Turner</a>&nbsp;was concerned about the amount of “hero worship” towards these thought leaders. If this small group is wrong, then the entire movement is wrong. As Jamie pointed out, AI safety is now a&nbsp;major voice in the AI policy world – making it even more concerning that AI safety is resting on the judgements of such a small number of people.</p><blockquote><p>“<i>There's maybe some jumping to like: what's the most official way that I can get involved in this? And what's the community-approved way of doing this or that? That's not the kind of question I think we should be asking.</i>” - Daniel Filan</p></blockquote><h1>Pausing is bad</h1><p>3 respondents thought that advocating for a <a href="https://pauseai.info/proposal">pause to AI</a>&nbsp;development is bad, while 1 respondent was pro-pause<span class="footnote-reference" id="fnref29m9mwiex5m"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn29m9mwiex5m">[1]</a></sup></span>. Nora referred me to a <a href="https://bounded-regret.ghost.io/ai-pause-will-likely-backfire-by-nora/">post</a>&nbsp;she wrote arguing that pausing is bad.<i>&nbsp;</i>In that post, she argues that pausing will a) reduce the quality of alignment research because researchers will be forced to test their ideas on weak models, b) make a <a href="https://www.lesswrong.com/tag/ai-takeoff">hard takeoff</a>&nbsp;more likely when the pause is lifted, and c) push capabilities research underground, where regulations are looser.</p><h1>Discounting public outreach &amp; governance as a route to safety</h1><p>Historically, the AI safety movement has underestimated the potential of getting the public on-side&nbsp;and getting policy passed, 3 people said. There is a lot of work in AI governance these days, but for a long time most in AI safety considered it a dead end. <strong>The </strong><i><strong>only hope</strong></i><strong>&nbsp;to reduce existential risk from AI was to solve the technical problems ourselves, and hope that those who develop the first AGI implement them</strong>. Jamie put this down to a general mistrust of governments in rationalist circles, not enough faith in our ability to solve coordination problems, and a general dislike of “consensus views”.</p><p>Holly thought there was <strong>a general unconscious desire for the solution to be technical</strong>. AI safety people were guilty of motivated reasoning that “the best way to save the world is to do the work that I also happen to find fun and interesting”.&nbsp;When the Singularity Institute pivoted towards safety and became <a href="https://intelligence.org/">MIRI</a>, they never gave up on the goal of building AGI – just started prioritizing making it safe.</p><blockquote><p>“<i>Longtermism was developed basically so that AI safety could be the most important cause by the utilitarian EA calculus. That's my take.</i>” - Holly Elmore</p></blockquote><p>She also condemned the way <strong>many in AI safety hoped to solve the alignment problem via “elite shady back-room deals”</strong>, like influencing the values of the first AGI system by getting into powerful positions in the relevant AI companies.</p><p><a href="https://www.richardcngo.com/">Richard Ngo</a>&nbsp;gave me similar vibes, arguing that AI safety is <strong>too structurally power-seeking</strong>: trying to raise lots of money, trying to gain influence in corporations and governments, trying to control the way AI values are shaped, favoring people who are concerned about AI risk for jobs and grants, maintaining the secrecy of information, and recruiting high school students to the cause. We can justify activities like these to some degree, but Richard worried that AI safety was leaning too much in this direction. This has led many outside of the movement to deeply mistrust AI safety (<a href="https://www.youtube.com/watch?v=j4sYtMpetoc">for example</a>).</p><blockquote><p>“<i>From the perspective of an external observer, it’s difficult to know how much to trust stated motivations, especially when they tend to lead to the same outcomes as deliberate power-seeking.</i>” - Richard Ngo</p></blockquote><p>Richard thinks that a better way for AI safety to achieve its goals is to instead <strong>gain more legitimacy </strong>by being open, informing the public of the risks in a legible way, and prioritizing competence.</p><p>More abstractly, both Holly and Richard reckoned that <strong>there is too much focus on individual impact in AI safety</strong>&nbsp;and not enough focus on helping the world solve the problem collectively. More power to do good lies in the hands of the public and governments than many AI safety folk and effective altruists think. Individuals <i>can</i>&nbsp;make a big difference by playing 4D chess, but it’s harder to get right and often backfires.</p><blockquote><p>“<i>The agent that is actually having the impact is much larger than any of us, and in some sense, the role of each person is to facilitate the largest scale agent, whether that be the AI safety community or civilization or whatever. Impact is a little meaningless to talk about, if you’re talking about the impact of individuals in isolation.</i>” - Richard Ngo</p></blockquote><h1>Conclusion</h1><p>Participants pointed to a range of mistakes they thought the AI safety movement had made. An overreliance on overly theoretical argumentation, being too insular, putting the public off by pushing weird or extreme views, supporting the leading AGI companies, not enough independent thought, advocating for an unhelpful pause to AI development, and ignoring policy as potential a route to safety.</p><p>Personally, I hope this can help the AI safety movement avoid making similar mistakes in the future! Despite the negative skew of my questioning, I walked away from these conversations feeling pretty optimistic about the direction the movement is heading. I believe that as long as we continue to be honest, curious and open-minded about what we’re doing right and wrong, AI safety as a concept will overall have a positive effect on humanity’s future.</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fn29m9mwiex5m"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref29m9mwiex5m">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;Other respondents may also have been pro or anti-pause, but since the pause debate did not come up in their interviews I didn’t learn what their positions on this issue were.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/cGzQBRDrpNHoYtbKN/what-mistakes-has-the-ai-safety-movement-made#comments">Discuss</a>
]]></content:encoded>
<pubDate>Fri, 24 May 2024 05:11:21 GMT</pubDate>
</item>
<item>
<title>The case for stopping AI safety research</title>
<link>https://www.lesswrong.com/posts/vkzmbf4Mve4GNyJaF/the-case-for-stopping-ai-safety-research</link>
<guid>https://www.lesswrong.com/posts/vkzmbf4Mve4GNyJaF/the-case-for-stopping-ai-safety-research</guid>
<content:encoded><![CDATA[
<div> AI safety research, failure modes, capability development, human biometric data, x-risk

总结:<br /><br />本文指出AI安全研究正在解决易解决的问题，但这种情况将会发生变化，因为我们将无法预测和解决AI系统不断增长的失败模式。AI安全研究为AI能力的发展提供了免费服务，加速了这一过程。未来将有大量的人类生物特征和神经数据输入AI系统培训，这将带来巨大突破，但也会增加潜在风险。停止AI安全研究或将其内部化可能会减慢AI发展速度，但也会让能力开发者更直接面对有管理能力的有害后果。 <div>
Published on May 23, 2024 3:55 PM GMT<br /><br /><p>TLDR: AI systems are failing in obvious and manageable ways for now. Fixing them will push the failure modes beyond our ability to understand and anticipate, let alone fix. The AI safety community is also doing a huge economic service to developers. Our belief that our minds can "fix" a super-intelligence - especially bit by bit - needs to be re-thought.&nbsp;</p><p>I wanted to write this post forever, but now seems like a good time. &nbsp;The case is simple, I hope it takes you 1min to read.</p><ol><li><strong>AI safety research is still solving easy problems. &nbsp;</strong>We are patching up the most obvious (to us) problems. As time goes we will no longer be able to play this existential risk game of chess with AI systems. I've argued this a lot (<a href="https://arxiv.org/abs/2305.19223">preprint</a>; ICML paper accepted (shorter read, will repost), will be out in a few days; <a href="https://www.agencyfoundations.ai">www.agencyfoundations.ai</a>). Seems others have this thought.</li><li><strong>Capability development is getting AI safety research for free. </strong>It's likely in the millions to tens of millions of dollars. All the "hackathons", and "mini" prizes to patch something up or propose a new way for society to digest/adjust to some new normal (and increasingly incentivizing existing academic labs).&nbsp;</li><li><strong>AI safety research is speeding up capabilities</strong>. I hope this is somewhat obvious to most.</li></ol><p>I write this now because in my view we are about 5-7 years before massive human biometric and neural datasets will enter our AI training. &nbsp;These will likely generate amazing breakthroughs in long-term planning and emotional and social understanding of the human world. &nbsp;They will also most likely increase x-risk radically.</p><p>Stopping AI safety research or taking it in-house with security guarantees etc, will &nbsp;slow down capabilities somewhat - and may expose capabilities developers more directly to public opinion of still manageable harmful outcomes.&nbsp;</p><br /><br /><a href="https://www.lesswrong.com/posts/vkzmbf4Mve4GNyJaF/the-case-for-stopping-ai-safety-research#comments">Discuss</a>
]]></content:encoded>
<pubDate>Fri, 24 May 2024 01:52:56 GMT</pubDate>
</item>
<item>
<title>EIS XIII: Reflections on Anthropic’s SAE Research Circa May 2024</title>
<link>https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may</link>
<guid>https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may</guid>
<content:encoded><![CDATA[
<div> Anthropic, interpretability, SAE, safety, research
总结:
Anthropic的最新SAE研究集中于展示示例，但并未展示出SAE在实际应用中的竞争力和实用性。虽然他们声称工作是为了安全，但似乎更着重于炫耀研究成果而非解决实际问题。他们的方法论似乎忽视了过去的可解释性偏差，缺乏实用性和竞争性。对于机械解释性研究，需要展示出在实际任务中的竞争力，而Anthropic却仍旧停留在街灯式示例和举例说明阶段。他们的工作似乎并未真正应用于实际任务和击败基准线，而更多地是集中于表面的演示和示范。所以，Anthropic的解释团队似乎在推行错误的范式，并未真正展示出SAE和其他解释技术在实际场景中的实用性。 <div>
Published on May 21, 2024 8:15 PM GMT<br /><br /><p>Part 13 of 12 in the&nbsp;<a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7"><u>Engineer’s Interpretability Sequence</u></a>.</p><h1>TL;DR</h1><p>On May 5, 2024,&nbsp;<a href="https://x.com/StephenLCasper/status/1787270794017702045"><u>I made a set of 10 predictions</u></a> about what the next sparse autoencoder (SAE) paper from Anthropic would and wouldn’t do.&nbsp;<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"><u>Today’s new SAE paper from Anthropic</u></a> was full of brilliant experiments and interesting insights, but it underperformed my expectations. I am beginning to be concerned that Anthropic’s recent approach to interpretability research might be better explained by safety washing than practical safety work.&nbsp;</p><p>Think of this post as a curt editorial instead of a technical piece. I hope to revisit my predictions and this post in light of future updates.&nbsp;</p><h1>Reflecting on predictions</h1><p><a href="https://x.com/StephenLCasper/status/1787270794017702045"><u>See my original post</u></a> for 10 specific predictions about what&nbsp;<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"><u>today’s paper</u></a> would and wouldn’t accomplish. I think that Anthropic obviously did 1 and 2 and obviously did not do 4, 5, 7, 8, 9, and 10. Meanwhile, I think that their experiments to identify&nbsp;<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#searching"><u>specific</u></a> and&nbsp;<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#safety-relevant"><u>safety-relevant</u></a> features should count for 3 (proofs of concept for a useful type of task) but definitely do not count for 6 (*competitively* finding and removing a harmful behavior that was represented in the training data).</p><p>Thus, my assessment is that Anthropic did 1-3 but not 4-10.&nbsp;<a href="https://www.alignmentforum.org/posts/EjsA2M8p8ERyFHLLY/takeaways-from-the-mechanistic-interpretability-challenges"><u>I have been wrong with mech interp predictions in the past</u></a>, but this time, everything I predicted with &gt;50% probability happened, and everything I predicted with &lt;50% probability did not happen.&nbsp;</p><p>The predictions were accurate in one sense. But overall, the paper&nbsp;<i>underperformed</i> expectations. If you scored the paper relative to my predictions by giving it <i>(1-p)</i> points when it did something that I predicted it would do with probability <i>p</i> and <i>-p</i> points when it did not, the paper would score -0.74.&nbsp;</p><h1>A review + thoughts</h1><p>I think that Anthropic’s new SAE work has continued to be like lots of prior high-profile work on mechanistic interpretability – it has focused on presenting illustrative examples, streetlight demos, and cherry-picked proofs of concept. This is useful for science, but it does not yet show that SAEs are helpful and competitive for diagnostic and debugging tasks that could improve AI&nbsp;<i>safety</i>.&nbsp;</p><p>I feel increasingly worried about how Anthropic motivates and sells its interpretability research in the name of safety. Today’s paper makes some major Motte and Bailey claims that oversell what was accomplished like “<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#searching:~:text=Eight%20months%20ago%2C%20we%20demonstrated%C2%A0that%20sparse%20autoencoders%20could%20recover%20monosemantic%20features%20from%20a%20small%20one%2Dlayer%20transformer."><u>Eight months ago, we demonstrated that sparse autoencoders could recover monosemantic features from a small one-layer transformer</u></a>,” “<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#searching:~:text=Sparse%20autoencoders%20produce%20interpretable%20features%20for%20large%20models."><u>Sparse autoencoders produce interpretable features for large models</u></a>,” and “<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#searching:~:text=The%20resulting%20features%20are%20highly%20abstract%3A%20multilingual%2C%20multimodal%2C%20and%20generalizing%20between%20concrete%20and%20abstract%20references."><u>The resulting features are highly abstract: multilingual, multimodal, and generalizing between concrete and abstract references.</u></a>” The paper also made some omissions of past literature on interpretability illusions (e.g., <a href="https://arxiv.org/abs/2104.07143">Bolukbasi et al., 2021</a>), which their methodology seems prone to. Normally, problems like this are mitigated by peer review, which Anthropic does not participate in. Meanwhile, whenever Anthropic puts out new interpretability research, I always see a laundry list of posts from the company and employees to promote it. They always seem to claim the same things – that some ‘groundbreaking new progress has been made’ and that ‘the model was even more interpretable than they thought’ but that ‘there remains progress to be made before interpretability is solved’. I won’t link to any specific person’s posts, but here are Anthropic’s posts from&nbsp;<a href="https://x.com/AnthropicAI/status/1792935506587656625"><u>today</u></a> and&nbsp;<a href="https://x.com/AnthropicAI/status/1709986949711200722"><u>October 2023</u></a>.&nbsp;</p><p>The way that Anthropic presents its interpretability work has real-world consequences. For example, it led to&nbsp;this <a href="https://x.com/brickroad7/status/1710084070758355319"><u>viral claim that interpretability will be solved and that we are bound for safe models</u></a>. It also seems to have led to at least one claim in a policy memo that&nbsp;<a href="https://committees.parliament.uk/writtenevidence/127070/pdf/"><u>advocates of AI safety are being silly because mechanistic interpretability was solved</u></a>. Meanwhile today, it seems that Anthropic orchestrated a <a href="https://www.nytimes.com/2024/05/21/technology/ai-language-models-anthropic.html">New York Times article</a> to be released alongside the paper, claiming to the public that exciting progress has been made (although the article also made helpful critical commentary on limitations!).</p><p>If interpretability is ever going to be helpful for safety, it will need to be useful and competitive in practical applications. This point has been made consistently for the better part of a decade (e.g.&nbsp;<a href="https://journals.sagepub.com/doi/10.1177/1461444816676645"><u>Ananny and Crawford, 2016</u></a>;&nbsp;<a href="https://arxiv.org/abs/1606.03490"><u>Lipton, 2016</u></a>;&nbsp;<a href="https://arxiv.org/abs/1702.08608"><u>Doshi-Velez and Kim, 2017</u></a>;&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0004370218305988"><u>Miller, 2018</u></a>;&nbsp;<a href="https://link.springer.com/article/10.1007/s13347-019-00372-9"><u>Krishnan, 2020</u></a>;&nbsp;<a href="https://arxiv.org/abs/2207.13243"><u>Rauker et al., 2022</u></a>). Despite this, it seems to me that Anthropic has so far failed to apply its interpretability techniques to practical tasks and show that they are competitive. Instead of testing applications and beating baselines, the recent approach has been to keep focusing on streetlight demos and showing lots of cherry-picked examples.&nbsp;</p><p><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#discussion:~:text=Inability%20to%20Evaluate,we%20care%20about."><u>The discussion section of today’s paper seems to suggest</u></a> that Anthropic’s interpretability team is doubling down on the wrong paradigm.&nbsp;</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pH6tyhEnngqWAXi9i/yzo7qx3uoswp6kbbxxlh" /><figcaption>&nbsp;</figcaption></figure><p>This seems to stem from confusion about&nbsp;<a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/MyvkTKfndx9t4zknh"><u>what “interpretability” is and what it’s for</u></a>. Unlike what this excerpt suggests, ther indeed is an evaluation standard – usefulness for engineers.&nbsp;</p><p>I don't think that SAE research is misguided. In <a href="https://x.com/StephenLCasper/status/1787270794017702045">my post</a>, I pointed out 6 things that I think they could be useful for. Meanwhile, some good recent work has demonstrated proofs of concept that SAEs can be useful on certain non-cherry-picked tasks of practical value and interest (<a href="https://arxiv.org/abs/2403.19647">Marks et al., 2024</a>). I think that it's very possible that SAEs and other interpretability techniques can be lenses into models that can help us find useful clues and insights. However, Anthropic's research on SAEs has yet to demonstrate practical usefulness that could help engineers in real applications.&nbsp;</p><p>I know that members of the Anthropic interpretability team have been aware of this critique. Meanwhile, Anthropic and its employees consistently affirm that their work is motivated by safety in the real world. But <a href="https://www.alignmentforum.org/posts/fpws8WNo6Mp4KsQjH/analogies-between-scaling-labs-and-misaligned">is</a> <a href="https://joinreboot.org/p/alignment">it</a>? I am starting to wonder about the extent to which the interpretability team’s current agenda is better explained by practical safety work versus doing machiavellian<u> </u><a href="https://www.alignmentforum.org/posts/fpws8WNo6Mp4KsQjH/analogies-between-scaling-labs-and-misaligned"><u>safety washing</u></a> to score points in&nbsp;<a href="https://x.com/brickroad7/status/1710084070758355319"><u>social media</u></a>,&nbsp;<a href="https://www.nytimes.com/2024/05/21/technology/ai-language-models-anthropic.html"><u>news</u></a>, and&nbsp;<a href="https://committees.parliament.uk/writtenevidence/127070/pdf/"><u>government</u></a>.&nbsp;</p><p>—</p><p>Thanks to Ryan Greenblatt and Buck Shlegris. I did not consult with them on this post, but they pointed out some useful things in a Slack thread that I put in here.</p><p><br />&nbsp;</p><br /><br /><a href="https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may#comments">Discuss</a>
]]></content:encoded>
<pubDate>Wed, 22 May 2024 23:26:15 GMT</pubDate>
</item>
<item>
<title>"Which chains-of-thought was that faster than?"</title>
<link>https://www.lesswrong.com/posts/7oGfJG2BuvTgdCHQH/which-chains-of-thought-was-that-faster-than</link>
<guid>https://www.lesswrong.com/posts/7oGfJG2BuvTgdCHQH/which-chains-of-thought-was-that-faster-than</guid>
<content:encoded><![CDATA[
<div> chain-of-thought, heuristic, cognitive trace, faster, effective
<br />
提供了一些来自Eliezer的好建议，主要包括两个启发式方法：完成一条思路链后，反思如何更快地想到，以及完成一次良好思路链后，反思比起哪些思路更快。文章中举例说明了这两种方法的应用场景，分析了它们的优势和适用范围，最后给出了另一个启发式方法：当接收到好建议时，询问如何让这些建议更好。整篇文章围绕思维方式的优化展开，提供了一系列实用方法帮助读者更高效地思考和学习。 
<br /><br />总结: 提供了两种启发式方法：完成思路链后反思如何更快地想到，以及反思良好思路链相比其他思路更快的优势；展示了具体实例和方法应用场景；分析了方法的优势和适用范围；提出了另一种启发式方法：接收好建议后询问如何改进建议。 <div>
Published on May 22, 2024 8:21 AM GMT<br /><br /><p>Here's some good advice from Eliezer:</p><h2>TAP: "How could I have thought that faster?"</h2><ul><li>WHEN<span class="footnote-reference" id="fnrefjf37bd9flbt"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnjf37bd9flbt">[1]</a></sup></span>&nbsp;<i>you complete a chain-of-thought</i></li><li>THEN <i>ask yourself, </i><a href="https://www.lesswrong.com/posts/rYq6joCrZ8m62m7ej/how-could-i-have-thought-that-faster"><i>"<strong>how could I have thought that faster?</strong>"</i></a></li></ul><p>I really like this heuristic, and it's already <a href="https://www.lesswrong.com/tag/making-beliefs-pay-rent">paid its rent</a> several times over for me. Most recently today, so I'll share the (slightly edited) cognitive trace of it as an example:</p><h3>Example: To find the inverse of something, trace the chain forward a few times first</h3><ol><li>I was in the context of having just asked myself <i>"what's the set of functions which have this function as its derivative?"</i></li><li>This is of course its integral, but I didn't want to use cached abstractions, and instead sought to get a generalized view of the landscape <a href="https://www.lesswrong.com/posts/fg9fXrHpeaDD6pEPL/truly-part-of-you">from first-principles</a>.</li><li>For about ~10 seconds, I tried to hold the function&nbsp;<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span></span></span></span></span>&nbsp;in my mind while trying to directly generate the integral landscape from it.&nbsp;</li><li>This seemed awfwly inefficient, so I changed tack: I already know some specific functions whose derivatives equal&nbsp;<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span></span></span></span></span>, so I held <i>those</i> as the proximal thing in my mind while retracing the cognitive steps involved in their derivation.</li><li>After making those steps more salient in the forward direction (integral→derivative), it was easier to retrace the path in the opposite direction.</li><li>And once the derivative→integral trace was salient for a few examples, it was easier to <i>generalize from the examples</i> to produce the landscape of all the integrals.</li><li>There are multiple takeaways here, but one is:<ol><li><i>"If you struggle to generalize something, find a way to generate specific examples first, then generalize from the examples."</i></li></ol></li></ol><h2>TAP: "Which chains-of-thought was that faster than?"</h2><p>Imo, more important than asking <i>"how could I have thought that faster?"</i> is the inverse heuristic:</p><ul><li>WHEN <i>you complete a good chain-of-thought</i></li><li>THEN <i>ask yourself, "<strong>which chains-of-thought was that faster than?</strong>"</i></li></ul><p>Although, ideally, I wouldn't scope the trigger to <i>every</i> time you complete a thought, since that overburdens the general cue. Instead, maybe limit it to those times when you have an especially clear trace of it AND you have a hunch that something about it was unusually good.</p><ul><li>WHEN <i>you complete a good chain of thought</i><ul><li>AND <i>you have its trace in short-term memory</i></li><li>AND <i>you hunch that something about it was unusually effective</i></li></ul></li><li>THEN <i>ask yourself, "<strong>which chains-of-thought was that faster than?</strong>"</i></li></ul><h3>Example: Sketching out my thoughts with pen-and-paper</h3><ol><li>Yesterday I was writing out some plans explicitly with pen and paper—enumerating my variables and drawing arrows between them.</li><li>I noticed—for the umpteenth time—that forcing myself to explicitly sketch out the problem (even with improvised visualizations) is far more <i>cognitively ergonomic</i> than keeping it in my head (see eg <a href="https://www.lesswrong.com/posts/Qyr9tfBLck53em4hf/how-to-write-pseudocode-and-why-you-should">why you should write pseudocode</a>).</li><li>But instead of just noting "yup, I should force myself to do more pen-and-paper", I asked myself two questions:<ol><li><i><strong>"When does it help me think, and when does it just slow me down?"</strong></i><ol><li>This part is important: <i>scope</i> your insight <i>sharply </i>to contexts where it's usefwl—hook your idea into the contexts where you want it triggered—so you avoid wasting memory-capacity on linking it up to useless stuff.</li><li>In other words, you want to minimize (unwanted) <a href="https://en.wikipedia.org/wiki/Associative_interference#Effect_on_recall:~:text=Many%20studies%20have%20concluded%20that%20associative%20interference%20reduces%20a%20subject%27s%20ability%20to%20recall.">associative interference</a> so you can remember stuff at lower cost.</li><li>My conclusion was that pen-and-paper is good when I'm trying to map complex relations between a handfwl of variables.</li><li>And it is NOT good when I have just a single proximal idea that I want to compare against a myriad of samples with high false-positive rate—that's instead where I should be doing inside-head thinking to exploit the brain's <a href="https://www.wikiwand.com/en/Connectionism">massively parallel distributed processor</a>.</li></ol></li><li><i><strong>"Why am I so reluctant to do it?"</strong></i><ol><li>This seems related to the brain's myopic tendency for <i>hastening subgoal completion.</i><span class="footnote-reference" id="fnref8jzoqcq85ko"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn8jzoqcq85ko">[2]</a></sup></span></li><li>So I resolved to try to notice exactly which subgoal(s) my brain biases motivation toward, so I can trigger this concept <i>specifically</i> in the contexts where top-down override is most needed—instead of relying on an overly general sense of "uuh I gotta do this more somehow".</li></ol></li></ol></li></ol><h3>Why is it better?</h3><p>While obviously both heuristics are good to use, the reasons I think asking <i>"which chains-of-thought was that faster than?"</i> tends to be more epistemically profitable than <i>"how could I have thought that faster?"</i> include:</p><ul><li>It is easier to find suboptimal thinking-habits to propagate an unusually good idea <i>into</i>, than to find good ideas for improving a particular suboptimal thinking-habit.<ul><li>Notice that in my technique, the good idea<u> </u>is <i><strong>cognitively proximal</strong></i> and the suboptimal thinking-habits are <i><strong>cognitively&nbsp;distal</strong></i>, whereas in Eliezer's suggestion it's the other way around.</li><li>A premise here is that good ideas are unusual (hard-to-find) and suboptimal thinking-habits are common (easy-to-find)—the advice flips in domains where it's the opposite.</li><li>It relates to the difference between propagating specific solutions to plausible problem-domains, vs searching for specific solutions to a specific problem.<ul><li>The brain tends to be biased against the former approach because it's preparatory work with <i>upfront cost </i>("prophylaxis"), whereas the latter context sort of forces you to search for solutions.</li></ul></li></ul></li></ul><h2>TAP: "What's the appropriate scope?"</h2><ul><li>WHEN <i>you notice that a heuristic is usefwl in specific cases</i></li><li><u>THEN </u><i><u>ask yourself, </u><strong><u>"can I generalize this to new domains?"</u></strong></i></li></ul><p>Especially notice that there's nothing about the structure of <i>"how could I have <strong>thought</strong> that faster?"</i> that implies it's only usefwl in the domain of specific short chains-of-thought. "Thought" here is an unconstrained variable. It generalizes to everything where the trace of specific examples is likely to contain information which profitably generalizes to other examples. The general pattern is:&nbsp;</p><ul><li><i>"What went <strong>wrong</strong> this time?"</i><ul><li>And its more-profitable inverse: <i>"What went <strong>right </strong>this time?"</i></li></ul></li></ul><p>So let's propagate this pattern across some domains:</p><ul><li><i>"How could I have <strong>learned</strong> that faster?"</i><ul><li>What's the most usefwl lessons you acquired from studying X? And could you have predicted that in advance so you could <i>avoid </i>wasting time learning [useless subsets of X]?</li></ul></li><li><i>"How could I have <strong>finished</strong> that faster?"</i><ul><li>I don't know about you, but I have wasted an outrageous number of hours perfecting the UI of my programs when, realistically, the benefit was extremely marginal.</li></ul></li><li><i>"How could I have <strong>failed</strong> that faster?"</i><ul><li>If you have a plan, go straight for the bottlenecks that have the <a href="https://lesswrong.com/posts/Eav2BizSejDcztFC8/focus-on-the-hardest-part-first">largest probability of making you realize the plan is intractable</a>. Also, <a href="https://www.lesswrong.com/posts/FMkQtPvzsriQAow5q/the-correct-response-to-uncertainty-is-not-half-speed">the correct response to uncertainty is not half-speed</a>.</li><li>Failing projects as fast as you can, may feel like making no progress at all. But keep in mind that you're sampling the search-space faster this way. I call it <i>"horizontal progress" </i>because that makes me feel better.</li></ul></li><li><i>"How could I have <strong>remembered</strong> that more reliably?"</i><ul><li>Especially don't forget the inverse: <i>"What <strong>enabled me to recall</strong> that?"</i></li></ul></li><li><i>"How could I have <strong>read</strong> that faster?"</i><ul><li>If you are guilty of reading this sentence after having read all previous sentences in this post, consider whether you ought to be skimming more. I'm pretty sure some of the above sentences were predictably less usefwl to you.</li></ul></li></ul><h2>TAP: "How can I make this advice better?"</h2><p>Lastly, another generally usefwl heuristic, which also happens to have caused the insights which led to this post:</p><ul><li>WHEN <i>you receive good advice</i><ul><li>AND <i>you especially trust the author of that advice</i></li></ul></li><li>THEN <i>ask yourself, "<strong>how can I make this advice better?</strong>"</i></li></ul><ol class="footnote-section footnotes"><li class="footnote-item" id="fnjf37bd9flbt"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefjf37bd9flbt">^</a></strong></sup></span><div class="footnote-content"><p>Formatted as a <strong>trigger-action-plan (TAP)</strong> to make the cue more separately salient, so you're more likely to notice the event that should trigger the action.</p></div></li><li class="footnote-item" id="fn8jzoqcq85ko"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref8jzoqcq85ko">^</a></strong></sup></span><div class="footnote-content"><blockquote><p><i>We asked university students to pick up either of two buckets, one to the left of an alley and one to the right, and to carry the selected bucket to the alley’s end. In most trials, one of the buckets was closer to the end point. We emphasized choosing the easier task, expecting participants to prefer the bucket that would be carried a shorter distance. Contrary to our expectation, participants chose the bucket that was closer to the start position, carrying it farther than the other bucket. </i>— <a href="https://gwern.net/doc/psychology/2014-rosenbaum.pdf">Pre-Crastination: Hastening Subgoal Completion at the Expense of Extra Physical Effort</a></p></blockquote></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/7oGfJG2BuvTgdCHQH/which-chains-of-thought-was-that-faster-than#comments">Discuss</a>
]]></content:encoded>
<pubDate>Thu, 23 May 2024 06:20:17 GMT</pubDate>
</item>
</channel>
</rss>