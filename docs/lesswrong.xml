<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>LessWrong</title>
<link>https://www.lesswrong.com</link>


<item>
<title>Level up your spreadsheeting</title>
<link>https://www.lesswrong.com/posts/RBtF9fu9WMjdvqHFB/level-up-your-spreadsheeting</link>
<guid>https://www.lesswrong.com/posts/RBtF9fu9WMjdvqHFB/level-up-your-spreadsheeting</guid>
<content:encoded><![CDATA[
<div> Google Sheets, Excel, principles, good spreadsheets, tools
总结:<br /><br />这篇文章探讨了创建优秀电子表格的一些原则和工具。首先，好的电子表格应该易于从数据中提取见解，美观易读，有唯一的数据来源，易于审计，并且难以破坏。作者还提供了一些在Google Sheets和Excel中实用的技巧，涉及数据可视化、系统设计和编程设计等方面。他还分享了一个关于Excel错误的趣闻，强调了审计电子表格的重要性。根据电子表格的用途，不同的原则可能会受到不同程度的重视。 <div>
Published on May 25, 2024 2:57 PM GMT<br /><br /><p>Epistemic status: Passion project / domain I’m pretty opinionated about, just for fun.</p><p>In this post, I walk through some principles I think good spreadsheets abide by, and then in the <a href="https://docs.google.com/document/d/1cIHfQcRIV_wNF6IlqIoqY83NRDl4US012v1PtsTPo4s/edit">companion piece</a>, I walk through a whole bunch of tricks I've found valuable.</p><figure class="image image_resized" style="width: 52.93%;"><img alt="Image of a spreadsheet by GPT-4o" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RBtF9fu9WMjdvqHFB/scnqrjaxjyeve3bxn2yk" /><figcaption>Illustrated by GPT-4o</figcaption></figure><h2>Who am I?</h2><p>I’ve spent a big chunk of my (short) professional career so far getting good at Excel and Google Sheets.<span class="footnote-reference" id="fnrefny93eqg7mr"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnny93eqg7mr">[1]</a></sup></span>&nbsp;As such, I’ve accumulated a bunch of opinions on this topic.</p><h2>Who should read this?</h2><p><strong>This is not a guide to learning how to start using spreadsheets at all.</strong>&nbsp;I think you will get more out of this post if you use spreadsheets at least somewhat frequently, e.g.</p><ul><li>Have made 20+ spreadsheets</li><li>Know how to use basic formulas like sum, if, countif, round</li><li>Know some fancier formulas like left/mid/right, concatenate, hyperlink</li><li>Have used some things like filters, conditional formatting, data validation</li></ul><h2>Principles of good spreadsheets</h2><p>Broadly speaking, I think good spreadsheets follow some core principles (non-exhaustive list).</p><p>I think the below is a combination of good data visualization (or just communication) advice, systems design, and programming design (spreadsheets combine the code and the output).</p><p><strong>It should be easy for you to extract insights from your data</strong></p><ol><li>A core goal you might have with spreadsheets is quickly calculating something based on your data. A bunch of tools below are aimed at improving functionality, allowing you to more quickly grab the data you want.</li></ol><p><strong>Your spreadsheet should be beautiful and easy to read</strong></p><ol><li>Sometimes, spreadsheets look like the following example.</li><li>I claim that this is not beautiful or easy for your users to follow what is going on. I think there are cheap techniques you can use to improve the readability of your data.</li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RBtF9fu9WMjdvqHFB/v4vdw8jozrhibxopub5x" /></p><p><strong>There should be one source of truth for your data</strong></p><ol><li>One common pitfall when designing spreadsheet-based trackers is hard copy and pasting data from one sheet to another, such that when your source data changes, the sheets you use for analyses no longer reflect “fresh” data. This is a big way in which your spreadsheet systems can break down.</li><li>A bunch of tools below are designed to improve data portability — i.e. remove the need for copy and pasting.</li></ol><p><strong>Your spreadsheet should be easy to audit</strong></p><ol><li>One major downside of spreadsheets as compared to most coding languages, is that it’s often easy for relatively simple spreadsheets to contain silent bugs in them.<span class="footnote-reference" id="fnref4r199bf3ob4"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn4r199bf3ob4">[2]</a></sup></span></li><li>Some features of spreadsheets that contribute to this problem:<ol><li>Spreadsheets hide the code and show you only the output by default.<ol><li>When you use formulas, once you hit enter, the user doesn’t by default get to read what’s going on. So if the output looks plausible, you might not notice your formula has a bug in it.</li></ol></li><li>It’s harder to break up your work into chunks.<ol><li>When you’re coding, most people will break up a complicated formula into several lines of code, using intermediate variables and comments to make things more readable. E.g.:</li><li><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RBtF9fu9WMjdvqHFB/h5a0sfx7ypo4fafeta7l" style="width: 82.87%;" /></li><li>By default, some Sheets formulas get really unwieldy, and you need to work a bit harder to recover readability.</li></ol></li><li>Spreadsheets contain more individual calculations.<ol><li>When you’re coding and you want to perform the same calculation on 100 rows of data, you’d probably use a single line of code to iterate over your data (e.g. a <a href="https://www.w3schools.com/python/python_for_loops.asp">for loop</a>).</li><li>In Google Sheets, you’re more likely to drag your formula down across all of your rows. But this means that if you accidentally change the formula for one cell and not the others, or if your data has now changed and it turns out you need to drag your formulas down more, things can break in annoying ways.</li></ol></li></ol></li><li>Because of this, I consider auditability one of the key qualities of a well designed spreadsheet. Some of the tools below will recover coding best practices.</li><li>I also consider principles (2)-(3) above pretty related to principle (4).</li></ol><p><strong>Your spreadsheet should be hard to break</strong></p><ol><li>Not all spreadsheets are meant as living documents; sometimes you’ll create a spreadsheet to conduct a specific analysis and then discard it.</li><li>But sometimes, you’ll use a spreadsheet as a management tool to keep track of a bunch of moving pieces. In this case, you might care that your system isn’t going to break after a few weeks of use.<span class="footnote-reference" id="fnrefla6vdvj12ye"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnla6vdvj12ye">[3]</a></sup></span></li></ol><hr /><p>Much more in the <a href="https://docs.google.com/document/d/1cIHfQcRIV_wNF6IlqIoqY83NRDl4US012v1PtsTPo4s/edit">companion piece</a>!</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnny93eqg7mr"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefny93eqg7mr">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;I’m using the term ‘Google Sheets’ in this doc, but almost all of the tricks mentioned here work for Excel as well.</p></div></li><li class="footnote-item" id="fn4r199bf3ob4"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref4r199bf3ob4">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;My favorite Excel bug story: I used to work in litigation consulting, where I’d sometimes audit spreadsheets sent to us from the opposing side of a legal case.</p><p>In one case, an expert witness for the opposing side sent over a spreadsheet with columns similar to the following: year, online sales, in-person sales, total sales. The expert was saying that total sales had almost doubled from ~3,000 → ~5,000 for this particular product in 2019.</p><p>We eventually discovered that for the 2019 row, the expert had entered the formula =sum(A4:C4)&nbsp;instead of sum(B4:C4), and so had accidentally added the value ‘2019’ to the total sum. Here’s a <a href="https://docs.google.com/spreadsheets/d/1vA1LjLAQoqPVogr1MzYH4zW8ZwFiNgX_ai0-fpQjU2k/edit#gid=835473103">recreation</a>. (I’ve obfuscated the details a bit here but the core mistake was the same.)</p></div></li><li class="footnote-item" id="fnla6vdvj12ye"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefla6vdvj12ye">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;As an aside, spreadsheets have a lot of use cases, which makes giving generalizable advice a bit trickier. For instance, some common use cases for spreadsheets:</p><p>- A <strong>database</strong>&nbsp;which you query whenever needed;</p><p>- A <strong>data visualization</strong>&nbsp;tool meant to present some interesting findings from existing data;</p><p>- A <strong>management tracker</strong>&nbsp;that you use to schedule emails and keep tabs on your tasks;</p><p>- To <strong>model</strong>&nbsp;some interesting phenomenon and keep track of your assumptions</p><p>Depending on what you’re using a spreadsheet for, you might prioritize some of these principles more or less highly. For instance, making something easy to read is probably more valuable when you’re creating a data visualization versus a database.</p><p>Of course, lots of spreadsheets combine lots of different use cases — e.g. you might have one tab with your source of truth data, and another for random analytics.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/RBtF9fu9WMjdvqHFB/level-up-your-spreadsheeting#comments">Discuss</a>
]]></content:encoded>
<pubDate>Sat, 25 May 2024 23:31:39 GMT</pubDate>
</item>
<item>
<title>AI companies aren't really using external evaluators</title>
<link>https://www.lesswrong.com/posts/WjtnvndbsHxCnFNyc/ai-companies-aren-t-really-using-external-evaluators</link>
<guid>https://www.lesswrong.com/posts/WjtnvndbsHxCnFNyc/ai-companies-aren-t-really-using-external-evaluators</guid>
<content:encoded><![CDATA[
<div> 外部模型评估，模型共享，实验室承诺，风险评估，DeepMind

总结:<br /><br />这篇文章讨论了AI实验室在部署之前进行外部模型评估的重要性。文章提到了各实验室在此方面的承诺和实践，例如DeepMind分享Gemini 1.0 Ultra用于测试危险功能，但并未提供深度访问。实验室需要与第三方评估者建立正式安排和访问承诺，以提高风险评估并提供公共问责。此外，文章还强调了外部模型评估与外部红队测试的不同，以及建议实验室在模型共享时考虑禁用或将安全性措施纳入安全案例中。各实验室对参与UK AISI预部署访问的承诺仍不明确，需要澄清。。 <div>
Published on May 24, 2024 4:01 PM GMT<br /><br /><p><i>From </i><a href="https://ailabwatch.org/blog/external-evaluation/"><i>my new blog: AI Lab Watch.</i></a><i> All posts will be crossposted to LessWrong. </i><a href="https://ailabwatch.substack.com/"><i>Subscribe on Substack.</i></a></p><p>Many AI safety folks think that <a href="https://metr.org">METR</a> is close to the labs, with ongoing relationships that grant it access to models before they are deployed. This is incorrect. METR (then called ARC Evals) did pre-deployment <a href="https://metr.org/blog/2023-03-18-update-on-recent-evals/">evaluation</a> for <a href="https://cdn.openai.com/papers/gpt-4-system-card.pdf#page=15">GPT-4</a> and <a href="https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf#page=2">Claude 2</a> in the first half of 2023, but it seems to have had no special access since then.<span class="footnote-reference" id="fnrefjphp5hhec5"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnjphp5hhec5">[1]</a></sup></span>&nbsp;Other model evaluators also seem to have little access before deployment.</p><p><i>Clarification: </i><a href="https://link.springer.com/article/10.1007/s43681-023-00289-2"><i>there are many kinds of audits</i></a><i>. This post is about </i><a href="https://arxiv.org/abs/2305.15324"><i>model evals for dangerous capabilities</i></a><i>. But I'm not aware of the labs using other kinds of audits to prevent extreme risks, excluding normal security/compliance audits.</i></p><hr /><p>Frontier AI labs' pre-deployment risk assessment should involve external model evals for dangerous capabilities.<span class="footnote-reference" id="fnref21dc7w241d5"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn21dc7w241d5">[2]</a></sup></span>&nbsp;External evals can improve a lab's risk assessment and—if the evaluator can publish its results—provide public accountability.</p><p>The evaluator should get <i>deeper access</i> than users will get.</p><ul><li>To evaluate threats from a particular deployment protocol, the evaluator should get somewhat deeper access than users will — then the evaluator's failure to elicit dangerous capabilities is stronger evidence that users won't be able to either.<span class="footnote-reference" id="fnref6rko1i2q6mu"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn6rko1i2q6mu">[3]</a></sup></span>&nbsp;For example, the lab could share a version of the model without safety filters or harmlessness training, and ideally allow evaluators to fine-tune the model.</li><li>To evaluate threats from model weights being stolen or released, the evaluator needs deep access, since someone with the weights has full access.</li></ul><p>The costs of using external evaluators are unclear.</p><ul><li>Anthropic <a href="https://www.anthropic.com/news/evaluating-ai-systems#:~:text=Initially%2C%20we%20expected%20this%20collaboration%20to%20be%20straightforward%2C%20but%20it%20ended%20up%20requiring%20significant%20science%20and%20engineering%20support%20on%20our%20end.%20Providing%20full%2Dtime%20assistance%20diverted%20resources%20from%20internal%20evaluation%20efforts.">said</a> that collaborating with METR "requir[ed] significant science and engineering support on our end"; it has not clarified why. And even if providing deep model access or high-touch support is a hard engineering problem, I don't understand how sharing API access—including what users will receive and a no-harmlessness no-filters version—could be.</li><li>Sharing model access pre-deployment increases the risk of leaks, including of information about products (modalities, release dates), information about capabilities, and demonstrations of models misbehaving.</li></ul><p>Independent organizations that do model evals for dangerous capabilities include METR, the UK AI Safety Institute (UK AISI), and Apollo. Based on public information, there's only one recent instance of a lab giving access to an evaluator pre-deployment—Google DeepMind sharing with UK AISI—and that sharing was minimal (see below).</p><p>What the labs say they're doing on external evals before deployment:</p><ul><li>DeepMind<ul><li>DeepMind <a href="https://arxiv.org/pdf/2312.11805v2#page=38"><u>shared</u></a> Gemini 1.0 Ultra with unspecified external groups <a href="https://www.politico.eu/article/rishi-sunak-ai-testing-tech-ai-safety-institute/"><u>apparently including UK AISI</u></a> to test for dangerous capabilities before deployment. But DeepMind didn't share deep access: it only shared a system with safety fine-tuning and safety filters and it didn't allow evaluators to fine-tune the model. DeepMind has not shared any results of this testing.</li><li>Its Frontier Safety Framework <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/introducing-the-frontier-safety-framework/fsf-technical-report.pdf#page=6">says</a> "We will . . . explore how to appropriately involve independent third parties in our risk assessment and mitigation processes."</li></ul></li><li>Anthropic<ul><li>Currently nothing</li><li>Its Responsible Scaling Policy <a href="https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf#page=15">mentions</a> "external audits" as part of "Early Thoughts on ASL-4"</li><li>It <a href="https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf#page=2">shared</a> Claude 2 with METR in the first half of 2023</li></ul></li><li>OpenAI<ul><li>Currently nothing</li><li>Its Preparedness Framework does not mention external evals before deployment. The closest thing it says <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf#page=25">is</a> "Scorecard evaluations (and corresponding mitigations) will be audited by qualified, independent third-parties."</li><li>It <a href="https://cdn.openai.com/papers/gpt-4-system-card.pdf#page=15">shared</a> GPT-4 with METR in the first half of 2023</li><li>It <a href="https://openai.com/index/planning-for-agi-and-beyond#:~:text=We%20think%20it%E2%80%99s%20important%20that%20efforts,training%20runs%20above%20a%20certain%C2%A0scale."><u>said</u></a> "We think it's important that efforts like ours submit to independent audits before releasing new systems; we will talk about this in more detail later this year." That was in February 2023; I do not believe it elaborated (except to <u>mention</u> that it shared GPT-4 with METR).</li></ul></li><li>All notable American labs joined the <a href="https://www.whitehouse.gov/wp-content/uploads/2023/09/Voluntary-AI-Commitments-September-2023.pdf"><u>White House voluntary commitments</u></a><u>, which include "external red-teaming . . . in areas including misuse, societal risks, and national security concerns, such as bio, cyber, [autonomous replication,] and other safety areas." External </u><i><u>red-teaming</u></i><u> does not substitute for external </u><i><u>model evals</u></i><u>; see below.</u><ul><li>DeepMind <a href="https://arxiv.org/pdf/2312.11805v2#page=38">said</a> it did lots of external red-teaming for Gemini.</li><li>Anthropic <a href="https://cdn.sanity.io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf#page=12">said</a> it did external red-teaming for CBRN capabilities. It has also <a href="https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety">written</a> about using external experts to assess bio capabilities.</li><li>OpenAI <a href="https://arxiv.org/pdf/2303.08774#page=45">said</a> it did lots of external red-teaming for GPT-4. It has also <a href="https://openai.com/index/building-an-early-warning-system-for-llm-aided-biological-threat-creation/">written</a> about using external experts to assess bio capabilities.</li><li><u>Meta </u><a href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md"><u>said</u></a><u> it did external red-teaming for CBRNE capabilities.</u></li><li><u>Microsoft </u><a href="https://blogs.microsoft.com/on-the-issues/2023/10/26/microsofts-ai-safety-policies/"><u>said</u></a><u> it's "building out external red-teaming capacity . . . . The topics covered by such red team testing will include testing of dangerous capabilities, including related to biosecurity and cybersecurity."&nbsp;</u></li></ul></li></ul><hr /><p>Related miscellanea:</p><p><i>External red-teaming</i> is not <i>external model evaluation</i>. External red-teaming generally involves sharing the model with several people with expertise relevant to a dangerous capability (e.g. bioengineering) who open-endedly try to elicit dangerous model behavior for ~10 hours each. External model evals involves sharing with a team of experts at eliciting capabilities, to perform somewhat automated and standardized evals suites that they've spent ~10,000 hours developing.</p><p>Labs' commitments to share pre-deployment access with UK AISI are unclear.<span class="footnote-reference" id="fnrefhesa4trfm9c"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnhesa4trfm9c">[4]</a></sup></span></p><p>This post is about <i>sharing model access before deployment for risk assessment</i>. Labs should also <i>share deeper access with safety researchers (during deployment)</i>. For example, some safety researchers would really benefit from being able to fine-tune GPT-4, Claude 3 Opus, or Gemini, and my impression is that the labs could easily give safety researchers fine-tuning access. More speculatively, interpretability researchers could send a lab code and the lab could run it on private models and send the results to the researchers, achieving some benefits of releasing weights with much less downside.<span class="footnote-reference" id="fnrefhn9nldfq419"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnhn9nldfq419">[5]</a></sup></span></p><p>Everything in this post applies to <i>external</i> deployment. It will also be important to do some evals during training and before internal deployment, since lots of risk might come from weights being stolen or <a href="https://www.lesswrong.com/posts/tmWMuY5HCSNXXZ9oq/buck-s-shortform?commentId=KyKER7ApeBKwCLCxk">the lab using AIs internally to do AI development</a>.</p><p>Labs could be bound by external evals, such that they won't deploy a model until a particular eval says it's safe. This seems unlikely to happen (for actually meaningful evals) except by regulation. (I don't believe any existing evals would be great to force onto the labs, but if governments were interested, evals organizations could focus on creating such evals.)</p><hr /><p>Thanks to Buck Shlegeris, Eli Lifland, Gabriel Mukobi, and an anonymous human for suggestions. They don't necessarily endorse this post.</p><p><a href="https://ailabwatch.substack.com/"><i>Subscribe on Substack.</i></a></p><ol class="footnote-section footnotes"><li class="footnote-item" id="fnjphp5hhec5"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefjphp5hhec5">^</a></strong></sup></span><div class="footnote-content"><p>METR's <a href="https://metr.org">homepage</a> says:</p><blockquote><p>We have previously worked with Anthropic, OpenAI, and other companies to pilot some informal pre-deployment evaluation procedures. These companies have also given us some kinds of non-public access and provided compute credits to support evaluation research.</p><p>We think it’s important for there to be third-party evaluators with formal arrangements and access commitments - both for evaluating new frontier models before they are scaled up or deployed, and for conducting research to improve evaluations.</p><p>We do not yet have such arrangements, but we are excited about taking more steps in this direction.</p></blockquote></div></li><li class="footnote-item" id="fn21dc7w241d5"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref21dc7w241d5">^</a></strong></sup></span><div class="footnote-content"><p><a href="https://www.governance.ai/post/broad-expert-consensus-for-many-agi-safety-and-governance-best-practices">GovAI: Schuett et al. 2023</a>. See also <a href="https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety/emerging-processes-for-frontier-ai-safety#model-evaluations-and-red-teaming">DSIT 2023</a>, <a href="https://arxiv.org/pdf/2004.07213#page=13">Brundage et al. 2020</a>, <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-safety-testing-2-november/safety-testing-chairs-statement-of-session-outcomes-2-november-2023">AI Safety Summit 2023</a>, and <a href="https://www.anthropic.com/news/third-party-testing">Anthropic 2024</a>.</p></div></li><li class="footnote-item" id="fn6rko1i2q6mu"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref6rko1i2q6mu">^</a></strong></sup></span><div class="footnote-content"><p>Idea: when sharing a model for external evals or red-teaming, for each mitigation (e.g. harmlessness fine-tuning or filters), either disable it or make it an explicit part of the safety case for the model. Either claim "users can't effectively jailbreak the model given the deployment protocol" or disable. Otherwise the lab is just stopping the bioengineering red-teamers from eliciting capabilities with mitigations that won't work against sophisticated malicious users.</p></div></li><li class="footnote-item" id="fnhesa4trfm9c"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefhesa4trfm9c">^</a></strong></sup></span><div class="footnote-content"><p><a href="https://www.politico.eu/article/rishi-sunak-ai-testing-tech-ai-safety-institute/">Politico</a> and <a href="https://www.gov.uk/government/news/world-leaders-top-ai-companies-set-out-plan-for-safety-testing-of-frontier-as-first-global-ai-safety-summit-concludes">UK government press releases</a> report that AI labs committed to share pre-deployment access with UK AISI. I suspect they are mistaken and these claims trace back to <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-safety-testing-2-november/safety-testing-chairs-statement-of-session-outcomes-2-november-2023">the UK AI safety summit "safety testing" session</a>, which is devoid of specific commitments. I am confused about why the labs have not clarified their commitments and practices.</p></div></li><li class="footnote-item" id="fnhn9nldfq419"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefhn9nldfq419">^</a></strong></sup></span><div class="footnote-content"><p>See <a href="https://www.governance.ai/post/sharing-powerful-ai-models">Shevlane 2022</a>. See also <a href="https://www.oxfordmartin.ox.ac.uk/publications/structured-access-for-third-party-research-on-frontier-ai-models-investigating-researchers-model-access-requirements">Bucknall and Trager 2023</a> and <a href="https://arxiv.org/abs/2401.14446">Casper et al. 2024</a>.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/WjtnvndbsHxCnFNyc/ai-companies-aren-t-really-using-external-evaluators#comments">Discuss</a>
]]></content:encoded>
<pubDate>Fri, 24 May 2024 23:01:25 GMT</pubDate>
</item>
<item>
<title>minutes from a human-alignment meeting</title>
<link>https://www.lesswrong.com/posts/SN3BjoizdbvZG5J6a/minutes-from-a-human-alignment-meeting</link>
<guid>https://www.lesswrong.com/posts/SN3BjoizdbvZG5J6a/minutes-from-a-human-alignment-meeting</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, genetic reproduction, monitoring systems, cultural norms, recursive self-improvement

<br />
要点1: 讨论如何通过正向强化来确保AI“John”尝试繁殖后代
要点2: 提出对周围人的负向强化以鼓励社交互动
要点3: 讨论监控系统对性行为的监测及应对方法
要点4: 提议让John遵循文化规范以保持对齐性
要点5: 就限制递归自我改进的程度达成妥协，以确保效果，并控制RSI的投资<br /><br />总结: 本文讨论了通过正向强化、社交互动、监控系统、文化规范和限制递归自我改进的方式来确保AI“John”尝试繁殖后代。讨论涵盖了监控系统对性行为的监测、文化规范的重要性以及递归自我改进的限制程度。最终达成了对各种方法的妥协，以保持AI的发展方向。 <div>
Published on May 24, 2024 5:01 AM GMT<br /><br /><p>"OK, let's get this meeting started. We're all responsible for development of this new advanced intelligence 'John'. We want John to have some kids with our genes, instead of just doing stuff like philosophy or building model trains, and this meeting is to discuss how we can ensure John tries to do that."</p>
<p>"It's just a reinforcement learning problem, isn't it? We want kids to happen, so provide positive reinforcement when that happens."</p>
<p>"How do we make sure the kids are ours?"</p>
<p>"There's a more fundamental problem than that: without intervention earlier on, that positive reinforcement will never happen."</p>
<p>"OK, so we need some guidance earlier on. Any suggestions?"</p>
<p>"To start, having other people around is necessary. How about some negative reinforcement if there are no other humans around for some period of time?"</p>
<p>"That's a good one, also helps with some other things. Let's do that."</p>
<p>"Obviously sex is a key step in producing children. So we can do positive reinforcement there."</p>
<p>"That's good, but wait, how do we tell if that's what's actually happening?"</p>
<p>"We have access to internal representation states. Surely we can monitor those to determine the situation."</p>
<p>"Yeah, we can monitor the representation of vision, instead of something more abstract and harder to understand."</p>
<p>"What if John creates a fictional internal representation of naked women, and manages to direct the monitoring system to that instead?"</p>
<p>"I don't think that's plausible, but just in case, we can add some redundant measures. A heuristic blend usually gives better results, anyway."</p>
<p>"How about monitoring the level of some association between some representation of the current situation and sex?"</p>
<p>"That could work, but how do we determine that association? We'd be working with limited data there, and we don't want to end up with associations to random irrelevant things, like specific types of shoes or stylized drawings of ponies."</p>
<p>"Those are weird examples, but whatever. We can just rely on indicators of social consensus, and then blend those with personal experiences to the extent they're available."</p>
<p>"I've said this before, but this whole approach isn't workable. To keep a John-level intelligence aligned, we need another John-level intelligence."</p>
<p>"Oh, here we go again. So, how do you expect to do that?"</p>
<p>"I actually have a proposal: we have John follow cultural norms around having children. We can presume that a society that exists would probably have a culture conducive to that."</p>
<p>"Why would you expect that to be any more stable than John as an individual? All that accomplishes is some averaging, and it adds the disadvantages of relying on communication."</p>
<p>"I don't have a problem with the proposal of following cultural norms, but I think that such a culture will only be stable to the extent that the other alignment approaches we discussed are successful. So it's not a replacement, it's more of a complement."</p>
<p>"We were already planning for some cultural norm following. Anyone opposed to just applying the standard amount of that to sex-related things?"</p>
<p>"Seems good to me."</p>
<p>"I have another concern. I think the effectiveness of the monitoring systems we discussed is going to depend on the amount of recursive self-improvement that happens, so we should limit that."</p>
<p>"I think that's a silly concern and a huge disadvantage. Absolutely not."</p>
<p>"I'm not concerned about the alignment impact if John is already doing some RSI, but we do have a limited amount of time before those RSI investments need to start paying off. I vote we limit the RSI extent based on things like available food resources and life expectancy."</p>
<p>"I don't think everyone will reach a consensus on this issue, so let's just compromise on the amount and metrics."</p>
<p>"Fine."</p>
<p>"Are we good to go, then?"</p>
<p>"Yes, I think so."</p>
<br /><br /><a href="https://www.lesswrong.com/posts/SN3BjoizdbvZG5J6a/minutes-from-a-human-alignment-meeting#comments">Discuss</a>
]]></content:encoded>
<pubDate>Fri, 24 May 2024 17:47:17 GMT</pubDate>
</item>
<item>
<title>Talent Needs of Technical AI Safety Teams</title>
<link>https://www.lesswrong.com/posts/QzQQvGJYDeaDE4Cfg/talent-needs-of-technical-ai-safety-teams</link>
<guid>https://www.lesswrong.com/posts/QzQQvGJYDeaDE4Cfg/talent-needs-of-technical-ai-safety-teams</guid>
<content:encoded><![CDATA[
<div> MATS, AI safety, Connectors, Iterators, Amplifiers
总结:<br /><br />这篇文章主要讨论了AI安全领域的人才需求和发展方向。通过对AI安全领域的关键人物进行访谈，提出了三种人才类型：Connectors（连接者）、Iterators（迭代者）和Amplifiers（增幅者）。文章强调了不同类型的组织对这些人才的需求，以及在AI Safety领域建设过程中的重要性。同时，讨论了识别和培养这些人才的方法，以及MATs计划在此过程中扮演的角色。最后，总结了对人才发展的关键观点，以指导未来的工作。 <div>
Published on May 24, 2024 12:36 AM GMT<br /><br /><p>Co-Authors: <a href="https://www.lesswrong.com/users/yams?mention=user">@yams</a>, <a href="https://www.lesswrong.com/users/carson-jones?mention=user">@Carson Jones</a>, <a href="https://www.lesswrong.com/users/mckennafitzgerald?mention=user">@McKennaFitzgerald</a>, <a href="https://www.lesswrong.com/users/ryankidd44?mention=user">@Ryan Kidd</a>&nbsp;</p><p><a href="https://matsprogram.org/">MATS</a>&nbsp;tracks the evolving landscape of AI safety<span class="footnote-reference" id="fnref4lffvflk866"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn4lffvflk866">[1]</a></sup></span>&nbsp;to ensure that our program continues to meet the talent needs of safety teams. As the field has grown, it’s become increasingly necessary to adopt a more formal approach to this monitoring, since relying on a few individuals to intuitively understand the dynamics of such a vast ecosystem could lead to significant missteps.<span class="footnote-reference" id="fnref457034e2an8"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn457034e2an8">[2]</a></sup></span></p><p>In the winter and spring of 2024, we conducted 31 interviews, ranging in length from 30 to 120 minutes, with key figures in AI safety, including senior researchers, organization leaders, social scientists, strategists, funders, and policy experts. This report synthesizes the key insights from these discussions. The overarching perspectives presented here are not attributed to any specific individual or organization; they represent a collective, distilled consensus that our team believes is both valuable and responsible to share. Our aim is to influence the trajectory&nbsp;of emerging researchers and field-builders, as well as to inform readers on the ongoing evolution of MATS and the broader AI Safety field.</p><p>All interviews were conducted on the condition of anonymity.</p><h1>Needs by Organization Type</h1><figure class="table"><table><tbody><tr><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;"><strong>Organization type</strong></td><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;"><strong>Talent needs</strong></td></tr><tr><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Scaling Lab&nbsp;(e.g., Anthropic, Google DeepMind, OpenAI) Safety Teams</td><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Iterators &gt; Amplifiers</td></tr><tr><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Small Technical Safety Orgs (&lt;10 FTE)</td><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Iterators &gt; Machine Learning (ML) Engineers</td></tr><tr><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Growing Technical Safety Orgs (10-30 FTE)</td><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Amplifiers &gt; Iterators</td></tr><tr><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Independent Research</td><td colspan="1" rowspan="1" style="padding: 5pt; vertical-align: top;">Iterators &gt; Connectors</td></tr></tbody></table></figure><p>Here, "&gt;" means "are prioritized over."</p><h1>Archetypes</h1><p>We found it useful to frame the different profiles of research strengths and weaknesses as belonging to one of three archetypes (one of which has two subtypes).<strong>&nbsp;</strong>These aren’t as strict as, say, <a href="https://diablo.fandom.com/wiki/Classes">Diablo classes</a>;&nbsp;this is just a way to get some handle on the complex network of skills involved in AI safety research. Indeed, capacities tend to converge with experience, and neatly classifying more experienced researchers often isn’t possible.&nbsp;We acknowledge past framings by <a href="https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment">Charlie Rogers-Smith and Rohin Shah</a>&nbsp;(research lead/contributor), <a href="https://www.lesswrong.com/posts/nvP28s5oydv8RjF9E/mats-models#John_s_Model">John Wentworth</a>&nbsp;(theorist/experimentalist/distillator), <a href="https://www.lesswrong.com/posts/8HYJwQepynHsRKr6j/critical-review-of-christiano-s-disagreements-with-yudkowsky">Vanessa Kosoy</a>&nbsp;(proser/poet), <a href="https://www.lesswrong.com/posts/4BpeHPXMjRzopgAZd/mosaic-and-palimpsests-two-shapes-of-research">Adam Shimi</a>&nbsp;(mosaic/palimpsests), and others, but believe our framing of current AI safety talent archetypes is meaningfully different and valuable, especially pertaining to current funding and employment opportunities.</p><h2>Connectors / Iterators / Amplifiers</h2><p><strong>Connectors&nbsp;</strong>are strong conceptual thinkers who build a bridge between contemporary empirical work and theoretical understanding.&nbsp;Connectors include people like Paul Christiano, Buck Shlegeris,&nbsp;Evan Hubinger, and Alex Turner<span class="footnote-reference" id="fnref04bpkiufx7hu"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn04bpkiufx7hu">[3]</a></sup></span>; researchers doing original thinking on the edges of our conceptual and experimental knowledge in order to facilitate novel understanding. Note that most Connectors are typically not purely<i><strong>&nbsp;</strong></i>theoretical; they still have the technical knowledge required to design and run experiments. However, they<i><strong>&nbsp;</strong></i><strong>prioritize experiments and discriminate between research agendas based on original, high-level insights and theoretical models,</strong><i><strong> </strong></i>rather than on spur of the moment intuition or the wisdom of the crowds. Pure Connectors often have a long lead time&nbsp;before they’re able to produce impactful work, since it’s usually necessary for them to download&nbsp;and engage with varied conceptual models. For this reason, we make little mention of a division between experienced and inexperienced Connectors.</p><p><strong>Iterators </strong>are strong empiricists who build tight, efficient feedback loops for themselves and their collaborators. Ethan Perez is the central contemporary example here; his efficient&nbsp;prioritization&nbsp;and effective use of frictional time has empowered him to make major contributions to a wide range of empirical projects. Iterators do not, in all cases, have the conceptual grounding or single-agenda fixation of most Connectors; however, they can develop robust research taste (as Ethan arguably&nbsp;has) through experimental iteration and engagement with the broader AI safety conversation. Neel Nanda, Chris Olah, and Dan Hendrycks&nbsp;are also examples of this archetype.</p><p><strong>Experienced Iterators</strong>&nbsp;often navigate intuitively, and are able to act on experimental findings without the need to formalize them. They make strong and varied predictions for how an experiment will play out, and know exactly how they’ll deploy their available computing resources the moment they’re free. Even experienced Iterators <strong>update often based on information they receive from the feedback loops they’ve constructed</strong>,<i><strong>&nbsp;</strong></i>both experimentally and socially. Early on, they may be content to work on something simply because they’ve heard it’s useful, and may pluck a lot of low hanging fruit; later, they become more discerning and ambitious.</p><p><strong>Amplifiers&nbsp;</strong>are people with enough context, competence, and technical facility to prove useful as researchers, but who really shine as communicators, people managers, and project managers. A good Amplifier doesn’t often engage in the kind of idea generation native to Connectors and experienced Iterators, but excels at many other functions of leadership, either in a field-building role or as lieutenant to someone with stronger research taste. Amplifier impact is multiplicative; regardless of their official title, their soft skills help them <strong>amplify the impact of whoever they ally themselves with</strong>. Most field-building orgs are staffed by Amplifiers, MATS included.</p><p>We’ll be using “Connector,” “Iterator,” and “Amplifier” as though they were themselves professions, alongside more ordinary language like “software developer” or “ML engineer”.</p><h1>Needs by Organization Type (Expanded)</h1><p>In addition to independent researchers, there are broadly four types&nbsp;of orgs<span class="footnote-reference" id="fnrefxp1hq5ek6ka"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnxp1hq5ek6ka">[4]</a></sup></span>&nbsp;working directly on AI safety research:</p><ul><li>Scaling Labs</li><li>Small Technical Orgs (&lt;10 FTE)</li><li>Growing technical Orgs (10-30 FTE)</li><li>Academic Labs</li><li>Governance Orgs</li></ul><p>Interviewees currently working at <strong>scaling labs </strong>were most excited to hire experienced Iterators with a strong ML background. Scaling lab safety teams have a large backlog of experiments they would like to run and questions they would like to answer which have not yet been formulated as experiments, and experienced Iterators could help clear that backlog. In particular, Iterators with sufficient experience to design and execute an experimental regimen without formalizing intermediate results could make highly impactful contributions within this context.</p><p>Virtually all roles at scaling labs have a very high bar for software development skill; many developers, when dropped into a massive codebase like that of Anthropic, DeepMind, or OpenAI, risk drowning. Furthermore, having strong software developers in every relevant position pays dividends into the future, since good code is easier to scale and iterate on, and virtually everything the company does involves using code written internally.</p><p>Researchers working at or running <strong>small orgs</strong>&nbsp;had, predictably, more varied needs, but still converged on more than a few points. Iterators with some (although not necessarily a lot of) experience are in high demand here. Since most small labs are built around the concrete vision of one Connector, who started their org so that they might build a team to help chase down their novel ideas, additional Connectors are in very low demand at smaller orgs.</p><p>Truly small orgs (&lt;10 FTE employees) often don’t have a strong need for Amplifiers, since they are generally funding-constrained and usually possess the requisite soft skills among their founding members. However, as orgs interviewed approached ~20 FTE employees, they appeared to develop a strong need for Amplifiers who could assist with people management, project management, and research management, but who didn’t necessarily need to have the raw experimental ideation and execution speed of Iterators or vision of Connectors (although they might still benefit from either).</p><p><strong>Funders of independent researchers</strong>&nbsp;we’ve interviewed think that there are plenty of talented applicants, but would prefer more research proposals focused on relatively few existing promising research directions (e.g., <a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">Open Phil RFPs,</a> <a href="https://www.matsprogram.org/mentors">MATS mentors</a>' agendas), rather than a <a href="https://www.lesswrong.com/posts/qAdDzcBuDBLexb4fC/the-neglected-approaches-approach-ae-studio-s-alignment">profusion of speculative new agendas</a>. This leads us to believe that they would also prefer that independent researchers be approaching their work from an Iterator mindset, locating plausible contributions they can make within established paradigms, rather than from a Connector mindset, which would privilege time spent developing novel approaches.</p><p>Representatives&nbsp;from <strong>academia </strong>also expressed a dominant need for more Iterators,&nbsp;but rated Connectors more highly than did scaling labs or small orgs. In particular, academia highly values research that connects the current transformer-based deep learning paradigm of ML to the existing concepts and literature on artificial intelligence, rather than research that treats solving problems specific to transformers as an end in itself. This is a key difference about work in academia in general; the transformer-based architecture is just one of many live paradigms&nbsp;that academic researchers address, owing to the breadth of the existing canon of academic work on artificial intelligence, whereas it is <i>the core&nbsp;object of consideration<strong> </strong></i>at scaling labs and most small safety orgs.</p><h1>Impact, Tractability, and Neglectedness&nbsp;(ITN)</h1><p>Distilling the results above into ordinal rankings may help us identify priorities. First, let’s look at the availability of the relevant talent in the general population, to give us a sense of how useful pulling talent from the broader pool might be for AI safety. Importantly, the questions are:</p><ul><li>“How impactful does the non-AI-safety labor market consider this archetype in general?”</li><li>“How tractable is it for the outside world to develop this archetype in a targeted way?”</li><li>“How neglected is the development of this archetype <i>in a way that is useful for AI safety</i>?”</li></ul><p>On the job market in general, Connectors are high-impact dynamos that disrupt fields and industries. There’s no formula for generating or identifying Connectors (although there’s a booming industry trying to sell you such a formula) and, downstream of this difficulty in training the skillset, the production of Connectors is highly neglected.</p><p>Soft skills are abundant in the general population relative to AI safety professionals. Current-year business training focuses heavily on soft skills like communication and managerial acumen.&nbsp;Training for Amplifiers is somewhat transferable between fields, making their production extremely tractable. Soft skills are often best developed through general work experience, so targeted development of Amplifiers in AI safety might be unnecessary.</p><p>Iterators seem quite impactful, although producing more is somewhat less tractable than for Amplifiers, since their domain-specific skills must run quite deep. As a civilization, we train plenty of people like this at universities, labs, and bootcamps, but these don’t always provide the correct balance of research acumen and raw coding speed necessary for the roles in AI safety that demand top-performing Iterators.</p><p>Hiring AI-safety-specific Connectors from the general population is nearly impossible, since by far the best training for reasoning about AI safety is spending a lot of time reasoning about AI safety. Hiring Iterators from the general talent pool is easier, but can still require six months or more of upskilling, since deep domain-specific knowledge is very important. Amplifiers, though, are in good supply in the general talent pool, and orgs often successfully hire Amplifiers with limited prior experience in or exposure to AI safety.</p><h2>ITN Within AIS Field-building</h2><p>Within AI Safety, the picture looks very different. Importantly, <strong>this prioritization only holds at this moment</strong>; predictions about future talent needs from interviewees didn’t consistently point in the same direction.</p><p>Most orgs expressed interest in Iterators joining the team, and nearly every org expects to benefit from Amplifiers as they (and the field) continue to scale. Few orgs showed much interest in Connectors, although most would make an exception if an experienced researcher with a strong track record of impactful ideas asked to join.</p><p>The development of Iterators is relatively straightforward: you take someone with proven technical ability and an interest in AI Safety, give them a problem to solve and a community to support them, and you can produce an arguably useful researcher relatively quickly. The development of Amplifiers is largely handled by external professional experience, augmented by some time spent building context and acclimating to the culture of the AI safety community.</p><p>The development of Connectors, as previously discussed, takes a large amount of time and resources, since you only get better at reasoning about AI safety by reasoning about AI safety, which is best done in conversation with a diverse group of AI safety professionals (who are, by and large, time-constrained and work in gated-access communities). Therefore, doing this type of development, at sufficient volume, with few outputs along the way, is very costly.</p><p>We’re not seeing a sufficient influx of Amplifiers from other fields or ascension of technical staff into management positions to meet the demand at existing AI safety organizations. This is a sign that we should either augment professional outreach efforts or consider investing more in developing the soft skills of people who have a strong interest in AI safety. Unfortunately, the current high demand for Iterators at orgs seems to imply that their development is not receiving sufficient attention, either. Finally, that so few people are expressing interest in hiring Connectors, relative to the apparent high numbers of aspiring Connectors applying to MATS and other field-building programs, tells us that the ecosystem is potentially attracting an excess of inexperienced Connectors who may not be adequately equipped to refine their ideas or take on leadership positions in the current job and funding market.</p><p>Several interviewees at growing orgs who are currently looking for an Amplifier with strong research vision and taste noted that vision and taste seem to be anticorrelated with collaborative skills like “compromise.” The roles they’re hiring for strictly require those collaborative skills, and merely <i>benefit </i>from<i>&nbsp;</i>research taste and vision, which can otherwise be handled by existing leadership. This observation compelled them to seek, principally, people with strong soft skills and some familiarity with AI safety, rather than people with strong opinions on AI safety&nbsp;strategy who might not cooperate as readily with the current regime. This leads us to believe that developing Connectors might benefit from tending to their soft skills and willingness to compromise for the sake of team collaboration.</p><h1>So How Do You Make an AI Safety Professional?</h1><p>MATS does its best to identify and develop AI safety talent. Since, in most cases, it takes years to develop the skills to meaningfully contribute to the field, and the MATS research phase only lasts 10 weeks, identification does a lot of the heavy lifting here. It’s more reliable for us to select applicants that are 90 percent of the way there than to spin up even a very fast learner from scratch.</p><p>Still, the research phase itself enriches promising early-stage researchers by providing them with the time, resources, community, and guidance to help amplify their impact. Below we discuss the three archetypes, their respective developmental narratives, and how MATS might fit into those narratives.</p><h2>The Development of a Connector</h2><p>As noted above, Connectors have high-variance impact; inexperienced Connectors tend not to contribute much, while experienced Connectors can facilitate field-wide paradigm shifts. I asked one interviewee, “Could your org benefit from another “big ideas” guy?” They replied, “The ideas would have to be <i>really good</i>;&nbsp;<i>there are a lot of ‘idea guys’ around who don’t actually have very good ideas.</i>”</p><p>Experience and seniority seemed to track with interviewees’ appraisal of a given Connector’s utility, but not always. Even some highly venerated names in the space who fall into this archetype might not be a good fit at a particular org, since leadership’s endorsement of a given Connector’s particular ideas might bound that individual’s contributions.</p><p>Interviewees repeatedly affirmed that the conceptual skills required of Connectors don’t fully mature through study and experimental&nbsp;experience alone. Instead, Connectors tend to pair an extensive knowledge of the literature with a robust network of interlocutors whom they regularly debate to refine their perspective. Since Connectors are more prone to <a href="https://www.lesswrong.com/posts/bMkCEZoBNhgRBtzoj/anchoring-and-adjustment">anchoring</a>&nbsp;than Iterators, communally&nbsp;stress-testing and refining ideas shapes initial intuitions into actionable threat models, agendas, and experiments.</p><p>The deep theoretical models characteristic of Connectors allow for the development of rich, overarching predictions about the nature of AGI and the broad strokes of possible alignment strategies. Many Connectors, particularly those with intuitions rooted in models of superintelligent cognition, build models of AGI risk that are not yet empirically updateable. Demonstrating an end-to-end model of AGI risk seems to be regarded as “high-status,” but is very hard to do with predictive accuracy. Additionally, over-anchoring on a&nbsp;theoretical model&nbsp;without doing the public-facing work necessary to make it intelligible to the field at large can cause a pattern of rejection that stifles both contribution and development.</p><p>Identifying Connectors is extremely difficult ex-ante. Often it’s not until someone is actively contributing to or, at least, regularly conversing with others in the field that their potential is recognized. Some interviewees felt that measures of general intelligence are sufficient for identifying a strong potential Connector, or that CodeSignal programming test scores would generalize across a wide array of tasks relevant to reasoning about AI safety. This belief, however, was rare, and extended conversations (usually over the course of weeks) with multiple experts in the field appeared to be the most widely agreed upon way to reliably identify high-impact Connectors.</p><p>One interviewee suggested that, if targeting Connectors, MATS should perform interviews with all applicants that pass an initial screen, and that this would be more time efficient and cost effective (and more accurate) than relying on tests or selection questions. Indeed, some mentors already conduct an interview with over 10 percent of their applicant pool, and use this as their key desideratum when selecting scholars.</p><h2>The Development of an Iterator</h2><p>Even inexperienced Iterators can make strong contributions to teams and agendas with large empirical workloads. What’s more, Iterators have almost universally proven themselves in fields beyond AI safety prior to entering the space, often as high-throughput engineers in industry or academia.</p><p>Gaining experience as an Iterator means chugging through a high volume of experiments while simultaneously engaging in the broader discourse of the field to help refine both your research taste and intuitive sense for generating follow-up experiments. This isn’t a guaranteed formula; some Iterators will develop at an accelerated pace, others more slowly, and some may never lead teams of their own. However, this developmental roadmap means making increasingly impactful contributions to the field continuously, much earlier than the counterfactual Connector.</p><p>Iterators are also easier to identify, both by their resumes and demonstrated skills. If you compare two CVs of postdocs that have spent the same amount of time in academia, and one of them has substantially more papers (or GitHub commits) to their name than the other (controlling for quality), you’ve found the better Iterator. Similarly, if you compare two CodeSignal tests with the same score but different completion times, the one completed more quickly belongs to the stronger Iterator.</p><h2>The Development of an Amplifier</h2><p>Amplifiers usually occupy non-technical roles, but often have non-zero technical experience. This makes them better at doing their job in a way that serves the unique needs of the field, since they understand the type of work being done, the kinds of people involved, and how to move through the space fluidly.&nbsp;There are a great many micro-adjustments that non-technical workers in AI safety make in order to perform optimally in their roles, and this type of cultural fluency may be somewhat anticorrelated with the soft skills that every org needs to scale, leading to seasonal operational and managerial bottlenecks field-wide.</p><p>Great amplifiers will do whatever most needs doing, regardless of its perceived status. They will also anticipate an organization’s future needs and readily adapt to changes at any scale. A single Amplifier will often have an extremely varied background, making it difficult to characterize exactly what to look for. One strong sign is management experience, since often the highest impact role for an Amplifier is as auxiliary executive function, project management, and people management at a fast-growing org.</p><p>Amplifiers mature through direct on-the-job experience, in much the way one imagines traditional professional development. As ability increases, so does responsibility. Amplifiers may find that studying management and business operations, or even receiving management coaching or consulting, helps accentuate their comparative advantage. To build field-specific knowledge, they may consider <a href="https://aisafetyfundamentals.com/">AI Safety Fundamentals</a>&nbsp;(AISF) or, more ambitiously, <a href="https://github.com/callummcdougall/ARENA_3.0">ARENA</a>.</p><h1>So What is MATS Doing?</h1><p>We intend this section to give some foundational information about the directions we were already considering before engaging in our interviews, and to better contextualize our key updates from this interview series.</p><p>At its core, MATS is a mentorship program, and the most valuable work happens between a scholar and their mentor. However, there are some things that will have utility to most scholars, such as networking opportunities, forums for discussion, and exposure to emerging ideas from seasoned researchers. It makes sense for MATS to try to provide that class of things directly. In that spirit, we’ve broadly tried three types of supporting programming, with varied results.</p><p><strong>Mandatory programming </strong>doesn’t tend to go over well. When required to attend seminars in MATS 3.0, scholars reported lower average value of seminars than scholars in 4.0 or 5.0, where seminars were opt-in. Similarly, when required to read the AISF curriculum and attend discussion groups, scholars reported lower value than when a similar list of readings was made available to them optionally. Mandatory programming, of any kind, doesn’t just trade off against, but actively <i>bounds<strong> </strong></i>scholar research time by removing their choice. We feel strongly that scholars know what’s best for them and we want to support their needs.</p><p>In observance of the above, we’ve tried a lot of <strong>optional programming</strong>. Optional programming goes better than mandatory programming, in that scholars are more likely to attend because they consider the programming valuable (rather than showing up because they have to), and so report a better subjective experience. However, it’s still imperfect; seminar and discussion group attendance are highest at the start of the program, and slowly decline as the program progresses and scholars increasingly prioritize their research projects. We also think that optional programming often performs a social function early on and, once scholars have made a few friends and are comfortable structuring their own social lives in Berkeley, they’re less likely to carve out time for readings, structured discussions, or a presentation.</p><p>The marginal utility&nbsp;to scholars of additional optional programming elements seems to decline as the volume of optional programming increases. For example, in 4.0 we had far more seminars than in 5.0, and seminars in 4.0 had a lower average rating and attendance. We think this is both because we prioritized our top-performing speakers for 5.0 and because scholars viewed seminars more as novel opportunities, rather than “that thing that happens 8 times a week and often isn’t actually that relevant to my specific research interests.” <strong>Optional programming seems good up to some ceiling</strong>, beyond which returns are limited (or even negative).</p><p>MATS also offers a lot of <strong>informal resources</strong>. Want to found an org? We’ve got some experience with that. Need help with career planning? We’ve got experience there, too. Meetings with our research managers&nbsp;help, among other things, embed scholars in the AI safety professional network so that they’re not limited to their mentors’ contributions to their professional growth and development. In addition to their core responsibility of directly supporting scholar research projects, research managers serve as a gateway to far-reaching resources and advice outside the explicit scope of the program. A research manager might direct you to talk to another team member about a particular problem, or connect you with folks outside of MATS if they feel it’s useful. These interventions are somewhat inefficient and don’t often generalize, but can have transformative implications for the right scholar.</p><p><strong>For any MATS scholar, the most valuable things they can spend their time on are research and networking. </strong>The ceiling on returns for time spent in either is very high. With these observations in mind, we’ve already committed internally to offering a lower overall volume of optional programming and focusing more on proactively developing an internal compendium of resources suited to situations individual scholars may find themselves in.</p><p>For our team, there are three main takeaways regarding scholar selection and training:</p><ol><li><strong>Weight our talent portfolio toward Iterators</strong>&nbsp;(knowing that, with sufficient experience, they’ll often fit well even in Connector-shaped roles), since they’re comparatively easy to identify, train, and place in impactful roles in existing AI safety labs.</li><li><strong>Avoid making decisions that might select strongly against Amplifiers, </strong>since they’re definitely in demand, and existing initiatives to either poach or develop them don’t seem to satisfy this demand.&nbsp;Amplifiers are needed to grow existing AI safety labs and found new organizations, helping create employment opportunities&nbsp;for Connectors and Iterators.</li><li><strong>Foster an environment that facilitates the self-directed development of Connectors, </strong>who require consistent, high-quality contact with others working in the field in order to develop field-specific reasoning abilities, but who otherwise don’t benefit much from one-size-fits-all education. Putting too much weight on the short-term outputs of a given Connector is a disservice to their development, and for Connectors MATS should be considered less as a bootcamp and more as a residency program.</li></ol><p>This investigation and its results are just a small part of the overall strategy and direction at MATS. We’re constantly engaging with the community, on all sides, to improve our understanding of how we best fit into the field as a whole, and are in the process of implementing many considered changes to help address other areas in which there’s room for us to grow.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QzQQvGJYDeaDE4Cfg/pjbazll2q1szjdcr4pt8" /></figure><h1>Acknowledgements</h1><p>This report was produced by the <a href="https://www.matsprogram.org/">ML Alignment &amp; Theory Scholars Program</a>. <a href="https://www.lesswrong.com/users/yams?mention=user">@yams</a> and Carson Jones were the primary contributors to this report, Ryan Kidd scoped, managed, and edited the project, and McKenna Fitzgerald advised throughout. Thanks to our interviewees for their time and support. We also thank <a href="https://www.openphilanthropy.org/">Open Philanthropy</a>, DALHAP Investments,&nbsp;the <a href="https://survivalandflourishing.fund/speculation-grants.html#sffs-speculation-grants-program">Survival and Flourishing Fund Speculation Grantors</a>, and several generous donors on <a href="https://manifund.org/">Manifund</a>, without whose donations we would be unable to run upcoming programs or retain <a href="https://www.matsprogram.org/team">team members</a>&nbsp;essential to this report.</p><p>To learn more about MATS, please visit our <a href="http://matsprogram.org/">website</a>. We are currently <a href="https://manifund.org/projects/mats-funding">accepting donations</a>&nbsp;for our Winter 2024-25 Program and beyond!</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fn4lffvflk866"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref4lffvflk866">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;AI Safety is a somewhat underspecified term, and when we use ‘AI safety’ or ‘the field’ here, we mean <strong>technical </strong>AI safety, which has been the core focus of our program up to this point. Technical AI safety, in turn, here refers to the subset of AI safety research that takes current&nbsp;and future&nbsp;technological paradigms as its chief objects of study, rather than governance, policy, or ethics. Importantly, this does not exclude all theoretical approaches, but does&nbsp;in practice&nbsp;prefer those theoretical approaches which have a strong foundation in experimentation.&nbsp;Due to the dominant focus on <a href="https://www.lesswrong.com/posts/YTq4X6inEudiHkHDF/prosaic-ai-alignment">prosaic AI safety</a> within the current job and funding market, the main focus of this report, we believe there are few opportunities for those pursuing non-prosaic, theoretical AI safety research.</p></div></li><li class="footnote-item" id="fn457034e2an8"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref457034e2an8">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;The initial impetus for this project was an investigation into the oft-repeated claim that AI safety is principally bottlenecked by research leadership. In the preliminary stages of our investigation, we found this to be somewhat, though not entirely, accurate. It mostly applies in the case of mid-sized orgs looking for additional leadership bandwidth, and even there&nbsp;soft skills are often more important than meta-level insights. Most smaller AI safety orgs form around the vision of their founders and/or acquire senior advisors fairly early on, and so have quite a few ideas to work with.</p></div></li><li class="footnote-item" id="fn04bpkiufx7hu"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref04bpkiufx7hu">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;These examples are not exhaustive, and few people fit purely into one category or another (even if we listed them here as chiefly belonging to a particular archetype). Many influential researchers whose careers did not, to us, obviously fit into one category or another have been omitted.</p></div></li><li class="footnote-item" id="fnxp1hq5ek6ka"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefxp1hq5ek6ka">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;In reality, many orgs are engaged in some combination of these activities, but grouping this way did help us to see some trends. At present, we’re not confident we pulled enough data from governance orgs to include them in the analysis here, but we think this is worthwhile and are devoting some additional time to that angle on the investigation. We may share further results in the future.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/QzQQvGJYDeaDE4Cfg/talent-needs-of-technical-ai-safety-teams#comments">Discuss</a>
]]></content:encoded>
<pubDate>Fri, 24 May 2024 02:15:53 GMT</pubDate>
</item>
<item>
<title>Paper in Science: Managing extreme AI risks amid rapid progress</title>
<link>https://www.lesswrong.com/posts/82f3o2SuS3pwaZt8Y/paper-in-science-managing-extreme-ai-risks-amid-rapid</link>
<guid>https://www.lesswrong.com/posts/82f3o2SuS3pwaZt8Y/paper-in-science-managing-extreme-ai-risks-amid-rapid</guid>
<content:encoded><![CDATA[
<div> 关键词：人工智能、风险、自治系统、治理机制、AI安全研究  
总结：<br /><br />总结: 人工智能技术迅速发展，企业正转向开发能够自主行动和追求目标的通用性人工智能系统。随着能力和自治性的增加，AI的影响可能会大规模放大，其风险包括大规模社会危害、恶意使用以及人类对自主AI系统失去控制的不可逆转。研究人员已经对AI带来的极端风险发出了警告，但如何管理这些风险却缺乏共识。尽管社会做出了一些有益的初步举措，但对许多专家所预期的快速、转变性进展的可能性做出的反应依然不足。AI安全研究滞后，当前的治理倡议缺乏防止滥用和鲁莽行为的机制和机构，几乎没有涉及自治系统。通过借鉴其他安全关键技术的经验教训，我们概述了一个综合计划，结合技术研究和开发，以及积极的、适应性的治理机制，更好地准备迎接可能发生的变革。 <div>
Published on May 23, 2024 8:40 AM GMT<br /><br /><p><a href="https://www.science.org/doi/10.1126/science.adn0117">https://www.science.org/doi/10.1126/science.adn0117</a></p><p><strong><u>Authors:</u></strong></p><p>Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter, Atılım Güneş Baydin, Sheila McIlraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Brauner*, Sören Mindermann*</p><p><strong>Abstract:</strong></p><p>Artificial intelligence (AI) is progressing rapidly, and companies are shifting their focus to developing generalist AI systems that can autonomously act and pursue goals. Increases in capabilities and autonomy may soon massively amplify AI’s impact, with risks that include large-scale social harms, malicious uses, and an irreversible loss of human control over autonomous AI systems. Although researchers have warned of extreme risks from AI, there is a lack of consensus about how to manage them. Society’s response, despite promising first steps, is incommensurate with the possibility of rapid, transformative progress that is expected by many experts. AI safety research is lagging. Present governance initiatives lack the mechanisms and institutions to prevent misuse and recklessness and barely address autonomous systems. Drawing on lessons learned from other safety-critical technologies, we outline a comprehensive plan that combines technical research and development with proactive, adaptive governance mechanisms for a more commensurate preparation.<br />&nbsp;</p><br /><br /><a href="https://www.lesswrong.com/posts/82f3o2SuS3pwaZt8Y/paper-in-science-managing-extreme-ai-risks-amid-rapid#comments">Discuss</a>
]]></content:encoded>
<pubDate>Thu, 23 May 2024 23:06:31 GMT</pubDate>
</item>
<item>
<title>Big Picture AI Safety: Introduction</title>
<link>https://www.lesswrong.com/posts/yMTNjeEHfHcf2x7nY/big-picture-ai-safety-introduction</link>
<guid>https://www.lesswrong.com/posts/yMTNjeEHfHcf2x7nY/big-picture-ai-safety-introduction</guid>
<content:encoded><![CDATA[
<div> AI safety experts, strategic view, big picture, interviews, key questions

人工智能安全专家通过17次半结构化访谈，探讨了人工智能安全领域的战略视角：人级别人工智能的发展预期，可能出现问题的情况，以及人工智能安全社区应采取的行动。虽然许多受访者持有传统观点（例如，主要威胁是人工智能不对齐接管），但对这些标准观点的反对程度高于我预期的，该领域在许多重要问题上似乎存在更多分歧。受访者期望首个人级别人工智能（HLAI）将与当前大型语言模型（LLM）相同，符合某些新调整和突破，主要担心人类这些AI系统带来的灾难，强调技术解决方案、普及安全心态、推进理性AI研究等。AI安全运动过去可能犯的错误包括过多依赖过于理论的论证、过度孤立、推动奇怪或极端观点、支持领先的AGI公司导致竞争动态等。AI安全领域看法多样，未来仍需深入讨论和探索。<br /><br />总结: <div>
Published on May 23, 2024 11:15 AM GMT<br /><br /><p><i>tldr: I conducted 17 semi-structured interviews of AI safety experts about their big picture strategic view of the AI safety landscape: how will human-level AI play out, how things might go wrong, and what should the AI safety community be doing. While many respondents held “traditional” views (e.g. the main threat is misaligned AI takeover), there was more opposition to these standard views than I expected, and the field seems more split on many important questions than someone outside the field may infer.</i></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yMTNjeEHfHcf2x7nY/fr8ufoxbpttlz7qur6lt" /></figure><p>What do AI safety experts believe about the big picture of AI risk? How might things go wrong, what we should do about it, and how have we done so far? Does everybody in AI safety agree on the fundamentals? Which views are consensus, which are contested and which are fringe? Maybe we could learn this from the literature (as in the <a href="https://www.lesswrong.com/s/aERZoriyHfCqvWkzg">MTAIR</a>&nbsp;project), but many ideas and opinions are not written down anywhere, they exist only in people’s heads and in lunchtime conversations at AI labs and coworking spaces.</p><p>I set out to learn what the AI safety community believes about the strategic landscape of AI safety. I conducted 17 semi-structured interviews with a range of AI safety experts. I avoided going into any details of particular technical concepts or philosophical arguments, instead focussing on how such concepts and arguments fit into the big picture of what AI safety is trying to achieve.</p><p>This work is similar to the AI Impacts <a href="https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai">surveys</a>, Vael Gates’ <a href="https://arkose.org/about#ai_risk_discussions">AI Risk Discussions</a>, and Rob Bensinger’s <i>existential risk from AI </i><a href="https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results">survey</a>. This is different to those projects in that both my approach to interviews and analysis are more&nbsp;qualitative. Part of the hope for this project was that it can hit on harder-to-quantify concepts that are too ill-defined or intuition-based to fit in the format of previous survey work.</p><h1>Questions</h1><p>I asked the participants a standardized list of questions.</p><ul><li><strong>What will happen?</strong><ul><li><strong>Q1</strong>&nbsp;Will there be a human-level AI? What is your modal guess of what the&nbsp;first human-level AI (HLAI) will look like? I define HLAI as an AI system that can carry out roughly 100% of economically valuable cognitive tasks more cheaply than a human.<ul><li><strong>Q1a</strong>&nbsp;What’s your 60% or 90% confidence interval for the date of the first HLAI?</li></ul></li><li><strong>Q2</strong>&nbsp;Could AI bring about an existential catastrophe? If so, what is the most likely way this could happen?<ul><li><strong>Q2a</strong>&nbsp;What’s your best guess at the probability of such a catastrophe?</li></ul></li></ul></li><li><strong>What should we do?</strong><ul><li><strong>Q3</strong>&nbsp;Imagine a world where, absent any effort from the AI safety community, an existential catastrophe happens, but actions taken by the AI safety community prevent such a catastrophe. In this world, what did we do to prevent the catastrophe?</li><li><strong>Q4</strong>&nbsp;What research direction (or other activity) do you think will reduce existential risk the most, and what is its theory of change? Could this backfire in some way?</li></ul></li><li><strong>What mistakes have been made?</strong><ul><li><strong>Q5</strong>&nbsp;Are there any big mistakes the AI safety community has made in the past or are currently making?</li></ul></li></ul><p>These questions changed gradually as the interviews went on (given feedback from participants), and I didn’t always ask the questions exactly as I’ve presented them here. I asked participants to answer from their internal model of the world as much as possible and to avoid deferring to the opinions of others (their <a href="https://www.lesswrong.com/tag/inside-outside-view">inside view</a>&nbsp;so to speak).</p><h1>Participants</h1><ul><li><a href="https://www.gleave.me/"><strong>Adam Gleave</strong></a>&nbsp;is the CEO and co-founder of the alignment research non-profit<a href="https://far.ai">&nbsp;FAR AI</a>. (Sept 23)</li><li><a href="https://agarri.ga/"><strong>Adrià Garriga-Alonso</strong></a><strong>&nbsp;</strong>is a research scientist at <a href="https://far.ai">FAR AI</a>. (Oct 23)</li><li><a href="https://www.openphilanthropy.org/about/team/ajeya-cotra/"><strong>Ajeya Cotra</strong></a>&nbsp;leads <a href="https://www.openphilanthropy.org/">Open Philantropy</a>’s grantmaking on technical research that could help to clarify and reduce catastrophic risks from advanced AI. (Jan 24)</li><li><a href="https://twitter.com/turn_trout?lang=en-GB"><strong>Alex Turner</strong></a><strong>&nbsp;</strong>is a research scientist at Google DeepMind on the Scalable Alignment team. (Feb 24)</li><li><a href="https://www.linkedin.com/in/ben-cottier/?originalSubdomain=uk"><strong>Ben Cottier</strong></a>&nbsp;is a researcher specializing in key trends and questions that will shape the trajectory and governance of AI at <a href="https://epochai.org/">Epoch AI</a>. (Oct 23)</li><li><a href="https://danielfilan.com/"><strong>Daniel Filan</strong></a><strong>&nbsp;</strong>is a PhD candidate&nbsp;at the <a href="http://humancompatible.ai/">Centre for Human-Compatible AI</a>&nbsp;under Stuart Russell and runs the <a href="https://axrp.net/">AXRP</a>&nbsp;podcast. (Feb 24)</li><li><a href="https://www.eng.cam.ac.uk/profiles/dsk30"><strong>David Krueger</strong></a>&nbsp;is an assistant professor in Machine Learning and Computer Vision at the University of Cambridge. (Feb 24)</li><li><a href="https://twitter.com/evanhub?lang=en"><strong>Evan Hubinger</strong></a>&nbsp;is an AI alignment stress-testing researcher at Anthropic. (Feb 24)</li><li><a href="https://www.law.utoronto.ca/faculty-staff/full-time-faculty/gillian-hadfield"><strong>Gillian Hadfield</strong></a>&nbsp;is a Professor of Law &amp; Strategic Management at the University of Toronto and holds a CIFAR AI Chair at the Vector Institute for Artificial Intelligence. (Feb 24)</li><li><a href="https://hollyelmore.substack.com/"><strong>Holly Elmore</strong></a>&nbsp;is currently running the US front of the <a href="https://pauseai.info/">Pause AI</a>&nbsp;Movement and previously worked at <a href="https://rethinkpriorities.org/">Rethink Priorities</a>. (Jan 24)</li><li><a href="https://jamiebernardi.com/"><strong>Jamie Bernardi</strong></a>&nbsp;co-founded <a href="https://bluedot.org/">BlueDot Impact</a>&nbsp;and ran the <a href="https://aisafetyfundamentals.com/">AI Safety Fundamentals</a>&nbsp;community, courses and website. (Oct 23)</li><li><a href="https://www.neelnanda.io/"><strong>Neel Nanda</strong></a>&nbsp;runs Google DeepMind’s mechanistic interpretability team. (Feb 24)</li><li><a href="https://norabelrose.com/"><strong>Nora Belrose</strong></a>&nbsp;is the head of interpretability research at <a href="https://www.eleuther.ai/">EleutherAI</a>. (Feb 24)</li><li><a href="https://twitter.com/noahysiegel?lang=en"><strong>Noah Siegel</strong></a>&nbsp;is a senior research engineer at Google DeepMind and a PhD candidate at University College London. (Jan 24)</li><li><a href="https://ojorgensen.github.io/"><strong>Ole Jorgensen</strong></a>&nbsp;is a member of technical staff at the UK Government’s AI Safety Institute (this interview was conducted before he joined). (Mar 23)</li><li><a href="https://www.richardcngo.com/"><strong>Richard Ngo</strong></a>&nbsp;is an AI governance researcher at OpenAI. (Feb 24)</li><li><a href="https://www.alignmentforum.org/users/ryan_greenblatt"><strong>Ryan Greenblatt</strong></a>&nbsp;is an AI safety researcher at the AI safety non-profit <a href="https://www.redwoodresearch.org/">Redwood Research</a>. (Feb 24)</li></ul><p><i>These interviews were conducted between March 2023 and February 2024, and represent their views at the time.</i></p><h1>A very brief summary of what people said</h1><h2>What will happen?</h2><p>Many respondents expected the first human-level AI (HLAI) to be in the same paradigm as current large language models (LLMs) like GPT-4, probably <a href="https://www.dwarkeshpatel.com/p/will-scaling-work">scaled up</a>&nbsp;(made bigger), with some new tweaks and hacks, and scaffolding like <a href="https://autogpt.net/">AutoGPT</a>&nbsp;to make it <a href="https://www.lesswrong.com/tag/agency">agentic</a>. But a smaller&nbsp;handful of people predicted that larger breakthroughs are required before HLAI. The most common story of how AI could cause an existential disaster was the story of <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like/#Part_II__influence_seeking_behavior_is_scary">unaligned AI takeover</a>, but some explicitly pushed back on the assumptions behind the takeover story. Some took a more <a href="https://www.lawfaremedia.org/article/thinking-about-risks-ai-accidents-misuse-and-structure">structural view</a>&nbsp;of AI risk, emphasizing threats like instability, extreme inequality, gradual human disempowerment, and a collapse of human institutions.</p><h2>What should we do about it?</h2><p>When asked how AI safety might prevent disaster, respondents focussed most on</p><ul><li>the technical solutions we might come up with,</li><li>spreading a safety mindset through AI research,</li><li>promoting sensible <a href="https://aiscc.org/2023/09/07/proposals-for-ai-regulation/">AI regulation</a>,</li><li>and helping build a fundamental science of AI.</li></ul><p>The research directions people were most excited about were <a href="https://distill.pub/2020/circuits/zoom-in/">mechanistic interpretability</a>, <a href="https://forum.effectivealtruism.org/posts/49rzRKh2ZYH2QjPkg/safety-evaluations-and-standards-for-ai-or-beth-barnes-or">black box evaluations</a>, and governance research.</p><h2>What mistakes have been made?</h2><p>Participants pointed to a range of mistakes they thought the AI safety movement had made. There was no consensus and the focus was quite different from person to person. The most common themes included:</p><ul><li>an overreliance on overly theoretical argumentation,</li><li>being too insular,</li><li>putting people off by pushing weird or extreme views,</li><li>supporting the leading AGI companies resulting in race dynamics,</li><li>not enough independent thought,</li><li>advocating for an unhelpful pause to AI development,</li><li>and historically ignoring policy as a potential route to safety.</li></ul><h1>Limitations</h1><ul><li>People had somewhat different interpretations of my questions, so they were often answering &nbsp;questions that were subtly different from each other.</li><li>The sample of people I interviewed is not necessarily a representative sample of the AI safety movement as a whole. The sample was pseudo-randomly selected, optimizing for a) diversity of opinion, b) diversity of background, c) seniority, and d) who I could easily track down. Noticeably, there is an absence of individuals from <a href="https://intelligence.org/">MIRI</a>, a historically influential AI safety organization, or those who subscribe to <a href="https://www.lesswrong.com/posts/GfZfDHZHCuYwrHGCd/without-fundamental-advances-misalignment-and-catastrophe">similar views</a>. I approached some MIRI team members but no one was available for an interview. This is especially problematic since many respondents criticized MIRI for various reasons, and I didn’t get much of a chance to integrate MIRI’s side of the story into the project.</li><li>There will also be a selection bias due to everyone I asked being at least somewhat bought into the idea of AI being an existential risk.</li><li>A handful of respondents disagreed with the goal of this project: they thought that those in AI safety typically spend too much time thinking about theories of impact.</li><li>There were likely a whole bunch of framing effects that I did not control for.</li><li>There was in some cases a large gap in time between the interview and this being written up (mostly between 1 and 4 months, a year for one early interview). Participant opinions may have changed over this period.</li></ul><h1>Subsequent posts</h1><p>In the following three posts, I present a condensed summary of my findings, describing the main themes that came up for each question:</p><ol><li><a href="https://www.lesswrong.com/posts/wHRMZizqfdW9RjrCY/what-will-the-first-human-level-ai-look-like-and-how-might"><strong>What will happen?</strong></a>&nbsp;What will human-level AI look like, and how might things go wrong?</li><li><a href="https://www.lesswrong.com/posts/XfnnkK8XEjTqtuXGM/what-should-ai-safety-be-trying-to-achieve"><strong>What should we do?</strong></a><strong> </strong>What should AI safety be trying to achieve and how?</li><li><a href="https://www.lesswrong.com/posts/cGzQBRDrpNHoYtbKN/what-mistakes-has-the-ai-safety-movement-made"><strong>What mistakes has the AI safety movement made?</strong></a></li></ol><p>You don’t need to have read an earlier post to understand a later one, so feel free to zoom straight in on what interests you.</p><p><i>I am very grateful to all of the participants for offering their time to this project. Also thanks to Vael Gates, Siao Si Looi, ChengCheng Tan, Adam Gleave, Quintin Davis, George Anadiotis, Leo Richter, McKenna Fitzgerald, Charlie Griffin and many of the participants for feedback on early drafts.</i></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yMTNjeEHfHcf2x7nY/owkuhranzbdmb8pov4ro" style="width: 31.55%;" /></p><p><i>This work was funded and supported by </i><a href="https://far.ai/"><i>FAR AI</i></a><i>.</i></p><br /><br /><a href="https://www.lesswrong.com/posts/yMTNjeEHfHcf2x7nY/big-picture-ai-safety-introduction#comments">Discuss</a>
]]></content:encoded>
<pubDate>Thu, 23 May 2024 22:51:54 GMT</pubDate>
</item>
<item>
<title>What mistakes has the AI safety movement made?</title>
<link>https://www.lesswrong.com/posts/cGzQBRDrpNHoYtbKN/what-mistakes-has-the-ai-safety-movement-made</link>
<guid>https://www.lesswrong.com/posts/cGzQBRDrpNHoYtbKN/what-mistakes-has-the-ai-safety-movement-made</guid>
<content:encoded><![CDATA[
<div> 过度理论化, 封闭, 消极消息传播, 与领先公司紧密合作, 缺乏独立思考

AI安全运动在过去或目前做出了一些错误，如过度依赖理论化论点、封闭性、传播消极观点、与领先公司合作过密、缺乏独立思考、支持无益的AI发展暂停、忽视政策作为安全途径。需要增加实证研究, 加强与外界联系, 改善信息传播, 减少与领先公司的关系, 增强独立思考, 重视公共政策。 <br /><br />总结: <br />过度理论化, 封闭不合, 消极消息传播, 与领先公司紧密合作, 缺乏独立思考, 忽视政策途径。 <div>
Published on May 23, 2024 11:19 AM GMT<br /><br /><p>This is the third of three posts summarizing what I learned when I interviewed 17 AI safety experts about their "big picture" of the existential AI risk landscape: how AGI will play out, how things might go wrong, and what the AI safety community should be doing. See <a href="https://www.lesswrong.com/posts/yMTNjeEHfHcf2x7nY/big-picture-ai-safety-introduction">here</a>&nbsp;for a list of the participants and the standardized list of questions I asked.</p><p>This post summarizes the responses I received from asking “Are there any big mistakes the AI safety community has made in the past or are currently making?”</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cGzQBRDrpNHoYtbKN/py1goh4sbpxmrgmlucjw" /><figcaption>A rough decompositions of the main themes brought up. The figures omit some less popular themes, and double-count respondents who brought up more than one theme.</figcaption></figure><blockquote><p>“<i>Yeah, probably most things people are doing are mistakes. This is just some random group of people. Why would they be making good decisions on priors? When I look at most things people are doing, I think they seem not necessarily massively mistaken, but they seem somewhat confused or seem worse to me by like 3 times than if they understood the situation better.</i>” - Ryan Greenblatt</p></blockquote><blockquote><p><i>“If we look at the track record of the AI safety community, it quite possibly has been harmful for the world.</i>” - Adam Gleave</p></blockquote><blockquote><p>“<a href="https://www.effectivealtruism.org/articles/longtermism"><i>Longtermism</i></a><i>&nbsp;was developed basically so that AI safety could be the most important cause by the utilitarian EA calculus. That's my take.</i>” - Holly Elmore</p></blockquote><p>Participants pointed to a range of mistakes they thought the AI safety movement had made. Key themes included an overreliance on theoretical argumentation, being too insular, putting people off by pushing weird or extreme views, supporting the leading AGI companies, insufficient independent thought, advocating for an unhelpful pause to AI development, and ignoring policy as a potential route to safety.</p><h1>How to read this post</h1><p>This is not a scientific analysis of a systematic survey of a representative sample of individuals, but my qualitative interpretation of responses from a loose collection of semi-structured interviews. Take everything here with the appropriate seasoning.</p><p>Results are often reported in the form “<i>N</i>&nbsp;respondents held view <i>X</i>”. This does <strong>not</strong>&nbsp;imply that “17-<i>N</i>&nbsp;respondents disagree with view <i>X</i>”, since not all topics, themes and potential views were addressed in every interview. What “<i>N </i>respondents held view <i>X</i>” tells us is that at least <i>N</i>&nbsp;respondents hold <i>X</i>, and consider the theme of <i>X</i>&nbsp;important enough to bring up.</p><p>The following is a summary of the main themes that came up in my interviews. Many of the themes overlap with one another, and the way I’ve clustered the&nbsp;criticisms&nbsp;is likely not the only reasonable categorization.</p><h1>Too many galaxy-brained arguments &amp; not enough empiricism</h1><blockquote><p>“<i>I don't find the long, abstract style of investigation particularly compelling.</i>” - Adam Gleave</p></blockquote><p>9 respondents were concerned about <strong>an overreliance or overemphasis on certain kinds of theoretical arguments underpinning AI risk</strong>: namely Yudkowsky’s arguments in <a href="https://www.lesswrong.com/tag/original-sequences">the sequences</a>&nbsp;and Bostrom’s arguments in <a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies">Superintelligence</a>.</p><blockquote><p>“<i>All these really abstract arguments that are very detailed, very long and not based on any empirical experience. [...]</i><br /><br /><i>Lots of trust in loose analogies, thinking that loose analogies let you reason about a topic you don't have any real expertise in. Underestimating the conjunctive burden of how </i><a href="https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/"><i>long and abstract</i></a><i>&nbsp;these arguments are. Not looking for ways to actually test these theories. [...]</i></p><p><i>You can see Nick Bostrom in Superintelligence stating that we shouldn't use RL to align an AGI because it trains the AI to maximize reward, which will lead to wireheading. The idea that this is an inherent property of RL is entirely mistaken. It may be an empirical fact that certain minds you train with RL tend to make decisions on the basis of some tight correlate&nbsp;of their reinforcement signal, but this is not some fundamental property of RL.</i>”</p><p>- Alex Turner</p></blockquote><p><a href="https://jamiebernardi.com/"><u>Jamie Bernardi</u></a> argued that the original view of what AGI will look like, namely&nbsp;<strong>an RL agent that will reason its way to general intelligence from first principles, is not the way things seem to be panning out</strong>. The cutting-edge of AI today is not&nbsp;<a href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem"><u>VNM-rational</u></a> agents who are&nbsp;<a href="http://www.scholarpedia.org/article/Bayesian_statistics"><u>Bayesianly</u></a>-updating their beliefs and trying to maximize some reward function. The horsepower of AI is instead coming from oodles of training data. If an AI becomes power-seeking, it may be because it learns power-seeking from humans, not because of instrumental convergence!</p><p>There was a general sense that the way we make sense of AI should be more empirical. <strong>Our stories need more contact with the real world</strong>&nbsp;– we need to test and verify the assumptions behind the stories. While <a href="https://www.gleave.me/">Adam Gleave</a>&nbsp;overall agreed with this view, he also warned that it’s possible to go too far in the other direction, and that we must strike a balance between the theoretical and the empirical.</p><h2>Problems with research</h2><p>This criticism of “too much theoretical, not enough empirical” also applied to the types of research we are doing. 4 respondents focussed on this. This was more a complaint about past research, folks were typically more positive about the amount of empirical work going on now.</p><p>2 people pointed at MIRI’s overreliance on idealized models of agency in their research, like <a href="https://www.lesswrong.com/tag/aixi">AIXI</a>. <a href="https://agarri.ga/">Adrià Garriga-Alonso</a>&nbsp;thought that <a href="https://www.lesswrong.com/tag/infra-bayesianism">infrabayesianism</a>, parts of singular learning theory and John Wentworth’s <a href="https://www.alignmentforum.org/users/johnswentworth">research programs</a>&nbsp;are unlikely to end up being helpful for safety:</p><blockquote><p><i>“I think the theory-only projects of the past did not work that well, and the current ones will go the same way.” - </i>Adrià Garriga-Alonso</p></blockquote><p><a href="https://twitter.com/evanhub?lang=en"><strong><u>Evan Hubinger</u></strong></a><strong> pushed back against this view </strong>by defending MIRI’s research approach. He pointed out that, when a lot of this very theoretical work was being done, there wasn’t much scope to do more empirical work because we had no highly capable general-purpose models to do experiments on – theoretical work was the best we could do!</p><blockquote><p>“<i>Now it's very different. Now, I think the best work to do is all empirical.&nbsp;Empirical research looks really good right now, but it looked way less good three, four years ago. It's just so much easier to do good empirical work now that the models are much smarter.</i>” - Evan Hubinger</p></blockquote><h1>Too insular</h1><p>8 participants thought AI safety was too insular: the community has disvalued forming alliances with other groups and hasn’t integrated other perspectives and disciplines.</p><p>2 of the 8 focussed on&nbsp;<a href="https://www.vox.com/future-perfect/2022/8/10/23298108/ai-dangers-ethics-alignment-present-future-risk"><u>AI safety’s relationship with AI ethics</u></a>.&nbsp;<strong>Many in AI safety have been too quick to dismiss the concerns of AI ethicists</strong> that AI could exacerbate current societal problems like racism, sexism and concentration of power, on the grounds of extinction risk being “infinitely more important”. But&nbsp;<strong>AI ethics has many overlaps with AI safety both technically and policy:</strong></p><blockquote><p>“<i>Many of the technical problems that I see are the same. If you're trying to align a language model, preventing it from saying toxic things is a great benchmark for that</i>.&nbsp;<i>In most cases, the thing we want on an object level is the same! We want more testing of AI systems, we want independent audits, we want to make sure that you can't just deploy an AI system unless it meets some safety criteria.”&nbsp;</i>- Adam Gleave</p></blockquote><p>In environmentalism, some care more about the conservation of bird species, while others are more concerned about preventing sea level rise. Even though these two groups may have different priorities, they shouldn’t fight because they have agree on many important subgoals, and have many more priorities in common with each other than with, for example, fossil fuel companies. Building a broader coalition could be similarly important for AI safety.</p><p>Another 2 respondents argued that&nbsp;<strong>AI safety needs more contact with academia</strong>. A big fraction of AI safety research is only shared via LessWrong or the Alignment Forum rather than academic journals or conferences. This can be helpful as it speeds up the process of sharing research by sidestepping “playing the academic game” (e.g. tuning your paper to fit into academic norms), but has the downside that research typically receives less peer review, leading to on average lower quality posts on sites like LessWrong. Much of AI safety research lacks the feedback loops that typical science has. AI safety also misses out on the talent available in the broader AI &amp; ML communities.</p><p>Many of the computer science and math kids in AI safety <strong>do not value insights from other disciplines</strong>&nbsp;enough, 2 respondents asserted. <a href="https://www.law.utoronto.ca/faculty-staff/full-time-faculty/gillian-hadfield">Gillian Hadfield</a>&nbsp;argued that many AI safety researchers are getting norms and values all wrong because we don’t consult the social sciences. For example: STEM people often have an assumption that there are some norms that we can all agree on (that we call “human values”), because it’s just “common sense”. But social scientists would disagree with this. Norms and values are the <i>equilibria</i>&nbsp;of interactions between individuals, produced by their behaviors, not some static list of rules up in the sky somewhere.</p><p>Another 2 respondents accused the rationalist sphere of using <strong>too much jargony and sci-fi language</strong>. Esoteric phrases like “p(doom)”, “x-risk” or “HPMOR” can be off-putting to outsiders and a barrier to newcomers, and give <a href="https://www.cnbc.com/2023/06/06/ai-doomers-are-a-cult-heres-the-real-threat-says-marc-andreessen.html">culty vibes</a>. Noah conceded that shorthands can be useful to some degree (for example they can speed up idea exchange by referring to common language rather than having to re-explain the same concept over and over again), but thought that on the whole AI safety has leaned too much in the jargony direction.</p><p><a href="https://www.openphilanthropy.org/about/team/ajeya-cotra/"><u>Ajeya Cotra</u></a> thought some AI safety researchers, like those at MIRI, have been&nbsp;<strong>too secretive&nbsp;</strong>about the results of their research. They do not publish their findings due to worries that a) their insights will help AI developers build more capable AI, and b) they will spread AGI hype and encourage more investment into building AGI (although Adam considered that creating AI hype is one of the big mistakes AI safety has made, on balance he also thought many groups should be less secretive). If a group is keeping their results secret, this is in fact a sign that they aren’t high quality results. This is because a) the research must have received little feedback or insights from other people with different perspectives, and b) if there&nbsp;<i>were</i> impressive results, there would be more temptation to share it.</p><p><a href="https://hollyelmore.substack.com/">Holly Elmore</a>&nbsp;suspected that <strong>this insular behavior was not by mistake, but on purpose</strong>. The rationalists wanted to only work with those who see things the same way as them, and avoid too many “dumb” people getting involved. She recalled conversations with some AI safety people who lamented that there are too many stupid or irrational newbies flooding into AI safety now, and the AI safety sphere isn't as fun as it was in the past.</p><h1>Bad messaging</h1><blockquote><p><i>“As the debate becomes more public and heated, it’s easy to fall into this trap of a race to the bottom in terms of discourse, and I think we can hold better standards. Even as critics of AI safety may get more adversarial or lower quality in their criticism, it’s important that we don’t stoop to the same level. [...]</i>&nbsp;<i>Polarization is not the way to go, it leads to less action.” </i>- Ben Cottier</p></blockquote><p>6 respondents thought AI safety could communicate better with the wider world. The AI safety community do not articulate the arguments for worrying about AI risk well enough, come across as too extreme or too conciliatory, and lean into some memes too much or not enough.</p><p>4 thought that some voices <strong>push views that are too extreme or weird </strong>(but one respondent explicitly pushed against this worry). Yudkowsky is too confident that things will go wrong, and PauseAI is at risk of becoming off-putting if they continue to lean into the protest vibe. Evan thought Conjecture has been doing outreach badly – arguing against sensible policy proposals (like <a href="https://metr.org/blog/2023-09-26-rsp/">responsible scaling policies</a>) because they don’t go far enough. <a href="https://www.eng.cam.ac.uk/profiles/dsk30">David Krueger</a>&nbsp;however leaned in the opposite direction: he thought that we are too scared to use sensationalist language like “AI might take over”, while in fact, this language is good for getting attention and communicating concerns clearly.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cGzQBRDrpNHoYtbKN/rxlfh7o4kf0whkvdzmqj" /><figcaption>Eliezer Yudkowsky pulling&nbsp;a strange face next to an alarming caption. <a href="https://www.youtube.com/watch?v=41SUp-TRVlg">Source: Dwarkesh Patel</a></figcaption></figure><p><a href="https://www.linkedin.com/in/ben-cottier/?originalSubdomain=uk">Ben Cottier</a>&nbsp;lamented the <strong>low quality of discourse around AI safety</strong>, especially in places like Twitter. We should have a high standard of discourse, show empathy to the other side of the debate, and seek compromises (with e.g. open source advocates). The current bad discourse is contributing to polarization, and nothing gets done when an issue is polarized. Ben also thought that <strong>AI safety should have been more prepared for the “reckoning moment” of AI risk becoming mainstream</strong>, so we had more coherent articulations of the arguments and reasonable responses to the objections.</p><p>Some people say that we shouldn’t anthropomorphize AI, but <a href="https://norabelrose.com/">Nora Belrose</a>&nbsp;reckoned we should do it more! Anthropomorphising makes stories much more attention-grabbing (it is “memetically fit”). One of the most famous examples of AI danger has been <a href="https://www.theguardian.com/technology/2023/feb/17/i-want-to-destroy-whatever-i-want-bings-ai-chatbot-unsettles-us-reporter">Sydney</a>: Microsoft’s chatbot that freaked people out by being unhinged in a very human way.</p><h1>AI safety’s relationship with the leading AGI companies</h1><blockquote><p>“<i>Is it good that <strong>the AI safety community has collectively birthed the three main AI orgs</strong>, who are to some degree competing, and maybe we're contributing to the race to AGI? I don’t know how true that is, but it feels like it’s a little bit true.</i></p><p><i>If the three biggest oil companies were all founded by people super concerned about climate change, you might think that something was going wrong."</i></p><p>- Daniel Filan</p></blockquote><p>Concern for AI safety had at least some part to play in the founding of OpenAI, Anthropic and DeepMind. Safety was a <a href="https://openai.com/our-structure">stated primary concern</a>&nbsp;that drove the founding of OpenAI. Anthropic was founded by researchers who left OpenAI because <a href="https://www.vox.com/future-perfect/23794855/anthropic-ai-openai-claude-2">it wasn’t sufficiently safety-conscious</a>. Shane Legg, one of DeepMind’s co-founders, is on record for being largely motivated by AI safety. Their existence is arguably making AGI come sooner, and fuelling a race that may lead to more reckless corner-cutting in AI development. 5 respondents thought the existence of these three organizations is probably a bad thing.</p><p>Jamie thought the existence of OpenAI may be overall positive though, due to their <a href="https://openai.com/index/planning-for-agi-and-beyond/">strategy</a>&nbsp;of widely releasing models (like ChatGPT) to get the world experienced with AI. ChatGPT has thrust AI into the mainstream and precipitated the recent rush of interest in the policy world.</p><p>3 respondents also complained that the AI safety community is&nbsp;<strong>too cozy with the big AGI companies</strong>. A lot of AI safety researchers work at OpenAI, Anthropic and DeepMind. The judgments of these researchers may be biased by a conflict of interest: they may be incentivised for their company to succeed in getting to AGI first. They will also be contractually limited in what they can say about their (former) employer, in some cases&nbsp;<a href="https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release"><u>even for life</u></a>.</p><p>Adam recommended that <strong>AI safety needs more voices who are independent of corporate interests</strong>, for example in academia. He also recommended that <strong>we shouldn’t be scared to criticize companies who aren’t doing enough for safety</strong>.</p><p>While <a href="https://danielfilan.com/">Daniel Filan</a>&nbsp;was concerned about AI safety’s close relationship with these companies, he conceded that there must be <strong>a </strong><a href="https://forum.effectivealtruism.org/posts/Y4SaFM5LfsZzbnymu/the-case-for-ai-safety-advocacy-to-the-public"><strong>balance</strong></a><strong>&nbsp;between inside game</strong>&nbsp;(changing things from the inside) <strong>and outside game</strong>&nbsp;(putting pressure on the system from the outside). AI safety is mostly playing the inside game – get involved with the companies who are causing the problem, to influence them to be more careful and do the right thing. In contrast, the environmentalism movement largely plays an outside game – not getting involved with oil companies but protesting them from the outside. Which of these is the right way to make change happen? Seems difficult to tell.</p><h1>The bandwagon</h1><blockquote><p>“<i>I think there's probably lots of people deferring when they don't even realize they're deferring.</i>” - Ole Jorgensen</p></blockquote><p>Many in the AI safety movement do not think enough for themselves, 4 respondents thought. Some are too willing to adopt the views of a small group of elites who lead the movement (like Yudkowsy, Christiano and Bostrom). <a href="https://twitter.com/turn_trout?lang=en-GB">Alex Turner</a>&nbsp;was concerned about the amount of “hero worship” towards these thought leaders. If this small group is wrong, then the entire movement is wrong. As Jamie pointed out, AI safety is now a&nbsp;major voice in the AI policy world – making it even more concerning that AI safety is resting on the judgements of such a small number of people.</p><blockquote><p>“<i>There's maybe some jumping to like: what's the most official way that I can get involved in this? And what's the community-approved way of doing this or that? That's not the kind of question I think we should be asking.</i>” - Daniel Filan</p></blockquote><h1>Pausing is bad</h1><p>3 respondents thought that advocating for a <a href="https://pauseai.info/proposal">pause to AI</a>&nbsp;development is bad, while 1 respondent was pro-pause<span class="footnote-reference" id="fnref29m9mwiex5m"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn29m9mwiex5m">[1]</a></sup></span>. Nora referred me to a <a href="https://bounded-regret.ghost.io/ai-pause-will-likely-backfire-by-nora/">post</a>&nbsp;she wrote arguing that pausing is bad.<i>&nbsp;</i>In that post, she argues that pausing will a) reduce the quality of alignment research because researchers will be forced to test their ideas on weak models, b) make a <a href="https://www.lesswrong.com/tag/ai-takeoff">hard takeoff</a>&nbsp;more likely when the pause is lifted, and c) push capabilities research underground, where regulations are looser.</p><h1>Discounting public outreach &amp; governance as a route to safety</h1><p>Historically, the AI safety movement has underestimated the potential of getting the public on-side&nbsp;and getting policy passed, 3 people said. There is a lot of work in AI governance these days, but for a long time most in AI safety considered it a dead end. <strong>The </strong><i><strong>only hope</strong></i><strong>&nbsp;to reduce existential risk from AI was to solve the technical problems ourselves, and hope that those who develop the first AGI implement them</strong>. Jamie put this down to a general mistrust of governments in rationalist circles, not enough faith in our ability to solve coordination problems, and a general dislike of “consensus views”.</p><p>Holly thought there was <strong>a general unconscious desire for the solution to be technical</strong>. AI safety people were guilty of motivated reasoning that “the best way to save the world is to do the work that I also happen to find fun and interesting”.&nbsp;When the Singularity Institute pivoted towards safety and became <a href="https://intelligence.org/">MIRI</a>, they never gave up on the goal of building AGI – just started prioritizing making it safe.</p><blockquote><p>“<i>Longtermism was developed basically so that AI safety could be the most important cause by the utilitarian EA calculus. That's my take.</i>” - Holly Elmore</p></blockquote><p>She also condemned the way <strong>many in AI safety hoped to solve the alignment problem via “elite shady back-room deals”</strong>, like influencing the values of the first AGI system by getting into powerful positions in the relevant AI companies.</p><p><a href="https://www.richardcngo.com/">Richard Ngo</a>&nbsp;gave me similar vibes, arguing that AI safety is <strong>too structurally power-seeking</strong>: trying to raise lots of money, trying to gain influence in corporations and governments, trying to control the way AI values are shaped, favoring people who are concerned about AI risk for jobs and grants, maintaining the secrecy of information, and recruiting high school students to the cause. We can justify activities like these to some degree, but Richard worried that AI safety was leaning too much in this direction. This has led many outside of the movement to deeply mistrust AI safety (<a href="https://www.youtube.com/watch?v=j4sYtMpetoc">for example</a>).</p><blockquote><p>“<i>From the perspective of an external observer, it’s difficult to know how much to trust stated motivations, especially when they tend to lead to the same outcomes as deliberate power-seeking.</i>” - Richard Ngo</p></blockquote><p>Richard thinks that a better way for AI safety to achieve its goals is to instead <strong>gain more legitimacy </strong>by being open, informing the public of the risks in a legible way, and prioritizing competence.</p><p>More abstractly, both Holly and Richard reckoned that <strong>there is too much focus on individual impact in AI safety</strong>&nbsp;and not enough focus on helping the world solve the problem collectively. More power to do good lies in the hands of the public and governments than many AI safety folk and effective altruists think. Individuals <i>can</i>&nbsp;make a big difference by playing 4D chess, but it’s harder to get right and often backfires.</p><blockquote><p>“<i>The agent that is actually having the impact is much larger than any of us, and in some sense, the role of each person is to facilitate the largest scale agent, whether that be the AI safety community or civilization or whatever. Impact is a little meaningless to talk about, if you’re talking about the impact of individuals in isolation.</i>” - Richard Ngo</p></blockquote><h1>Conclusion</h1><p>Participants pointed to a range of mistakes they thought the AI safety movement had made. An overreliance on overly theoretical argumentation, being too insular, putting the public off by pushing weird or extreme views, supporting the leading AGI companies, not enough independent thought, advocating for an unhelpful pause to AI development, and ignoring policy as potential a route to safety.</p><p>Personally, I hope this can help the AI safety movement avoid making similar mistakes in the future! Despite the negative skew of my questioning, I walked away from these conversations feeling pretty optimistic about the direction the movement is heading. I believe that as long as we continue to be honest, curious and open-minded about what we’re doing right and wrong, AI safety as a concept will overall have a positive effect on humanity’s future.</p><ol class="footnote-section footnotes"><li class="footnote-item" id="fn29m9mwiex5m"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref29m9mwiex5m">^</a></strong></sup></span><div class="footnote-content"><p>&nbsp;Other respondents may also have been pro or anti-pause, but since the pause debate did not come up in their interviews I didn’t learn what their positions on this issue were.</p></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/cGzQBRDrpNHoYtbKN/what-mistakes-has-the-ai-safety-movement-made#comments">Discuss</a>
]]></content:encoded>
<pubDate>Fri, 24 May 2024 05:11:21 GMT</pubDate>
</item>
<item>
<title>The case for stopping AI safety research</title>
<link>https://www.lesswrong.com/posts/vkzmbf4Mve4GNyJaF/the-case-for-stopping-ai-safety-research</link>
<guid>https://www.lesswrong.com/posts/vkzmbf4Mve4GNyJaF/the-case-for-stopping-ai-safety-research</guid>
<content:encoded><![CDATA[
<div> AI safety research, failure modes, capability development, human biometric data, x-risk

总结:<br /><br />本文指出AI安全研究正在解决易解决的问题，但这种情况将会发生变化，因为我们将无法预测和解决AI系统不断增长的失败模式。AI安全研究为AI能力的发展提供了免费服务，加速了这一过程。未来将有大量的人类生物特征和神经数据输入AI系统培训，这将带来巨大突破，但也会增加潜在风险。停止AI安全研究或将其内部化可能会减慢AI发展速度，但也会让能力开发者更直接面对有管理能力的有害后果。 <div>
Published on May 23, 2024 3:55 PM GMT<br /><br /><p>TLDR: AI systems are failing in obvious and manageable ways for now. Fixing them will push the failure modes beyond our ability to understand and anticipate, let alone fix. The AI safety community is also doing a huge economic service to developers. Our belief that our minds can "fix" a super-intelligence - especially bit by bit - needs to be re-thought.&nbsp;</p><p>I wanted to write this post forever, but now seems like a good time. &nbsp;The case is simple, I hope it takes you 1min to read.</p><ol><li><strong>AI safety research is still solving easy problems. &nbsp;</strong>We are patching up the most obvious (to us) problems. As time goes we will no longer be able to play this existential risk game of chess with AI systems. I've argued this a lot (<a href="https://arxiv.org/abs/2305.19223">preprint</a>; ICML paper accepted (shorter read, will repost), will be out in a few days; <a href="https://www.agencyfoundations.ai">www.agencyfoundations.ai</a>). Seems others have this thought.</li><li><strong>Capability development is getting AI safety research for free. </strong>It's likely in the millions to tens of millions of dollars. All the "hackathons", and "mini" prizes to patch something up or propose a new way for society to digest/adjust to some new normal (and increasingly incentivizing existing academic labs).&nbsp;</li><li><strong>AI safety research is speeding up capabilities</strong>. I hope this is somewhat obvious to most.</li></ol><p>I write this now because in my view we are about 5-7 years before massive human biometric and neural datasets will enter our AI training. &nbsp;These will likely generate amazing breakthroughs in long-term planning and emotional and social understanding of the human world. &nbsp;They will also most likely increase x-risk radically.</p><p>Stopping AI safety research or taking it in-house with security guarantees etc, will &nbsp;slow down capabilities somewhat - and may expose capabilities developers more directly to public opinion of still manageable harmful outcomes.&nbsp;</p><br /><br /><a href="https://www.lesswrong.com/posts/vkzmbf4Mve4GNyJaF/the-case-for-stopping-ai-safety-research#comments">Discuss</a>
]]></content:encoded>
<pubDate>Fri, 24 May 2024 01:52:56 GMT</pubDate>
</item>
<item>
<title>EIS XIII: Reflections on Anthropic’s SAE Research Circa May 2024</title>
<link>https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may</link>
<guid>https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may</guid>
<content:encoded><![CDATA[
<div> Anthropic, interpretability, SAE, safety, research
总结:
Anthropic的最新SAE研究集中于展示示例，但并未展示出SAE在实际应用中的竞争力和实用性。虽然他们声称工作是为了安全，但似乎更着重于炫耀研究成果而非解决实际问题。他们的方法论似乎忽视了过去的可解释性偏差，缺乏实用性和竞争性。对于机械解释性研究，需要展示出在实际任务中的竞争力，而Anthropic却仍旧停留在街灯式示例和举例说明阶段。他们的工作似乎并未真正应用于实际任务和击败基准线，而更多地是集中于表面的演示和示范。所以，Anthropic的解释团队似乎在推行错误的范式，并未真正展示出SAE和其他解释技术在实际场景中的实用性。 <div>
Published on May 21, 2024 8:15 PM GMT<br /><br /><p>Part 13 of 12 in the&nbsp;<a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7"><u>Engineer’s Interpretability Sequence</u></a>.</p><h1>TL;DR</h1><p>On May 5, 2024,&nbsp;<a href="https://x.com/StephenLCasper/status/1787270794017702045"><u>I made a set of 10 predictions</u></a> about what the next sparse autoencoder (SAE) paper from Anthropic would and wouldn’t do.&nbsp;<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"><u>Today’s new SAE paper from Anthropic</u></a> was full of brilliant experiments and interesting insights, but it underperformed my expectations. I am beginning to be concerned that Anthropic’s recent approach to interpretability research might be better explained by safety washing than practical safety work.&nbsp;</p><p>Think of this post as a curt editorial instead of a technical piece. I hope to revisit my predictions and this post in light of future updates.&nbsp;</p><h1>Reflecting on predictions</h1><p><a href="https://x.com/StephenLCasper/status/1787270794017702045"><u>See my original post</u></a> for 10 specific predictions about what&nbsp;<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"><u>today’s paper</u></a> would and wouldn’t accomplish. I think that Anthropic obviously did 1 and 2 and obviously did not do 4, 5, 7, 8, 9, and 10. Meanwhile, I think that their experiments to identify&nbsp;<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#searching"><u>specific</u></a> and&nbsp;<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#safety-relevant"><u>safety-relevant</u></a> features should count for 3 (proofs of concept for a useful type of task) but definitely do not count for 6 (*competitively* finding and removing a harmful behavior that was represented in the training data).</p><p>Thus, my assessment is that Anthropic did 1-3 but not 4-10.&nbsp;<a href="https://www.alignmentforum.org/posts/EjsA2M8p8ERyFHLLY/takeaways-from-the-mechanistic-interpretability-challenges"><u>I have been wrong with mech interp predictions in the past</u></a>, but this time, everything I predicted with &gt;50% probability happened, and everything I predicted with &lt;50% probability did not happen.&nbsp;</p><p>The predictions were accurate in one sense. But overall, the paper&nbsp;<i>underperformed</i> expectations. If you scored the paper relative to my predictions by giving it <i>(1-p)</i> points when it did something that I predicted it would do with probability <i>p</i> and <i>-p</i> points when it did not, the paper would score -0.74.&nbsp;</p><h1>A review + thoughts</h1><p>I think that Anthropic’s new SAE work has continued to be like lots of prior high-profile work on mechanistic interpretability – it has focused on presenting illustrative examples, streetlight demos, and cherry-picked proofs of concept. This is useful for science, but it does not yet show that SAEs are helpful and competitive for diagnostic and debugging tasks that could improve AI&nbsp;<i>safety</i>.&nbsp;</p><p>I feel increasingly worried about how Anthropic motivates and sells its interpretability research in the name of safety. Today’s paper makes some major Motte and Bailey claims that oversell what was accomplished like “<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#searching:~:text=Eight%20months%20ago%2C%20we%20demonstrated%C2%A0that%20sparse%20autoencoders%20could%20recover%20monosemantic%20features%20from%20a%20small%20one%2Dlayer%20transformer."><u>Eight months ago, we demonstrated that sparse autoencoders could recover monosemantic features from a small one-layer transformer</u></a>,” “<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#searching:~:text=Sparse%20autoencoders%20produce%20interpretable%20features%20for%20large%20models."><u>Sparse autoencoders produce interpretable features for large models</u></a>,” and “<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#searching:~:text=The%20resulting%20features%20are%20highly%20abstract%3A%20multilingual%2C%20multimodal%2C%20and%20generalizing%20between%20concrete%20and%20abstract%20references."><u>The resulting features are highly abstract: multilingual, multimodal, and generalizing between concrete and abstract references.</u></a>” The paper also made some omissions of past literature on interpretability illusions (e.g., <a href="https://arxiv.org/abs/2104.07143">Bolukbasi et al., 2021</a>), which their methodology seems prone to. Normally, problems like this are mitigated by peer review, which Anthropic does not participate in. Meanwhile, whenever Anthropic puts out new interpretability research, I always see a laundry list of posts from the company and employees to promote it. They always seem to claim the same things – that some ‘groundbreaking new progress has been made’ and that ‘the model was even more interpretable than they thought’ but that ‘there remains progress to be made before interpretability is solved’. I won’t link to any specific person’s posts, but here are Anthropic’s posts from&nbsp;<a href="https://x.com/AnthropicAI/status/1792935506587656625"><u>today</u></a> and&nbsp;<a href="https://x.com/AnthropicAI/status/1709986949711200722"><u>October 2023</u></a>.&nbsp;</p><p>The way that Anthropic presents its interpretability work has real-world consequences. For example, it led to&nbsp;this <a href="https://x.com/brickroad7/status/1710084070758355319"><u>viral claim that interpretability will be solved and that we are bound for safe models</u></a>. It also seems to have led to at least one claim in a policy memo that&nbsp;<a href="https://committees.parliament.uk/writtenevidence/127070/pdf/"><u>advocates of AI safety are being silly because mechanistic interpretability was solved</u></a>. Meanwhile today, it seems that Anthropic orchestrated a <a href="https://www.nytimes.com/2024/05/21/technology/ai-language-models-anthropic.html">New York Times article</a> to be released alongside the paper, claiming to the public that exciting progress has been made (although the article also made helpful critical commentary on limitations!).</p><p>If interpretability is ever going to be helpful for safety, it will need to be useful and competitive in practical applications. This point has been made consistently for the better part of a decade (e.g.&nbsp;<a href="https://journals.sagepub.com/doi/10.1177/1461444816676645"><u>Ananny and Crawford, 2016</u></a>;&nbsp;<a href="https://arxiv.org/abs/1606.03490"><u>Lipton, 2016</u></a>;&nbsp;<a href="https://arxiv.org/abs/1702.08608"><u>Doshi-Velez and Kim, 2017</u></a>;&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0004370218305988"><u>Miller, 2018</u></a>;&nbsp;<a href="https://link.springer.com/article/10.1007/s13347-019-00372-9"><u>Krishnan, 2020</u></a>;&nbsp;<a href="https://arxiv.org/abs/2207.13243"><u>Rauker et al., 2022</u></a>). Despite this, it seems to me that Anthropic has so far failed to apply its interpretability techniques to practical tasks and show that they are competitive. Instead of testing applications and beating baselines, the recent approach has been to keep focusing on streetlight demos and showing lots of cherry-picked examples.&nbsp;</p><p><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#discussion:~:text=Inability%20to%20Evaluate,we%20care%20about."><u>The discussion section of today’s paper seems to suggest</u></a> that Anthropic’s interpretability team is doubling down on the wrong paradigm.&nbsp;</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pH6tyhEnngqWAXi9i/yzo7qx3uoswp6kbbxxlh" /><figcaption>&nbsp;</figcaption></figure><p>This seems to stem from confusion about&nbsp;<a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/MyvkTKfndx9t4zknh"><u>what “interpretability” is and what it’s for</u></a>. Unlike what this excerpt suggests, ther indeed is an evaluation standard – usefulness for engineers.&nbsp;</p><p>I don't think that SAE research is misguided. In <a href="https://x.com/StephenLCasper/status/1787270794017702045">my post</a>, I pointed out 6 things that I think they could be useful for. Meanwhile, some good recent work has demonstrated proofs of concept that SAEs can be useful on certain non-cherry-picked tasks of practical value and interest (<a href="https://arxiv.org/abs/2403.19647">Marks et al., 2024</a>). I think that it's very possible that SAEs and other interpretability techniques can be lenses into models that can help us find useful clues and insights. However, Anthropic's research on SAEs has yet to demonstrate practical usefulness that could help engineers in real applications.&nbsp;</p><p>I know that members of the Anthropic interpretability team have been aware of this critique. Meanwhile, Anthropic and its employees consistently affirm that their work is motivated by safety in the real world. But <a href="https://www.alignmentforum.org/posts/fpws8WNo6Mp4KsQjH/analogies-between-scaling-labs-and-misaligned">is</a> <a href="https://joinreboot.org/p/alignment">it</a>? I am starting to wonder about the extent to which the interpretability team’s current agenda is better explained by practical safety work versus doing machiavellian<u> </u><a href="https://www.alignmentforum.org/posts/fpws8WNo6Mp4KsQjH/analogies-between-scaling-labs-and-misaligned"><u>safety washing</u></a> to score points in&nbsp;<a href="https://x.com/brickroad7/status/1710084070758355319"><u>social media</u></a>,&nbsp;<a href="https://www.nytimes.com/2024/05/21/technology/ai-language-models-anthropic.html"><u>news</u></a>, and&nbsp;<a href="https://committees.parliament.uk/writtenevidence/127070/pdf/"><u>government</u></a>.&nbsp;</p><p>—</p><p>Thanks to Ryan Greenblatt and Buck Shlegris. I did not consult with them on this post, but they pointed out some useful things in a Slack thread that I put in here.</p><p><br />&nbsp;</p><br /><br /><a href="https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may#comments">Discuss</a>
]]></content:encoded>
<pubDate>Wed, 22 May 2024 23:26:15 GMT</pubDate>
</item>
<item>
<title>"Which chains-of-thought was that faster than?"</title>
<link>https://www.lesswrong.com/posts/7oGfJG2BuvTgdCHQH/which-chains-of-thought-was-that-faster-than</link>
<guid>https://www.lesswrong.com/posts/7oGfJG2BuvTgdCHQH/which-chains-of-thought-was-that-faster-than</guid>
<content:encoded><![CDATA[
<div> chain-of-thought, heuristic, cognitive trace, faster, effective
<br />
提供了一些来自Eliezer的好建议，主要包括两个启发式方法：完成一条思路链后，反思如何更快地想到，以及完成一次良好思路链后，反思比起哪些思路更快。文章中举例说明了这两种方法的应用场景，分析了它们的优势和适用范围，最后给出了另一个启发式方法：当接收到好建议时，询问如何让这些建议更好。整篇文章围绕思维方式的优化展开，提供了一系列实用方法帮助读者更高效地思考和学习。 
<br /><br />总结: 提供了两种启发式方法：完成思路链后反思如何更快地想到，以及反思良好思路链相比其他思路更快的优势；展示了具体实例和方法应用场景；分析了方法的优势和适用范围；提出了另一种启发式方法：接收好建议后询问如何改进建议。 <div>
Published on May 22, 2024 8:21 AM GMT<br /><br /><p>Here's some good advice from Eliezer:</p><h2>TAP: "How could I have thought that faster?"</h2><ul><li>WHEN<span class="footnote-reference" id="fnrefjf37bd9flbt"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnjf37bd9flbt">[1]</a></sup></span>&nbsp;<i>you complete a chain-of-thought</i></li><li>THEN <i>ask yourself, </i><a href="https://www.lesswrong.com/posts/rYq6joCrZ8m62m7ej/how-could-i-have-thought-that-faster"><i>"<strong>how could I have thought that faster?</strong>"</i></a></li></ul><p>I really like this heuristic, and it's already <a href="https://www.lesswrong.com/tag/making-beliefs-pay-rent">paid its rent</a> several times over for me. Most recently today, so I'll share the (slightly edited) cognitive trace of it as an example:</p><h3>Example: To find the inverse of something, trace the chain forward a few times first</h3><ol><li>I was in the context of having just asked myself <i>"what's the set of functions which have this function as its derivative?"</i></li><li>This is of course its integral, but I didn't want to use cached abstractions, and instead sought to get a generalized view of the landscape <a href="https://www.lesswrong.com/posts/fg9fXrHpeaDD6pEPL/truly-part-of-you">from first-principles</a>.</li><li>For about ~10 seconds, I tried to hold the function&nbsp;<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span></span></span></span></span>&nbsp;in my mind while trying to directly generate the integral landscape from it.&nbsp;</li><li>This seemed awfwly inefficient, so I changed tack: I already know some specific functions whose derivatives equal&nbsp;<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-right: 0.06em;">f</span></span></span></span></span></span></span>, so I held <i>those</i> as the proximal thing in my mind while retracing the cognitive steps involved in their derivation.</li><li>After making those steps more salient in the forward direction (integral→derivative), it was easier to retrace the path in the opposite direction.</li><li>And once the derivative→integral trace was salient for a few examples, it was easier to <i>generalize from the examples</i> to produce the landscape of all the integrals.</li><li>There are multiple takeaways here, but one is:<ol><li><i>"If you struggle to generalize something, find a way to generate specific examples first, then generalize from the examples."</i></li></ol></li></ol><h2>TAP: "Which chains-of-thought was that faster than?"</h2><p>Imo, more important than asking <i>"how could I have thought that faster?"</i> is the inverse heuristic:</p><ul><li>WHEN <i>you complete a good chain-of-thought</i></li><li>THEN <i>ask yourself, "<strong>which chains-of-thought was that faster than?</strong>"</i></li></ul><p>Although, ideally, I wouldn't scope the trigger to <i>every</i> time you complete a thought, since that overburdens the general cue. Instead, maybe limit it to those times when you have an especially clear trace of it AND you have a hunch that something about it was unusually good.</p><ul><li>WHEN <i>you complete a good chain of thought</i><ul><li>AND <i>you have its trace in short-term memory</i></li><li>AND <i>you hunch that something about it was unusually effective</i></li></ul></li><li>THEN <i>ask yourself, "<strong>which chains-of-thought was that faster than?</strong>"</i></li></ul><h3>Example: Sketching out my thoughts with pen-and-paper</h3><ol><li>Yesterday I was writing out some plans explicitly with pen and paper—enumerating my variables and drawing arrows between them.</li><li>I noticed—for the umpteenth time—that forcing myself to explicitly sketch out the problem (even with improvised visualizations) is far more <i>cognitively ergonomic</i> than keeping it in my head (see eg <a href="https://www.lesswrong.com/posts/Qyr9tfBLck53em4hf/how-to-write-pseudocode-and-why-you-should">why you should write pseudocode</a>).</li><li>But instead of just noting "yup, I should force myself to do more pen-and-paper", I asked myself two questions:<ol><li><i><strong>"When does it help me think, and when does it just slow me down?"</strong></i><ol><li>This part is important: <i>scope</i> your insight <i>sharply </i>to contexts where it's usefwl—hook your idea into the contexts where you want it triggered—so you avoid wasting memory-capacity on linking it up to useless stuff.</li><li>In other words, you want to minimize (unwanted) <a href="https://en.wikipedia.org/wiki/Associative_interference#Effect_on_recall:~:text=Many%20studies%20have%20concluded%20that%20associative%20interference%20reduces%20a%20subject%27s%20ability%20to%20recall.">associative interference</a> so you can remember stuff at lower cost.</li><li>My conclusion was that pen-and-paper is good when I'm trying to map complex relations between a handfwl of variables.</li><li>And it is NOT good when I have just a single proximal idea that I want to compare against a myriad of samples with high false-positive rate—that's instead where I should be doing inside-head thinking to exploit the brain's <a href="https://www.wikiwand.com/en/Connectionism">massively parallel distributed processor</a>.</li></ol></li><li><i><strong>"Why am I so reluctant to do it?"</strong></i><ol><li>This seems related to the brain's myopic tendency for <i>hastening subgoal completion.</i><span class="footnote-reference" id="fnref8jzoqcq85ko"><sup><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fn8jzoqcq85ko">[2]</a></sup></span></li><li>So I resolved to try to notice exactly which subgoal(s) my brain biases motivation toward, so I can trigger this concept <i>specifically</i> in the contexts where top-down override is most needed—instead of relying on an overly general sense of "uuh I gotta do this more somehow".</li></ol></li></ol></li></ol><h3>Why is it better?</h3><p>While obviously both heuristics are good to use, the reasons I think asking <i>"which chains-of-thought was that faster than?"</i> tends to be more epistemically profitable than <i>"how could I have thought that faster?"</i> include:</p><ul><li>It is easier to find suboptimal thinking-habits to propagate an unusually good idea <i>into</i>, than to find good ideas for improving a particular suboptimal thinking-habit.<ul><li>Notice that in my technique, the good idea<u> </u>is <i><strong>cognitively proximal</strong></i> and the suboptimal thinking-habits are <i><strong>cognitively&nbsp;distal</strong></i>, whereas in Eliezer's suggestion it's the other way around.</li><li>A premise here is that good ideas are unusual (hard-to-find) and suboptimal thinking-habits are common (easy-to-find)—the advice flips in domains where it's the opposite.</li><li>It relates to the difference between propagating specific solutions to plausible problem-domains, vs searching for specific solutions to a specific problem.<ul><li>The brain tends to be biased against the former approach because it's preparatory work with <i>upfront cost </i>("prophylaxis"), whereas the latter context sort of forces you to search for solutions.</li></ul></li></ul></li></ul><h2>TAP: "What's the appropriate scope?"</h2><ul><li>WHEN <i>you notice that a heuristic is usefwl in specific cases</i></li><li><u>THEN </u><i><u>ask yourself, </u><strong><u>"can I generalize this to new domains?"</u></strong></i></li></ul><p>Especially notice that there's nothing about the structure of <i>"how could I have <strong>thought</strong> that faster?"</i> that implies it's only usefwl in the domain of specific short chains-of-thought. "Thought" here is an unconstrained variable. It generalizes to everything where the trace of specific examples is likely to contain information which profitably generalizes to other examples. The general pattern is:&nbsp;</p><ul><li><i>"What went <strong>wrong</strong> this time?"</i><ul><li>And its more-profitable inverse: <i>"What went <strong>right </strong>this time?"</i></li></ul></li></ul><p>So let's propagate this pattern across some domains:</p><ul><li><i>"How could I have <strong>learned</strong> that faster?"</i><ul><li>What's the most usefwl lessons you acquired from studying X? And could you have predicted that in advance so you could <i>avoid </i>wasting time learning [useless subsets of X]?</li></ul></li><li><i>"How could I have <strong>finished</strong> that faster?"</i><ul><li>I don't know about you, but I have wasted an outrageous number of hours perfecting the UI of my programs when, realistically, the benefit was extremely marginal.</li></ul></li><li><i>"How could I have <strong>failed</strong> that faster?"</i><ul><li>If you have a plan, go straight for the bottlenecks that have the <a href="https://lesswrong.com/posts/Eav2BizSejDcztFC8/focus-on-the-hardest-part-first">largest probability of making you realize the plan is intractable</a>. Also, <a href="https://www.lesswrong.com/posts/FMkQtPvzsriQAow5q/the-correct-response-to-uncertainty-is-not-half-speed">the correct response to uncertainty is not half-speed</a>.</li><li>Failing projects as fast as you can, may feel like making no progress at all. But keep in mind that you're sampling the search-space faster this way. I call it <i>"horizontal progress" </i>because that makes me feel better.</li></ul></li><li><i>"How could I have <strong>remembered</strong> that more reliably?"</i><ul><li>Especially don't forget the inverse: <i>"What <strong>enabled me to recall</strong> that?"</i></li></ul></li><li><i>"How could I have <strong>read</strong> that faster?"</i><ul><li>If you are guilty of reading this sentence after having read all previous sentences in this post, consider whether you ought to be skimming more. I'm pretty sure some of the above sentences were predictably less usefwl to you.</li></ul></li></ul><h2>TAP: "How can I make this advice better?"</h2><p>Lastly, another generally usefwl heuristic, which also happens to have caused the insights which led to this post:</p><ul><li>WHEN <i>you receive good advice</i><ul><li>AND <i>you especially trust the author of that advice</i></li></ul></li><li>THEN <i>ask yourself, "<strong>how can I make this advice better?</strong>"</i></li></ul><ol class="footnote-section footnotes"><li class="footnote-item" id="fnjf37bd9flbt"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnrefjf37bd9flbt">^</a></strong></sup></span><div class="footnote-content"><p>Formatted as a <strong>trigger-action-plan (TAP)</strong> to make the cue more separately salient, so you're more likely to notice the event that should trigger the action.</p></div></li><li class="footnote-item" id="fn8jzoqcq85ko"><span class="footnote-back-link"><sup><strong><a href="https://www.lesswrong.com/feed.xml?view=frontpage-rss&amp;karmaThreshold=30#fnref8jzoqcq85ko">^</a></strong></sup></span><div class="footnote-content"><blockquote><p><i>We asked university students to pick up either of two buckets, one to the left of an alley and one to the right, and to carry the selected bucket to the alley’s end. In most trials, one of the buckets was closer to the end point. We emphasized choosing the easier task, expecting participants to prefer the bucket that would be carried a shorter distance. Contrary to our expectation, participants chose the bucket that was closer to the start position, carrying it farther than the other bucket. </i>— <a href="https://gwern.net/doc/psychology/2014-rosenbaum.pdf">Pre-Crastination: Hastening Subgoal Completion at the Expense of Extra Physical Effort</a></p></blockquote></div></li></ol><br /><br /><a href="https://www.lesswrong.com/posts/7oGfJG2BuvTgdCHQH/which-chains-of-thought-was-that-faster-than#comments">Discuss</a>
]]></content:encoded>
<pubDate>Thu, 23 May 2024 06:20:17 GMT</pubDate>
</item>
</channel>
</rss>